@article{ZHOU20211491,
title = {Transcriptome based functional identification and application of regulator AbrB on alkaline protease synthesis in Bacillus licheniformis 2709},
journal = {International Journal of Biological Macromolecules},
volume = {166},
pages = {1491-1498},
year = {2021},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2020.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0141813020349539},
author = {Cuixia Zhou and Huitu Zhang and Honglei Fang and Yanqing Sun and Huiying Zhou and Guangcheng Yang and Fuping Lu},
keywords = {Transcriptome analysis, Gene regulation, AbrB, Alkaline protease, },
abstract = {Bacillus licheniformis 2709 is the major alkaline protease producer, which has great potential value of industrial application, but how the high-producer can be regulated rationally is still not completely understood. It's meaningful to understand the metabolic processes during alkaline protease production in industrial fermentation medium. Here, we collected the transcription database at various enzyme-producing stages (preliminary stage, stable phase and decline phase) to specifically research the synthesized and regulatory mechanism of alkaline protease in B. licheniformis. The RNA-sequencing analysis showed differential expression of numerous genes related to several processes, among which genes correlated with regulators were concerned, especially the major differential gene abrB on enzyme (AprE) synthesis was investigated. It was further verified that AbrB is a repressor of AprE by plasmid-mediated over-expression due to the severely descending enzyme activity (11,300 U/mL to 2695 U/mL), but interestingly it is indispensable for alkaline protease production because the enzyme activity of the null abrB mutant was just about 2279 U/mL. Thus, we investigated the aprE transcription by eliminating the theoretical binding site (TGGAA) of AbrB protein predicated by computational strategy, which significantly improved the enzyme activity by 1.21-fold and gene transcription level by 1.77-fold in the mid-log phase at a cultivation time of 18 h. Taken together, it is of great significance to improve the production strategy, control the metabolic process and oriented engineering by rational molecular modification of regulatory network based on the high throughput sequencing and computational prediction.}
}
@article{SMITH2018325,
title = {The Developing Infant Creates a Curriculum for Statistical Learning},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {4},
pages = {325-336},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318300275},
author = {Linda B. Smith and Swapnaa Jayaraman and Elizabeth Clerkin and Chen Yu},
keywords = {statistical learning, egocentric vision, face perception, object perception},
abstract = {New efforts are using head cameras and eye-trackers worn by infants to capture everyday visual environments from the point of view of the infant learner. From this vantage point, the training sets for statistical learning develop as the sensorimotor abilities of the infant develop, yielding a series of ordered datasets for visual learning that differ in content and structure between timepoints but are highly selective at each timepoint. These changing environments may constitute a developmentally ordered curriculum that optimizes learning across many domains. Future advances in computational models will be necessary to connect the developmentally changing content and statistics of infant experience to the internal machinery that does the learning.}
}
@article{DELIMA2023100590,
title = {Managing the plot structure of character-based interactive narratives in games},
journal = {Entertainment Computing},
volume = {47},
pages = {100590},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100590},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000459},
author = {Edirlei Soares {de Lima} and Bruno Feijó and Antonio L. Furtado},
keywords = {Interactive storytelling, Narrative generation, Drama management, Plot structure, Automated planning},
abstract = {The use of narrative generation methods in games is a complex challenge that involves multiple problems of plot-based processes integrated with character-based methods. Examples of these problems are the high computational complexity of many story generation algorithms, the difficulties associated with the generation of interactive narratives that are compelling and emotionally impactful, the complex interactions among characters, and the need for tools and methods to support story writers in the process of creating and managing the narrative structure of interactive stories. In this work, we present and evaluate a new approach to generate and manage the plot structure of character-based interactive narratives in games, which combines multi-agent planning with a drama management strategy based on narrative structures. The proposed method is supported by an authoring tool that allows authors to create and test interactive narratives using graphical interfaces and intuitive diagrams. The results of our study suggest the effectiveness of our approach in generating interactive narratives for highly interactive game environments. In addition, a user study of the proposed authoring tool indicates that it can successfully support the development of character-based interactive narratives without requiring programming knowledge.}
}
@incollection{TAIEF2025582,
title = {1.44 - The Application of Machine Learning for Green Hydrogen Production},
editor = {Professor Abdul Ghani Olabi},
booktitle = {Comprehensive Green Materials (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {582-593},
year = {2025},
isbn = {978-0-443-15739-4},
doi = {https://doi.org/10.1016/B978-0-443-15738-7.00030-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157387000301},
author = {Wafa Taief and Amani Al-Othman and Muhammad Tawalbeh},
keywords = {Green hydrogen, Hydrogen production, Machine learning, Optimization algorithms., Water electrolysis},
abstract = {It is no secret that the amount of pollution that fossil fuels cause in the environment when burned. When searching for alternative solutions, it is noted that hydrogen is an attractive option because of its many advantages that are included in this work. Hydrogen can be produced by different methods. In this work, there is a comprehensive review of the hydrogen production methods divided according to the sources, either renewable or nonrenewable, and the type of energy used based on both single and combined forms. In this work, there is a general view of the machine learning algorithms and models and the categories that they belong to, as well as their application in green hydrogen production. This work is very useful for the readers to organize their ideas about hydrogen production methods and machine learning and its applications to produce green hydrogen, so after reading this work, they can know their specific interests and search for details about it.}
}
@article{LATIF2023726,
title = {Design and Development a Virtual Planetarium Learning Media Using Augmented Reality},
journal = {Procedia Computer Science},
volume = {227},
pages = {726-733},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.577},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017441},
author = {Jazlyn Jan Keyla Latif and Augustinus Adrian Triputra and Michael Awarsa Kesuma and Fairuz Iqbal Maulana},
keywords = {Augmented Reality, Planetarium, Virtual, Virtual Planetarium, Application},
abstract = {The solar system has always been a mystery to many. If not for the advanced technologies, most humans would not have the opportunity to gain knowledge about the planets. Although Earth is a part of the solar system, the solar system is simply too dangerous and expensive for humans to explore casually. Humans do not interact with the Sun or planets actively. This is especially concerning for children who often require visual aids in studying. An Augmented Reality (AR) based application can solve that problem. Through Virtual Planetarium, children may interact with the Sun or the planets and gain information. This will help aid children's guardians in studying the solar system. The application is made by Systems Development Life Cycle (SDLC) method. Through the making of this application, it is expected that children will have a better understanding of the solar system.}
}
@article{KAPETANIOU2025106115,
title = {Beyond impulse control – toward a comprehensive neural account of future-oriented decision making},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {172},
pages = {106115},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2025.106115},
url = {https://www.sciencedirect.com/science/article/pii/S0149763425001150},
author = {Georgia E. Kapetaniou and Alexander Soutschek},
keywords = {Self-control, Delay discounting, Dorsolateral prefrontal cortex, Construal theory, Prospection, Metacognition},
abstract = {The dominant focus of current neural models of future-oriented decision making is on the interplay between the brain’s reward system and a frontoparietal network thought to implement impulse control. Here, we propose a re-interpretation of the contribution of frontoparietal activation to future-oriented behavior and argue that future-oriented decisions are influenced by a variety of psychological mechanisms implemented by dissociable brain mechanisms. We review the literature on the neural mechanisms underlying the influence of prospection, retrospection, framing, metacognition, and automatization on future-oriented decisions. We propose that the prefrontal cortex contributes to future-oriented decisions not by exerting impulse control but by constructing and updating the value of abstract future rewards. These prefrontal value representations interact with regions involved in reward processing (neural reward system), prospection (hippocampus, temporal cortex), metacognition (frontopolar cortex), and habitual behavior (dorsal striatum). The proposed account of the brain mechanisms underlying future-oriented decisions has several implications for both basic and clinical research: First, by reconciling the idea of frontoparietal control processes with construal accounts of intertemporal choice, we offer an alternative interpretation of the canonical prefrontal activation during future-oriented decisions. Second, we highlight the need for obtaining a better understanding of the neural mechanisms underlying future-oriented decisions beyond impulse control and of their contribution to myopic decisions in clinical disorders. Such a widened focus may, third, stimulate the development of novel neural interventions for the treatment of pathological impulsive decision making.}
}
@incollection{BERMAN201691,
title = {Chapter 3 - Indexing Text},
editor = {Jules J. Berman},
booktitle = {Data Simplification},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {91-133},
year = {2016},
isbn = {978-0-12-803781-2},
doi = {https://doi.org/10.1016/B978-0-12-803781-2.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037812000035},
author = {Jules J. Berman},
keywords = {Index, Concordance, Autocoding, Autoencoding, Page rank, Search, Retrieval},
abstract = {Data has no value if it cannot be sensibly examined. In past centuries, the index has been the key to searching and retrieving text. Today, it is tempting to think that the index is obsolete, being replaced the by the "find" box that pops onto the screen when we press the "Ctrl-F" keys. This is not the case; simple "find" searches cannot cope with the variations and complexities of textual information. A thoughtful index is a reconceptualization of the document that permits rapid retrieval of terms that are related to the search term. An index, aided by proper annotation of data, permits us to understand data in ways that were not anticipated when the original content was collected. With the use of computers, multiple indexes, designed for different purposes, can be created for a single document or data set. As data accrues, indexes can be updated. When data sets are combined, their respective indexes can be merged. A good way of thinking about indexes is that the document contains all of the complexity; the index contains all of the simplicity. Data scientists who understand how to create and use indexes will be in the best position to search, retrieve, and analyze textual data.}
}
@incollection{SENNEWALD2011139,
title = {15 - Planning and Budgeting},
editor = {Charles A. Sennewald},
booktitle = {Effective Security Management (Fifth Edition)},
publisher = {Butterworth-Heinemann},
edition = {Fifth Edition},
address = {Boston},
pages = {139-151},
year = {2011},
isbn = {978-0-12-382012-9},
doi = {https://doi.org/10.1016/B978-0-12-382012-9.00015-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123820129000150},
author = {Charles A. Sennewald},
abstract = {Publisher Summary
This chapter focuses on the planning and budgeting process in a Security Department. Planning and budgeting go hand in hand as budget is a plan stated in financial terms. Budgeting requires a realistic estimate of programs and their costs and an allocation of resources to achieve planned objectives. As budgets are prepared well in advance, effective budget management requires thinking ahead and anticipating needs based on relatively predictable conditions; thus it becomes a tool to ensure that plans are carried out. It gives direction to planning by defining specific programs, their timing, and their costs. The top–down and bottom–up approaches to budgeting in which senior management establishes goals and guidelines; the Security Manager provides the detailed planning and cost estimates; and senior management reviews these recommendations, establishes priorities, and allocates funds is recommended. Budget costs are generally broken down into capital, salary, and sundry expenses. The capital expenses include costs of physical improvements, physical additions, or major expenditures for hardware, which are usually fixed and hence easily determined. The use of detailed records from month to month and year to year makes it possible to arrive at realistic salary and sundry projections. All ongoing nonsalary expenses are considered sundry expenses, which are categorized according to factors such as the volume of expenditures in a given category and the distinctive nature of such expenditures. It is generally easier and efficient to manage a number of smaller sundry accounts than to rely on large accounts, which can be cumbersome.}
}
@incollection{ZEMAN2013373,
title = {Chapter 31 - The nature of consciousness},
editor = {James L. Bernat and H. Richard Beresford},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {118},
pages = {373-407},
year = {2013},
booktitle = {Ethical and Legal Issues in Neurology},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-444-53501-6.00031-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444535016000317},
author = {Adam Zeman and Jan Adriaan Coebergh},
keywords = {awareness, consciousness, philosophy, self-consciousness, theories of consciousness, unconsciousness}
}
@article{NEUMAN2014650,
title = {Personality from a cognitive-biological perspective},
journal = {Physics of Life Reviews},
volume = {11},
number = {4},
pages = {650-686},
year = {2014},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1571064514001584},
author = {Yair Neuman},
keywords = {Personality, Threat, Trust, Distrust, Psychology, Interdisciplinarity},
abstract = {The term “personality” is used to describe a distinctive and relatively stable set of mental traits that aim to explain the organism's behavior. The concept of personality that emerged in human psychology has been also applied to the study of non-human organisms from birds to horses. In this paper, I critically review the concept of personality from an interdisciplinary perspective, and point to some ideas that may be used for developing a cognitive-biological theory of personality. Integrating theories and research findings from various fields such as cognitive ethnology, clinical psychology, and neuroscience, I argue that the common denominator of various personality theories are neural systems of threat/trust management and their emotional, cognitive, and behavioral dimensions. In this context, personality may be also conceived as a meta-heuristics both human and non-human organisms apply to model and predict the behavior of others. The paper concludes by suggesting a minimal computational model of personality that may guide future research.}
}
@article{YANG2012381,
title = {Combining means-end chain and fuzzy ANP to explore customers’ decision process in selecting bundles},
journal = {International Journal of Information Management},
volume = {32},
number = {4},
pages = {381-395},
year = {2012},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2011.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401211001307},
author = {Hao-Wei Yang and Kuei-Feng Chang},
keywords = {Means-end chain, Fuzzy analytic network process, Customer value, Product attributes},
abstract = {Although some researches had submitted the hierarchical model of customer value, there are still questions remaining in the model. How to achieve a more effective method for obtaining and analyzing data from customers concerning their expectations is still lacking. Therefore, this research first applied the modification of the means-end chain (MEC) to construct a hierarchy framework of customer value to allow product attributes to be linked. Next, this research combined fuzzy analytic network process (FANP) in exploring customer preference to catch the multiple needs of customers. Meanwhile, in the measurement of customer preference, fuzzy logic and linguistic variables are utilized to overcome human subjective and imprecise thinking. Overall, this research proposes the hierarchy framework of customer value including the causal relationships of attributes–consequence–value to fill previous model's gap, and identified the factors which could enhance the value of bundle, in contrast to the many monetary research treatments of product bundles. Finally, this research presented the value implications of cosmetics bundles and implication for management and marketing.}
}
@article{TERZIEVA2024106,
title = {Trends, Challenges, Opportunities, and Innovations in STEM Education},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {3},
pages = {106-111},
year = {2024},
note = {22nd IFAC Conference on Technology, Culture and International Stability TECIS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.134},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324002179},
author = {Valentina Terzieva and Elena Paunova-Hubenova and Savina Slavcheva},
keywords = {STEM education, STEM teaching approaches, STEM challenges, innovative teaching methods},
abstract = {STEM education aims to prepare students for their future jobs, providing authentic tasks and problems to solve. Usually, approaches to teaching STEM subjects are based on a constructivist learning theory that accentuates active, practical, and interactive learning approaches. Nowadays, the implementation of STEM education faces several logistical and pedagogical challenges, which can impact the effectiveness of STEM education programs. The conditions for applying information technologies in STEM education in Bulgarian schools and universities are presented. The paper proposes a conceptual model of the innovative STEM educational system, which includes personalization and optimization of the applied teaching methods.}
}
@article{DEPARDIEU1994110,
title = {Optical properties of cermets: 3D real space renormalization using the connection machine},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {207},
number = {1},
pages = {110-114},
year = {1994},
issn = {0378-4371},
doi = {https://doi.org/10.1016/0378-4371(94)90360-3},
url = {https://www.sciencedirect.com/science/article/pii/0378437194903603},
author = {G. Depardieu and P. Fiorini and S. Berthier},
abstract = {We propose a 3D position space renormalization approach based on Kadanoff's block method, for the calculation of the effective dielectric function of cermet type film. This model has been applied to simulated cubic lattices with particle aggregation. Our calculations were performed on a Thinking Machine CM5 massively parallel supercomputer. The renormalization process maps very naturally on to hypercube parallel architecture. We obtain good agreement with most of the experimental data.}
}
@article{SCHAFER2020100360,
title = {Lenstool-HPC: A High Performance Computing based mass modelling tool for cluster-scale gravitational lenses},
journal = {Astronomy and Computing},
volume = {30},
pages = {100360},
year = {2020},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2019.100360},
url = {https://www.sciencedirect.com/science/article/pii/S2213133719301349},
author = {C. Schäfer and G. Fourestey and J.-P. Kneib},
keywords = {Gravitational lensing software, High performance computing algorithms, Applied computing: astronomy, Galaxies: clusters, Galaxies: halos, Lenstool},
abstract = {With the upcoming generation of telescopes, cluster scale strong gravitational lenses will act as an increasingly relevant probe of cosmology and dark matter. The better resolved data produced by current and future facilities requires faster and more efficient lens modelling software. Consequently, we present Lenstool-HPC, a strong gravitational lens modelling and map generation tool based on High Performance Computing (HPC) techniques and the renowned Lenstool software. We also showcase the HPC concepts needed for astronomers to increase computation speed through massively parallel execution on supercomputers. Lenstool-HPC was developed using lens modelling algorithms with high amounts of parallelism. Each algorithm was implemented as a highly optimised CPU, GPU and Hybrid CPU–GPU version. The software was deployed and tested on the Piz Daint cluster of the Swiss National Supercomputing Centre (CSCS). Lenstool-HPC perfectly parallel lens map generation and derivative computation achieves a factor 30 speed-up using only 1 GPU compared to Lenstool. Lenstool-HPC hybrid Lens-model fit generation tested at Hubble Space Telescope precision is scalable up to 200 CPU–GPU nodes and is faster than Lenstool using only 4 CPU–GPU nodes.}
}
@article{VEKONY2025111703,
title = {Mind wandering enhances statistical learning},
journal = {iScience},
volume = {28},
number = {2},
pages = {111703},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.111703},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224029304},
author = {Teodóra Vékony and Bence C. Farkas and Bianka Brezóczki and Matthias Mittner and Gábor Csifcsák and Péter Simor and Dezső Németh},
keywords = {Psychology},
abstract = {Summary
The human brain spends 30–50% of its waking hours engaged in mind-wandering (MW), a common phenomenon in which individuals either spontaneously or deliberately shift their attention away from external tasks to task-unrelated internal thoughts. Despite the significant amount of time dedicated to MW, its underlying reasons remain unexplained. Our pre-registered study investigates the potential adaptive aspects of MW, particularly its role in predictive processes measured by statistical learning. We simultaneously assessed visuomotor task performance as well as the capability to extract probabilistic information from the environment while assessing task focus (on-task vs. MW). We found that MW was associated with enhanced extraction of hidden, but predictable patterns. This finding suggests that MW may have functional relevance in human cognition by shaping behavior and predictive processes. Overall, our results highlight the importance of considering the adaptive aspects of MW, and its potential to enhance certain fundamental cognitive abilities.}
}
@article{CHO2025101169,
title = {How age and culture influence cognition: A lifespan developmental perspective},
journal = {Developmental Review},
volume = {75},
pages = {101169},
year = {2025},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2024.101169},
url = {https://www.sciencedirect.com/science/article/pii/S0273229724000534},
author = {Isu Cho and Angela Gutchess},
keywords = {Cognitive Aging, Culture, Cognition, Culture and Cognition, Children, Lifespan perspective},
abstract = {It has long been assumed that cognitive aging is a universal phenomenon. However, increasing evidence substantiates the importance of individual differences in cognitive aging. How do experiential factors related to culture shape developmental trajectories of cognition? We propose a new model examining how age and culture influence cognitive processes, building on past models and expanding upon them to incorporate a lifespan developmental perspective. The current model posits that how age and culture interact to influence cognition depends on (a) the extent to which the cognitive task relies on top-down or bottom-up processes, and (b) for more top-down processes, the level of cognitive resources required to perform the task. To assess the validity of the model, we review literature not only from adulthood but also childhood, making this the first model to adopt a lifespan perspective in the study of culture and cognition. The current work advances understanding of cognitive aging by delineating the combined effects of biological aging processes, assumed to apply across cultures, and culture-dependent experiential aging processes, which reflect unique cultural experiences throughout one’s lifespan. This approach enables understanding of comprehensive potential mechanisms that underlie the influence of culture on cognitive development across life stages.}
}
@article{ELZANFALY201579,
title = {[I3] Imitation, Iteration and Improvisation: Embodied interaction in making and learning},
journal = {Design Studies},
volume = {41},
pages = {79-109},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X1500071X},
author = {Dina El-Zanfaly},
keywords = {computational making, design education, design technology, interaction design, reflective practice},
abstract = {I introduce in this paper a new learning and making process that fosters a new ability to make things through the body's direct, iterative engagement with materials, tools, machines and objects. Tested in a variety of educational settings, this method, which I call ‘I3’ for its three-layer operation of ‘Imitation, Iteration and Improvisation’, allows learners to develop their sensory experiences to improvise and create on their own. I introduce case studies in order to test I3. I challenge the separation of design and construction often reinforced by the use of digital fabrication. I show that learning to make and learning from making emerge together through a situated and embodied interaction among the learner, the materials, the tools and the object in-the-making.}
}
@article{FIORELLI2025150,
title = {Digital models and 3D biomechanics analysis in orthodontics. Part 1: Vector calculations},
journal = {Seminars in Orthodontics},
volume = {31},
number = {1},
pages = {150-157},
year = {2025},
issn = {1073-8746},
doi = {https://doi.org/10.1053/j.sodo.2024.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1073874624001348},
author = {Giorgio Fiorelli},
keywords = {Biomechanics, Vectors, 3D Force System},
abstract = {Biomechanics is essential for optimizing orthodontic appliances and controlling dental movement. Charles J. Burstone pioneered a three-dimensional (3D) approach in orthodontics, advocating for a shift beyond appliance-focused methods. Initially, biomechanics studies were constrained to two-dimensional (2D) analysis due to the complexities of 3D evaluation. Despite progress in computational tools and digital modeling, orthodontic biomechanics has largely maintained a 2D orientation. This paper advances orthodontic biomechanics into 3D, re-evaluating concepts previously limited to 2D frameworks. A dedicated software, DDP-Ortho (Ortolab, Poland), is introduced to enable orthodontists to analyze and resolve biomechanical challenges in 3D, facilitating appliance designs with precise 3D force systems. The representation and calculation of force vectors and moments in 3D are detailed, emphasizing the inherent complexity absent computational support. Key processes such as vector subtraction and addition, fundamental for assessing and refining orthodontic force systems, are explained. Additionally, the vector split (couple replacement) method, previously described in 2D, is extended to 3D, addressing the unique constraints and challenges of this approach. These tools promise to refine the accuracy and effectiveness of orthodontic treatments, setting the stage to examine the interactions between 3D force systems and dental movement, which will be addressed in a subsequent paper, to broaden the potential of contemporary orthodontic therapy.}
}
@article{LEHRACH201126,
title = {ITFoM – The IT Future of Medicine},
journal = {Procedia Computer Science},
volume = {7},
pages = {26-29},
year = {2011},
note = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911006776},
author = {Hans Lehrach and Ralf Subrak and Peter Boyle and Markus Pasterk and Kurt Zatloukal and Heimo Müller and Tim Hubbard and Angela Brand and Mark Girolami and Daniel Jameson and Frank J. Bruggeman and Hans V. Westerhoff},
keywords = {nonlinear information and communication technology, distributive computing, high throughput data analysis, personalized medicine, systems medicine, healthcare revolution: virtual human},
abstract = {Molecular medicine is undergoing a revolution, creating a data fog that may obscure understanding. The functioning human is analogous to a biological factory controlled by an incredibly complex Information and Communication (IC) network. It is proposed that 7 billion computational replicas be made of those 7 billion human IC networks to enable interrogation and manipulation, for understanding and personalized healthcare. This requires a revolutionary ICT that follows the organization of the biological information and communication flows, with implications for hardware, software and connectivity.}
}
@article{TOSUN2013121,
title = {Does obligatory linguistic marking of source of evidence affect source memory? A Turkish/English investigation},
journal = {Journal of Memory and Language},
volume = {69},
number = {2},
pages = {121-134},
year = {2013},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2013.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X13000235},
author = {Sümeyra Tosun and Jyotsna Vaid and Lisa Geraci},
keywords = {Evidentiality, Turkish, Source monitoring, Recognition memory, Linguistic relativity, Thinking for speaking},
abstract = {This study examined the influence of obligatory linguistic marking of the source of information on source memory. Turkish grammar requires speakers to indicate if an assertion is based on first hand knowledge or non-firsthand knowledge (hearsay or inference); English grammar does not require this distinction. We hypothesized that obligatory coding of source of evidence leads to a greater weighting of first hand relative to non-firsthand accounts of events (an “evidentiality effect”), resulting in better memory for first hand sources. In support of this hypothesis, across two experiments native Turkish speaking adults showed significantly better recognition and source memory for assertions coded with first hand than non-firsthand evidential markers. Further, among Turkish speakers who also knew English, those who learned English later had less accurate recognition and source memory for non-firsthand sources presented in English than those who learned English earlier, suggesting a carryover from the first language (Turkish). English monolingual speakers showed no difference in recognition or source memory as a function of source type, but showed better memory than Turkish speakers for non-firsthand sources. These findings provide the first empirical support for an evidentiality effect, suggesting that when marking the source of evidence is required by the grammar first hand sources are privileged in memory and non-firsthand sources are discounted.}
}
@article{ZHENG2025,
title = {Machine Memory Intelligence: Inspired by Human Memory Mechanisms},
journal = {Engineering},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2025.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S2095809925000293},
author = {Qinghua Zheng and Huan Liu and Xiaoqing Zhang and Caixia Yan and Xiangyong Cao and Tieliang Gong and Yong-Jin Liu and Bin Shi and Zhen Peng and Xiaocen Fan and Ying Cai and Jun Liu},
keywords = {Machine memory intelligence, Neural mechanism, Associative representation, Continual learning, Collaborative reasoning},
abstract = {Large models, exemplified by ChatGPT, have reached the pinnacle of contemporary artificial intelligence (AI). However, they are plagued by three inherent drawbacks: excessive training data and computing power consumption, susceptibility to catastrophic forgetting, and a deficiency in logical reasoning capabilities within black-box models. To address these challenges, we draw insights from human memory mechanisms to introduce “machine memory,” which we define as a storage structure formed by encoding external information into a machine-representable and computable format. Centered on machine memory, we propose the brand-new machine memory intelligence (M2I) framework, which encompasses representation, learning, and reasoning modules and loops. We explore the key issues and recent advances in the four core aspects of M2I, including neural mechanisms, associative representation, continual learning, and collaborative reasoning within machine memory. M2I aims to liberate machine intelligence from the confines of data-centric neural networks and fundamentally break through the limitations of existing large models, driving a qualitative leap from weak to strong AI.}
}
@article{FORTINI2023107058,
title = {An experimental and numerical study of the solid particle erosion damage in an industrial cement large-sized fan},
journal = {Engineering Failure Analysis},
volume = {146},
pages = {107058},
year = {2023},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2023.107058},
url = {https://www.sciencedirect.com/science/article/pii/S1350630723000122},
author = {Annalisa Fortini and Alessio Suman and Nicola Zanini},
keywords = {Wear damage, Hardfacing, Centrifugal fan, Computational fluid dynamics, Solid particle erosion, Metallographic analysis},
abstract = {The present paper addresses the wear failure analysis of a large-sized centrifugal fan operating in a cement clinker grinding plant. Within cement production, the calcination at middle and high-temperature values (from 120 °C to 400 °C depending on the process parameters) of the raw material requires such a process fan, which also ensures the draft and feed of the flue gases and combustion air needed for the operation of the main equipment of the cement factory. To detect and analyze the impact conditions within the heavy-duty fan, Computational Fluid Dynamics (CFD) analyses were performed. The analysis of the numerical results shows that the relevant fan surfaces are affected by different impact velocities and angles, generating non-uniform erosion patterns similar to the on-field detections. Besides, the obtained comprehensive description of the flow and contaminants behaviors through the entire flow path enables setting up the subsequent experimental investigation. The erosive wear behavior of a Fe-Cr-C hardfacing cast iron and wear-resistant steel was tested through a test rig constructed for the purpose of being in accordance with the ASTM G76 standard. The test bench was adapted to manage the raw meal powder used in the cement factory to reproduce the actual operating conditions. The results show a greater capability of Fe-Cr-C hardfacing cast iron to face the erosion phenomenon in terms of lower values of material loss over the exposure time. These findings, coupled with the metallographic analysis to detect the erosion mechanisms (ductile and/or brittle), help a better prediction of the fan operating life. The investigation showed the reliability of the numerical/experimental coupled approach in assessing the actual erosion magnitude and the influence of the impact angle on the erosion phenomena. This coupled approach gains a further understanding of the proper design of manufacturing and maintenance activities, covering several project steps from material selections to the scheduled and overhaul operations. A reliable operating-life prediction allows manufacturers and operators to obtain production and economic goals.}
}
@article{ZHANG2022101922,
title = {Multiple-symbol noncoherent learning detection of coded QAM signals in IEEE 802.15.3 Wireless Multi-media Networks},
journal = {Physical Communication},
volume = {55},
pages = {101922},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101922},
url = {https://www.sciencedirect.com/science/article/pii/S1874490722001999},
author = {Gaoyuan Zhang and Congfang Ma and Kai Chen and Yongen Li and Haiqiong Li and Congzheng Han},
keywords = {Wireless Multi-media Networks, Noncoherent detection, Deep learning, Uniform quantization},
abstract = {We consider the noncoherent deep learning problem for coded signal detection under the phase noncoherent channels for remote home healthcare applications with high data rate. In particular, a multiple-symbol noncoherent learning detection (MNLD) scheme based on neural networks is proposed for low-density parity-check (LDPC) coded noncoherent quadrature amplitude modulation (QAM) signals in IEEE 802.15.3 Wireless Multi-media Networks. Our derivation shows that extensive operations for the first kind zero-order modified Bessel function is unavoidable for the implementation of the optimal bit log-likelihood ratio (LLR) for decoding in traditional multiple-symbol detection (MSD) scheme. The perfect estimation of the channel state information (CSI), i.e., a priori information about the variance of the additive white Gaussian noise (AWGN), is also required for the receiver. This is clearly not computationally practical for Wireless Multi-media Networks. Consequently, we developed an improved approach based on feed-forward neural networks to accurately calculate the bit LLR. Furthermore, to decrease the generation size of training set and thus increase the training speed of the proposed neural networks, we uniformly quantize the continuous carrier phase offset (CPO), which is random and unknown, into discrete status. Our simulation results verify the learning efficiency of this simplified training-set generation configuration. The decoding convergence is successfully accelerated and much performance gain is finally achieved when compared with traditional decoding using the perfect bit LLR. This is clearly critical for high reliable transmission of home healthcare information.}
}
@incollection{AGGARWAL2022135,
title = {Chapter 5 - Models for improving fresh produce chains},
editor = {Wojciech J. Florkowski and Nigel H. Banks and Robert L. Shewfelt and Stanley E. Prussia},
booktitle = {Postharvest Handling (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
address = {San Diego},
pages = {135-164},
year = {2022},
isbn = {978-0-12-822845-6},
doi = {https://doi.org/10.1016/B978-0-12-822845-6.00005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228456000051},
author = {Deepak Aggarwal and Stanley E. Prussia},
keywords = {Soft systems, modeling, simulation, postharvest quality simulator, supply chain game, soft systems methodologies},
abstract = {Fresh produce passes through various links of refrigerated or nonrefrigerated value chains from the farmer’s plot to the consumer’s plate. Extensive research since the 1960s has focused on deciphering the ideal postharvest handling conditions to maximize product shelf life for different produce species. However, ideal conditions are seldom met in real-life value chains as the produce travels from field to sorting area, packaging, loading, transportation, unloading, retail display, and the consumer’s car. Further, retail and food service managers often are not provided information about previous handling that affects the remaining shelf life at their link of a value chain. The primary goal is to provide fresh produce with desirable qualities such as firmness, color, ripeness, rupture strength, and taste to the consumers. Postharvest value chains require assimilation of systems thinking, systems dynamics, and physiology of the fresh produce. These can be woven together using modeling and simulation games. Various types of models such as mental models, conceptual models, soft systems, and others can be applied. The underlying equations for the models are the value chain dynamics and the physiological changes in produce at varying storage conditions. The models can then be provided to the intended user as a simple spreadsheet model or as visually appealing games or videos, with the simulations running in the background. The models predicting postharvest quality changes could help decision makers alter shipment destinations and storage conditions so that fresh produce arrives with the desired consumer characteristics. Playing simulation games can be an entertaining method for everyone interested in fresh fruits and vegetables to experience the challenges and satisfactions of learning the consequences of decisions they make while playing the role of a manager at each link in a selected chain. Developing a model requires understanding about the interactions within a system and its surroundings. When developing models, information gaps often are found that require research to learn how a system works. Learning continues as users of the model gain experience without the costs or risks of changing real-life situations. Using soft systems methodology to study fresh fruit and vegetable value chains would include developing models of their activities, interconnections, flows of information, and political and social environments. This could improve our understanding of how chains could function as if they were systems. Other types of models would result from using the critical systems practice methodology as a guide for learning the technical, managerial, and social changes necessary to increase per capita consumption, reduce losses and waste, and improve profits for family farms and other global issues related to postharvest handling of fresh produce. Multiplayer simulation games would help managers and leaders learn how to improve entire fresh fruit and vegetable chains.}
}
@article{FARAHI2021100326,
title = {A simulation–optimization approach for measuring emergency department resilience in times of crisis},
journal = {Operations Research for Health Care},
volume = {31},
pages = {100326},
year = {2021},
issn = {2211-6923},
doi = {https://doi.org/10.1016/j.orhc.2021.100326},
url = {https://www.sciencedirect.com/science/article/pii/S2211692321000424},
author = {Sorour Farahi and Khodakaram Salimifard},
keywords = {Crisis, Healthcare responsiveness, Resilience, Simulation–optimization},
abstract = {Crisis occurrence in the healthcare context is, for different reasons, a phenomenon that happens abundantly. The priority of the healthcare system during a crisis is to provide quality care and superior services to the injured people. However, given the usually extreme severity of the crisis that results in a significant number of injured people, proper and timely responsiveness of healthcare systems is a challenging issue This study proposes a novel framework using a hybrid simulation–optimization approach to measure the healthcare responsiveness in crisis to address this real-world problem. This paper closely connects operations research techniques to critical systems thinking notions to evaluate the behavior of a system in the face of crisis. Since all arriving casualties to the hospital are first taken to the emergency department (ED), the ED in a case study is used to illustrate the performance of the presented approach. We designed seven crisis scenarios and one scenario of the ED system in a normal situation and modeled them using discrete-event simulation (DES). Patients’ interarrival times act as the driver of workload experienced in ED during crisis scenarios of varying severity. For crisis simulation scenarios that are unable to cope with the severity of the crisis, we developed an optimization model in an optimization tool to determine the optimal configuration of resources. The optimal configuration can improve healthcare resilience. The results show that an interarrival time of 13.8 min is the maximum threshold, below which feasible solutions could not be found, and the ED system is likely to collapse.}
}
@article{LAENG2014263,
title = {Scrutinizing visual images: The role of gaze in mental imagery and memory},
journal = {Cognition},
volume = {131},
number = {2},
pages = {263-283},
year = {2014},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0010027714000043},
author = {Bruno Laeng and Ilona M. Bloem and Stefania D’Ascenzo and Luca Tommasi},
keywords = {Imagery, Memory, Eye-tracking, Enactment},
abstract = {Gaze was monitored by use of an infrared remote eye-tracker during perception and imagery of geometric forms and figures of animals. Based on the idea that gaze prioritizes locations where features with high information content are visible, we hypothesized that eye fixations should focus on regions that contain one or more local features that are relevant for object recognition. Most importantly, we predicted that when observers looked at an empty screen and at the same time generated a detailed visual image of what they had previously seen, their gaze would probabilistically dwell within regions corresponding to the original positions of salient features or parts. Correlation analyses showed positive relations between gaze’s dwell time within locations visited during perception and those in which gaze dwelled during the imagery generation task. Moreover, the more faithful an observer’s gaze enactment, the more accurate was the observer’s memory, in a separate test, of the dimension or size in which the forms had been perceived. In another experiment, observers saw a series of pictures of animals and were requested to memorize them. They were then asked later, in a recall phase, to answer a question about a property of one of the encoded forms; it was found that, when retrieving from long-term memory a previously seen picture, gaze returned to the location of the part probed by the question. In another experimental condition, the observers were asked to maintain fixation away from the original location of the shape while thinking about the answer, so as to interfere with the gaze enactment process; such a manipulation resulted in measurable costs in the quality of memory. We conclude that the generation of mental images relies upon a process of enactment of gaze that can be beneficial to visual memory.}
}
@article{NERSESSIAN2009178,
title = {Hybrid analogies in conceptual innovation in science},
journal = {Cognitive Systems Research},
volume = {10},
number = {3},
pages = {178-188},
year = {2009},
note = {Special Issue on Analogies - Integrating Cognitive Abilities},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389041709000035},
author = {Nancy J. Nersessian and Sanjay Chandrasekharan},
keywords = {Conceptual innovation, Hybrid analogies, Simulation, Visual reasoning, Engineering sciences},
abstract = {Analogies are ubiquitous in science, both in theory and experiments. Based on an ethnographic study of a research lab in neural engineering, we focus on a case of conceptual innovation where the cross-breeding of two types of analogies led to a breakthrough. In vivo phenomena were recreated in two analogical forms: one, as an in vitro physical model, and the other, as a computational model of the first physical model. The computational model also embodied constraints drawn from the neuroscience and engineering literature. Cross connections and linkages were then made between these two analogical models, over time, to solve problems. We describe how the development of the intermediary, hybrid computational model led to a conceptual innovation, and subsequent engineering innovations. Using this case study, we highlight some of the peculiar features of such hybrid analogies that are now used widely in the sciences and engineering sciences, and the significant questions they raise for current theories of analogy.}
}
@article{KOPONEN202557,
title = {Sales managers' perceptions of interpersonal communication competence in leading AI-integrated sales teams},
journal = {Industrial Marketing Management},
volume = {124},
pages = {57-72},
year = {2025},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2024.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0019850124001846},
author = {Jonna Koponen and Saara Julkunen and Anne Laajalahti and Marianna Turunen and Brian Spitzberg},
keywords = {Artificial intelligence (AI), Interpersonal communication competence, Management},
abstract = {Adoption of artificial intelligence (AI) is no longer the issue for most professional organizations—the question is how to integrate it into the functions and organizational processes. Considering the current integration of AI in work processes, the requirements for sales managers' interpersonal communication competence (ICC) are likely to be modified. However, research on sales management competencies is surprisingly scarce. This longitudinal case study investigates sales managers' perceptions of their ICC needs in leading AI-integrated sales teams in the financial sector. During the years 2019–2024, 35 expert interviews with sales managers were collected from one of Scandinavia's largest financial groups. The findings indicate that AI system integration brought benefits, concerns and communication challenges to sales managers' job content. The main components related to sales managers' ICC in leading AI-integrated sales teams encompass both traditional competencies (motivation, knowledge, communication skills, and adaptability) but also include contextual AI factors and a concern for ethical reflectivity. A component model of managerial interpersonal communication competence in AI-integrated teams (MICCAIT) is produced and its implications are examined. Given the greater reliance on technology, sales managers may increasingly need to place greater emphasis on their empathy and people-oriented skills for the human employees remaining in the workplace.}
}
@article{FOSTER2021101214,
title = {Translating the grid: How a translational approach shaped the development of grid computing},
journal = {Journal of Computational Science},
volume = {52},
pages = {101214},
year = {2021},
note = {Case Studies in Translational Computer Science},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101214},
url = {https://www.sciencedirect.com/science/article/pii/S187775032030510X},
author = {Ian Foster and Carl Kesselman},
keywords = {Translational computer science, Grid computing},
abstract = {A growing gap between progress in biological knowledge and improved health outcomes inspired the new discipline of translational medicine, in which the application of new knowledge is an explicit part of a research plan. Abramson and Parashar argue that a similar gap between complex computational technologies and ever-more-challenging applications demands an analogous discipline of translational computer science, in which the deliberate movement of research results into large-scale practice becomes a central research focus rather than an afterthought. We revisit from this perspective the development and application of grid computing from the mid-1990s onwards, and find that a translational framing is useful for understanding the technology’s development and impact. We discuss how the development of grid computing infrastructure, and the Globus Toolkit, in particular, benefited from a translational approach. We identify lessons learned that can be applied to other translational computer science initiatives.}
}
@article{BENEDETTO2014167,
title = {Rebound effects due to economic choices when assessing the environmental sustainability of wine},
journal = {Food Policy},
volume = {49},
pages = {167-173},
year = {2014},
issn = {0306-9192},
doi = {https://doi.org/10.1016/j.foodpol.2014.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0306919214001250},
author = {Graziella Benedetto and Benedetto Rugani and Ian Vázquez-Rowe},
keywords = {Carbon footprint, Consequential LCA, Indirect effects, Life Cycle Assessment, Sustainable consumption},
abstract = {The identification and working mechanisms of Rebound Effects (REs) have important policy implications. The intensity of these impacts is crucial when it comes to detecting strategies to promote sustainable consumption of food and beverages, as in the case of wine. In fact, neglecting the occurrence of REs in wine production and delivery leads to under- or over-estimating the effects that novel more sustainable technologies may produce. An in-depth analysis on the ways in which the stakeholders may react to the availability of a new product (e.g. wine produced through a process oriented to the reduction of CO2 emissions) may be particularly useful to allow producers and consumers to target the REs with respect to the overall goals of desired sustainability. In this article, we first provide a definition and a classification of different types of REs and then analyse some exemplificative cases applied to the supply and consumption of wine produced through technologies that reduce environmental emissions or resource consumptions. A final step analyses the methodological tools that may be useful when including REs in life cycle thinking as applied to the wine sector.}
}
@incollection{MISTRY2021467,
title = {17 - Bio-inspired design},
editor = {Igor Yadroitsev and Ina Yadroitsava and Anton {du Plessis} and Eric MacDonald},
booktitle = {Fundamentals of Laser Powder Bed Fusion of Metals},
publisher = {Elsevier},
pages = {467-489},
year = {2021},
series = {Additive Manufacturing Materials and Technologies},
isbn = {978-0-12-824090-8},
doi = {https://doi.org/10.1016/B978-0-12-824090-8.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012824090800010X},
author = {Yash Mistry and Daniel Anderson and Dhruv Bhate},
keywords = {Additive manufacturing, Bio-inspired design, Biomimicry, Cellular materials, Complexity, Hierarchy, Laser powder bed fusion, Periodicity, Texture, Topology optimization},
abstract = {Advances in additive manufacturing, computational design tools, and digitization techniques are converging in an exciting new era of engineering design, as humanity has never experienced before. Within this convergent domain, Bio-Inspired Design (BID) is a particularly promising area of research since the potential space for establishing structure-function correlation is vast, and the majority of it is untapped. In this chapter, bio-inspired design is first introduced, specifically in the context of the Laser Powder Bed Fusion (L-PBF) process. Practical approaches for implementing BID for L-PBF are discussed, followed by a discussion of key general design concepts that can be abstracted for BID. Examples of how BID and L-PBF have been combined are then presented, followed by a consideration of some of the most important design constraints posed by the L-PBF process that the bio-inspired designer needs to be aware of. The chapter concludes with a forward looking discussion of opportunities in this domain.}
}
@article{LENNON2022104608,
title = {Young children's social and independent behavior during play with a coding app: Digital game features matter in a 1:1 child to tablet setting},
journal = {Computers & Education},
volume = {190},
pages = {104608},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104608},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522001798},
author = {Maya Lennon and Sarah Pila and Rachel Flynn and Ellen A. Wartella},
keywords = {Applications in coding, Cooperative/collaborative learning, Games, early years education},
abstract = {The overarching aim of this study was to explore young children's (N = 25, Mage = 5.16 years) play with two coding games (Daisy the Dinosaur and Kodable) in a 1:1 child to tablet setting. We had three research questions focused on children's game play: 1) How does the structure of each game influence children's play? 2) Do children play more or less independently depending on the game they play? 3) Do children who play the games more independently learn more coding skills? Three researchers coded more than 6 h of video data showing children's play with digital coding games. Findings include, that the type of game did influence the different ways that children behaved while playing. However, during both games, children had the same amount of independent play. Children who played more independently during Daisy the Dinosaur learned more coding skills. This may be because these children were focusing more on the game than their peers as we did not find a similar effect for the game Kodable. We discuss the ways that children play structured vs. open structured (i.e., sandbox) digital games with a particular focus on how game play may influence learning. As opportunities for individual device ownership in classrooms increase, future work should continue to explore how game features influence learning.}
}
@article{LIN2006709,
title = {On integration of interface design methods: Can debates be resolved?},
journal = {Interacting with Computers},
volume = {18},
number = {4},
pages = {709-722},
year = {2006},
note = {Special Theme Papers from Special Editorial Board Members (contains Regular Papers)},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2005.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0953543805001104},
author = {Y. Lin and W.J. Zhang and R.J. Koubek and Ronald R. Mourant},
keywords = {Human–computer interface, Total interface design, Interface design principle},
abstract = {There have been many debates on how to design the human–computer interface (HCI). Often, one can find that different views in a debate are simply because these views are attached to different aspects which embody the same thing. In other words, prior to giving an effective judgment of a debate, one needs to establish an understanding of the ‘total’ aspects of a thing the debate is about. Following this line of thinking, in this paper, we propose an understanding of the ‘total’ aspects of designing HCI, which is called the total interface design framework. We then judge several debates under this framework with the purpose of exemplifying the judgment process for any other debate related to designing HCI. At the end, the debates used for exemplifying our judgment process can be resolved. The effectiveness of the total interface design framework for integrating the different HCI approaches is also demonstrated.}
}
@article{SINGH2025448,
title = {Unlocking microbial reservoirs for antimicrobial peptides and beyond},
journal = {Trends in Plant Science},
volume = {30},
number = {5},
pages = {448-450},
year = {2025},
note = {Special issue:Root biology and soil health for a sustainable future},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2024.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S1360138524003145},
author = {Akanksha Singh and Shivam Chauhan and Prabodh Kumar Trivedi},
keywords = {antimicrobial peptides, global microbiome, machine learning, peptide based biologicals},
abstract = {Recently, Santos-Júnior et al. utilized a machine learning approach to identify nearly a million novel antimicrobial peptides (AMPs) from the global microbiome. Here we explore the untapped potential of plant- and soil-associated microbiomes as a source of novel peptides, highlighting their promising applications in advancing agricultural innovation and sustainability.}
}
@article{VANDECRUYS2021107,
title = {Mental distress through the prism of predictive processing theory},
journal = {Current Opinion in Psychology},
volume = {41},
pages = {107-112},
year = {2021},
note = {Psychopathology},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2021.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X21001056},
author = {Sander {Van de Cruys} and Pieter {Van Dessel}},
keywords = {Predictive processing, Mental distress, Psychopathology, Emotion, Depression, Anxiety, Active inference, Addiction, Learning, Psychotherapy, Computational psychiatry},
abstract = {Summary
We review the predictive processing theory’s take on goals and affect, to shed new light on mental distress and how it develops into psychopathology such as in affective and motivational disorders. This analysis recovers many of the classical factors known to be important in those disorders, like uncertainty and control, but integrates them in a mechanistic model of adaptive and maladaptive cognition and behavior. We derive implications for treatment that have so far remained underexposed in existing predictive processing accounts of mental disorder, specifically with regard to the model-dependent construction of value, the importance of model validation (evidence), and the introduction and learning of new, adaptive beliefs that relieve suffering.}
}
@article{ZHANG202581,
title = {Utilizing neuroimaging visualization technology to enhance standardized neurosurgical training for Traditional Chinese Medicine residents: A neuroanatomical education study},
journal = {Brain Hemorrhages},
volume = {6},
number = {2},
pages = {81-85},
year = {2025},
issn = {2589-238X},
doi = {https://doi.org/10.1016/j.hest.2024.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2589238X24000640},
author = {Rongjun Zhang and Zhigang Gong and Wenbing Jiang and Zhaofeng Su},
keywords = {Neuroimaging visualization, Traditional Chinese Medicine, Neurosurgical training, Neuroanatomy, DSI Studio, Clinical Education, Meridians, Neural fiber tracts},
abstract = {Objective
This study aims to address the difficulties encountered by Traditional Chinese Medicine (TCM) students in learning neuroanatomy during clinical training by utilizing neuroimaging visualization technology.
Methods
81 students were divided into a control group (40 students) and an observation group (41 students). The control group followed traditional teaching methods as prescribed by the curriculum, while the observation group received additional training with the neuroimaging visualization software DSI Studio. This included whole-brain neural fiber reconstruction and cortical spinal tract evaluation in the context of stroke. Upon completion of the training, both groups were assessed on neuroanatomical theory, case analysis, neurological examination, and clinical skills. The teaching effectiveness was compared based on assessment results and feedback from questionnaires administered to the observation group.
Results
The observation group significantly outperformed the control group in theoretical knowledge, case analysis, and physical examination (P < 0.05). Over 90 % of students in the observation group reported via questionnaire that the integration of neuroimaging visualization technology significantly enhanced their understanding of neuroanatomy and clinical reasoning skills.
Conclusion
The clinical teaching approach augmented with neuroimaging visualization technology significantly improves the standardized training outcomes for TCM neurosurgical residents.}
}
@article{KUMARSHRIVASTAVA20217025,
title = {Bike clutch plate thermal analysis with using different materials},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {7025-7029},
year = {2021},
note = {International Conference on Advances in Design, Materials and Manufacturing},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.300},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321038943},
author = {Ashish {Kumar Shrivastava} and Rohit Pandey and Rahaman {Khan Pathan} and Yogesh {Kumar Tembhurne} and T {Ravi Kiran}},
keywords = {ANSYS, CATIA, Wet-Clutch plat, Thermal stress, Total deformation, Heat flux},
abstract = {In this present paper, wet clutch is analysed by Computational Modeling. 2-D drawings are designed for multi plate clutch from computational calculations. 3D model is created in the Solid Edge modeling software for bike clutch. FEM analysis of clutch is carried out by varying friction materials with some non metals and composite materials. The best material suited for the lining of friction surfaces is found. Thermal transient investigation will be done on the wet grating plates to check the quality. Modeling of multi plate clutch is done by using Solid edge Software and then the model is imported into ANSYS Software for Structural, Thermal analysis to check the quality and temperature contours. Results show that friction materials like Al alloy, Al oxide, Cast iron, Carbon fiber and Al nitride generate 146.31 °C, 136.77 °C, 146.44 °C, 149.99 °C and 144.54 °C respectively. Heat flux results with the above friction materials are respectively 1.18 w/mm2, 1.04 w/mm2, 0.56 w/mm2, 0.0085 w/mm2 and 1.25 w/mm2. A twin grasp transmission is very like the traditional programmed, the primary distinction being the twofold grip structure contrasted with the single programmed grasp utilized in automatics. Automatics utilize a torque converter to move motor torque from the motor to the transmission. Basic investigation for grip plate has done utilizing the properties of three materials which are utilized for liner (for example carbon–carbon composites, Kevlar, Ceramic composites). A twin grasp transmission is very like the traditional programmed, the primary distinction being the twofold grip structure contrasted with the single programmed grasp utilized in automatics.}
}
@incollection{MACHINMASTROMATTEO2025569,
title = {Information Literacy and the Information Science Curriculum},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {569-578},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00191-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001917},
author = {Juan D. Machin-Mastromatteo and César Saavedra-Alamillas and Alejandro Villegas-Muro},
keywords = {Advocacy, Critical competences, Curricular implementation, Curriculum, Curriculum methodologies, Digital literacy, Information literacy, Information literacy programs, Library and information science, Media literacy, Professional education and training, Social implications, Teaching and learning methodologies, Workplace implications},
abstract = {This entry provides a background to information literacy education in the library and information science (LIS) curriculum by presenting a summary of the elements and characteristics that information literacy courses should include. These are divided into six categories: (1) curricular implementation and general challenges; (2) topics the curriculum must include; (3) inclusion of education-related topics; (4) integration of other literacies; (5) methodologies for the information literacy curriculum; and (6) workplace and social implications. Then, it includes a brief and non-exhaustive review of 41 LIS programs, including courses on information literacy and related subjects. Finally, we offer some brief considerations for the future perspectives of this topic.}
}
@article{PEREIRA2014126,
title = {Analytical model for calculating indeterminate results interval of screening tests, the effect on seroconversion window period: A brief evaluation of the impact of uncertain results on the blood establishment budget},
journal = {Transfusion and Apheresis Science},
volume = {51},
number = {2},
pages = {126-131},
year = {2014},
issn = {1473-0502},
doi = {https://doi.org/10.1016/j.transci.2014.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1473050214001736},
author = {Paulo Pereira and James O. Westgard and Pedro Encarnação and Jerard Seghatchian},
keywords = {Bias, Delta-value, Precision, Seroconversion window period, Total analytical error},
abstract = {The evaluation of measurement uncertainty is not required by the European Union regulation for blood establishments' laboratory tests. However, it is required for tests accredited by ISO 15189. Also, the forthcoming ISO 9001 edition requires “risk based thinking” with risk described as “the effect of uncertainty on an expected result”. ISO recommends GUM models for determination of measurement uncertainty, but their application is not intended for ordinal value measurements, such as what happens with screening test binary results. This article reviews, discusses and proposes concepts intended for measurement uncertainty of screening test results. The precision model focuses on cutoff level allowing the evaluation of the indeterminate interval using analytical sources of variance. The interval is considered in the estimation of the seroconversion window period. The delta-value of patients and healthy subjects' samples allows ranking two tests according to the probability of the two classes of indeterminate results: chance of false negative results and chance of false positive results (waste on budget).}
}
@article{TAMM2016251,
title = {Slow sluggish cognitive tempo symptoms are associated with poorer academic performance in children with ADHD},
journal = {Psychiatry Research},
volume = {242},
pages = {251-259},
year = {2016},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2016.05.054},
url = {https://www.sciencedirect.com/science/article/pii/S0165178115304686},
author = {Leanne Tamm and Annie A. Garner and Richard E.A. Loren and Jeffery N. Epstein and Aaron J. Vaughn and Heather A. Ciesielski and Stephen P. Becker},
keywords = {Learning difficulties, School children, Apathy/disinterest, Slowed behavior/thinking},
abstract = {Sluggish cognitive tempo (SCT) symptoms may confer risk for academic impairment in attention-deficit/hyperactivity disorder (ADHD). We investigated SCT in relation to academic performance and impairment in 252 children (ages 6–12, 67% boys) with ADHD. Parents and teachers completed SCT and academic impairment ratings, and achievement in reading, math, and spelling was assessed. Simultaneous regressions controlling for IQ, ADHD, and comorbidities were conducted. Total SCT predicted parent-rated impairments in writing, mathematics, and overall school but not reading. Parent-rated SCT Slow predicted poorer reading and spelling, but not math achievement. Teacher-rated SCT Slow predicted poorer spelling and math, but not reading achievement. Parent-rated SCT Slow predicted greater academic impairment ratings across all domains, whereas teacher-rated SCT Slow predicted greater impairment in writing only. Age and gender did not moderate these relationships with the exception of math impairment; SCT slow predicted math impairment for younger but not older children. Parent and teacher SCT Sleepy and Daydreamy ratings were not significant predictors. SCT Slow appears to be uniquely related to academic problems in ADHD, and may be important to assess and potentially target in intervention. More work is needed to better understand the nature of SCT Slow symptoms in relation to inattention and amotivation.}
}
@incollection{FOTOPOULOS2022241,
title = {Chapter 8 - The edge-cloud continuum in wearable sensing for respiratory analysis},
editor = {Rui Pedro Paiva and Paulo de Carvalho and Vassilis Kilintzis},
booktitle = {Wearable Sensing and Intelligent Data Analysis for Respiratory Management},
publisher = {Academic Press},
pages = {241-271},
year = {2022},
isbn = {978-0-12-823447-1},
doi = {https://doi.org/10.1016/B978-0-12-823447-1.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234471000026},
author = {Anaxagoras Fotopoulos and Pantelis Z. Lappas and Alexis Melitsiotis},
keywords = {Artificial intelligence, Edge computing, Internet of Medical Things, Multisource fusion, P4 health care},
abstract = {Edge computing is seen as a set of remotely available computer system resources that drive the computing power at the source of data to improve energy efficiency and security, as well as decrease latency. Although the computation capability of biomedical wearables has increased extremely during the past decade, it is still challenging to perform sophisticated artificial intelligence (AI) algorithms in a resource-constrained environment for energy-efficiency and (near) real-time processing, along the edge-cloud continuum. The aim of this chapter is twofold. The first is to outline the role of edge computing on the Internet of Medical Things, in which wearable technologies are used as the sensory equipment for respiratory analysis, at the transition of patient monitoring from hospital to home. The second is to discuss the potential of explainable AI in the P4 health-care context for respiratory analysis, by highlighting computational intelligence and multisource fusion approaches to achieve continuous monitoring of respiratory analysis.}
}
@article{MARTIN2023120366,
title = {An energy future beyond climate neutrality: Comprehensive evaluations of transition pathways},
journal = {Applied Energy},
volume = {331},
pages = {120366},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120366},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922016233},
author = {Nick Martin and Laura Talens-Peiró and Gara Villalba-Méndez and Rafael Nebot-Medina and Cristina Madrid-López},
keywords = {Sustainable energy transition, Renewable energy, Energy modelling, Integrated assessment, Life cycle assessment, Critical raw materials},
abstract = {Many of the long-term policy decisions surrounding the sustainable energy transition rely on models that fail to consider environmental impacts and constraints beyond direct greenhouse gas emissions and land occupation. Such assessments offer incomplete and potentially misleading information about the true sustainability issues of transition pathways. Meanwhile, although decision-makers desire greater access to a broader range of environmental, material and socio-economic indicators, few tools currently address this gap. Here, we introduce ENBIOS, a framework that integrates a broader range of such indicators into energy modelling and policymaking practices. By calculating sustainability-related indicators across hierarchical levels, we reach deeper understandings of the potential energy systems to be derived. With ENBIOS, we analyse a series of energy pathways designed by the Calliope energy system optimization model for the European energy system in 2030 and 2050. Although overall emissions will drop significantly, considerable rises in land, labour and critical raw material requirements are likely. These outcomes are further reflected in unfavourable shifts in key metabolic indicators during this period; energy metabolic rate of the system will drop by 25.6%, land requirement-to-energy will quadruple, while the critical raw material supply risk-to-energy ratio will rise by 74.2%. Heat from biomass and electricity from wind and solar are shown to be the dominant future processes across most indicator categories.}
}
@incollection{LUO2015401,
title = {18 - Factor Investing and Portfolio Construction Techniques},
editor = {Emmanuel Jurczenko},
booktitle = {Risk-Based and Factor Investing},
publisher = {Elsevier},
pages = {401-433},
year = {2015},
isbn = {978-1-78548-008-9},
doi = {https://doi.org/10.1016/B978-1-78548-008-9.50018-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480089500186},
author = {Yin Luo and Spyros Mesomeris},
keywords = {Alpha parameter, Asset-class allocation, Beta portfolios, Copula model, CVaR optimization theory, CVaR portfolio, Efficient frontier analysis, Portfolio construction, Risk-based portfolio, Risk premia},
abstract = {Factor investing has been growing in popularity within institutional investment circles over the last few years. At the same time, risk-based portfolio construction techniques have become more mainstream, particularly following the global financial crisis of 2008. We cannot be blamed for thinking that these two concepts are very closely related: factor investing is about understanding the sources of risk that underlie a particular portfolio and taking investment decisions directly at the factor level, while risk-based portfolio construction techniques can be used to put together risk-factor portfolios. However, we will argue that factor investing is ultimately an “asset allocation” concept (even though it may not be directly used as such), whereas risk-based portfolio construction is a methodology that can be pursued in building risk-factor portfolios; in fact, under certain assumptions, it may actually be the optimal technique. Risk-based portfolio construction methods are readily used outside the risk-factor area, and, at the same time, risk-factor portfolios can be constructed through a number of different approaches such as mean-variance optimization, etc.}
}
@incollection{FOX2005103,
title = {7 Knowledge, arguments, and intentions in clinical decision-making},
editor = {Ray Paton and Laura A. McNamara},
series = {Studies in Multidisciplinarity},
publisher = {Elsevier},
volume = {3},
pages = {103-129},
year = {2005},
booktitle = {Multidisciplinary Approaches to Theory in Medicine},
issn = {1571-0831},
doi = {https://doi.org/10.1016/S1571-0831(06)80011-0},
url = {https://www.sciencedirect.com/science/article/pii/S1571083106800110},
author = {John Fox and David Glasspool},
abstract = {Publisher Summary
The most influential theories of reasoning and decision making were developed by mathematicians and logicians, often informed by problems in some practical domain such as medicine or economics. Their work led to theoretical concepts with great intellectual depth and formal rigor, such as statistical decision theory (SDT). There are difficulties with expected utility and other mathematical techniques for practical decision making. Any quantitative decision procedure depends upon the ability to estimate the required parameters. This can be problematic in real-world applications. Classical decision theory focuses on only a small part of the decision process—making the choice. There are deep issues about the adequacy of quantitative formalisms to represent the kinds of knowledge and forms of reasoning that are routinely employed in medical thinking. This chapter presents an alternative framework that is formally sound but avoids the shortcomings of standard quantitative decision procedures.}
}
@article{LUO2020151,
title = {Three-way decision with incomplete information based on similarity and satisfiability},
journal = {International Journal of Approximate Reasoning},
volume = {120},
pages = {151-183},
year = {2020},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2020.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X19303421},
author = {Junfang Luo and Mengjun Hu and Keyun Qin},
keywords = {Three-way decision, Rough set, Incomplete information, Similarity, Satisfiability, Fuzzy logic},
abstract = {Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a brief review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using α-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using α-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions.}
}
@article{2025335,
title = {In This Issue},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {10},
number = {4},
pages = {335-336},
year = {2025},
note = {Cognitive Neuroscience of Mindfulness},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2025.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S2451902225000692}
}
@article{GATI2021298,
title = {Differentially private data fusion and deep learning Framework for Cyber–Physical–Social Systems: State-of-the-art and perspectives},
journal = {Information Fusion},
volume = {76},
pages = {298-314},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000890},
author = {Nicholaus J. Gati and Laurence T. Yang and Jun Feng and Xin Nie and Zhian Ren and Samwel K. Tarus},
keywords = {Differential privacy, Deep computation, Data fusion, CPSS},
abstract = {The modern technological advancement influences the growth of the cyber–physical system and cyber–social system to a more advanced computing system cyber–physical–social system (CPSS). Therefore, CPSS leads the data science revolution by promoting tri-space information resource from a single space. The establishment of CPSSs increases the related privacy concerns. To provide privacy on CPSSs data, various privacy-preserving schemes have been introduced in the recent past. However, technological advancement in CPSSs requires the modifications of previous techniques to suit its dynamics. Meanwhile, differential privacy has emerged as an effective method to safeguard CPSSs data privacy. To completely comprehend the state-of-the-art developments and learn the field’s research directions, this article provides a comprehensive review of differentially private data fusion and deep learning in CPSSs. Additionally, we present a novel differentially private data fusion and deep learning Framework for Cyber–Physical–Social Systems , and various future research directions for CPSSs.}
}
@article{CAMELODAZA2024101760,
title = {Parameter estimation in single-phase transformers via the generalized normal distribution optimizer while considering voltage and current measurements},
journal = {Results in Engineering},
volume = {21},
pages = {101760},
year = {2024},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2024.101760},
url = {https://www.sciencedirect.com/science/article/pii/S2590123024000136},
author = {Juan David Camelo-Daza and Diego Noel Betancourt-Alonso and Oscar Danilo Montoya and Ernesto Gómez-Vargas},
keywords = {Nonlinear optimization, Metaheuristic optimization algorithms, Generalized normal distribution optimizer, Parameter estimation, Single-phase transformers, Mean square error minimization, Voltage and current measurements},
abstract = {This research addresses, from a perspective of metaheuristic optimization, the problem regarding parametric estimation in single-phase transformers while considering voltage and current measures at the terminals of the transformer and weighing linear loads. Transformer parametric estimation is modeled as a nonlinear problem in order to minimize the mean square error between the calculated voltage and current variables and the measurements taken. The nonlinearities are associated with Kirchhoff's first and second laws applied to the equivalent electrical circuit of the single-phase transformer. The nonlinear optimization problem is solved by applying a metaheuristic optimization algorithm known as the generalized normal distribution optimizer (GNDO), which uses evolution rules that allow exploring and exploiting the solution space via the classical probability function based on normal distributions. Numerical results in three test transformers of 20, 45, and 112.5 kVA demonstrate the effectiveness and robustness of the proposed GNDO approach when compared to other optimizers reported in the literature, such as the crow search algorithm, the coyote optimization algorithm, and the exact solution of the nonlinear optimization model using the fmincon solver of the MATLAB software. All numerical simulations confirm the potential of the GNDO approach to deal with complex optimization problems in engineering and science with promising results and low computational effort.}
}
@article{SMITH2017274,
title = {The hierarchical basis of neurovisceral integration},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {75},
pages = {274-296},
year = {2017},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2017.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S014976341630673X},
author = {Ryan Smith and Julian F. Thayer and Sahib S. Khalsa and Richard D. Lane},
keywords = {Neurovisceral integration, Cardiac vagal control, Heart rate variability, Interoception, Emotion, Predictive coding, Cognitive control},
abstract = {The neurovisceral integration (NVI) model was originally proposed to account for observed relationships between peripheral physiology, cognitive performance, and emotional/physical health. This model has also garnered a considerable amount of empirical support, largely from studies examining cardiac vagal control. However, recent advances in functional neuroanatomy, and in computational neuroscience, have yet to be incorporated into the NVI model. Here we present an updated/expanded version of the NVI model that incorporates these advances. Based on a review of studies of structural/functional anatomy, we first describe an eight-level hierarchy of nervous system structures, and the contribution that each level plausibly makes to vagal control. Second, we review recent work on a class of computational models of brain function known as “predictive coding” models. We illustrate how the computational dynamics of these models, when implemented within our proposed vagal control hierarchy, can increase understanding of the relationship between vagal control and both cognitive performance and emotional/physical health. We conclude by discussing novel implications of this updated NVI model for future research.}
}
@article{REINHOLD2025122653,
title = {Perspectives: The license to fail – Steps towards an adaptive paradigm for forest management in times of unprecedented uncertainty},
journal = {Forest Ecology and Management},
volume = {585},
pages = {122653},
year = {2025},
issn = {0378-1127},
doi = {https://doi.org/10.1016/j.foreco.2025.122653},
url = {https://www.sciencedirect.com/science/article/pii/S0378112725001616},
author = {Simon Reinhold and Olef Koch and Andreas Schweiger and Roderich {von Detten}},
keywords = {Forest management, Uncertainty, Resilience, Adaptive management, Experimental framework},
abstract = {This article offers a new perspective on possible consequences of looking at uncertainty in forest ecosystem management as something irreducible. Management strategies suggested by forest science are often focused on minimizing uncertainty by using predictive or probabilistic models and risk management via diversification in order to enable the achievement of relatively rigid targets or desired ecosystem conditions. These approaches, however, do not fully capture the complex, dynamic nature of forest ecosystems and the challenges ecosystem managers face as societal needs and the world’s climate are changing in an unprecedented manner. The fact that a good proportion of future events is unpredictable and that forest ecosystem management has to find ways to act upon this uncertainty precisely because it is irreducible is often overlooked. The conceptual background for this publication was drawn from considerations on uncertainty and entrepreneurial action in both forest sciences and economics. Building on that, we argue that the understanding of forests as complex and adaptive socio-ecological systems has to be better represented in systematic and hierarchical structures. Central to sector-wide innovation and adaptation is a new management paradigm: A better understanding of ecosystem dynamics in real-time and a substitution of approaches that are designed to plan or steer ecosystems with experimental proceedings via trial and error. Therefore, we propose a theoretical workflow that bolsters the decisions of individual forest managers with the results of a landscape-wide experimental network. By promoting a culture of “not-knowing”, bottom-up exchange between hierarchical levels and continuous learning and experimentation, institutions can encourage individual forest managers to learn from their own management process and thus improve the way they navigate the complexities of ecosystem management in an uncertain future.}
}
@article{PESTI20132487,
title = {Symposium: Experimental design for poultry production and genomics research1 1Presented as part of the Experimental Design for Poultry Production and Genomics Research Symposium at the Poultry Science Association's annual meeting in Athens, Georgia, July 12, 2012.},
journal = {Poultry Science},
volume = {92},
number = {9},
pages = {2487-2489},
year = {2013},
issn = {0032-5791},
doi = {https://doi.org/10.3382/ps.2012-02733},
url = {https://www.sciencedirect.com/science/article/pii/S0032579119394623},
author = {Gene M. Pesti and Samuel E. Aggrey and Bryan I. Fancher},
keywords = {statistics, biometrics, experimental design, genomics},
abstract = {This symposium dealt with the theoretical and practical aspects of choosing and evaluating experimental designs, and how experimental results may be related to poultry production through modeling. Additionally, recent advances in techniques for generating high-throughput genomic sequencing data, genomic breeding values, genomics selection, and genome-wide association studies have provided unique computational challenges to the poultry industry. Such challenges were presented and discussed.}
}
@article{CHIRIMUUTA201934,
title = {Synthesis of contraries: Hughlings Jackson on sensory-motor representation in the brain},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {75},
pages = {34-44},
year = {2019},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1369848618300359},
author = {M. Chirimuuta},
abstract = {This paper examines the concept of representation in the brain which occurs in the writings of the neurologist John Hughlings Jackson (1835–1911). Jackson was immersed in Victorian physiological psychology, a hybrid of British associationism and a reflex theory of the operation of the nervous system. Furthermore, Jackson was deeply influenced by Herbert Spencer, and I argue that Spencer's progressivist evolutionary ideas are in tension with the more mechanistic approach of the reflex theory. I also discuss Jackson's legacy in the 20th century and the longstanding debate about localisation of function in the brain.}
}
@incollection{SAAVEDRAALAMILLAS2025623,
title = {Library Instruction and Research Training in the Context of Artificial Intelligence},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {623-629},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00122-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395689500122X},
author = {César Saavedra-Alamillas and Josmel Pacheco-Mendoza and Erik M. Ortiz-Díaz and Youness {El Hamzaoui} and Marc A. Astbury},
keywords = {Academic production., Artificial intelligence, Embedded librarian, Information literacy, Liaison librarian, Librarian, Library instruction, Research, Researcher training, Scientific communication},
abstract = {The librarian has played a crucial role throughout history, evolving from a guardian of humanity׳s collective memory to guiding the use of collections and, in recent years, a trainer in the use of information and emerging digital technologies. Currently, the librarian is an active collaborator who understands scientific communication processes and the mechanisms for improving high-impact academic production.}
}
@article{WU2024103772,
title = {Fuser: An enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103772},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103772},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001328},
author = {Fan Wu and Bin Gao and Xiaoou Pan and Linlin Li and Yujiao Ma and Shutian Liu and Zhengjun Liu},
keywords = {Hateful memes detection, Multimodal fusion, Congruent reinforced perceptron, Main semantic, Auxiliary context},
abstract = {As a multimodal form of hate speech on social media, hateful memes are more aggressive and cryptic threats to the real life of humans. Automatic detection of hateful memes is crucial, but the images and texts in most memes are only weakly consistent or even irrelevant. Although existing works have achieved the initial goal of detecting hateful memes with pre-trained models, they are limited to monolithic inference methods while ignoring the semantic differences between multimodal representations. To strengthen the comprehension and reasoning of the hidden meaning behind the memes by combining real-world knowledge, we propose an enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection. Inspired by the human cognitive mechanism, we first divide the extracted multisource representations into main semantics and auxiliary contexts based on their strength and relevance, and then precode them into lightly correlated embeddings with unified spatial dimensions via a novel prefix uniform layer, respectively. To jointly learn the intrinsic correlation between primary and secondary semantics, a congruent reinforced perceptron with brain-like perceptual integration is designed to seamlessly fuse multimodal representations in a shared latent space while maintaining the feature integrity in the sub-fusion space, thereby implicitly reasoning about the subtle metaphors behind the memes. Extensive experiments on four benchmark datasets fully demonstrate the effectiveness and superiority of our architecture compared with previous state-of-the-art methods.}
}
@article{GEFEN2016828,
title = {Cytoskeleton and plasma-membrane damage resulting from exposure to sustained deformations: A review of the mechanobiology of chronic wounds},
journal = {Medical Engineering & Physics},
volume = {38},
number = {9},
pages = {828-833},
year = {2016},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2016.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S1350453316301114},
author = {Amit Gefen and Daphne Weihs},
keywords = {Chronic wounds, Mechanical loading, Sustained deformation, Cell damage},
abstract = {The purpose of this review paper is to summarize the current knowledge on cell-scale mechanically-inflicted deformation-damage, which is at the frontier of cell mechanobiology and biomechanics science, specifically in the context of chronic wounds. The dynamics of the mechanostructure of cells and particularly, the damage occurring to the cytoskeleton and plasma-membrane when cells are chronically deformed (as in a weight-bearing static posture) is correlated to formation of the most common chronic wounds and injuries, such as pressure ulcers (injuries). The first occurrence is microscopic injury which onsets as damage in individual cells and then progresses macroscopically to the tissue-scale. Here, we specifically focus on sub-catastrophic and catastrophic damage to cells that can result from mechanical loads that are delivered statically or at physiological rates; this results in apoptosis at prolonged times or necrosis, rapidly. We start by providing a basic background of cell mechanics and dynamics, focusing on the plasma-membrane and the cytoskeleton, and discuss approaches to apply and estimate deformations in cells. We then consider the effects of different levels of mechanical loads, i.e. low, high and intermediate, and describe the expected damage in terms of time-scales of application and in terms of cell response, providing experimental examples where available. Finally, we review different theoretical and computational modeling approaches that have been used to describe cell responses to sustained deformation. We highlight the insights that those models provide to explain, for example, experimentally observed variabilities in cell damage and death under loading.}
}
@article{MORAVEC2023147,
title = {Global trends in disruptive technological change: social and policy implications for education},
journal = {On the Horizon},
volume = {31},
number = {34},
pages = {147-173},
year = {2023},
issn = {1074-8121},
doi = {https://doi.org/10.1108/OTH-02-2023-0007},
url = {https://www.sciencedirect.com/science/article/pii/S1074812123000325},
author = {John W. Moravec and María Cristina Martínez-Bravo},
keywords = {Literature review, Meta-analysis, Technology, Education policy, Disruptive technology, Global trends},
abstract = {Purpose
The purpose of this study is to identify global trends in disruptive technological change and map the social and policy implications, particularly as they relate to the educational ecosystem and main stakeholders across all levels of education.
Design/methodology/approach
The authors conducted a two-stage meta-analysis of 1,155 scholarly, peer-reviewed articles. The investigation involves a systematized literature review for data identification and collation adhering to defined selection criteria, and a network analysis to scrutinize data, consolidate information and unveil correlations and patterns from the literature review to produce a set of recommendations.
Findings
The study unveiled educational trends related to disruptive technologies and delineated four principal clusters representing how these technologies are transforming the education ecosystem. Additionally, a series of transversal aspects that reveal a societal vulnerability toward future prospects in the realms of ethics, sustainability, resilience, security, and policy were identified.
Practical implications
The findings spotlight an enlarging chasm between industry (and society at large) and conventional education, where many transformations triggered by disruptive technologies remain absent from teaching and learning systems. The study further offers recommendations and envisions potential scenarios, urging stakeholders to respond based on their positions concerning disruptive technologies.
Originality/value
Expanding from the meta-analysis of pertinent literature, this paper offers four collections of curated resources, four mini case studies and four scenarios for policymakers and local communities to consider, enabling them to plot courses for their optimal futures.}
}
@article{DOERR2000431,
title = {How Can I Find a Pattern in this Random Data?: The Convergence of Multiplicative and Probabilistic Reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {4},
pages = {431-454},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00023-7},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000237},
author = {Helen M Doerr},
abstract = {This classroom-based research study examines the thinking of pre-calculus students about multiplicative growth and decay within a probabilistic context, thus bringing together two research strands in mathematics education: students' understanding of exponential functions and students' reasoning about random events. Using a multi-stage approach to model development, a curriculum unit was designed to elicit students' creation of a model or system that could be used to describe and explain the behavior of an experienced, probabilistic system. The evidence suggests that while the students made sense of the underlying multiplicative structure of the problem situation, many students experienced a conflict between the concept of a pattern and the concept of randomness. Students encountered difficulty in reconciling the deterministic nature of a closed-form analytic solution with the non-deterministic nature of a sequence of random events. These results suggest that there is a need for students to gain experience with non-deterministic models using contexts that provide meaningful empirical data.}
}
@article{GRAZZINI201726,
title = {Bayesian estimation of agent-based models},
journal = {Journal of Economic Dynamics and Control},
volume = {77},
pages = {26-47},
year = {2017},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2017.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0165188917300222},
author = {Jakob Grazzini and Matteo G. Richiardi and Mike Tsionas},
keywords = {Agent-based, Estimation, Bayes, Approximate Bayesian computation, Likelihood},
abstract = {We consider Bayesian inference techniques for agent-based (AB) models, as an alternative to simulated minimum distance (SMD). Three computationally heavy steps are involved: (i) simulating the model, (ii) estimating the likelihood and (iii) sampling from the posterior distribution of the parameters. Computational complexity of AB models implies that efficient techniques have to be used with respect to points (ii) and (iii), possibly involving approximations. We first discuss non-parametric (kernel density) estimation of the likelihood, coupled with Markov chain Monte Carlo sampling schemes. We then turn to parametric approximations of the likelihood, which can be derived by observing the distribution of the simulation outcomes around the statistical equilibria, or by assuming a specific form for the distribution of external deviations in the data. Finally, we introduce Approximate Bayesian Computation techniques for likelihood-free estimation. These allow embedding SMD methods in a Bayesian framework, and are particularly suited when robust estimation is needed. These techniques are first tested in a simple price discovery model with one parameter, and then employed to estimate the behavioural macroeconomic model of De Grauwe (2012), with nine unknown parameters.}
}
@article{HOOD2012181,
title = {Systems Approaches to Biology and Disease Enable Translational Systems Medicine},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {10},
number = {4},
pages = {181-185},
year = {2012},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2012.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1672022912000526},
author = {Leroy Hood and Qiang Tian},
keywords = {Systems biology, P4 medicine, Family genome sequencing, Targeted proteomics, Single-cell analysis},
abstract = {The development and application of systems strategies to biology and disease are transforming medical research and clinical practice in an unprecedented rate. In the foreseeable future, clinicians, medical researchers, and ultimately the consumers and patients will be increasingly equipped with a deluge of personal health information, e.g., whole genome sequences, molecular profiling of diseased tissues, and periodic multi-analyte blood testing of biomarker panels for disease and wellness. The convergence of these practices will enable accurate prediction of disease susceptibility and early diagnosis for actionable preventive schema and personalized treatment regimes tailored to each individual. It will also entail proactive participation from all major stakeholders in the health care system. We are at the dawn of predictive, preventive, personalized, and participatory (P4) medicine, the fully implementation of which requires marrying basic and clinical researches through advanced systems thinking and the employment of high-throughput technologies in genomics, proteomics, nanofluidics, single-cell analysis, and computation strategies in a highly-orchestrated discipline we termed translational systems medicine.}
}
@article{SOUSA2014569,
title = {Sociomaterial Enactment Drive of Business/IT Alignment: From Small Data to Big Impact},
journal = {Procedia Technology},
volume = {16},
pages = {569-582},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314002321},
author = {José L.R. Sousa and Ricardo J. Machado},
keywords = {Business / IT alignment, complex-networks, profiling framework, information systems;},
abstract = {Business/IT alignment is an information systems research field with a long existence and a high number of researchers and represents a central direction on the thinking about the relation between business and the information systems. It aims to achieve a paradigm, one in which there is a high degree of visibility and availability of information, about the information systems sociomateriality. Complex-networks constitute an approach to the study of emergent properties of complex systems that strongly focuses and relies on models and measures, through which build the system interdependence. Several contributions of complex-networks are: topology always affects the function; separated from the domain; quantification of element's relationships; visibility and capture of emergent properties. This work expects to contribute for the appropriate use of complex-networks models and measure in the drive of the information systems alignment. This work considers an exploratory case research strategy. It uses an exploratory case developed in the field of information systems that directed its deployment to sustain the business development and evolution. This paper illustrates a profiling framework that introduces a global and elementary use of the sociomaterial enactment. From the analysis of the exploratory case, the paper infers drivers of the information systems Business/IT alignment.}
}
@article{RAMESH2021375,
title = {Activation energy process in hybrid CNTs and induced magnetic slip flow with heat source/sink},
journal = {Chinese Journal of Physics},
volume = {73},
pages = {375-390},
year = {2021},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2021.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0577907321001696},
author = {G.K. Ramesh and J.K. Madhukesh},
keywords = {Carbon nanotubes, Slip flow, Induce magnetic flux, Activation energy, Chemical reaction},
abstract = {Effect of induced magnetic field is critical as a result of much controlled and focused on liquid flow is wanted in numerous modern and clinical procedures for example electromagnetic casting, drug delivery and cooling of nuclear reactors. Hence this investigation explains the behaviour of hybrid carbon nanotubes (CNTs) flow through slipped surface with induced magnetic field. Accumulation of SWCNTs (single wall) and MWCNTs (multi wall) nanomaterial with water base liquid is considered. Thermal performance is analyzed with regular heat source/sink effect. Chemical reaction and activation energy impacts are incorporated in mass equation. Solution of the similarity equations are obtained by adopting RKF45 method. Influence of flow variables are illustrated through graphs and computational values of drag force, Nusselt number and Sherwood number are presented in tables. It is noted that activation energy enhance the concentration field whereas opposite behaviour for reaction rate. Also induce magnetic field boosted with the larger values of magnetic Prandtl number. Furthermore it is observed that hybrid CNTs nanomaterial having higher rate of heating/cooling compare to singular CNTs nanomaterial.}
}
@article{JU2017180,
title = {Single image haze removal based on the improved atmospheric scattering model},
journal = {Neurocomputing},
volume = {260},
pages = {180-191},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217307051},
author = {Mingye Ju and Zhenfei Gu and Dengyin Zhang},
keywords = {Improved atmospheric scattering model, Linear model, Gaussian–Laplacian pyramid, Image haze removal, Haze aware density feature},
abstract = {In this paper, we propose an improved atmospheric scattering model (IASM) to overcome the inherent limitation of the traditional atmospheric scattering model. Based on the IASM, a fast single image dehazing algorithm is also presented. In this algorithm, by constructing a linear model between the transmission and the haze aware density feature, the transmission map can be directly estimated through a linear operation on three components: luminance, saturation and gradient. Combining the sky-relevant feature and the proposed guided energy model (GEM), we can accurately estimate the atmospheric light and scene incident light, and can further restore the scene albedo via the IASM. Finally, an accelerating framework (AF) based on the Gaussian–Laplacian pyramid is proposed to increase the computational speed. Experimental results demonstrate that the proposed algorithm outperforms most of the prevalent algorithms in terms of visual effect and computational efficiency. Besides, it is also capable of processing various types of degraded images in addition to hazy images.}
}
@article{SUE20219,
title = {Generative design in factory layout planning},
journal = {Procedia CIRP},
volume = {99},
pages = {9-14},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121002584},
author = {Marian Süße and Matthias Putz},
keywords = {Generative design, Computational design, Evolutionary design, Factory planning, Layout planning, Literature search},
abstract = {Planning and optimization of facility layouts have been investigated for decades and manifold approaches are applied for structuring and design of production layouts. However, results heavily depend on the experience and creativity of involved planning experts. Currently, complexity of planning processes constantly increases, e.g. due to further requirements of energy and media supply. Generative Design, hitherto mainly applied in component development, provides opportunities to cope with a much larger solution space and develop creative layout concepts. Thus, based on a structured overview on established planning methods, a concept and first results of factory layout planning with Generative Design are described.}
}
@article{ANTONIDES2022101010,
title = {A learning trajectory for enumerating permutations: Applying and elaborating a theory of levels of abstraction},
journal = {The Journal of Mathematical Behavior},
volume = {68},
pages = {101010},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.101010},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000785},
author = {Joseph Antonides and Michael T. Battista},
keywords = {Permutations, Learning trajectories, Abstraction, Combinatorics, Teaching experiment, Concreteness fading},
abstract = {Permutations are fundamental to combinatorics and other areas of mathematics, and it is important that students develop efficient and conceptually supported ways of mentally constructing, listing, and enumerating them. To date, there is still much to learn about how students reason about enumerating permutations, and how instruction can support students’ conceptual development. We address this gap in the research literature by carefully tracing the evolution of two preservice middle school teachers’ permutation enumeration strategies and conceptualizations, which led to the formulation of levels of sophistication for combinatorial reasoning. These levels are explained by applying and extending a constructivist theory of levels of abstraction. Additionally, we outline an instructional approach that was instrumental in facilitating student learning. Together, the proposed levels and linked instructional approach constitute an initial learning trajectory for permutations that we believe could be useful for understanding and supporting post-secondary non-STEM students’ meaningful conceptualizations and enumerations of permutations.}
}
@article{ZHU2024106594,
title = {Distilling mathematical reasoning capabilities into Small Language Models},
journal = {Neural Networks},
volume = {179},
pages = {106594},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106594},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024005185},
author = {Xunyu Zhu and Jian Li and Yong Liu and Can Ma and Weiping Wang},
keywords = {Large language models, Knowledge Distillation, Mathematical reasoning, Chain-of-Thought, Program-of-Thought},
abstract = {This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental performance demonstrates that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.}
}
@article{PRATAMA2023338,
title = {WizardOfMath: A top-down puzzle game with RPG elements to hone the player's arithmetic skills},
journal = {Procedia Computer Science},
volume = {216},
pages = {338-345},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.144},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022220},
author = {Mulia Pratama and Yanfi Yanfi and Pualam Dipa Nusantara},
keywords = {Game, math, Game Development Life Cycle, Game Experience Questionnaire},
abstract = {As one of the important education subjects’ mathematics difficulties can lead to tension and be described as the most hated or feared subject. This study aims to create a puzzle game application with RPG elements called WizardOfMath to increase a user's interest in mathematics subject. The research method includes a method called Game Development Life Cycle (GDLC), which has a pre-production stage that is suitable for game development rather than the Waterfall method. A game application is built based on the prior requirement gathering. The evaluation was arranged using the Game Experience Questionnaire (GEQ) survey which is performed by providing an online form to the public. Reliability test of GEQ modules meets Cronbach's Alpha value above 0.7 and the validity test of the r table is greater than 0.05. The results calculation of the Game Experience Questionnaire (GEQ) from a total of 55 participants and 3 modular structures, which are the Core module, In-game Module, and Post-game Module obtained an average score of 4.06, 3.88, and 3.57 for positive aspects and 2,72, 2.67, and 2,61 for the negative aspect. The contribution of this study shows this puzzle game application with RPG elements decreased user tension and the negative effect of being involved with mathematics subjects.}
}
@article{ROUGIER2009155,
title = {Implicit and explicit representations},
journal = {Neural Networks},
volume = {22},
number = {2},
pages = {155-160},
year = {2009},
note = {What it Means to Communicate},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2009.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0893608009000112},
author = {Nicolas P. Rougier},
keywords = {Computational neuroscience, Representation, Symbol, Embodied cognition},
abstract = {During the past decades, the symbol grounding problem, as has been identified by Harnard [Harnard, S. (1990). The symbol grounding problem. Physica D: Nonlinear Phenomena, 42, 335–346], became a prominent problem in the cognitive science society. The idea that a symbol is much more than a mere meaningless token that can be processed through some algorithm, sheds new light on higher brain functions such as language and cognition. We present in this article a computational framework that may help in our understanding of the nature of grounded representations. Two models are briefly introduced that aim at emphasizing the difference we make between implicit and explicit representations.}
}
@article{MEMARIAN2023100022,
title = {ChatGPT in education: Methods, potentials, and limitations},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {1},
number = {2},
pages = {100022},
year = {2023},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2023.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2949882123000221},
author = {Bahar Memarian and Tenzin Doleck},
keywords = {ChatGPT, Large language models, Education, Artificial intelligence, Machine learning, Data science, Pedagogy},
abstract = {ChatGPT has been under the scrutiny of public opinion including in education. Yet, less work has been done to analyze studies conducted on ChatGPT in educational contexts. This review paper examines where ChatGPT is employed in educational literature and areas of potential, challenges, and future work. A total of 63 publications were included in this review using the general framework of open and axial coding. We coded and summarized the methods, and reported potentials, limitations, and future work of each study. Thematic analysis of reviewed studies revealed that most extant studies in the education literature explore ChatGPT through a commentary and non-empirical lens. The potentials of ChatGPT include but are not limited to the development of personalized and complex learning, specific teaching and learning activities, assessments, asynchronous communication, feedback, accuracy in research, personas, and task delegation and cognitive offload. Several areas of challenge that ChatGPT is or will be facing in education are also shared. Examples include but are not limited to plagiarism deception, misuse or lack of learning, accountability, and privacy. There are both concerns and optimism about the use of ChatGPT in education, yet the most pressing need is to ensure student learning and academic integrity are not sacrificed. Our review provides a summary of studies conducted on ChatGPT in education literature. We further provide a comprehensive and unique discussion on future considerations for ChatGPT in education.}
}
@article{ISMAEEL2019599,
title = {Drawing the operating mechanisms of green building rating systems},
journal = {Journal of Cleaner Production},
volume = {213},
pages = {599-609},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.12.115},
url = {https://www.sciencedirect.com/science/article/pii/S095965261833823X},
author = {Walaa S.E. Ismaeel},
keywords = {Green certification, Greenmarket, LEED, Performance measurement and verification, Sustainable building guidelines, Green building rating systems},
abstract = {Abstract:
Green Building Rating and Certification systems (GBRSs) were developed to provide guidelines and benchmarking criteria for conducting sustainable building processes. Yet, they lack a ‘know how’ defining their role and value-contribution, which may eventually limit their role and affect their credibility and application in the decision-making process. Subsequently, this study presents its research hypothesis assuming two mechanisms and four scopes of operation; the rating mechanism operates using the guidelines and measurement metrics while the certification mechanism operates using the verification and certification metrics. The research has adopted an integrated qualitative and quantitative approach where the development of the four interrelated scopes of operation has been traced through literature, and an assigned score weighting has been used to compare them according to different GBRSs- and more specifically for the Leadership in Energy and Environmental Design (LEED) credits. The results present an integrated application framework (IAF) with a particular focus to energy and materials' credits and based on the system's targets as well as credits' intent and interrelations. The proposed framework applies system thinking to the level of individual practices as well as the entire building process. This is followed by two case-study validations and amendments to reflect dominance, temporal precedence and iterative action of some scopes along different project phases. It indicates how the difference in building type and context may alter opportunities for scoring potentials in addition to means of supporting important decisions such as setting building reuse and CWM plans as well as specifying and procuring sustainable materials. The result provides a consistent mean to manage and document building activities and finally report buildings' performance. This shall prove very useful for researchers, practitioners and system developers. Finally, the study provides insights for developing the LEED system as well as different GBRSs using the IAF; this may take the form of a more interactive decision support tool, software management application or a better user friendly system interface.}
}
@article{LEAVY2011235,
title = {Elementary and middle grade students’ constructions of typicality},
journal = {The Journal of Mathematical Behavior},
volume = {30},
number = {3},
pages = {235-254},
year = {2011},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312311000174},
author = {Aisling M. Leavy and James A. Middleton},
keywords = {Statistical reasoning, Mathematical thinking, Elementary students, Middle grade students, Typicality, Data and statistics},
abstract = {This study addresses the measures chosen by students when selecting or constructing indices to properties of distributions of data. A series of individual teaching experiments were conducted to provide insight into the development of five 4th to 8th grade students’ conceptualizations of distribution over the course of 8 weeks of instruction. During the course of the teaching experiment (emergent) statistical tasks and analogous teacher activities were created and refined in an effort to support the development of understanding. In the process of development, attempts were made by students to coordinate center and variability when constructing measures to index properties of distributions. The results indicate that consideration of representativeness was a major factor that motivated modification of approaches to constructing indices of distributions, and subsequent coordination of indices of variation and center. In particular, the defining features of student's self-constructed “typical” values and notions of spread were examined, resulting in a model of development constituting eight “categories” ranging from the construction of values that did not reflect properties of the data (Category 1) to measures employing conceptual use of the mean in combination with other indices of center and spread (Category 8).}
}
@article{SIYAL20252637,
title = {Adaptive Attribute-Based Honey Encryption: A Novel Solution for Cloud Data Security},
journal = {Computers, Materials and Continua},
volume = {82},
number = {2},
pages = {2637-2664},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.058717},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825001614},
author = {Reshma Siyal and Muhammad Asim and Long Jun and Mohammed Elaffendi and Sundas Iftikhar and Rana Alnashwan and Samia Allaoua Chelloug},
keywords = {Cybersecurity, data security, cloud storage, hadoop encryption and decryption, privacy protection, attribute-based honey encryption},
abstract = {A basic procedure for transforming readable data into encoded forms is encryption, which ensures security when the right decryption keys are used. Hadoop is susceptible to possible cyber-attacks because it lacks built-in security measures, even though it can effectively handle and store enormous datasets using the Hadoop Distributed File System (HDFS). The increasing number of data breaches emphasizes how urgently creative encryption techniques are needed in cloud-based big data settings. This paper presents Adaptive Attribute-Based Honey Encryption (AABHE), a state-of-the-art technique that combines honey encryption with Ciphertext-Policy Attribute-Based Encryption (CP-ABE) to provide improved data security. Even if intercepted, AABHE makes sure that sensitive data cannot be accessed by unauthorized parties. With a focus on protecting huge files in HDFS, the suggested approach achieves 98% security robustness and 95% encryption efficiency, outperforming other encryption methods including Ciphertext-Policy Attribute-Based Encryption (CP-ABE), Key-Policy Attribute-Based Encryption (KB-ABE), and Advanced Encryption Standard combined with Attribute-Based Encryption (AES+ABE). By fixing Hadoop’s security flaws, AABHE fortifies its protections against data breaches and enhances Hadoop’s dependability as a platform for processing and storing massive amounts of data.}
}
@incollection{SHEN20231,
title = {Interdisciplinary science learning},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-9},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.13030-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305130301},
author = {Ji Shen and Changzhao Wang},
keywords = {Cross-disciplinary learning, Integrated learning, Interdisciplinary science learning, Interdisciplinary understanding, Interdisciplinary practices, Knowledge integration, Multidisciplinary learning, STEM education, STEAM education},
abstract = {This article presents a conceptual review of studies and programs related to interdisciplinary science learning in different boundary-crossing scenarios including within sciences, across STEM, and with non-STEM fields. Specific examples are also included to illuminate the four core interdisciplinary practices, namely, translation, transfer, integration, and transformation, that cut across these interdisciplinary learning contexts. The article also discusses challenges for interdisciplinary science learning and strategies proposed to address these challenges. More empirical studies are called to test the effectiveness of these strategies to facilitate and assess interdisciplinary science learning in different domains and contexts.}
}
@article{GORCUN2025110793,
title = {Strategic tour operator selection in the tourism sector using a quantum picture fuzzy rough set-based multi-criteria decision-making approach},
journal = {Engineering Applications of Artificial Intelligence},
volume = {153},
pages = {110793},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110793},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625007936},
author = {Ömer Faruk Görçün and Dragan Pamucar and Hasan Dinçer and Serhat Yüksel and Ismail İyigün and Vladimir Simic},
keywords = {Strategic selection, Tour operators, Tourism industry, Quantum picture fuzzy rough sets, Decision-making trial and evaluation laboratory},
abstract = {Tour operator selection is critical for ensuring high-quality services, customer satisfaction, and sustainable tourism development. However, traditional decision-making methods often fail to address the complexities and uncertainties involved in this process. This study introduces a robust decision-making framework that integrates quantum picture fuzzy rough sets (QPFR) with advanced Multi-Criteria Decision-Making (MCDM) techniques to enhance the evaluation and selection of tour operators. The methodology incorporates QPFR, the Decision-Making Trial and Evaluation Laboratory (DEMATEL), and the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) to assess and rank seven prominent tour operators in the Turkish tourism sector. The evaluation is based on 16 comprehensive criteria: quality, safety, environmental impact, authenticity, and economic contribution. Expert inputs and artificial intelligence techniques were utilized to ensure the model's reliability and accuracy. The findings reveal that the proposed model effectively minimizes uncertainties, provides consistent rankings, and highlights the critical importance of specific criteria in decision-making. Sensitivity analysis confirms the robustness of the results, demonstrating the model's applicability to dynamic and complex decision-making contexts. This study offers theoretical contributions and practical insights for decision-makers, emphasizing the value of integrating advanced computational methods to support sustainable tourism development.}
}
@article{HAPPLE2017283,
title = {Effects of air infiltration modeling approaches in urban building energy demand forecasts},
journal = {Energy Procedia},
volume = {122},
pages = {283-288},
year = {2017},
note = {CISBAT 2017 International ConferenceFuture Buildings & Districts – Energy Efficiency from Nano to Urban Scale},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.07.323},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217329260},
author = {Gabriel Happle and Jimeno A. Fonseca and Arno Schlueter},
keywords = {Urban Building Energy Modeling, Air Infiltration, Ventilation, Heating Energy Demand, Cooling Energy Demand, Forecasting, City Energy Analyst (CEA)},
abstract = {The air infiltration rate is a highly sensitive variable that influences heating and cooling demand forecasts in urban building energy modeling. This paper analyses the effect of two different simplified modeling techniques of air infiltration - fixed air change rate vs. a model based on wind pressure and air temperatures - on the heating and cooling demand in a district. The urban energy simulation toolbox City Energy Analyst (CEA) is used to simulate a case study in Switzerland, comprising of 24 buildings of various functions. Results indicate that despite the large differences for individual buildings, a fixed infiltration rate model could be sufficient for early design studies of district energy systems, as the impact on the sizing of district energy systems remains relatively low. This comparison will contribute to the continued development of urban energy simulations that are robust, as well as computationally fast.}
}
@article{SINGH2023103044,
title = {A survey of mobility-aware Multi-access Edge Computing: Challenges, use cases and future directions},
journal = {Ad Hoc Networks},
volume = {140},
pages = {103044},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.103044},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522002165},
author = {Ramesh Singh and Radhika Sukapuram and Suchetana Chakraborty},
keywords = {Mobility, Multi-access Edge Computing, Task offloading, Service migration, Content caching, Resource allocation},
abstract = {Many mobile and pervasive applications avail cloud services to reduce overheads in on-device computation. The performance of these services depends on the available bandwidth of the underlying network, the physical proximity of the cloud server and the end devices, the volume of data, the computational capacity of the server, and, importantly, the mobility of the devices hosting the applications. Edge computing promises to provide better performance by bringing services (e.g., a video streaming service) from the cloud to servers near the user. It also enables partial or full offloading of the computation (tasks) and storage functionalities from the User Equipment (UE) to the edge of the network. This saves power and benefits from relatively more powerful devices at the edge. Multi-access Edge Computing (MEC), which supports wireless and wired access technologies, has gained significant research interest. When UEs move, services must continue to operate, tasks may need to be offloaded again, and states related to tasks and services may need to be migrated. In this paper, we focus on four functional components (task/service offloading, resource allocation, content/task caching, and service/task migration) of MEC. We survey the challenges to these and their solutions in the context of UE mobility. Mobility creates challenges during offloading resource-intensive tasks as the user may move while the task is being offloaded. Some of the other challenges are how to jointly allocate computing and communication resources, minimize service down time during migration, and share the backhaul network if the same MEC host must continue to be used. Some key research areas include intelligent task offloading and service migration algorithms, exploiting group mobility to improve task migration time, studying the interplay of MEC parameters such as capabilities of the target MEC host, etc. In addition, predicting the mobile trajectory through intelligent methods and implementations with datasets from real-world scenarios are required. We compare this paper on 11 parameters (service migration, task offloading, resource allocation, content caching, mobility, use cases, architecture, computing paradigm, mobility model, system model, virtualization/Software Defined Networks) with 31 other survey papers from 2018 to April 2022 in MEC and related domains. We discuss the Edge Computing paradigm, the system architecture and model descriptions, and use cases. We briefly explain the relevant challenges and future directions in emerging domains, such as the Internet of drones and Digital twins. We also discuss future research directions in task/service migration, offloading, resource management, distributed computing, reliability, and Quality of Service, all related to mobility in MEC.}
}
@article{WANG2023107152,
title = {scASGC: An adaptive simplified graph convolution model for clustering single-cell RNA-seq data},
journal = {Computers in Biology and Medicine},
volume = {163},
pages = {107152},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107152},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523006170},
author = {Shudong Wang and Yu Zhang and Yulin Zhang and Wenhao Wu and Lan Ye and YunYin Li and Jionglong Su and Shanchen Pang},
keywords = {ScRNA-seq, Clustering, Bioinformatics, Graph convolution, Computational biology, Machine learning},
abstract = {Single-cell RNA sequencing (scRNA-seq) is now a successful technique for identifying cellular heterogeneity, revealing novel cell subpopulations, and forecasting developmental trajectories. A crucial component of the processing of scRNA-seq data is the precise identification of cell subpopulations. Although many unsupervised clustering methods have been developed to cluster cell subpopulations, the performance of these methods is vulnerable to dropouts and high dimensionality. In addition, most existing methods are time-consuming and fail to adequately account for potential associations between cells. In the manuscript, we present an unsupervised clustering method based on an adaptive simplified graph convolution model called scASGC. The proposed method builds plausible cell graphs, aggregates neighbor information using a simplified graph convolution model, and adaptively determines the most optimal number of convolution layers for various graphs. Experiments on 12 public datasets show that scASGC outperforms both classical and state-of-the-art clustering methods. In addition, in a study of mouse intestinal muscle containing 15,983 cells, we identified distinct marker genes based on the clustering results of scASGC. The source code of scASGC is available at https://github.com/ZzzOctopus/scASGC.}
}
@article{DAMBROT2020110,
title = {Theoretical and hypothetical pathways to real-time neuromorphic AGI/post-AGI ecosystems},
journal = {Procedia Computer Science},
volume = {169},
pages = {110-122},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.122},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302453},
author = {S. Mason Dambrot},
keywords = {artificial general intelligence, counterfactual quantum entanglement, enplants, graphene, mediated artificial superintelligence, neural prosthetics, photonics, recurrent neural networks, spintronics, synthetic genomics, transdisciplinarity, transentity universal intelligence},
abstract = {While Homo sapiens is without doubt our planet’s most advanced species capable of imagining, creating and implementing tools, one of the many observable trends in evolution is the accelerating merger of biology and technology at increasing levels of scale. This is not surprising, given that our technology can be seen from a perspective in which the sensorimotor and, subsequently, prefrontal areas of our brain increasingly extending its motor (as did our evolutionary predecessors), perceptual, and—with computational advances, cognitive and memory capacities—into the exogenous environment. As such, this trajectory has taken us to a point in the above-mentioned merger at which the brain itself is beginning to meld with its physically expressed hardware and software counterparts—functionally at first, but increasingly structurally as well, initially by way of neural prostheses and brain-machine interfaces. Envisioning the extension of this trend, I propose theoretical technological pathways to a point at which humans and non-biological human counterparts may have the option to have identical neural substrates that—when integrated with Artificial General Intelligence (AGI), counterfactual quantum communications and computation, and AGI ecosystems—provide a global advance in shared knowledge and cognitive function while ameliorating current concerns associated with advanced AGI, as well as suggesting (and, if realized, accelerating) the far-future emergence of Transentity Universal Intelligence (TUI).}
}
@incollection{MAIVALI201555,
title = {Chapter 2 - The Basis of Knowledge: Causality and Truth},
editor = {Ülo Maiväli},
booktitle = {Interpreting Biomedical Science},
publisher = {Academic Press},
address = {Boston},
pages = {55-108},
year = {2015},
isbn = {978-0-12-418689-7},
doi = {https://doi.org/10.1016/B978-0-12-418689-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124186897000028},
author = {Ülo Maiväli},
keywords = {Popper, philosophy of science, causality, induction, deduction, correlation, concordance, regression analysis, AIC, Akaike},
abstract = {As a basis for scientific methodology, and to enable sharper thinking on the topic of scientific discovery, concepts like realism, empiricism, instrumentalism, operationalism, and pragmatism are explained. Emphasis is put throughout on truth and causality, which are followed from the historical depths (Hume and Kant) through twentieth-century developments (Popper and Fisher) into modern statistical theories. The connections of causality with observational (correlational) and experimental studies is discussed through integrating real-life examples with theoretical concepts. Statistical methods of correlation, concordance, and regression analysis are discussed with emphasis on their scientific uses and misuses, assumptions, and interpretations in the context of formulating causal theories. Model selection, Granger causality, and convergent cross mapping are briefly touched on. Scientific experiments, with their dependence on defined experimental systems, manipulations and controls, are put into the context of testing of causal hypotheses.}
}
@incollection{GORI2018122,
title = {Chapter 3 - Linear Threshold Machines},
editor = {Marco Gori},
booktitle = {Machine Learning},
publisher = {Morgan Kaufmann},
pages = {122-184},
year = {2018},
isbn = {978-0-08-100659-7},
doi = {https://doi.org/10.1016/B978-0-08-100659-7.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006597000038},
author = {Marco Gori},
keywords = {Linear machines, Least mean square, Linear-threshold machines, Normal equations, Ridge regression, Linear separability, Predicate order, Gradient descent, Perceptron algorithm, Terminal attractors},
abstract = {One of the simplest ways of modeling the interactions of intelligent agents with the environment is to expose them to a collection of supervised pairs (example, target). This chapter is about the learning mechanisms that arise from the assumption of dealing with linear and linear-threshold machines. In most cases, the covered topics nicely intercept different disciplines, and are of remarkable importance to better grasp many approaches to machine learning. The chapter covers classic topics, like normal equations and ridge regression, as well as representational issues in pattern recognition that are connected with the notion of predicate order. Linear-threshold machines are described along with related computational geometry issues, and the view that arises from Bayesian decision. Classic gradient learning algorithms, including the stochastic version, are described in the continuum setting, as well as the Rosenblatt perceptron algorithm. Finally, some complexity issues are covered in both the discrete and continuous setting of computation.}
}
@article{ZHIHUI2024108706,
title = {Temperature measurement at turbine outlet achieved by a sensing net and infrared thermometry method},
journal = {International Journal of Thermal Sciences},
volume = {196},
pages = {108706},
year = {2024},
issn = {1290-0729},
doi = {https://doi.org/10.1016/j.ijthermalsci.2023.108706},
url = {https://www.sciencedirect.com/science/article/pii/S1290072923005677},
author = {Wang Zhihui and Ma Chaochen and Ji Nian},
keywords = {infrared thermometry, Temperature sensing net (TSN), Turbine outlet temperature, Conjugate heat transfer (CHT), Turbine adiabatic efficiency},
abstract = {The measurement of turbine outlet temperature is challenging because of an intense swirl and high speed at this position. However, accurate measurement of the turbine outlet temperature is fundamental for characterizing the turbine performance. The paper proposed an infrared thermometry method based on the temperature sensing net (TSN) to measure the temperature distribution at the turbine outlet. First, this article describes the design and operation of the measurement procedure through infrared technology to accomplish this difficult task. Then, the temperature and velocity distribution at the turbine outlet and the adiabatic efficiency of the turbine are obtained using the CFD (Computational Fluid Dynamics) method to verify the feasibility of the proposed scheme. And the CHT (Conjugate Heat Transfer) simulation results for the TSN show that the incoming flow mass rate has a great influence on TSN temperature. In contrast, the influence of the incoming flow temperature gradient on it is almost negligible. Moreover, the fluid flow behavior and static temperature distribution around the temperature-sensing wire (TSW) at different Mach numbers are analyzed, and the heat transfer mechanism between the TSW and the fluid is revealed. The results show that the temperature of the TSN is lower than that of the incoming flow, but the distribution law is similar. The main factor affecting the temperature difference between the TSW and the fluid is the incoming flow velocity.}
}
@article{LI20112824,
title = {Human Action Recognition Based on Template Matching},
journal = {Procedia Engineering},
volume = {15},
pages = {2824-2830},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.532},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811020339},
author = {Chengyou Li and Tao Hua},
keywords = {computer vison ;key frame, ℜ transform, string matching},
abstract = {This paper presents a new method of human action recognition, which is based on ℜ transform and template matching after the key frame is extracted from a cycle. For a key binary human silhouette, ℜ transform is employed to represent low-level features. The advantage of the ℜ transform lies in its low computational complexity and geometric invariance. We utilize a novel string matching scheme based on edit distance is proposed to analyze different human actions. Compared with other methods, ours is superior because the descriptor is robust to frame loss in the video sequence, disjoint silhouettes and holes in the shape, and thus achieves better performance in similar activities recognition, simple representation, computational complexity and template generalization. Sufficient experiments have proved the efficiency.}
}
@article{MUDJAHIDIN2019968,
title = {Testing Methods on System Dynamics: A Model of Reliability, Average Reliability, and Demand of Service},
journal = {Procedia Computer Science},
volume = {161},
pages = {968-975},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319179},
author = { Mudjahidin and Rully Agus Hendrawan and Andre Parvian Aristio and Joko Lianto Buliali and Muhammad Nur Yuniarto},
keywords = {System dynamics, Testing method, structural testing, algorithms testing, behavioural testing},
abstract = {As a model used to simulate policies by creating scenarios, system dynamics must have similarities with real systems. Therefore, the system dynamics model should test so declare as the right model and representing the behaviour of a system. Thus, in this article, we propose three test methods to ensure the system dynamics model have appropriate structure, correct value according to the specified equation, and can use to establish the parameter of the model. We study articles to propose the testing methods (the structural testing, algorithms testing, and behavioural testing) and present the case study about reliability, average reliability, and its affected demands. In this article, we prove that the testing methods can be used to show the system dynamics model appropriates and represents the real system, all computation generated by the simulation output is proper to the specified equation and can use to choose the best parameter.}
}
@article{AVINERI2012512,
title = {On the use and potential of behavioural economics from the perspective of transport and climate change},
journal = {Journal of Transport Geography},
volume = {24},
pages = {512-521},
year = {2012},
note = {Special Section on Theoretical Perspectives on Climate Change Mitigation in Transport},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2012.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0966692312000646},
author = {Erel Avineri},
keywords = {Behavioural economics, Travel behaviour, Nudge},
abstract = {It can be argued that the main thinking in transport planning and policy making stem from neoclassical economics in which individuals are largely assumed to make rational, consistent, and efficient choices, and apply cognitive processes of decision making that maximise their economic utility. Research in behavioural sciences indicates that individuals’ choices in a wide range of contexts deviate from the predictions of the rational man paradigm inspired the research agenda in the field of travel behaviour. New concepts and practices of government aim to apply some behavioural economics insights in the design of behavioural change initiatives and measures, an approach recently advocated in the US and the UK. This paper provides a brief review on the use and potential of behavioural economics from the perspective of transport and climate change, in two main contexts: travel demand modelling and design of behaviour change measures. The discussion of limitations and knowledge gaps associated with the implementation of behavioural economics to a travel behaviour context might contribute to the debate and help in defining research agenda in this area.}
}
@article{BORGIANNI2015388,
title = {Integration of OTSM-TRIZ and Analytic Hierarchy Process for Choosing the Right Solution},
journal = {Procedia Engineering},
volume = {131},
pages = {388-400},
year = {2015},
note = {TRIZ and Knowledge-Based Innovation in Science and Industry},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.12.431},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815043234},
author = {Yuri Borgianni and Francesco Saverio Frillici and Federico Rotini},
keywords = {Network of Problems, best solution selection, Analytic Hierarchy Process, hand steamer},
abstract = {A relevant part of TRIZ literature concerns the steps of the problem solving process, hence the analysis of the troublesome situation, the identification of the core problem and its resolution. Conversely, few efforts have been dedicated to support the last phase of the conceptual design process, which regards the selection of the most promising solutions to be further developed. The lack within TRIZ of an instrument capable to fulfill the abovementioned task led the authors to investigate the classical decision making methods and their applicability in the context of selecting the most valuable concepts downstream of problem solving phases characterized by divergent thinking. Several potential approaches have been surveyed and, among the others, the Weighted Sum Method and the Analytic Hierarchy Process seem to hold some of the characteristics requested by an ideal method to facilitate the decision making. In this paper, both of them have been tested through a real case study in order to verify their actual applicability and to reveal strengths and weaknesses with a particular focus on their capability to guide the decision process when a plurality of parties (e.g. policy makers, domain experts) are involved. The testing activity revealed that the Analytic Hierarchy Process resulted overall more appreciated by the experimenters, thanks to the systematic approach employed to select the best solution among a sample of alternatives developed through the Network of Problems.}
}
@article{JIANG2024100078,
title = {Human-AI interaction research agenda: A user-centered perspective},
journal = {Data and Information Management},
volume = {8},
number = {4},
pages = {100078},
year = {2024},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2543925124000147},
author = {Tingting Jiang and Zhumo Sun and Shiting Fu and Yan Lv},
keywords = {Human-AI interaction, Human-AI collaboration, Human-AI competition, Human-AI conflict, Human-AI symbiosis},
abstract = {The rapid growth of artificial intelligence (AI) has given rise to the field of Human-AI Interaction (HAII). This study meticulously reviewed the research themes, theoretical foundations, and methodological frameworks of the HAII field, aiming to construct a comprehensive overview of this field and provide robust support for future investigations. HAII research themes include human-AI collaboration, competition, conflict, and symbiosis. Theories drawn from communication, psychology, and sociology support these studies, while the employed methods include both self-reporting and observational approaches commonly utilized in user studies. It is suggested that future research should broaden its focus to encompass diverse user groups, AI roles, and tasks. Moreover, it is necessary to develop multi-disciplinary theories and integrate multi-level research methods to support the sustained development of the field. This study not only furnishes indispensable theoretical and practical insights for forthcoming research endeavors but also catalyzes the realization of a future distinguished by seamless interaction between humans and AI.}
}
@article{CLEGG201856,
title = {Analysis of a train-operating company's customer service system during disruptions: Conceptual requirements for gamifying frontline staff development},
journal = {Journal of Rail Transport Planning & Management},
volume = {8},
number = {1},
pages = {56-77},
year = {2018},
issn = {2210-9706},
doi = {https://doi.org/10.1016/j.jrtpm.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2210970617300495},
author = {Ben Clegg and Richard Orme and Chris Owen and Pavel Albores},
keywords = {Customer service systems, Gamification, Systems thinking, Mitigate-Plan-React-Recovery (MPRR) framework, Disruption management, Frontline staff training},
abstract = {This paper provides an account of an action research study into the systemic success factors which help frontline staff react to and recover from a rail service disruption. This study focuses on the effective use of information during a disruption to improve customer service, as this is a priority area for train-operating companies (TOCs) in Great Britain. A novel type of systems thinking, known as Process-Oriented Holonic (PrOH) Modelling, has been used to investigate and model the ‘Passenger Information During Disruption’ (PIDD) system. This paper presents conceptual requirements for a gamified learning environment; it describes ‘what’, ‘how’ and ‘when’ these systemic success factors could be gamified using a popular disruption management reference framework known as the Mitigate, Prepare, React and Recover (MPRR) framework. This paper will interest managers of and researchers into customer service system disruptions, as well as those wishing to develop new gamified learning environments to improve customer service systems.}
}
@article{FATTAHI2020107755,
title = {Stochastic optimization of disruption-driven supply chain network design with a new resilience metric},
journal = {International Journal of Production Economics},
volume = {230},
pages = {107755},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107755},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320301407},
author = {Mohammad Fattahi and Kannan Govindan and Reza Maihami},
keywords = {Resilience metrics, Supply chain network design, Stochastic programming, Conic mixed-integer program},
abstract = {The supply chain (SC) ability to return quickly and effectively to its initial condition or even a more desirable state after a disruption is critically important, and is defined as SC resilience. Nevertheless, it has not been sufficiently quantified in the related literature. This study provides a new metric to quantify the SC resilience by using the stochastic programming. Our metric measures the expected value of the SC's cost increase due to a possible disruption event during its recovery period. Based on this measure, we propose a two-stage stochastic program for the supply chain network design under disruption events that optimizes location, allocation, inventory and order-size decisions. The stochastic program is formulated using quadratic conic optimization, and the sample average approximation (SAA) method is employed to handle the large number of disruption scenarios. A comprehensive computational study is carried out to highlight the applicability of the presented metric, the computational tractability of the stochastic program, and the performance of the SAA. Several key managerial and practical insights are gained based on the computational results. This new metric captures the time and cost of the SC's recovery after disruption events contrast to most of previous studies and main impacts of these two aspects on design decisions are highlighted. Further, it is shown computationally that the increase of SC's capacity is not a suitable strategy for designing resilient SCs in some business environments.}
}
@incollection{AHAMED2017465,
title = {Chapter 29 - The Architecture of a Mind-Machine},
editor = {Syed V. Ahamed},
booktitle = {Evolution of Knowledge Science},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {465-479},
year = {2017},
isbn = {978-0-12-805478-9},
doi = {https://doi.org/10.1016/B978-0-12-805478-9.00029-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054789000297},
author = {Syed V. Ahamed},
keywords = {Mind, Knowledge, Machines, Technology, Human Needs, Knowledge Windows, Perceptual Spaces},
abstract = {Chapter Summary
In this chapter, we take bold step and propose the unthinkable: The genesis of a Customizable Mind-Machine. Thought that stems from the mind is deeply seated in a biological framework of neurons. The biological origin lies in the marvel of evolution over the eons and refined ever so fast, faster than in the prior centuries. Three (a, b, and c), triadic objects are ceaselessly at work. At a personal level (a) mind, knowledge, and machines have been intertwined like inspiration, words, and language since the dawn of the human evolution and more recently, (b) technology, manufacturing, and economics have formed a hub of progress, (c) wealth, global marketing, and insatiable needs of humans and civilization. These triadic cycles of nine essential objects of human existence are spinning quicker and quicker every year. The Internet offers the mind no choice but to leap and soar over history and over the globe. Alternatively, human mind can sink deeper and deeper into ignorance and oblivion. More recently, the Artificial Intelligence at work in the Internet had challenged the natural intelligence at the cognizance level in the mind to find its way to breakthroughs and innovations. We integrate functions of the mind with the processing of knowledge in the hardware of machines by freely traversing the neural, mental, physical, psychological, social, knowledge, and computational spaces. The laws of neural biology and mind, laws of knowledge and social sciences, and finally the laws of physics and mechanics in each of the spaces are unique and executed by distinctive processors for each space. Much as mind rules over matter, the triad of mind, space, and time creates a human-space that rules over the Relativistic-space of matter, space, and time.}
}
@article{WEIGL20231,
title = {Modelling learning for a better safety culture within an organization using a virtual safety coach: Reducing the risk of postpartum depression via improved communication with parents},
journal = {Cognitive Systems Research},
volume = {80},
pages = {1-36},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000153},
author = {Linn-Marie Weigl and Fakhra Jabeen and Jan Treur and H. Rob Taal and Peter H.M.P. Roelofsma},
keywords = {Shared mental models, virtual AI Coach in healthcare, Fathers/psychology, Depressive disorders/complications, Postpartum depression},
abstract = {This paper describes an extension of a safety culture within hospital organizations providing more transparency and acknowledgement of all actors, and in particular the parents. It contributes a model architecture to support a hospital to develop such an extended safety culture. It is illustrated for prevention of postpartum depression. Postpartum depression is a commonly known consequence of childbirth for both mothers and fathers. In this research, we computationally analyze the risk factors and lack of support received by fathers. Therefore, we use shared mental models to model the effects of poor and additional communication by healthcare practitioners to mitigate the development of postpartum depression in both the mother and the father. Both individual mental models and shared mental models are considered in the design of the computational model. The paper illustrates the benefits of simple support in terms of communication during childbirth, which has lasting effects, even outside the hospital. For the impact of additional communication, a Virtual Safety Coach is designed that intervenes when necessary to provide support, i.e., when a health care practitioner doesn’t. Moreover, organizational learning is also modelled to improve the mental models of both the Safety Coach and the Health Care Practitioner.}
}
@article{DIMAGGIO2025S297,
title = {486. Simulating Thought Disorder: Fine-Tuning Llama-2 for Synthetic Speech in Schizophrenia},
journal = {Biological Psychiatry},
volume = {97},
number = {9, Supplement },
pages = {S297-S298},
year = {2025},
note = {Abstract Supplement},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2025.02.724},
url = {https://www.sciencedirect.com/science/article/pii/S0006322325008224},
author = {Anthony DiMaggio and Gleb Melshin and Lena Palaniyappan and Alban Voppel}
}
@incollection{CARSTON2006559,
title = {Language of Thought},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {559-561},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/04780-5},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542047805},
author = {R. Carston},
keywords = {biosemantics, computational theory of mind, connectionism, intentional realism, intentionality, Mentalese, methodological solipsism, productivity (of thought), propositional attitude, psychosemantics, representational theory of mind, syntactic structure, systematicity (of thought)},
abstract = {Two key aspects of human public languages are syntax and semantics, where syntax concerns the combinatorial structure of linguistic expressions and semantics refers to their content or meaning. So, the claim that humans have a language of thought, defended in particular by Jerry Fodor, amounts to the view that thoughts are representational (semantic) and thought processes are computational, that is, they involve transformations of symbolic structures on the basis of their formal (syntactic) properties. The fact that thought, like language, exhibits ‘productivity’ and ‘systematicity’ argues for a system of mental representation that has language-like structure.}
}
@article{MAKKAR2019381,
title = {Cognitive spammer: A Framework for PageRank analysis with Split by Over-sampling and Train by Under-fitting},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {381-404},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18305703},
author = {Aaisha Makkar and Neeraj Kumar},
keywords = {Internet of Things(IoT), Cognitive IoT, Web spam, PageRank},
abstract = {From the past few years, there is an exponential increase in one of the most popular technologies of the modern era called as Internet of Things (IoT). In IoT, various objects perform the tasks of sensing, communication, and computation for providing uninterrupted services (e.g., e-health, e-transportation, security access, etc.) to the end users. In this era, Cognitive Internet of Things (CIoT) is an another paradigm of IoT developed to enhance the capabilities of intelligence in IoT objects where these objects can take independent decisions in any environment. IoT follows the service oriented architecture (SOA), in which the application layer is the topmost layer. It enables the IoT objects to interact with the other objects located across the globe. The power of learning, thinking, and understanding by these objects, can make the information access more accurate and reliable but Web spam is one of the challenges while accessing information from the web. It has been observed from the literature review that search engines are preferred mostly by the people for accessing information. The efficient ranking by the search engines can reduce the computational cost of information exchange by IoT objects. Search engines should be able to prevent the spam from being injected into the web. But, the existing techniques for this problem target in finding the spam after its occurrence in search engine result pages. So, in this proposal, we present an intelligent cognitive spammer framework, Cognitive spammer, which eliminates the spam pages during the web page rank score calculation by search engines. The framework update the Google’s ranking algorithm, PageRank in such a way that it automatically prevents link spam by considering the link structure of web for rank score computation. The updated PageRank algorithm provided the better ranking of web pages. The proposed framework is validated with the WEBSPAM-UK2007 dataset. Before processing, the dataset is preprocessed with a new technique, called as ‘Split by Over-sampling and Train by Under-fitting’ to remove the trade off between imbalanced instances of target class. After data cleaning, we applied machine learning techniques (Bagged model, Boosted linear model, etc) with the web page features to make accurate predictions. The detection classifiers only consider the link features of the web page irrespective of the page content. Out of the fifteen classifiers, best three are ensemble, which results in better performance with overall accuracy improvement. Ten-fold cross validation has also been applied with the resulted ensemble model, which results in getting the accuracy of 99.6% in the proposed scheme.}
}
@article{WAHYUNINGSIH2024349,
title = {Comparison of Effectiveness of Logistic Regression, Naive Bayes, and Random Forest Algorithms in Predicting Student Arguments},
journal = {Procedia Computer Science},
volume = {234},
pages = {349-356},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924003715},
author = {Tri Wahyuningsih and Danny Manongga and Irwan Sembiring and Sutarto Wijono},
keywords = {Comparison Algorithm, Logistic Regression, Naive Bayes, Random Forest, Student Arguments},
abstract = {Currently, in the process of assessing and giving feedback on students' argumentative writing, educators have to spend a considerable amount of time reading and analyzing each essay individually. This can be a complicated and time-consuming process, especially if the number of students to be assessed is quite large. The problem of this research is to find the most effective algorithm in providing accurate and reliable predictions in the context of evaluation and feedback of students' argumentation. This study compares three algorithms (logistic regression, Naive Bayes, and Random Forest) to predict student argumentation using essays from grades 6-12. Logistic regression performed best with 94.34% accuracy, followed by random forest with 91.98% accuracy, and Naive Bayes with 88.93% accuracy. The study optimized preprocessing and selected algorithms for an automated guidance model. It is the first stage of a three-part study for developing automated guidance models. Data came from Kaggle, and the study aims to improve the accuracy of automated guidance models for student argumentation.}
}
@article{DEBRUIJN2022101666,
title = {The perils and pitfalls of explainable AI: Strategies for explaining algorithmic decision-making},
journal = {Government Information Quarterly},
volume = {39},
number = {2},
pages = {101666},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101666},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21001027},
author = {Hans {de Bruijn} and Martijn Warnier and Marijn Janssen},
keywords = {Artificial intelligence, XAI, Algorithms, Computational intelligence, Data-driven decision, Socio-tech, Transparency, Accountability, Trust, E-government},
abstract = {Governments look at explainable artificial intelligence's (XAI) potential to tackle the criticisms of the opaqueness of algorithmic decision-making with AI. Although XAI is appealing as a solution for automated decisions, the wicked nature of the challenges governments face complicates the use of XAI. Wickedness means that the facts that define a problem are ambiguous and that there is no consensus on the normative criteria for solving this problem. In such a situation, the use of algorithms can result in distrust. Whereas there is much research advancing XAI technology, the focus of this paper is on strategies for explainability. Three illustrative cases are used to show that explainable, data-driven decisions are often not perceived as objective by the public. The context might raise strong incentives to contest and distrust the explanation of AI, and as a consequence, fierce resistance from society is encountered. To overcome the inherent problems of XAI, decisions-specific strategies are proposed to lead to societal acceptance of AI-based decisions. We suggest strategies to embrace explainable decisions and processes, co-create decisions with societal actors, move away from an instrumental to an institutional approach, use competing and value-sensitive algorithms, and mobilize the tacit knowledge of professionals}
}
@article{PAULIUK2022130997,
title = {Co-design of digital transformation and sustainable development strategies - What socio-metabolic and industrial ecology research can contribute},
journal = {Journal of Cleaner Production},
volume = {343},
pages = {130997},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130997},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622006321},
author = {Stefan Pauliuk and Maximilian Koslowski and Kavya Madhu and Simon Schulte and Sebastian Kilchert},
keywords = {Sustainable development, Digital transformation, Systems thinking, Research agenda, Technology scale-up, Development constraints},
abstract = {Sustainable development and digital transformation profoundly re-shape industrial societies but have been studied largely independently. In light of pressing global environmental and social challenges, both transformations need to be well aligned with each other to achieve multiple objectives such as listed under the UN Sustainable Development goals (SDGs). Quantitative research on interlinkages, energy and material implications, and co-dependencies between the different digital transformation (DT) and sustainable development (SD) strategies is emerging and has so far focused on estimating the overall potential and on life cycle assessment (LCA). To frame the problem systematically, we developed a hierarchy of system levels for studying society's material and energy use, including the four levels: product/process, process cluster, life cycle/material cycle, and economy-wide. We mapped major DT strategies and the SDGs to the hierarchy and found a wide gap in system coverage: While most DT strategies focus on the product, process and process cluster levels, the SDGs predominantly target the economy-wide level. Socio-metabolic and industrial ecology research is needed to inform decision makers on how the two transformations can be aligned to reach overarching societal goals, such as the SDGs, expanding on and moving beyond LCA. Future research needs to assess combinations of multiple DT and SD strategies. It needs to study how DT can help decouple human wellbeing from negative environmental and social impacts. Research needs to focus on the strategies’ deployment potential, infrastructure needs, impacts on material cycles, and potential to transform both service demand and industrial production.}
}
@article{GOLDIN1998137,
title = {Representational systems, learning, and problem solving in mathematics},
journal = {The Journal of Mathematical Behavior},
volume = {17},
number = {2},
pages = {137-165},
year = {1998},
note = {Representations and the Psychology of Mathematics Education: Part II},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0364-0213(99)80056-1},
url = {https://www.sciencedirect.com/science/article/pii/S0364021399800561},
author = {Gerald A. Goldin},
abstract = {This article explores aspects of a unified psychological model for mathematical learning and problem solving, based on several different types of representational systems and their stages of development. The goal is to arrive at a scientifically adequate theoretical framework, complex enough to account for diverse empirical results but sufficiently simple to be accessible and useful in mathematics education practice. Some perspectives on representational systems are discussed, and components of the model are described in relation to these ideas—including constructs related to imagistic thinking, heuristics and strategies, affect, and the fundamental role of ambiguity.}
}
@article{PAPADOPOULOS2008311,
title = {Students’ use of technological tools for verification purposes in geometry problem solving},
journal = {The Journal of Mathematical Behavior},
volume = {27},
number = {4},
pages = {311-325},
year = {2008},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2008.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312308000497},
author = {Ioannis Papadopoulos and Vassilios Dagdilelis},
keywords = {Verification, Problem solving, Technology},
abstract = {Despite its importance in mathematical problem solving, verification receives rather little attention by the students in classrooms, especially at the primary school level. Under the hypotheses that (a) non-standard tasks create a feeling of uncertainty that stimulates the students to proceed to verification processes and (b) computational environments – by providing more available tools compared to the traditional environment – might offer opportunities for more frequent usage of verification techniques, we posed to 5th and 6th graders non-routine problems dealing with area of plane irregular figures. The data collected gave us evidence that computational environments allow the development of verification processes in a wider variety compared to the traditional paper-and-pencil environment and at the same time we had the chance to propose a preliminary categorization of the students’ verification processes under certain conditions.}
}
@article{MAGALHAES20151157,
title = {Establishment ofAutomatization as a Requirement for Time Management Input Modules in Project Management Information Systems for Academic Activities – A Game Theory Approach},
journal = {Procedia Computer Science},
volume = {64},
pages = {1157-1162},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.596},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915027313},
author = {Sérgio Tenreiro de Magalhães and Maria José Magalhães and Vítor J. Sá},
keywords = {Academic Activities, Project Management, Project Management Information Systems, Game Theory.},
abstract = {Academics are expected to engage in several works in several different domains, namely research and development, general management and services to the community, while lecturing a set of courses. Academics might differ in their preference for some of these activities and also in their corresponding performance. Quality assurance in academic institutions implies monitoring performance, what is frequently done by measuring a set of quantitative results at the end of a certain period. Project Management best practices can change this frequent practice, introducing, for instance, the concept of cost efficiency, allowing for objective comparisons between different types of activities. For this to happen there is a need to monitor the time spent by each academic in each activities or, at least, in each set of activities of the same type. The challenge is to know how to do that. Game Theory has been studying decision making in competitive environment, which is increasingly the case in academic institutions. Therefore, there is a primary need to verify if a relevant percentage of the academics have a perception that there is an incentive to lie in their timesheets, due to competitive thinking. This paper presents a pilot study that allowed concluding that time management input modules in project management information systems for academic activities must be automated, eliminating the human factor in timesheet fillings.}
}
@article{SCHOLTE201894,
title = {Toward a systems theatre: Proposal for a program of non-trivial modeling},
journal = {Futures},
volume = {103},
pages = {94-105},
year = {2018},
note = {Futures of Society: The Interactions Revolution},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717302033},
author = {Tom Scholte},
keywords = {Augusto Boal, Theatre of the Oppressed, Theatre for Living, Systems theory, Cybernetics, Konstantin Stanislavski, Soft Systems Methodology, System Dynamics, Critical systems heurstics, Enactive Management},
abstract = {This paper makes the case for, and calls for participants in, an interdisciplinary research program exploring the development of theatrical methods of social system modeling. It combines argumentation that synthesizes concepts from the theatre and the system sciences with results from a pilot application of some of the modeling methods discussed. Theatrical methods of modeling facilitate surprising insights regarding the impacts of emotion and other non-trivial factors on system behaviour that are difficult to address in purely computational and diagrammatic forms of modeling. While a theoretical relationship between systems approaches and the theatrical techniques discussed has been articulated elsewhere, this paper is the first to propose a more fulsome exploration of the potentialities of this relationship for systems praxis.}
}
@article{VALERY201844,
title = {A collaborative CPU–GPU approach for principal component analysis on mobile heterogeneous platforms},
journal = {Journal of Parallel and Distributed Computing},
volume = {120},
pages = {44-61},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518303411},
author = {Olivier Valery and Pangfeng Liu and Jan-Jan Wu},
keywords = {OpenCL, GPGPU, Mobile computing, Heterogeneous system, PCA, Energy efficient, Acceleration, Data analysis, Machine learning},
abstract = {The advent of the modern GPU architecture has enabled computers to use General Purpose GPU capabilities (GPGPU) to tackle large scale problem at a low computational cost. This technological innovation is also available on mobile devices, addressing one of the primary problems with recent devices: the power envelope. Unfortunately, recent mobile GPUs suffer from a lack of accuracy that can prevent them from running any large scale data analysis tasks, such as principal component analysis (Shlens, 0000) (PCA). The goal of our work is to address this limitation by combining the high precision available on a CPU with the power efficiency of a mobile GPU. In this paper, we exploit the shared memory architecture of mobile devices in order to enhance the CPU–GPU collaboration and speed up PCA computation without sacrificing precision. Experimental results suggest that such an approach drastically reduces the power consumption of the mobile device while accelerating the overall workload. More generally, we claim that this approach can be extended to accelerate other vectorized computations on mobile devices while still maintaining numerical accuracy.}
}