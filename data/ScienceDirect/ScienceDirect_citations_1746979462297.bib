@article{GOUTAUDIER2021113755,
title = {Proper Generalized Decomposition with time adaptive space separation for transient wave propagation problems in separable domains},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {380},
pages = {113755},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.113755},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521000918},
author = {Dimitri Goutaudier and Laurent Berthe and Francisco Chinesta},
keywords = {Proper Generalized Decomposition (PGD), Transient wave propagation, Time adaptive space separation, Separable domain, Scalar wave equation, Elastodynamics},
abstract = {Transient wave propagation problems may involve rich discretizations, both in space and in time, leading to computationally expensive simulations, even for simple spatial domains. The Proper Generalized Decomposition (PGD) is an attractive model order reduction technique to address this issue, especially when the spatial domain is separable. In this work, we propose a space separation with a time adaptive number of modes to efficiently capture transient wave propagation in separable domains. We combine standard time integration schemes with this original space separated representation for empowering standard procedures. The numerical behavior of the proposed method is explored through several 2D wave propagation problems involving radial waves, propagation on long time analyses, and wave conversions. We show that the PGD solution approximates its standard finite element solution counterpart with acceptable accuracy, while reducing the storage needs and the computation time (CPU time). Numerical results show that the CPU time per time step linearly increases when refining the mesh, even with implicit time integration schemes, which is not the case with standard procedures.}
}
@article{MONROE2020293,
title = {Moral elevation: Indications of functional integration with welfare trade-off calibration and estimation mechanisms},
journal = {Evolution and Human Behavior},
volume = {41},
number = {4},
pages = {293-302},
year = {2020},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090513820300581},
author = {Amy Monroe},
keywords = {Moral elevation, Welfare trade-off ratios, Competitive altruism, Emotion},
abstract = {Moral elevation is a positive social emotion, which is triggered by observing third parties behaving benevolently, and which in turn triggers a motivation to behave benevolently towards others in general. It has been suggested that this relatively obscure emotion may be the output of a naturally selected cognitive adaptation which functions to help us retain our position in the competition for access to beneficial social relationships. This suggestion is here interpreted within the framework of ‘recalibrational emotions’. This framework offers the computational vocabulary necessary to understand how mental adaptations governing affect and motivation perform their functions at the cognitive level. Parallels are drawn between the suggested function and known phenomenological attributes of moral elevation, and the recently explicated functional operation of other social emotions (such as anger, guilt, and gratitude). Specifically, these other social emotions are thought to share a common computational pathway; recalibration of our welfare trade-off ratios (WTRs). WTRs are the computational element which dictate our willingness to benefit others at some cost to ourselves. A series of studies was conducted to explore whether a reliable relationship exists between moral elevation and WTRs. The results suggest that elevation does have a positive recalibrational effect on our WTRs, and that it may also be functionally integrated with a mental mechanism designed by natural selection to estimate the WTRs of other social actors.}
}
@article{MARSHALL2024R950,
title = {Where physics and biology meet},
journal = {Current Biology},
volume = {34},
number = {20},
pages = {R950-R960},
year = {2024},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2024.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0960982224011345},
author = {Wallace Marshall and Buzz Baum and Adrienne Fairhall and Carl-Philipp Heisenberg and Elena Koslover and Andrea Liu and Yanlan Mao and Alex Mogilner and Celeste M. Nelson and Ewa K. Paluch and Xavier Trepat and Alpha Yap}
}
@incollection{ULLMAN1988548,
title = {Visual routines**This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-80-C-0505 and in part by National Science Foundation Grant 79-23110MCS. Reprint requests should be sent to Shimon Ullman Department of Psychology and Artificial Intelligence Laboratory, M.I.T., Cambridge, MA 02139. U.S.A.},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {548-579},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50047-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500479},
author = {SHIMON ULLMAN},
abstract = {This paper examines the processing of visual information beyond the creation of the early representations. A fundamental requirement at this level is the capacity to establish visually abstract shape properties and spatial relations. This capacity plays a major role in object recognition, visually guided manipulation, and more abstract visual thinking. For the human visual system, the perception of spatial properties and relations that are complex from a computational standpoint nevertheless often appears deceivingly immediate and effortless. The proficiency of the human system in analyzing spatial information far surpasses the capacities of current artificial systems. The study of the computations that underlie this competence may therefore lead to the development of new more efficient methods for the spatial analysis of visual information. The perception of abstract shape properties and spatial relations raises fundamental difficulties with major implications for the overall processing of visual information. It will be argued that the computation of spatial relations divides the analysis of visual information into two main stages. The first is the bottom-up creation of certain representations of the visible environment. The second stage involves the application of processes called ‘visual routines’ to the representations constructed in the first stage. These routines can establish properties and relations that cannot be represented explicitly in the initial representations. Visual routines are composed of sequences of elemental operations. Routines for different properties and relations share elemental operations. Using a fixed set of basic operations, the visual system can assemble different routines to extract an unbounded variety of shape properties and spatial relations.}
}
@article{MILLI2021104881,
title = {A rational reinterpretation of dual-process theories},
journal = {Cognition},
volume = {217},
pages = {104881},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2021.104881},
url = {https://www.sciencedirect.com/science/article/pii/S0010027721003024},
author = {Smitha Milli and Falk Lieder and Thomas L. Griffiths},
keywords = {Bounded rationality, Dual-process theories, Meta-decision making, Bounded optimality, Metareasoning, Resource-rationality},
abstract = {Highly influential “dual-process” accounts of human cognition postulate the coexistence of a slow accurate system with a fast error-prone system. But why would there be just two systems rather than, say, one or 93? Here, we argue that a dual-process architecture might reflect a rational tradeoff between the cognitive flexibility afforded by multiple systems and the time and effort required to choose between them. We investigate what the optimal set and number of cognitive systems would be depending on the structure of the environment. We find that the optimal number of systems depends on the variability of the environment and the difficulty of deciding when which system should be used. Furthermore, we find that there is a plausible range of conditions under which it is optimal to be equipped with a fast system that performs no deliberation (“System 1”) and a slow system that achieves a higher expected accuracy through deliberation (“System 2”). Our findings thereby suggest a rational reinterpretation of dual-process theories.}
}
@article{MAO2022109671,
title = {A decision support engine for infill drilling attractiveness evaluation using rule-based cognitive computing under expert uncertainties},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109671},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109671},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521013000},
author = {Qiangqiang Mao and Xiaohua Ma and Yuhe Wang},
keywords = {Cognitive computing, Fuzzy inference, Infill well placement, Drilling attractiveness evaluation, Expert uncertainties quantification},
abstract = {Optimally drilling new wells in a developed reservoir is an essential strategy to potentially tap remaining oil for a complete life circle of oilfield development. Further, the determination of optimal infill drilling targets is a challenging issue which involves the integration of data, experts' knowledge and human decisions. The decision process can be essentially regarded as a systematic evaluation of drilling attractiveness. To automate drilling attractiveness evaluation, we develop a decision support engine using rule-based cognitive computing to rank and recommend drilling candidates. Such drilling candidates are chosen by the quantification of regional drilling attractiveness. Then we use two cases with different settings to show its general applicability and human-like reasoning abilities. The reasoning process considers expertise and human-involved uncertainties. The expertise is characterized by certain representation of fuzzy rules sets. Our results highlight the potential of our recommendation engine in pinpointing the most productive drilling location. And our method avoids the expensive reservoir simulation runs. Moreover, fuzzy drilling attractiveness evaluation can serve as an alternative initialization method of model-based infill well optimization, which avoids local optimum problem and greatly saves iteration time. Our approach extends human's reasoning capability and accelerates human's decision-making process with very low computational cost.}
}
@article{WEN201811,
title = {Fast ranking nodes importance in complex networks based on LS-SVM method},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {506},
pages = {11-23},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.03.076},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118303947},
author = {Xiangxi Wen and Congliang Tu and Minggong Wu and Xurui Jiang},
keywords = {Complex network, Node importance, AHP, LS-SVM},
abstract = {Achieving high accuracy and comprehensiveness in node importance evaluation of complex networks is time-consuming. To solve this problem, a method based on Least Square Support Vector Machine (LS-SVM) was proposed. Firstly, four complicated importance indicators which reflect the node importance globally and comprehensively were selected. Then analytic hierarchy process (AHP) method was applied to obtain the node’s importance evaluation. On this basis, three simple indicators with low computational complexity were proposed, and LS-SVM was adopted to find the mapping rules between simple indicators and AHP evaluation. The experiments on artificial network and actual network show the validity of proposed method: the evaluation based on complicated indicators is consistent with reality and reflects node importance accurately; simple indicators evaluation by LS-SVM saved a lot of computational time and improved the evaluating efficiency. Our method can provide guidance on influential node identification in large scale complex networks.}
}
@article{VELAVELUPILLAI201436,
title = {Constructive and computable Hahn–Banach theorems for the (second) fundamental theorem of welfare economics},
journal = {Journal of Mathematical Economics},
volume = {54},
pages = {36-39},
year = {2014},
issn = {0304-4068},
doi = {https://doi.org/10.1016/j.jmateco.2014.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0304406814001062},
author = {K. {Vela Velupillai}},
keywords = {Fundamental theorems of welfare economics, Hahn–Banach theorem, Constructive analysis, Computable analysis},
abstract = {The Hahn–Banach Theorem plays a crucial role in the second fundamental theorem of welfare economics. To date, all mathematical economics and advanced general equilibrium textbooks concentrate on using non-constructive or incomputable versions of this celebrated theorem. In this paper we argue for the introduction of constructive or computable Hahn–Banach theorems in mathematical economics and advanced general equilibrium theory. The suggested modification would make applied and policy-oriented economics intrinsically computational.}
}
@article{VARDOULI2015137,
title = {Making use: Attitudes to human-artifact engagements},
journal = {Design Studies},
volume = {41},
pages = {137-161},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000563},
author = {Theodora Vardouli},
keywords = {design theory, philosophy of design, user behavior, function theory, computational models},
abstract = {‘Function’ and ‘use’ are keywords that design researchers customarily employ when referring to human-artifact engagements. However, there is little consensus about how the concepts of function and use relate to each other, to the intentions of ‘designers’ and ‘users’, or to their actions and encompassing contexts. In this paper, I synthesize literature from design research, material culture studies, design anthropology, and function theory in order to critically compare different attitudes to human-artifact engagements, implicit in characterizations of function and use. I identify design-centric, communicative, and use-centric attitudes, and discuss their assumptions and implications for design theory. I conclude by outlining principles for theoretically and computationally approaching use as an embodied and temporally contingent process – as a form of ‘making’.}
}
@article{KASONGO2023113,
title = {A deep learning technique for intrusion detection system using a Recurrent Neural Networks based framework},
journal = {Computer Communications},
volume = {199},
pages = {113-125},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422004601},
author = {Sydney Mambwe Kasongo},
keywords = {Machine learning, Feature selection, Intrusion detection, Feature extraction},
abstract = {In recent years, the spike in the amount of information transmitted through communication infrastructures has increased due to the advances in technologies such as cloud computing, vehicular networks systems, the Internet of Things (IoT), etc. As a result, attackers have multiplied their efforts for the purpose of rendering network systems vulnerable. Therefore, it is of utmost importance to improve the security of those network systems. In this study, an IDS framework using Machine Learning (ML) techniques is implemented. This framework uses different types of Recurrent Neural Networks (RNNs), namely, Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and Simple RNN. To assess the performance of the proposed IDS framework, the NSL-KDD and the UNSW-NB15 benchmark datasets are considered. Moreover, existing IDSs suffer from low test accuracy scores in detecting new attacks as the feature dimension grows. In this study, an XGBoost-based feature selection algorithm was implemented to reduce the feature space of each dataset. Following that process, 17 and 22 relevant attributes were picked from the UNSW-NB15 and NSL-KDD, respectively. The accuracy obtained through the test subsets was used as the main performance metric in conjunction with the F1-Score, the validation accuracy, and the training time (in seconds). The results showed that for the binary classification tasks using the NSL-KDD, the XGBoost-LSTM achieved the best performance with a test accuracy (TAC) of 88.13%, a validation accuracy (VAC) of 99.49% and a training time of 225.46 s. For the UNSW-NB15, the XGBoost-Simple-RNN was the most efficient model with a TAC of 87.07%. For the multiclass classification scheme, the XGBoost-LSTM achieved a TAC of 86.93% over the NSL-KDD and the XGBoost-GRU obtained a TAC of 78.40% over the UNSW-NB15 dataset. These results demonstrated that our proposed IDS framework performed optimally in comparison to existing methods.}
}
@article{BAHK2013298,
title = {Analytical investigation of tooth profile modification effects on planetary gear dynamics},
journal = {Mechanism and Machine Theory},
volume = {70},
pages = {298-319},
year = {2013},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2013.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X13001584},
author = {Cheon-Jae Bahk and Robert G. Parker},
keywords = {Tooth profile modification, Planetary gear, Vibration, Nonlinear, Perturbation method},
abstract = {This study investigates the impact of tooth profile modification on spur planetary gear vibration. An analytical model is proposed to capture the excitation from tooth profile modifications at the sun–planet and ring–planet meshes. The accuracy of the proposed model for dynamic analysis is correlated against a benchmark finite element analysis. Perturbation analysis yields a closed-form approximation of the vibration response with tooth profile modifications. The perturbation solution is used to investigate the effects of tooth profile modification. The tooth profile modification parameters that minimize response are readily obtained. Static transmission error and dynamic response are minimized at different amounts of profile modification, which contradicts common practical thinking regarding the correlation between static transmission error and dynamic response. Contrary to expectations, the optimal sun–planet and ring–planet tooth profile modifications that minimize response when applied individually may increase dynamic response when applied simultaneously. System parameters such as mesh stiffness and mesh phase significantly affect the influence of tooth profile modification.}
}
@article{QIAO2024120105,
title = {Towards retraining-free RNA modification prediction with incremental learning},
journal = {Information Sciences},
volume = {660},
pages = {120105},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120105},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524000185},
author = {Jianbo Qiao and Junru Jin and Haoqing Yu and Leyi Wei},
keywords = {RNA modification, Deep learning, Incremental learning},
abstract = {RNA modifications are important for deciphering the function of cells and their regulatory mechanisms. In recent years, researchers have developed many deep learning methods to identify specific modifications. However, these methods require model retraining for each new RNA modification and cannot progressively identify the newly identified RNA modifications. To address this challenge, we propose an innovative incremental learning framework that incorporates multiple incremental learning methods. Our experimental results confirm the efficacy of incremental learning strategies in addressing the RNA modification challenge. By uniquely targeting 10 RNA modification types in a class incremental setting, our framework exhibits superior performance. Notably, it can be extended to new category methylation predictions without the need for retraining with previous data, improving computational efficiency. Through the accumulation of knowledge, the model is able to evolve and continuously learn the differences across methylation, mitigating the problem of catastrophic forgetting during deep learning model training. Overall, our framework provides various alternatives to enhance the prediction of novel RNA modifications and illuminates the potential of incremental learning in tacking numerous genome data.}
}
@article{ZHAO2025100817,
title = {User entertainment experience analysis of artificial intelligence entertainment robots based on convolutional neural networks in park plant landscape design},
journal = {Entertainment Computing},
volume = {52},
pages = {100817},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100817},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400185X},
author = {Jingjing Zhao and Juan Yin and Yaqi Shi and Liang Qiao and Guihua Ma},
keywords = {Convolutional neural network, AI entertainment robots, Park plants, Landscape design, User experience},
abstract = {Currently, the application of artificial intelligence entertainment robots in park plant landscape design has attracted increasing attention. This study aims to design an artificial intelligence entertainment robot that can provide a high-quality user experience. Through virtual reality and robotics technology, designers can be provided with visual and entertaining design solutions, and more interactive experiences can be provided for design clients. Convolutional neural networks can effectively extract features from images, and utilizing spectral feature extraction technology to further improve the accuracy of image recognition. Subsequently, this study designed a robot control system and calibrated the hand eye system. The robot control system can coordinate the various functions of the robot and ensure its smooth operation in the park plant landscape design. The calibration of the hand eye system is to ensure that the robot can accurately perceive the environment and locate its own position. Through real-time control strategies, robots can respond and adjust in a timely manner based on current environmental changes and user needs. By comparing with the actual position on the ground, the accuracy of robot positioning is obtained, and the system is further optimized and improved.}
}
@article{SANGAIAH2020347,
title = {Cognitive IoT system with intelligence techniques in sustainable computing environment},
journal = {Computer Communications},
volume = {154},
pages = {347-360},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.049},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419314616},
author = {Arun Kumar Sangaiah and Jerline Sheebha Anni Dhanaraj and Prabu Mohandas and Aniello Castiglione},
keywords = {Computational intelligence, Cognition, Multi-sensor, Data fusion, IoT},
abstract = {Forest border crossing animals creates major societal related issues, in addition to endangering their own lives. This is the objective focused in this paper targeting the species “The Elephant”, incorporating with technical methodologies namely, multi-sensor data fusion, cognition theories and computational intelligence techniques. Multi-sensor data fusion provides three level detection of target, along with its related outputs, which improves performance metrics. Cognition theory resulted in obtaining other interesting features about the target. Computational intelligence techniques integrate and conclude the presence of the target in the pseudo-boundary. The technical combination enhances the novelty of the research work, resulting in achieving remarkable accuracy and minimized false alert. An IoT kit was designed and deployed in the real time wild environment in Hosur forest region for collecting the data of Elephant. Further, the notification is sent to the registered mobile of the forest authority, as an early warning for chasing the pachyderm back to the forest.}
}
@article{ZADEH20082751,
title = {Is there a need for fuzzy logic?},
journal = {Information Sciences},
volume = {178},
number = {13},
pages = {2751-2779},
year = {2008},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2008.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025508000716},
author = {Lotfi A. Zadeh},
keywords = {Fuzzy logic, Fuzzy sets, Approximate reasoning, Computing with words, Computing with perceptions, Generalized theory of uncertainty},
abstract = {“Is there a need for fuzzy logic?” is an issue which is associated with a long history of spirited discussions and debate. There are many misconceptions about fuzzy logic. Fuzzy logic is not fuzzy. Basically, fuzzy logic is a precise logic of imprecision and approximate reasoning. More specifically, fuzzy logic may be viewed as an attempt at formalization/mechanization of two remarkable human capabilities. First, the capability to converse, reason and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information, conflicting information, partiality of truth and partiality of possibility – in short, in an environment of imperfect information. And second, the capability to perform a wide variety of physical and mental tasks without any measurements and any computations [L.A. Zadeh, From computing with numbers to computing with words – from manipulation of measurements to manipulation of perceptions, IEEE Transactions on Circuits and Systems 45 (1999) 105–119; L.A. Zadeh, A new direction in AI – toward a computational theory of perceptions, AI Magazine 22 (1) (2001) 73–84]. In fact, one of the principal contributions of fuzzy logic – a contribution which is widely unrecognized – is its high power of precisiation. Fuzzy logic is much more than a logical system. It has many facets. The principal facets are: logical, fuzzy-set-theoretic, epistemic and relational. Most of the practical applications of fuzzy logic are associated with its relational facet. In this paper, fuzzy logic is viewed in a nonstandard perspective. In this perspective, the cornerstones of fuzzy logic – and its principal distinguishing features – are: graduation, granulation, precisiation and the concept of a generalized constraint. A concept which has a position of centrality in the nontraditional view of fuzzy logic is that of precisiation. Informally, precisiation is an operation which transforms an object, p, into an object, p∗, which in some specified sense is defined more precisely than p. The object of precisiation and the result of precisiation are referred to as precisiend and precisiand, respectively. In fuzzy logic, a differentiation is made between two meanings of precision – precision of value, v-precision, and precision of meaning, m-precision. Furthermore, in the case of m-precisiation a differentiation is made between mh-precisiation, which is human-oriented (nonmathematical), and mm-precisiation, which is machine-oriented (mathematical). A dictionary definition is a form of mh-precisiation, with the definiens and definiendum playing the roles of precisiend and precisiand, respectively. Cointension is a qualitative measure of the proximity of meanings of the precisiend and precisiand. A precisiand is cointensive if its meaning is close to the meaning of the precisiend. A concept which plays a key role in the nontraditional view of fuzzy logic is that of a generalized constraint. If X is a variable then a generalized constraint on X, GC(X), is expressed as X isr R, where R is the constraining relation and r is an indexical variable which defines the modality of the constraint, that is, its semantics. The primary constraints are: possibilistic, (r=blank), probabilistic (r=p) and veristic (r=v). The standard constraints are: bivalent possibilistic, probabilistic and bivalent veristic. In large measure, science is based on standard constraints. Generalized constraints may be combined, qualified, projected, propagated and counterpropagated. The set of all generalized constraints, together with the rules which govern generation of generalized constraints, is referred to as the generalized constraint language, GCL. The standard constraint language, SCL, is a subset of GCL. In fuzzy logic, propositions, predicates and other semantic entities are precisiated through translation into GCL. Equivalently, a semantic entity, p, may be precisiated by representing its meaning as a generalized constraint. By construction, fuzzy logic has a much higher level of generality than bivalent logic. It is the generality of fuzzy logic that underlies much of what fuzzy logic has to offer. Among the important contributions of fuzzy logic are the following: 1.FL-generalization. Any bivalent-logic-based theory, T, may be FL-generalized, and hence upgraded, through addition to T of concepts and techniques drawn from fuzzy logic. Examples: fuzzy control, fuzzy linear programming, fuzzy probability theory and fuzzy topology.2.Linguistic variables and fuzzy if–then rules. The formalism of linguistic variables and fuzzy if–then rules is, in effect, a powerful modeling language which is widely used in applications of fuzzy logic. Basically, the formalism serves as a means of summarization and information compression through the use of granulation.3.Cointensive precisiation. Fuzzy logic has a high power of cointensive precisiation. This power is needed for a formulation of cointensive definitions of scientific concepts and cointensive formalization of human-centric fields such as economics, linguistics, law, conflict resolution, psychology and medicine.4.NL-Computation (computing with words). Fuzzy logic serves as a basis for NL-Computation, that is, computation with information described in natural language. NL-Computation is of direct relevance to mechanization of natural language understanding and computation with imprecise probabilities. More generally, NL-Computation is needed for dealing with second-order uncertainty, that is, uncertainty about uncertainty, or uncertainty2 for short. In summary, progression from bivalent logic to fuzzy logic is a significant positive step in the evolution of science. In large measure, the real-world is a fuzzy world. To deal with fuzzy reality what is needed is fuzzy logic. In coming years, fuzzy logic is likely to grow in visibility, importance and acceptance.}
}
@article{ALRAKHAMI2021107573,
title = {A deep learning-based edge-fog-cloud framework for driving behavior management},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107573},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107573},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005127},
author = {Mabrook S. Al-Rakhami and Abdu Gumaei and Mohammad Mehedi Hassan and Atif Alamri and Musaed Alhussein and Md. Abdur Razzaque and Giancarlo Fortino},
keywords = {Deep learning, Car mobile edge (CME), Fog and cloud computing, Aggressive driving behaviors},
abstract = {Among the various reasons behind vehicle accidents, drivers' aggressiveness and distractions play a significant role. Deep learning (DL) algorithms inside a car mobile edge (CME) have been used for driver monitoring and to perform automated decision-making processes. Training and retraining the DL models in resource-constrained CME devices come with several challenges, especially regarding computational and memory space costs. Moreover, training the DL models periodically on representative data nearest to CME without imposing communication overheads on the cloud improves the quality of service (QoS) parameters, such as memory demand, processing time, power consumption, and bandwidth. This paper investigates the deployment of a deep neural network (DNN) model on a cloud-fog-edge computing framework for aggressive driver behavior detection and monitoring. To reach this goal, our framework proposes utilizing effective systems and databases of sensor-based metrics and data, cost-effective wireless networks, cloud-and fog-edge computing technologies, and the Internet. Experimental results of the DNN model showed that the accuracy of detection is improved by 1.84% compared with the current related work without any pre-processing window on data points that come from bio-signal sensors. Moreover, the experimental results of the networking part prove the efficiency and effectiveness of the proposed framework.}
}
@article{ZHOU2019104484,
title = {Long-term forecasts for energy commodities price: What the experts think},
journal = {Energy Economics},
volume = {84},
pages = {104484},
year = {2019},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2019.104484},
url = {https://www.sciencedirect.com/science/article/pii/S0140988319302658},
author = {Fan Zhou and Lionel Page and Robert K. Perrons and Zuduo Zheng and Simon Washington},
keywords = {Crude oil prices, Natural gas prices, Expert elicitation, Bayesian Truth Serum, Surprisingly popular},
abstract = {The ability to forecast energy prices in the long-term is important for a wide range of reasons, from the formulation of countries' energy and transportation policies to the defensive strategies of nations to investment decisions within the private sector. Despite the importance of these predictions, however, forecasters and market pundits face a difficult challenge when trying to forecast over the long-term. While statistical models can credibly rely on assumptions about the relationship between variables in the short-term, they are frequently less reliable in the long-term as political and technological transformations profoundly change how the economy works over time. Towards improving long-term predictions for energy commodities, this paper uses the elicitation and aggregation of experts' beliefs to put forward forecasts for crude oil and natural gas prices by incentivizing experts to tell the truth and minimising their own biases through the application of the Bayesian Truth Serum. With this approach, we generated both short-term and long-term forecasts, and used the short-term forecast to validate the quality of the experts' predictions.}
}
@article{GUI2024111972,
title = {Molten salt-promoted MgO-based CO2 adsorbents: Selective adsorption on polycrystalline surfaces},
journal = {Journal of Environmental Chemical Engineering},
volume = {12},
number = {2},
pages = {111972},
year = {2024},
issn = {2213-3437},
doi = {https://doi.org/10.1016/j.jece.2024.111972},
url = {https://www.sciencedirect.com/science/article/pii/S2213343724001027},
author = {Changqing Gui and Zirui Wang and Changjian Ling and Zhongfeng Tang},
keywords = {CO, MgCl·6 HO, Molten salt, MgO, Capture},
abstract = {Molten salt-doped MgO adsorbent is considered one of the most promising CO2 adsorbents in the field. In this work, MgO-based adsorbents were prepared by one-step calcination using MgCl2·6 H2O as magnesium source. The CO2 adsorption performance of MgO-based adsorbents was investigated via different methods. Results showed that the maximum CO2 adsorption capacity of MgO doped by LiNO3-NaNO3-KNO3 was 57.1% at the CO2 concentration of 80% and 350 ℃, and the MgO-based adsorbents showed good regeneration. The nanosheet structure of the MgO-based adsorbents decreased with the increase in the number of cycles, whereas the crystal structures of MgO and alkali metal nitrates did not change because of multiple decarbonization. DFT computation revealed selective adsorption of CO2 on different crystal faces of MgO. The (200) crystal face of molten salt-doped MgO did not have CO2 trap ability. In addition, the doped nitrate did not directly participate in the reaction but reduced the adsorption energy of MgO carbonation. The adsorption energies of the MgO (220) and (222) crystal faces after doping with nitrate were reduced to − 2.07 and − 3.26 eV, respectively. The overall energy level of adsorption decreased as the number of resonance peaks and the stability of the structure increased. This study explains why MgO currently fails to reach the theoretical adsorption capacity and reveals the underlying mechanism of molten salts.}
}
@article{BORATYNSKA20165529,
title = {FsQCA in corporate bankruptcy research. An innovative approach in food industry},
journal = {Journal of Business Research},
volume = {69},
number = {11},
pages = {5529-5533},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.04.166},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316303733},
author = {Katarzyna Boratyńska},
keywords = {Complexity theory, fsQCA, Corporate bankruptcy, Food industry},
abstract = {This study focuses on fsQCA in corporate bankruptcy research. This research aims at revealing how an fsQCA approach can overcome the knowledge gap of current conceptual and methodological attempts to expose corporate bankruptcy's architecture of causalities. The article discusses the economic literature concerning using fsQCA in corporate bankruptcy studies through complexity theory and a critical perspective. The study concentrates on implementing fsQCA and asymmetric thinking to corporate bankruptcy cases in food industry. The research examines the main reasons for corporate bankruptcy, namely: lack of financial liquidity, too high level of liabilities, losses, weak management, and too late recovery actions. The study attempts to build theory from food industry cases.}
}
@article{KEBEDE2024131461,
title = {Transfer learning-based deep learning models for proton exchange membrane fuel remaining useful life prediction},
journal = {Fuel},
volume = {367},
pages = {131461},
year = {2024},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2024.131461},
url = {https://www.sciencedirect.com/science/article/pii/S0016236124006094},
author = {Getnet Awoke Kebede and Shih-Che Lo and Fu-Kwun Wang and Jia-Hong Chou},
keywords = {Drop method, Long-short term memory with attention, Remaining useful life prediction, Transfer learning, Variational autoencoder},
abstract = {Proton exchange membrane fuel cells (PEMFCs) offer power generation capabilities for diverse applications including commercial enterprises, industrial sectors, and residential technologies. Nevertheless, the comprehensive integration of PEMFC applications could be improved by challenges related to degradation and durability. The imperative development of efficient performance prognostic models assumes a pivotal role in the prognosis of remaining useful life (RUL), health monitoring, and effective utilization of PEMFCs. This paper centers on the prognostication of critical components within PEMFCs and introduces a transfer learning approach based on variational autoencoder and bi-directional long short-term memory with an attention mechanism (Bi-LSTM-AM) model. This approach combines feature fusion, knee-point detection, and a sophisticated deep-learning-based predictive model. Notably, incorporating the variational autoencoder as the framework for feature fusion introduces a novel perspective previously unexplored. Identifying the knee point and knowing the start point on the training data, facilitates optimized parameter computation. The application of transfer learning facilitates the transfer of optimal model parameters and weights from a source to a target dataset. Conclusively, the estimation of stack voltage degradation and real-time RUL prediction based on the test dataset is executed by implementing our proposed method. The stack voltage prediction findings showcase the Bi-LSTM-AM model’s superior performance relative to comparison models. The proposed online rolling prediction model, utilizing a sliding window technique for RUL prediction, yields significantly enhanced accuracy, culminating in a relative error margin ranging from approximately 1.69% to 5.04%.}
}
@incollection{MEHTA2025549,
title = {16 - State of the art in machine learning for the purpose of optimizing and predicting the properties of polymeric nanocomposites},
editor = {Alokesh Pramanik and Animesh Basak and Yu Dong and Chander Prakash and J. Paulo Davim},
booktitle = {Nanocomposite Manufacturing Technologies},
publisher = {Woodhead Publishing},
pages = {549-573},
year = {2025},
series = {Woodhead Publishing Reviews: Mechanical Engineering Series},
isbn = {978-0-12-824329-9},
doi = {https://doi.org/10.1016/B978-0-12-824329-9.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243299000164},
author = {Amrinder Mehta and Hitesh Vasudev and Chander Prakash and Alokesh Pramanik and Animesh Basak and S. Shankar},
keywords = {Nanocomposite materials, machine learning, polymeric nanocomposites, thermal properties, nanofiller, matrix},
abstract = {Polymer nanocomposites are made up of a continuous matrix phase and a nano-reinforcement phase that is spread throughout the matrix. The mechanical, electrical, and thermal properties of these materials have seen major advancements as a result of these materials. They are currently put to use in a wide number of technological applications, some of which may be found in the automotive, aeronautical, aerospace, maritime, and civil sectors, respectively. Aspect ratio, geometry, size, orientation, and dispersion are examples of some of the factors that contribute to the reinforcing effect of the nanofiller. The processes of melt-blending, compression molding, solution processing, and in-situ polymerization are the ones that are utilized most commonly when it comes to the creation of polymeric nanocomposites. In the field of material science, the creation of raw computational tools for use in the design of innovative materials has been largely superseded by the use of coupled approaches. Machine learning (ML) is a subset of artificial intelligence that allows computers to automatically improve themselves by learning from their previous mistakes and obtaining new information. This is accomplished through a process known as “machine learning.” ML makes it possible to successfully analyze the behavior of the produced composites by using a wider number of different approaches. In the case of polymeric nanocomposites, we are able to make educated guesses regarding a wide variety of multifunctional features. Because it is educated on enormous amounts of data, ML also helps to keep the cost of the models down.}
}
@article{LEPP2025100642,
title = {Does generative AI help in learning programming: Students’ perceptions, reported use and relation to performance},
journal = {Computers in Human Behavior Reports},
volume = {18},
pages = {100642},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2025.100642},
url = {https://www.sciencedirect.com/science/article/pii/S2451958825000570},
author = {Marina Lepp and Joosep Kaimre},
keywords = {Artificial Intelligence (AI), Programming education, Higher education, Student perceptions, Academic performance},
abstract = {In 2022, the release of ChatGPT marked a significant advancement in the use of Artificial Intelligence (AI) chatbots, particularly impacting fields like computer science and education. The ability to generate code snippets using AI chatbots has introduced new opportunities and challenges in teaching programming. However, there is limited agreement on how students integrate them into their learning processes. This study aims to explore how students utilize AI chatbots in the "Object-Oriented Programming" course and examine the relationship between chatbot usage and academic performance. To address this, 231 students completed a survey assessing the frequency and manner of chatbot usage. Descriptive statistical methods were employed to analyze usage and perceptions, while Spearman's correlation was used to investigate the connection between chatbot usage and course performance. Results indicated that students primarily relied on AI chatbots for programming tasks. Interestingly, students' performance negatively correlates with the reported frequency of using these tools. These findings provide valuable insights for programming educators, offering a better understanding of students' perceptions and use of AI chatbots. This knowledge can inform strategies for integrating these tools effectively into computer science education.}
}
@article{SWARTZ2004773,
title = {A multimethod approach to the combat air forces mix and deployment problem},
journal = {Mathematical and Computer Modelling},
volume = {39},
number = {6},
pages = {773-797},
year = {2004},
note = {Defense transportation: Algorithms, models, and applications for the 21st century},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(04)90554-7},
url = {https://www.sciencedirect.com/science/article/pii/S0895717704905547},
author = {S.M Swartz and A.W Johnson},
keywords = {Multiattribute decision analysis, Ranking and selection, Heuristics},
abstract = {The purpose of military logistics is to ensure that the material elements of combat capability come together at the right place and time and in the right configuration to be useful to the supported commander. These material elements are constrained in both quantity and location. The usefulness of any element to a commander is dependent upon both its extrinsic (qualitative; situation dependent) and intrinsic (quantitative; inherent) characteristics. Our research provides a methodology for rationally assigning relative value to material resources over time, in order to improve the linkage between what arrives (becomes available for use) in theater at any given time, and what is actually needed at that time. A blend of qualitative (value focused thinking and hierarchical weighting) and quantitative (a greedy matching algorithm) methods were used against the lift-constrained combat forces material selection/movement problem. The intent is to provide a decision support tool for the formulation of force mixes that best support desired time-phased battlefield objectives, given constraints on available transportation resources. This methodology is applicable to general crisis response planning, such as for disaster relief.}
}
@article{LI2021151508,
title = {Reverse vaccinology approach for the identifications of potential vaccine candidates against Salmonella},
journal = {International Journal of Medical Microbiology},
volume = {311},
number = {5},
pages = {151508},
year = {2021},
issn = {1438-4221},
doi = {https://doi.org/10.1016/j.ijmm.2021.151508},
url = {https://www.sciencedirect.com/science/article/pii/S1438422121000370},
author = {Jie Li and Jingxuan Qiu and Zhiqiang Huang and Tao Liu and Jing Pan and Qi Zhang and Qing Liu},
keywords = {, Reverse vaccinology, Computational model, Vaccine target, Immunoprotective},
abstract = {Salmonella is a leading cause of foodborne pathogen which causes intestinal and systemic diseases across the world. Vaccination is the most effective protection against Salmonella, but the identification and design of an effective broad-spectrum vaccine is still a great challenge, because of the multi-serotypes of Salmonella. Reverse vaccinology is a new tool to discovery and design vaccine antigens combining human immunology, structural biology and computational biology with microbial genomics. In this study, reverse vaccinology, an in-silico approach was established to screen appropriate immunogen targets by calculating the immunogenicity score of 583 non-redundant outer membrane and secreted proteins of Salmonella. Herein among 100 proteins identified with top-ranked scores, 15 representative antigens were selected randomly. Applying the sequence conservation test, four proteins (FliK, BcsZ, FhuA and FepA) remained as potential vaccine candidates for in vivo evaluation of immunogenicity and immunoprotection. All four candidates were capable to trigger the immune response and stimulate the production of antiserum in mice. Furthermore, top-ranked proteins including FliK and BcsZ provided wide antigenic coverage among the multi-serotype of Salmonella. The S. Typhimurium LT2 challenge model used in mice immunized with FliK and BcsZ showed a high relative percentage survival (RPS) of 52.74 % and 64.71 % respectively. In conclusion, this study constructed an in-silico pipeline able to successfully pre-screen the vaccine targets characterized by high immunogenicity and protective immunity. We show that reverse vaccinology allowed screening of appropriate broad-spectrum vaccines for Salmonella.}
}
@article{LIU2023115384,
title = {Trajectory planning for unmanned surface vehicles in multi-ship encounter situations},
journal = {Ocean Engineering},
volume = {285},
pages = {115384},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115384},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823017687},
author = {Jianjian Liu and Huizi Chen and Shaorong Xie and Yan Peng and Dan Zhang and Huayan Pu},
keywords = {Tordsdrajectory planning, Collision avoidance, Velocity obstacle, Multiship encounters, COLREGS},
abstract = {Unmanned surface vehicles (USVs) can encounter traffic ships while navigating toward the target location. For the USVs, collision avoidance (CA) trajectories need to be planned according to the international regulations for preventing collisions at sea (COLREGS). A novel trajectory planning approach is proposed for the collision-free trajectories planning of USVs in the case of multiship encounters. Unlike the existing trajectory planning approaches, the proposed approach uses the holistic thinking to simplify the analysis of encounter situations. Ships approaching from all sides of the USV are treated as one or two equivalent obstacles based on consistent offset velocity direction (COVD) method. Furthermore, planned velocity is designed using the proposed CA strategy and kinematic constraints. This strategy is compliant with COLREGS and includes an emergency CA module to further ensure a safe distance between the USV and traffic ships. The performance of the proposed trajectory planning approach is verified through physical simulations using an existing simulator. The simulation results show that the proposed trajectory planning approach can implement multiple USVs to simultaneously avoid collisions and reach their respective target positions. Moreover, the approach remains effective when other USVs do not follow the COLREGS protocols.}
}
@article{DAVIES2023100692,
title = {Idea generation and knowledge creation through maker practices in an artifact-mediated collaborative invention project},
journal = {Learning, Culture and Social Interaction},
volume = {39},
pages = {100692},
year = {2023},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2023.100692},
url = {https://www.sciencedirect.com/science/article/pii/S2210656123000089},
author = {Sini Davies and Pirita Seitamaa-Hakkarainen and Kai Hakkarainen},
keywords = {Constructionism, Design practices, Engineering practices, Epistemic object, Epistemic practices, Knowledge creation, Learning by making, Material mediation},
abstract = {This investigation involved carrying out interventions that engaged teams of lower-secondary (13–14-year-old) Finnish students in using traditional and digital fabrication technologies to make materially embodied collaborative inventions. By relying on video data and ethnographic observations of the student teams' collaborative invention processes, the investigation focused on investigating 1) how the teams generated and developed their design ideas in their materially anchored making process and 2) what kinds of maker practices they relied on during open-ended invention projects. The study focused on a microanalytic study of three teams of students, and we utilized and developed visual data analysis methods. Our findings reveal the complex nature of the student teams' materially contextualized ideation and the knowledge creation activities that took place within their projects. The findings suggest that open-ended, materially mediated co-invention projects offer ample opportunities for creative cultural participation and practice-based knowledge creation in schools.}
}
@article{NAKAMURA2021198,
title = {Explanation of emotion regulation mechanism of mindfulness using a brain function model},
journal = {Neural Networks},
volume = {138},
pages = {198-214},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100037X},
author = {Haruka Nakamura and Yoshimasa Tawatsuji and Siyuan Fang and Tatsunori Matsui},
keywords = {Emotion regulation in mindfulness, Mechanism, Brain function model, Top-down, Bottom-up},
abstract = {The emotion regulation mechanism of mindfulness plays an important role in the stress reduction effect. Many researchers in the fields of cognitive psychology and cognitive neuroscience have attempted to elucidate this mechanism by documenting the cognitive processes that occur and the neural activities that characterize each process. However, previous findings have not revealed the mechanism of information propagation in the brain that achieves emotion regulation during mindfulness. In this study, we constructed a functional brain model based on its anatomical network structure and a computational model representing the propagation of information between brain regions. We then examined the effects of mindfulness meditation on information propagation in the brain using simulations of changes in the activity of each region. These simulations of changes represent the degree of processing resource allocation to the neural activity via changes in the weights of each region’s output. As a result of the simulations, we reveal how the neural activity characteristic of emotion regulation in mindfulness, which has been reported in previous studies, is realized in the brain. Mindfulness meditation increases the weight of the output from each region of the thalamus and sensory cortex, which processes sensory stimuli from the external world. This sensory information activates the insula and anterior cingulate cortex (ACC). The orbitofrontal cortex and dorsolateral prefrontal cortex inhibit amygdala activity (i.e., top-down emotion regulation). However, when mindfulness meditation dominates bottom-up processing via sensory stimuli from the external world, amygdala activity increases through the insula and ACC activation.}
}
@article{BERGER2024537,
title = {Enmeshed with the digital: satellite navigation and the phenomenology of drivers’ spaces},
journal = {Mobilities},
volume = {19},
number = {3},
pages = {537-555},
year = {2024},
issn = {1745-0101},
doi = {https://doi.org/10.1080/17450101.2023.2285304},
url = {https://www.sciencedirect.com/science/article/pii/S1745010123001431},
author = {Viktor Berger},
keywords = {Satellite navigation, GPS, driving, automobilities, Merleau-Ponty, hybrid spaces, mesh, mediatization},
abstract = {This paper aims to develop a theoretical interpretation of how satellite navigation transforms drivers’ experience of automotive spaces. The use of satellite navigation has, so far, been predominantly studied from a cognitivist perspective based on the computer model of cognition and the theory of spatial disengagement. Experimental studies have concluded that over-reliance on digital navigation tools diminishes spatial orientation and spatial memory. According to the dominant interpretation, satellite navigation causes disengagement from space. After addressing these approaches, the paper introduces an embodied perspective of satellite navigation. This is accomplished by applying the phenomenology of perception of Maurice Merleau-Ponty, whose notions, such as perception, body schema, motor habit, and virtual body, illuminate otherwise undertheorized dimensions of drivers’ spaces. By using digital tools for wayfinding, drivers’ body schema, virtual body, and perception of space are modified, thereby enabling an engagement with convoluted ‘mesh spaces.’ This new term is integral to the interpretation of drivers’ spaces, as well as being distinct from that of ‘hybrid space,’ although both aim to conceptualize spaces, including physical objects and their visual representations. Conclusions will be drawn against the broader context of the mediatization of everyday life.}
}
@article{HE2025110716,
title = {The comprehensive safety assessment method for complex construction crane accidents based on scenario analysis – A case study of crane accidents},
journal = {Computers & Industrial Engineering},
volume = {199},
pages = {110716},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110716},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224008386},
author = {Wei He and Zelong Lin and Wei Li and CJ Wong and Dewei Kong and W.M. Edmund Loh},
keywords = {Scenario Analysis Theory, Improved Bayesian Network, Crane Accidents, Safety Assessment, Emergency Management},
abstract = {Crane accidents pose a significant safety hazard in the infrastructure construction process, making a scientifically reliable safety assessment crucial. Addressing the limitations of traditional methods in adequately considering the complexity of crane accidents, this study proposes a safety assessment model based on Scenario Analysis Theory (SAT) and an improved Bayesian Network (BN) algorithm. The model constructs accident scenario elements, utilizes improved BN to model influencing factors and their interactions, and designs safety assessment functions for a quantitative analysis of crane accident safety. This study demonstrates that the proposed safety assessment model more comprehensively reflects the dynamic evolution of crane accidents. It provides more accurate and interpretable assessment outcomes, significantly aiding in risk prediction and decision-making for emergency management. Key stakeholders, including site management teams, and regulatory bodies, can leverage these findings to enhance emergency management capabilities and reduce the risk of accidents in construction projects.}
}
@article{RECANATINI20208653,
title = {Drug Research Meets Network Science: Where Are We?},
journal = {Journal of Medicinal Chemistry},
volume = {63},
number = {16},
pages = {8653-8666},
year = {2020},
issn = {1520-4804},
doi = {https://doi.org/10.1021/acs.jmedchem.9b01989},
url = {https://www.sciencedirect.com/science/article/pii/S1520480420001702},
author = {Maurizio Recanatini and Chiara Cabrelle},
abstract = {Network theory provides one of the most potent analysis tools for the study of complex systems. In this paper, we illustrate the network-based perspective in drug research and how it is coherent with the new paradigm of drug discovery. We first present data sources from which networks are built, then show some examples of how the networks can be used to investigate drug-related systems. A section is devoted to network-based inference applications, i.e., prediction methods based on interactomes, that can be used to identify putative drug–target interactions without resorting to 3D modeling. Finally, we present some aspects of Boolean networks dynamics, anticipating that it might become a very potent modeling framework to develop in silico screening protocols able to simulate phenotypic screening experiments. We conclude that network applications integrated with machine learning and 3D modeling methods will become an indispensable tool for computational drug discovery in the next years.
}
}
@article{LI2018122,
title = {Uncertainty, politics, and technology: Expert perceptions on energy transitions in the United Kingdom},
journal = {Energy Research & Social Science},
volume = {37},
pages = {122-132},
year = {2018},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214629617303304},
author = {Francis G.N. Li and Steve Pye},
keywords = {Climate policy, Energy policy, Uncertainty analysis, Decision-making},
abstract = {Energy policy is beset by deep uncertainties, owing to the scale of future transitions, the long-term timescales for action, and numerous stakeholders. This paper provides insights from semi-structured interviews with 31 UK experts from government, industry, academia, and civil society. Participants were asked for their views on the major uncertainties surrounding the ability of the UK to meet its 2050 climate targets. The research reveals a range of views on the most critical uncertainties, how they can be mitigated, and how the research community can develop approaches to better support strategic decision-making. The study finds that the socio-political dimensions of uncertainty are discussed by experts almost as frequently as technological ones, but that there exist divergent perspectives on the role of government in the transition and whether or not there is a requirement for increased societal engagement. Finally, the study finds that decision-makers require a new approach to uncertainty assessment that overcomes analytical limits to existing practice, is more flexible and adaptable, and which better integrates qualitative narratives with quantitative analysis. Policy design must escape from ‘caged’ thinking concerning what can or cannot be included in models, and therefore what types of uncertainties can or cannot be explored.}
}
@article{WILKINS2007635,
title = {Inexpensive fusion methods for enhancing feature detection},
journal = {Signal Processing: Image Communication},
volume = {22},
number = {7},
pages = {635-650},
year = {2007},
note = {"Special Issue on Content-Based Multimedia Indexing and Retrieval"},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2007.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0923596507000732},
author = {Peter Wilkins and Tomasz Adamek and Noel E. O’Connor and Alan F. Smeaton},
keywords = {Feature detection, Data fusion, TRECVID},
abstract = {Recent successful approaches to high-level feature detection in image and video data have treated the problem as a pattern classification task. These typically leverage the techniques learned from statistical machine learning, coupled with ensemble architectures that create multiple feature detection models. Once created, co-occurrence between learned features can be captured to further boost performance. At multiple stages throughout these frameworks, various pieces of evidence can be fused together in order to boost performance. These approaches whilst very successful are computationally expensive, and depending on the task, require the use of significant computational resources. In this paper we propose two fusion methods that aim to combine the output of an initial basic statistical machine learning approach with a lower-quality information source, in order to gain diversity in the classified results whilst requiring only modest computing resources. Our approaches, validated experimentally on TRECVid data, are designed to be complementary to existing frameworks and can be regarded as possible replacements for the more computationally expensive combination strategies used elsewhere.}
}
@article{SURYADI2023730,
title = {”Read on”: comprehending challenging texts at university through gamification App},
journal = {Procedia Computer Science},
volume = {216},
pages = {730-738},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022682},
author = {Phillip Suryadi and Irfan Rifai and Hady Pranoto},
keywords = {gamification, reading, application, students, texts},
abstract = {Despite the common misperception of playing games as wasteful activity, studies found that some of its components may contribute to users’ knowledge generation, soft skill improvement, and foreign language learning. This article reports the development of an application for reading and the initial impacts of the gamification-based application on students’ reading comprehension in English. The application was aimed to support generation Z's university students who are well exposed to gadgets with the ability in comprehending challenging texts. In addition to the sociocultural theory of learning and second language acquisition theories, we considered factors like university students as users, texts’ complexity offered at the university level, and gamification features in designing the application. The study resulted in a prototype of a gaming activity called” ReadOn”. Surveys, interviews, and experiments were carried out on a small group of participants during, and after designing processes. The Survey data was used as a foundation to design the app while the interview and the experiments provided data to explore the usability of the newly built prototype. The data of students’ experience in using the prototype was used as feedback for future development of the platform.}
}
@article{GANNON2025R152,
title = {Motion integration: A case of misdirection},
journal = {Current Biology},
volume = {35},
number = {4},
pages = {R152-R154},
year = {2025},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2025.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0960982225000168},
author = {Sara M. Gannon and Lindsey L. Glickfeld},
abstract = {Summary
Integrating complex motion signals from the environment is essential for behavior. A recent study in the mouse has revealed that both encoding in the superior colliculus and the optokinetic reflex follow a novel motion integration rule.}
}
@article{BELLA2023123268,
title = {Vibrationally resolved deep–red circularly polarised luminescence spectra of C70 derivative through Gaussian curvature analysis of ground and excited states},
journal = {Journal of Molecular Liquids},
volume = {391},
pages = {123268},
year = {2023},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2023.123268},
url = {https://www.sciencedirect.com/science/article/pii/S0167732223020743},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Fullerene, Chirality, Curvature, Vibronic, Circularly polarized luminescence},
abstract = {Over the last years, the interaction of fullerene with circularly polarized light has attracted growing attention for potential electronic and optical applications. However, in literature there is only one example of fullerene derivative capable of emitting circularly polarized light, showing an active circularly polarized luminescence (CPL) signal in the deep-red visible region. This unique luminophore offered us the possibility to study the connection between the topological features of C70 spheroid and its chiral emission properties. In light of these considerations, we proposed a theoretical protocol that combines three different step: (1) The Ball Pivoting Algorithm for C70 surface reconstruction. (2) The discrete gaussian curvature analysis in the ground (S0) and excited states (S1). (3) The computation of the vibrationally-resolved CPL spectrum. The first step allowed us to extract useful information that linked the topological shape of C70 to the sp2 carbon network chemistry. The DFT benchmark in the second step guided us in grasping the best functional for the C70 curvature simulation in the ground state, spotlighting how B97D3 excellently succeed for this task. The curvature investigation in the first excited state showed that (for all the twenty exchange–correlation functional tested) the C70 fragment is more curved in S1 than in S0. The final step collected the topological information from the previous sections to provide a detailed overview of the theoretical factors (such as the quantum formalism, the potential energy surface description and the transition dipole moment approximation) impacting on the C70 vibrationally resolved CPL spectrum. We found that the adiabatic hessian model coupled with the Franck-Condon Herzberg-Teller approximation computed at PW6B95D3/6-311G(d,p) level provides excellent results in emulating the band-shape and position of the experimental CPL spectrum.}
}
@article{TERZIYAN20242540,
title = {Can ChatGPT Challenge the Scientific Impact of Published Research, Particularly in the Context of Industry 4.0 and Smart Manufacturing?},
journal = {Procedia Computer Science},
volume = {232},
pages = {2540-2550},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002497},
author = {Vagan Terziyan and Olena Kaikova and Mariia Golovianko and Oleksandra Vitko},
keywords = {Artificial Intelligence, ChatGPT, Industry 4.0, Smart Manufacturing, academic impact},
abstract = {The released ChatGPT as a powerful language model is capable of assisting with a wide range of tasks, including answering questions, summarizing, paraphrasing, proofreading, classifying, and integrating texts. In this study, we tested ChatGPT capability to assist researchers in evaluating the academic articles’ contribution. We suggest a dialogue schema in which ChatGPT is asked to answer research questions from the target article and then to compare its own answers with the answers from the article. Finally, ChatGPT is asked to integrate both solutions coherently. We experimented with Proceedings of ISM-2022 Conference on Industry 4.0 and Smart Manufacturing, utilizing explicit research questions. The chat context enabled assessing studied articles’ contributions to Industry 4.0, uncovering advancements beyond the state-of-the-art. However, ChatGPT demonstrates limitations in content understanding and contribution evaluation. We conclude that while it collaborates with humans on academic tasks, human guidance remains essential, while ChatGPT's assistance efficiently complements traditional academic processes.}
}
@article{LOW2020e03083,
title = {Induction approach via P-Graph to rank clean technologies},
journal = {Heliyon},
volume = {6},
number = {1},
pages = {e03083},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e03083},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019367428},
author = {C.X. Low and W.Y. Ng and Z.A. Putra and K.B. Aviso and M.A.B. Promentilla and R.R. Tan},
keywords = {Chemical engineering, Optimal selection, Simple additive weighting, Clean technologies, Induction, Decision analysis, P-Graph},
abstract = {Identification of appropriate clean technologies for industrial implementation requires systematic evaluation based on a set of criteria that normally reflect economic, technical, environmental and other aspects. Such multiple attribute decision-making (MADM) problems involve rating a finite set of alternatives with respect to multiple potentially conflicting criteria. Conventional MADM approaches often involve explicit trade-offs in between criteria based on the expert's or decision maker's priorities. In practice, many experts arrive at decisions based on their tacit knowledge. This paper presents a new induction approach, wherein the implicit preference rules that estimate the expert's thinking pathways can be induced. P-graph framework is applied to the induction approach as it adds the advantage of being able to determine both optimal and near-optimal solutions that best approximate the decision structure of an expert. The method elicits the knowledge of experts from their ranking of a small set of sample alternatives. Then, the information is processed to induce implicit rules which are subsequently used to rank new alternatives. Hence, the expert's preferences are approximated by the new rankings. The proposed induction approach is demonstrated in the case study on the ranking of Negative Emission Technologies (NETs) viability for industry implementation.}
}
@incollection{ALEKSANDER200599,
title = {Machine consciousness},
editor = {Steven Laureys},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {150},
pages = {99-108},
year = {2005},
booktitle = {The Boundaries of Consciousness: Neurobiology and Neuropathology},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(05)50008-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612305500086},
author = {Igor Aleksander},
abstract = {The work from several laboratories on the modeling of consciousness is reviewed. This ranges, on one hand, from purely functional models where behavior is important and leads to an attribution of consciousness to, on the other hand, material work closely derived from the information about the anatomy of the brain. At the functional end of the spectrum, applications are described specifically directed at a job-finding problem, where the person being served should not discern between being served by a conscious human or a machine. This employs an implementation of global workspace theories. At the material end, attempts at modeling attentional brain mechanisms, and basic biochemical processes in children are discussed. There are also general prescriptions for functional schemas that facilitate discussions for the presence of consciousness in computational systems and axiomatic structures that define necessary architectural features without which it would be difficult to represent sensations. Another distinction between these two approaches is whether one attempts to model phenomenology (material end) or not (functional end). The former is sometimes called “synthetic phenomenology.” The upshot of this chapter is that studying consciousness through the design of machines is likely to have two major outcomes. The first is to provide a wide-ranging computational language to express the concept of consciousness. The second is to suggest a wide-ranging set of computational methods for building competent machinery that benefits from the flexibility of conscious representations.}
}
@article{ZHANG2025127717,
title = {CCMA: A framework for cascading cooperative multi-agent in autonomous driving merging using Large Language Models},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127717},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127717},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425013399},
author = {Miao Zhang and Zhenlong Fang and Tianyi Wang and Shuai Lu and Xueqian Wang and Tianyu Shi},
keywords = {Large Language Model, Autonomous driving, Reinforcement Learning, In-context learning, Multi-agent system, Cooperative merging},
abstract = {Traditional Reinforcement Learning (RL) suffers from challenges in replicating human-like behaviors, generalizing effectively in multi-agent scenarios, and overcoming inherent interpretability issues. These tasks become even more difficult when they require a deep understanding of the environment, coordination of agents’ intentions and driving styles across various scenarios, and the overall optimization of safety, efficiency, and comfort in dynamic environments. Recently, Large Language Model (LLM) enhanced methods have shown promise in improving generalization and interoperability. However, these approaches primarily focus on single-agent scenarios and often neglect the necessary coordination among multiple road users. Therefore, in this paper, we introduce the Cascading Cooperative Multi-agent (CCMA) framework, designed to address these challenges by enhancing human-like behaviors and fostering multi-level cooperation across diverse multi-agent driving tasks, ultimately improving both micro and macro-level performance in complex driving environments. Specifically, the CCMA framework integrates RL for individual interactions, a fine-tuned LLM for regional cooperation, a reward function for global optimization, and the Retrieval-augmented Generation mechanism to dynamically optimize decision-making across complex driving scenarios. Our experiments demonstrate that our CCMA method not only enhances human-like behaviors and interpretability, but also outperforms other state-of-the-art RL methods in multi-agent environments. These findings highlight the significant impact of cascading coordinated communication and dynamic functional alignment in advanced, human-like multi-agent autonomous driving environments. Our project page is https://miaorain.github.io/rainrun.github.io/.}
}
@article{ISLAM2025,
title = {Memory-enhancing effects of daidzin, possibly through dopaminergic and AChEergic dependent pathways},
journal = {The Journal of Nutrition},
year = {2025},
issn = {0022-3166},
doi = {https://doi.org/10.1016/j.tjnut.2025.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S002231662500269X},
author = {Muhammad Torequl Islam and Abdullah {Al Shamsh Prottay} and Md. Shimul Bhuia and Md. Showkot Akbor and Raihan Chowdhury and Siddique Akber Ansari and Irfan Aamer Ansari and Md. Amirul Islam and Catarina Martins Tahim and Henrique Douglas {Melo Coutinho}},
keywords = {Daidzin, memory-enhancing capacity, dopamine receptors, AChEergic interaction, molecular docking},
abstract = {The soy isoflavone daidzin (DZN) possesses cognitive-enhancing effects in animals. However, the mechanism for this effect is yet to be discovered. For this, we investigate its memory-enhancing capacity using the mouse models of marble burying, dust removal, an open-field study, and in silico studies. Adult male Swiss albino mice were randomly divided into different groups consisting of Control (vehicle: 10 mL/kg), DZN 5, 10, and 20 mg/kg, dopamine (agonist: 22 mg/kg), galantamine (inhibitor: 3 mg/kg), and a combination of DZN-10 with standards. DZN dose-dependently and significantly (p <0.05) increased marble burying and removed dust while decreasing the total distance in OFT. DZN-10 enhanced dopamine’s effect significantly (p < 0.05). In silico findings suggest that DZN has strong binding capacities of –10.3, –7.5, –9.8, and –9.2 kcal/mol to the AChE, D1, D3, and D5 receptors, respectively. Taken together, DZN may exert its memory-enhancing ability by interacting with AChE and dopamine receptors.}
}
@article{GROOTHUIJSEN2024100290,
title = {AI chatbots in programming education: Students’ use in a scientific computing course and consequences for learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100290},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100290},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000936},
author = {Suzanne Groothuijsen and Antoine {van den Beemt} and Joris C. Remmers and Ludo W. {van Meeuwen}},
keywords = {AI chatbots, ChatGPT, Programming education, Pair programming, Student learning, Engineering education},
abstract = {Teaching and learning in higher education require adaptation following students' inevitable use of AI chatbots. This study contributes to the empirical literature on students' use of AI chatbots and how they influence learning. The aim of this study is to identify how to adapt programming education in higher engineering education. A mixed-methods case study was conducted of a scientific computing course in a Mechanical Engineering Master's program at a Eindhoven University of Technology in the Netherlands. Data consisted of 29 student questionnaires, a semi-structured group interview with three students, a semi-structured interview with the teacher, and 29 students' grades. Results show that students used ChatGPT for error checking and debugging of code, increasing conceptual understanding, generating, and optimizing solution code, explaining code, and solving mathematical problems. While students reported advantages of using ChatGPT, the teacher expressed concerns over declining code quality and student learning. Furthermore, both students and teacher perceived a negative influence from ChatGPT usage on pair programming, and consequently on student collaboration. The findings suggest that learning objectives should be formulated in more detail, to highlight essential programming skills, and be expanded to include the use of AI tools. Complex programming assignments remain appropriate in programming education, but pair programming as a didactic approach should be reconsidered in light of the growing use of AI Chatbots.}
}
@article{FARISCO2024106714,
title = {Is artificial consciousness achievable? Lessons from the human brain},
journal = {Neural Networks},
volume = {180},
pages = {106714},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106714},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006385},
author = {Michele Farisco and Kathinka Evers and Jean-Pierre Changeux},
keywords = {Brain, Consciousness, Artificial intelligence, Neuromorphic computing, Robotics, Cognition, Neuroscience},
abstract = {We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model or as a benchmark. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of human-like conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (i.e., structural and architectural) and extrinsic (i.e., related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make human-like conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it cannot be theoretically excluded that AI research can develop partial or potentially alternative forms of consciousness that are qualitatively different from the human form, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word “consciousness” for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify which level and/or type of consciousness AI research aims to develop, as well as what would be common versus differ in AI conscious processing compared to human conscious experience.}
}
@article{SRINIVAS199799,
title = {Strategic decision-making processes: network-based representation and stochastic simulation},
journal = {Decision Support Systems},
volume = {21},
number = {2},
pages = {99-110},
year = {1997},
note = {Special Issue: Expertise and Modeling Expert Decision Making},
issn = {0167-9236},
doi = {https://doi.org/10.1016/S0167-9236(97)00023-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167923697000237},
author = {V. Srinivas and B. Shekar},
keywords = {Qualitative probabilistic networks, Stochastic simulation, Cognitive maps, Strategic thinking, Decision-making process, Network-based representation},
abstract = {Representation of decision-making in organizations is an intricate process. Qualitative Probabilistic Network (QPN)-based approach offers a scheme which is useful for representing processes involved in decision-making. This paper demonstrates the usefulness of QPN-based scheme with an illustrative case study. The focus of the case study is on understanding the strategic behavior of a key player in the Indian Automobile Industry. This is done by transforming Cognitive Maps developed into QPN-based formalisms and analyzing them. In addition to this, stochastic simulation experiment is performed on the QPN-based networks to generate hypothetical scenarios.}
}
@article{PEZZULO2014647,
title = {Internally generated sequences in learning and executing goal-directed behavior},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {12},
pages = {647-657},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2014.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661314001570},
author = {Giovanni Pezzulo and Matthijs A.A. {van der Meer} and Carien S. Lansink and Cyriel M.A. Pennartz},
keywords = {forward sweep, generative models, hippocampus, decision making, reinforcement learning, spatial navigation, replay, inference, prospection, theta rhythm, ventral striatum},
abstract = {A network of brain structures including hippocampus (HC), prefrontal cortex, and striatum controls goal-directed behavior and decision making. However, the neural mechanisms underlying these functions are unknown. Here, we review the role of ‘internally generated sequences’: structured, multi-neuron firing patterns in the network that are not confined to signaling the current state or location of an agent, but are generated on the basis of internal brain dynamics. Neurophysiological studies suggest that such sequences fulfill functions in memory consolidation, augmentation of representations, internal simulation, and recombination of acquired information. Using computational modeling, we propose that internally generated sequences may be productively considered a component of goal-directed decision systems, implementing a sampling-based inference engine that optimizes goal acquisition at multiple timescales of on-line choice, action control, and learning.}
}
@article{SAAVEDRA20091324,
title = {Experimental transition state for the Corey–Bakshi–Shibata reduction},
journal = {Tetrahedron Letters},
volume = {50},
number = {12},
pages = {1324-1327},
year = {2009},
issn = {0040-4039},
doi = {https://doi.org/10.1016/j.tetlet.2009.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0040403909000793},
author = {Jaime Saavedra and Sean E. Stafford and Matthew P. Meyer},
abstract = {Asymmetric reductions of prochiral ketones are important transformations in the syntheses of natural products, pharmaceuticals, and fine chemicals. The Corey–Bakshi–Shibata reduction is unique among hydride transfer reductions in its tremendous substrate range and catalytic nature. Here, a coordinated computational and experimental approach is taken toward understanding the origins of the high selectivity and broad substrate range, which are hallmarks of this reduction.}
}
@article{MULLER2021103546,
title = {Kandinsky Patterns},
journal = {Artificial Intelligence},
volume = {300},
pages = {103546},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000977},
author = {Heimo Müller and Andreas Holzinger},
keywords = {Explainable AI, Explainability, Synthetic test data, Ground truth},
abstract = {Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple, self-contained hence controllable synthetic test data sets for the development, validation and training of visual tasks and explainability in artificial intelligence (AI). Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable by human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an “infallible authority” defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for testing explainability, interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns. We provide a Github repository and invite the international AI research community to a challenge to experiment with our Kandinsky Patterns. The goal is to help expand and advance the field of AI, and in particular to contribute to the increasingly important field of explainable AI.}
}
@article{ERIOLI2011729,
title = {Interwoven landscape},
journal = {Procedia Engineering},
volume = {21},
pages = {729-736},
year = {2011},
note = {2011 International Conference on Green Buildings and Sustainable Cities},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.11.2071},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811049058},
author = {Alessio Erioli and Mirco Bianchini and Piero Bruschi and Andrea Baschieri},
keywords = {architecture, ecology, infrastructure, highway, photocatalysis, dazzle, new materials ;},
abstract = {Human specie has always engineered the environment to set the conditions for its own settlement, producing in its evolutionary development superorganisms (cities) and the necessary networks of connections among them. Instead of rejecting cars as an extraneous object to a picturesque nature, this project starts from a perspective in which cities and technology are the metabolic extension of human specie and therefore a necessary part of its own nature; the vessels (vehicles) for human transportation, or better, the vehicle-host symbiotic system thus becomes a necessary part of human ecology, and so the network of connections upon which they live, operate and interact with: infrastructures. The project of an environmental enhancer for the Nogara mare highway in Veneto (Italy) provides the unique chance to bring together ecological thinking, host interaction and active materials. Its location (an open country planar area among cultivated fields) enucleates as critical variables the impact of pollutants and the phenomenon of dazzling. With respect to such criticalities, the project uses digital generative and parametric strategies to generate a performative structure in which densification and rarefaction of elements is a local morphological response to dazzle. The structure itself acts as a scaffold for a photo catalytic PET based material that, mimicking the behavior of coccoluti (marine microorganisms) is able to reduce CO2 (and potentially other pollutants) to salts and nitrates that are then naturally deployed to the neighboring cultivated fields as fertilizers. The material has been tested for photo catalytic integration and is currently under development. Present building and production techniques privilege the industrial assembly of inert materials, with a one-way flow of energy and process from raw material to finished product. Instead of this mono-directional energy consumption the project promotes the continuous exchange of information (as code and matter-energy) at all levels and from the digital to the material domains: use of dazzle information, morphogenetic rules and structural behavior to generate the scaffold, a photo catalytic material that responds to pollutants and produces fertilizers, making the structure symbiotic with their hosts and the environment.}
}
@article{YIN2024110392,
title = {Embrace sustainable AI: Dynamic data subset selection for image classification},
journal = {Pattern Recognition},
volume = {151},
pages = {110392},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110392},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001432},
author = {Zimo Yin and Jian Pu and Ru Wan and Xiangyang Xue},
keywords = {Data selection, Dynamic subset selection, Weighted sampling, Class distribution, Training efficiency},
abstract = {Data selection is commonly used to reduce costs and energy usage by training on a subset of available data. However, determining the appropriate subset size requires extensive dataset knowledge and experimentation, limiting transferability. Varying the validation set also produces unstable results and wastes computational resources. In this paper, we propose a data selection method for dynamically determining subset ratios based on model performance using only a training set. The data search space is narrowed through weighted sampling, leveraging statistical selection patterns. Parallel analysis of class distributions identifies the most representative samples with high selection potential. Extensive experiments validate our approach and demonstrate improved training efficiency. Our method speeds up various subset ratios by up to 2.2x on CIFAR-10, 1.9x on CIFAR-100, 2.0x on TinyImageNet, and 2.3x on ImageNet with negligible accuracy drops.}
}
@article{WU2024111235,
title = {Intelligent strategic bidding in competitive electricity markets using multi-agent simulation and deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {152},
pages = {111235},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111235},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624000097},
author = {Jiahui Wu and Jidong Wang and Xiangyu Kong},
keywords = {Intelligent bidding strategy, Competitive electricity markets, Multi-agent simulation(MAS), Deep reinforcement learning(DRL), Async n-step QL, Improved Async n-step QL},
abstract = {Aiming at the lack of comprehension of agents in Multi-Agent Simulation (MAS) based on classic Reinforcement Learning algorithms of competitive electricity markets, an intelligent strategic bidding method using Deep Reinforcement Learning (DRL) and MAS is proposed in this paper, which not only can provide more intelligent strategies for market participants to maximize their profits, but can enhance the performance of simulation models dealing with high-dimensional continuous data in electricity markets. Firstly, a theoretical framework of intelligent strategic bidding in competitive electricity markets based on MAS and DRL is proposed, and the process of intelligent bidding in electricity markets based on MAS and DRL is described. Then, three MAS models of intelligent strategic bidding are built based on three classic DRL algorithms, including Deep Q-Network (DQN), Double Deep Q-Network (DDQN), and Asynchronous n-step Q-learning (Async n-step QL), and three algorithms’ convergence speed, computational efficiency, and response sensitivity are compared and analyzed. Finally, a novel Improved Async n-step QL (IAsync n-step QL) algorithm is proposed, the MAS model based on the IAsync n-step QL algorithm for intelligent strategic bidding is established. Simulation results show that the model using the novel DRL algorithm is more profitable and responsive than the classic DRL algorithms.}
}
@article{STEANE2003469,
title = {A quantum computer only needs one universe},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {34},
number = {3},
pages = {469-478},
year = {2003},
note = {Quantum Information and Computation},
issn = {1355-2198},
doi = {https://doi.org/10.1016/S1355-2198(03)00038-8},
url = {https://www.sciencedirect.com/science/article/pii/S1355219803000388},
author = {A.M Steane},
keywords = {Quantum computation, Classical computation, Parallel universes, Entanglement},
abstract = {The nature of quantum computation is discussed. It is argued that, in terms of the amount of information manipulated in a given time, quantum and classical computation are equally efficient. Quantum superposition does not permit quantum computers to “perform many computations simultaneously” except in a highly qualified and to some extent misleading sense. Quantum computation is therefore not well described by interpretations of quantum mechanics which invoke the concept of vast numbers of parallel universes. Rather, entanglement makes available types of computation processes which, while not exponentially larger than classical ones, are unavailable to classical systems. The essence of quantum computation is that it uses entanglement to generate and manipulate a physical representation of the correlations between logical entities, without the need to completely represent the logical entities themselves.}
}
@article{LI2025103353,
title = {A multi-task engineering design intention recognition approach based on Vision Transformer and EEG data},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103353},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103353},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002460},
author = {Mingrui Li and Zuoxu Wang and Fan Li and Jihong Liu},
keywords = {Design intention recognition, Engineering design, EEG, Vision Transformer},
abstract = {Engineering product design involves a variety of tasks and scenarios, including design modeling, design calculation, process planning, etc. When performing these design tasks, designers generate constantly shifting design intentions. Accurately recognizing these design intentions allows for a more thorough exploration of design processes from the perspective of cognition, facilitating the advancement of intelligent engineering design. Electroencephalogram (EEG) technology has emerged as an effective tool in recent years, which can provide direct insight into designers’ cognitive processes and intentions. However, the current application of EEG technology in engineering design faces difficulties in adapting to multi-task scenarios and rarely targets the design process directly. This study proposed a design intention recognition approach based on Vision Transformer (ViT) and EEG data applicable to multiple engineering design tasks. An image-like representation matrix is introduced to organize designers’ EEG data with the retention of its spatial and frequency features. Then, standard EEG data under different design intentions as well as the EEG data from real design processes is utilized to train and fine-tune a ViT-based design intention recognition model. An experiment workflow for collecting the two types of EEG data is also presented, along with detailed examples of three design tasks. The comparative experiment results and the case study demonstrates the feasibility of the proposed design intention recognition approach.}
}
@article{PALANIYAPPAN2020109911,
title = {Cortical thickness and formal thought disorder in schizophrenia: An ultra high-field network-based morphometry study},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {101},
pages = {109911},
year = {2020},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2020.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0278584619310309},
author = {Lena Palaniyappan and Ali Al-Radaideh and Penny A. Gowland and Peter F. Liddle},
keywords = {Disorganisation, Thought disorder, Salience network, Cognitive control, Language network, Coherence},
abstract = {Background
Persistent formal thought disorder (FTD) is a core feature of schizophrenia. Recent cognitive and neuroimaging studies indicate a distinct mechanistic pathway underlying the persistent positive FTD (pFTD or disorganized thinking), though its structural determinants are still elusive. Using network-based cortical thickness estimates from ultra-high field 7-Tesla Magnetic Resonance Imaging (7T MRI), we investigated the structural correlates of pFTD.
Methods
We obtained speech samples and 7T MRI anatomical scans from medicated clinically stable patients with schizophrenia (n = 19) and healthy controls (n = 20). Network-based morphometry was used to estimate the mean cortical thickness of 17 functional networks covering the entire cortical surface from each subject. We also quantified the vertexwise variability of thickness within each network to quantify the spatial coherence of the 17 networks, estimated patients vs. controls differences, and related the thickness of the affected networks to the severity of pFTD.
Results
Patients had reduced thickness of the frontoparietal and default mode networks, and reduced spatial coherence affecting the salience and the frontoparietal control network. A higher burden of positive FTD related to reduced frontoparietal thickness and reduced spatial coherence of the salience network. The presence of positive FTD, but not its severity, related to the reduced thickness of the language network comprising of the superior temporal cortex.
Conclusions
These results suggest that cortical thickness of both cognitive control and language networks underlie the positive FTD in schizophrenia. The structural integrity of cognitive control networks is a critical determinant of the expressed severity of persistent FTD in schizophrenia.}
}
@article{YAHIAOUI20243958,
title = {Two parallel expansions for improving supersonic axisymmetric nozzle performance},
journal = {Advances in Space Research},
volume = {74},
number = {8},
pages = {3958-3982},
year = {2024},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2024.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S0273117724006562},
author = {Toufik Yahiaoui},
keywords = {MLN, BPN, DEN, HT, MOC, Error computation},
abstract = {The aim of this work is to develop a numerical computation program allowing designing new contours of a supersonic axisymmetric nozzle having two expansions at the throat, named by DEN (Dual Expansion Nozzle). This new nozzle gives a uniform and parallel flow at the exit section, to improve considerably the performances compared to the conventional Minimum Length Nozzle (MLN), and the Best Performances Nozzle (BPN). The present nozzle has a two unknowns external and central body curved walls. Each of them is started by an initial expansion angle to give a uniform and horizontal flow at the exit section. Two others transition regions are calculated in parallel with the contours points to give the desired exit Mach number. The walls are determined point by point by the High Temperature Method of Characteristics (HT MOC) model. The resolution of the four compatibility and characteristics equations is done numerically by the finite difference predictor corrector algorithm. The validation of the results is controlled by the convergence of the calculated critical sections ratio to that given by the theory. The design depends on four parameters, where MLN and BPN become special cases of DEN. A comparison is made with MLN, since it is currently used in the aerospace propulsion and with BPN aiming to improve their performances. The comparison is made for the same critical mass flow rate. The results demonstrate a remarkable reduction up of 45 %, and 52 % in the mass of DEN when the exit Mach number ME = 3.00 and the stagnation temperature T0 = 2000 K. The application is made for air and for future aerospace missiles in order to improve their trajectory parameters. The chosen example demonstrates an improvement of 13 % and 16 % on the missile range compared, respectively to MLN, and BPN.}
}
@article{LI2025113564,
title = {Exploring formal defeasible reasoning of large language models: A Chain-of-Thought approach},
journal = {Knowledge-Based Systems},
volume = {319},
pages = {113564},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113564},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125006100},
author = {Zhaoqun Li and Chen Chen and Mengze Li and Beishui Liao},
keywords = {Defeasible reasoning, Large language model, Defeasible logic programming},
abstract = {Defeasible reasoning, critical for commonsense reasoning and uncertainty handling, has garnered significant attention in AI community. This interest is particularly pronounced in the development and evaluation of Large Language Models (LLMs), which often involve reconciling inconsistent and incomplete knowledge. However, it remains uncertain whether LLMs possess generalizable defeasible reasoning abilities. Besides, the lack of a formal defeasible reasoning benchmark and appropriate evaluations limits further exploration in this domain. In this study, we aim to investigate the capacity of LLMs for defeasible reasoning, particularly within the framework of formal defeasible logic. Specifically, we select the popular defeasible logic framework, DeLP, as the basis for evaluating the LLMs’ defeasible logical reasoning capabilities. We initially create a synthetic dataset comprising logical programs that encompass a variety of DeLP programs with differing depths of reasoning. To address the challenges encountered during inference, we introduce a Chain-of-Thought (CoT) framework that prompts LLMs to conduct formal deduction and engage in multi-step defeasible reasoning, thereby enhancing problem-solving performance. Employing this argumentative solving approach, we observe that LLMs struggle to manage defeasible information effectively. This observation raises questions about whether contemporary LLMs possess reasoning abilities comparable to human intelligence, challenging the reliable deployment and advancement of AI systems in real-world scenarios.}
}
@incollection{MARCHAND201831,
title = {Chapter 2 - Analogical Mapping in Numerical Development},
editor = {Daniel B. Berch and David C. Geary and Kathleen {Mann Koepke}},
booktitle = {Language and Culture in Mathematical Cognition},
publisher = {Academic Press},
pages = {31-47},
year = {2018},
series = {Mathematical Cognition and Learning},
isbn = {978-0-12-812574-8},
doi = {https://doi.org/10.1016/B978-0-12-812574-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812574800002X},
author = {Elisabeth Marchand and David Barner},
keywords = {Analogy, Number acquisition, Successor function, Numerical estimation, Structure mapping},
abstract = {This chapter outlines the contribution of analogical thinking in numerical cognition and specifically to number-word learning and numerical estimation. We begin with an overview of number-word learning, followed by a description of analogical mapping as defined by Gentner, 1983, Gentner, 2010, and discuss how children might acquire the meaning of counting based on analogical mapping. Next, we review the claim that very similar processes of analogical mapping may support numerical estimation, based on findings from studies of dot-array and number-line estimation. These studies suggest that children's knowledge of how the count list is structured and in particular the ordering and distance between numbers affects their ability to make accurate estimates. Finally, we discuss extensions of this idea to other cases where analogy has been proposed as a source of representational change. We conclude that analogical mappings enrich how humans transcend core numerical abilities to represent abstract content.}
}
@article{ALONSO2025100509,
title = {A novel approach for job matching and skill recommendation using transformers and the O*NET database},
journal = {Big Data Research},
volume = {39},
pages = {100509},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000048},
author = {Rubén Alonso and Danilo Dessí and Antonello Meloni and Diego {Reforgiato Recupero}},
keywords = {Information extraction, Transformers, Online enrolling process, Natural language processing, Course recommendation},
abstract = {Today we have tons of information posted on the web every day regarding job supply and demand which has heavily affected the job market. The online enrolling process has thus become efficient for applicants as it allows them to present their resumes using the Internet and, as such, simultaneously to numerous organizations. Online systems such as Monster.com, OfferZen, and LinkedIn contain millions of job offers and resumes of potential candidates leaving to companies with the hard task to face an enormous amount of data to manage to select the most suitable applicant. The task of assessing the resumes of candidates and providing automatic recommendations on which one suits a particular position best has, therefore, become essential to speed up the hiring process. Similarly, it is important to help applicants to quickly find a job appropriate to their skills and provide recommendations about what they need to master to become eligible for certain jobs. Our approach lies in this context and proposes a new method to identify skills from candidates' resumes and match resumes with job descriptions. We employed the O*NET database entities related to different skills and abilities required by different jobs; moreover, we leveraged deep learning technologies to compute the semantic similarity between O*NET entities and part of text extracted from candidates' resumes. The ultimate goal is to identify the most suitable job for a certain resume according to the information there contained. We have defined two scenarios: i) given a resume, identify the top O*NET occupations with the highest match with the resume, ii) given a candidate's resume and a set of job descriptions, identify which one of the input jobs is the most suitable for the candidate. The evaluation that has been carried out indicates that the proposed approach outperforms the baselines in the two scenarios. Finally, we provide a use case for candidates where it is possible to recommend courses with the goal to fill certain skills and make them qualified for a certain job.}
}
@article{PIRES2024625,
title = {Selection of Naval Bases and Stations for submarines: a multimethodological approach},
journal = {Procedia Computer Science},
volume = {242},
pages = {625-632},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018386},
author = {Tullio Pires and Celio Manso {de Azevedo Junior} and Mateus Vanzetta and Marcos {dos Santos} and Carlos Francisco {Simões Gomes}},
keywords = {Submarines, Naval Base, Multicriteria, MPSI-MARA, SCA},
abstract = {With the PROSUB program, the Brazilian Navy (MB) has been renewing its feet of submarines. However, this is not a movement that is exclusively Brazilian. With the worsening of crises around the world, many countries are in the process of expanding their armed forces, and coastal nations, in particular, are paying significant attention to their submarine weapons. However, as it is not only convenient to acquire submarines but also to operate them, it is necessary to define from where they will do so. Given the above, this current work aims to present a framework with a multimethodological focus, that is, presenting a combined use of the problem structuring method (PSM) Strategic Choice Approach with the multicriteria method MPSI-MARA. As a result, the ordering of some points along the Brazilian coast, made non-specific, is presented as a suggestion for the implementation of new Submarine Bases and/or Naval Support Stations.}
}
@article{KWIATKOWSKA201335,
title = {Fuzzy logic and semiotic methods in modeling of medical concepts},
journal = {Fuzzy Sets and Systems},
volume = {214},
pages = {35-50},
year = {2013},
note = {Soft Computing in the Humanities and Social Sciences},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2012.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0165011412001376},
author = {Mila Kwiatkowska and Krzysztof Kielan},
keywords = {Fuzzy system models, Medicine, Cognitive sciences, Decision support systems, Depression},
abstract = {The field of medicine is a quickly growing area of application for computer-based systems. However, the use of computerized methods in this knowledge-intensive and expert-based discipline brings multiple challenges. The major problem is the modeling, representing, and interpreting of diverse medical concepts. For example, some symptoms and their etiologies are described in terms of molecular biology and genetics, physiological processes are defined using models from chemistry and physics; yet mental disorders are defined in more subjective terms of feelings, behaviours, habits, and life events. Thus, the representation of medical concepts must be sufficiently expressive to model concepts which are inherently complex, context-dependent, evolving, and often imprecise. Furthermore, the representation must be formal or, at least, sufficiently rigorous in order to be processed by computers and at the same time, the representation must be human-readable in order to be validated by humans. In this paper, we describe the modeling process of medical concepts as a mapping from the real-world medical concepts into their computational models, and further into their physical implementation. First, we define the notion of a concept as a fundamental unit of knowledge and specify the fundamental principles of the computational representation of a concept. Second, we describe the characteristics of medical concepts, specifically their historical and cultural changeability, their social and cultural ambiguity, and their varied levels of precision. Third, we present a meta-modeling framework for computational representation of medical concepts. Our framework is based on fuzzy logic and semiotic methods which allow us to explicitly model two important characteristics of medical concepts: imprecision and context-dependency. We present the framework using an example of a mental disorder, specifically, the concept of clinical depression. To exemplify the changeable and evolutionary character of medical concepts, we discuss the development of the diagnostic criteria for depression. Finally, we use the example of the assessment of depression to describe the computational representation for polythetic and multi-dimensional concepts and for categorical and non-categorical concepts. We demonstrate how the proposed modeling framework utilizes (1) a fuzzy-logic approach to represent the non-categorical (continuous) nature of the symptoms and (2) a semiotic approach to represent the polythetic (contextual interpretation) and dimensional nature of the symptoms.}
}
@article{MWAPE2025343,
title = {Life cycle sustainability assessment of staple food processing: A double and dynamic materiality approach},
journal = {Sustainable Production and Consumption},
volume = {56},
pages = {343-363},
year = {2025},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2025.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352550925000764},
author = {Mwewa Chikonkolo Mwape and Aditya Parmar and Franz Roman and Naushad M. Emmambux and Yaovi Ouézou Azouma and Oliver Hensel},
keywords = {LCSA, Staple foods, Double materiality, Dynamic materiality, MEFA, Global warming potential (GWP), Python modeling, ESG},
abstract = {Globally, 70 % of people are fed through peasant food systems that are responsible for growing 50 % of the world's food calories on 30 % of the land. In the global south, particularly in Sub-Saharan Africa, small-scale farming serves as a crucial lifeline for the food and income needs of local populations. Yet, it remains underfunded and under-researched in the context of sustainable development. Even if the traditional Life Cycle Sustainability Assessment offers a holistic approach to evaluating the impacts of staple food processing across environmental, economic, and social dimensions, its inability to track dynamic materiality limits its application in evaluating future impacts. Therefore, this study aimed to provide a comprehensive Life Cycle Sustainability Assessment framework for staple food processing, using cassava to produce gari, a staple food for more than 300 million West Africans, as a case study. This framework integrates Material and Energy Flow Analysis techniques to trace resource use and emissions. The research incorporated Environmental, Social and Governance pillars; double materiality, evaluating both the direct and indirect impacts of processing activities, alongside dynamic materiality to capture evolving environmental, financial, and social factors through scenarios. Python computational modeling was used to perform these complex analyses, ensuring accuracy and adaptability. The findings highlight significant energy inefficiencies (6.67 kWh kg-1) coupled with a high Global Warming Potential (GWP) of 9.02 kgCO2eq kg-1 and production costs of $0.56 kg-1. The most significant opportunities for improvement were identified in optimizing energy consumption and transforming waste into biogas. The dynamic model revealed that integrating renewable energy sources could substantially reduce environmental impacts and increase the Net Profit Margin from 34.43 to 52.52 %, as proposed in the energy transition from woodfuel and gasoline to a Hybrid Solar and Biogas energy system. This study contributes to the growing body of literature on Life Cycle Sustainability Assessment by applying a comprehensive framework to staple food processing. The findings offer valuable insights into the environmental, social, and economic trade-offs in food processing systems, providing practical recommendations for improving sustainability throughout the food supply chain. Extended studies using these methods on other staples are highly recommended.}
}
@article{YUAN202317,
title = {MFGAD: Multi-fuzzy granules anomaly detection},
journal = {Information Fusion},
volume = {95},
pages = {17-25},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523000490},
author = {Zhong Yuan and Hongmei Chen and Chuan Luo and Dezhong Peng},
keywords = {Granular computing, Fuzzy rough set theory, Unsupervised anomaly detection, Multi-granularity, Hybrid data},
abstract = {Unsupervised anomaly detection is an important research direction in the process of unsupervised knowledge acquisition. It has been successfully applied in many fields, such as online fraud identification, loan approval, and medical diagnosis. Multi-granularity thinking is an effective information fusion method for solving problems in a multi-granular environment, which allows people to understand and analyze problems from multiple perspectives. However, there are few studies on building anomaly detection models using the idea of multi-fuzzy granules. To this end, this paper constructs a multi-fuzzy granules anomaly detection method by using a fuzzy rough computing model. In this method, a hybrid metric is first used to calculate the fuzzy relations. Then, two ranking sequences are constructed based on the significance of attributes. Furthermore, forward and reverse multi-fuzzy granules are constructed to define anomaly scores based on the ranking sequences. Finally, a multi-fuzzy granules-based anomaly detection algorithm is designed to detect anomalies. The experimental results compared with existing algorithms show the effectiveness of the proposed algorithm.}
}
@article{UPADHYAY2024109796,
title = {Advancements in Alzheimer's disease classification using deep learning frameworks for multimodal neuroimaging: A comprehensive review},
journal = {Computers and Electrical Engineering},
volume = {120},
pages = {109796},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109796},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624007237},
author = {Prashant Upadhyay and Pradeep Tomar and Satya Prakash Yadav},
keywords = {Multimodal, Neuroimaging, Alzheimer's disease, Classification, Images, Feature extraction},
abstract = {Over the past years, Alzheimer's disease has emerged as a serious concern for people's health. Researchers are facing challenges in effectively categorizing and diagnosing the different stages of Alzheimer's disease (AD). Current promising studies have shown that multimodal Neuroimaging has the potential to offer vital information about the structural and functional alterations associated with Alzheimer's. Using advanced computational techniques, Machine Learning calculations have been demonstrated to be highly precise in deciphering patterns and connections within the multimodal Neuroimaging data, eventually aiding in the arrangement of Alzheimer's illness stages. This research aimed to survey the adequacy of Machine Learning techniques in correctly categorizing stages of Alzheimer's disease by working on multiple neuroimaging modalities. In this review, a detailed analysis was carried out on the classification algorithms included. The study specifically examines publications published between 2016 and 2024. From the review, it was found that deep learning frameworks are more robust in Alzheimer's disease classification.}
}
@incollection{CHOUBEY2025319,
title = {Chapter 25 - Future directions on systems biology},
editor = {Babak Sokouti},
booktitle = {Systems Biology and In-Depth Applications for Unlocking Diseases},
publisher = {Academic Press},
pages = {319-328},
year = {2025},
isbn = {978-0-443-22326-6},
doi = {https://doi.org/10.1016/B978-0-443-22326-6.00025-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223266000250},
author = {Jyotsna Choubey and Jyoti Kant Choudhari and Biju Prava Sahariah},
keywords = {Agriculture, Biomedical research, Bioremediation, Bioresources, Drug discovery, Genetics, Healthcare and medicine, Proteomics},
abstract = {Biologists integrate engineering principles to design, construct, and transform biological systems for specific intents. Biological engineering involves creating new biological components, technologies, and systems, as well as redesigning existing ones, to execute specific functions and solve specific biological problems. To accomplish this objective, engineers utilize their expertise in the creation, assembly, and manipulation of biological systems, such as DNA and cells. The shift in the research paradigm toward systems biology can be largely attributed to significant advancements in protein and DNA sequencing technology. This approach is characterized by its emphasis on creating predictive models that can be applied to all levels of structural hierarchies found in biological systems. In addition, there is a strong focus on integrating data from various scales. The ultimate goal of systems biology is to create bio-based technologies that can be applied in a wide range of fields, including pharmaceuticals, health science, environmental remediation, energy production, and biotechnology. The impact of systems biology is increasingly being observed in various aspects of our lives, and this influence is anticipated to gain momentum in the future. This chapter centers on the historical background and extent of systems biology, its implementation in diverse domains, and its prospective future in various branches of biology.}
}
@article{LAWSON2013284,
title = {Sensory connection, interest/attention and gamma synchrony in autism or autism, brain connections and preoccupation},
journal = {Medical Hypotheses},
volume = {80},
number = {3},
pages = {284-288},
year = {2013},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2012.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306987712005415},
author = {Wendy Lawson},
abstract = {Does motivational interest increase gamma synchrony across neuronal networking to enable computation of related sensory inputs that might lead to greater social understanding in autism spectrum conditions (ASC)? Meaning, is it possible/likely that in autism because individuals process one aspect of sensory input at any one time (therefore missing the wider picture in general) when they are motivated/interested or attending to particular stimuli their attention window is widened due to increased gamma synchrony and they might be enabled to connect in ways that do not occur when they are not motivated? This is my current research question. If gamma synchrony is helping with the binding of information from collective sensory inputs, in ASC, when and only if the individual is motivated, then this has huge potential for how learning might be encouraged for individuals with an ASC.}
}
@article{PHILLIPS200930,
title = {Fiber tractography reveals disruption of temporal lobe white matter tracts in schizophrenia},
journal = {Schizophrenia Research},
volume = {107},
number = {1},
pages = {30-38},
year = {2009},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2008.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0920996408004854},
author = {Owen R. Phillips and Keith H. Nuechterlein and Kristi A. Clark and Liberty S. Hamilton and Robert F. Asarnow and Nathan S. Hageman and Arthur W. Toga and Katherine L. Narr},
keywords = {Diffusion tensor imaging, White matter, Uncinate fasciculus, Inferior longitudinal fasciculus, Arcuate fasciculus, Fractional anisotropy},
abstract = {Diffusion tensor imaging (DTI) studies have demonstrated abnormal anisotropic diffusion in schizophrenia. However, examining data with low spatial resolution and/or a low number of gradient directions and limitations associated with analysis approaches sensitive to registration confounds may have contributed to mixed findings concerning the regional specificity and direction of results. This study examined three major white matter tracts connecting lateral and medial temporal lobe regions with neocortical association regions widely implicated in systems-level functional and structural disturbances in schizophrenia. Using DTIstudio, a previously validated regions of interest tractography method was applied to 30 direction diffusion weighted imaging data collected from demographically similar schizophrenia (n=23) and healthy control subjects (n=22). The diffusion tensor was computed at each voxel after intra-subject registration of diffusion-weighted images. Three-dimensional tract reconstruction was performed using the Fiber Assignment by Continuous Tracking (FACT) algorithm. Tractography results showed reduced fractional anisotropy (FA) of the arcuate fasciculi (AF) and inferior longitudinal fasciculi (ILF) in patients compared to controls. FA changes within the right ILF were negatively correlated with measures of thinking disorder. Reduced volume of the left AF was also observed in patients. These results, which avoid registration issues associated with voxel-based analyses of DTI data, support that fiber pathways connecting lateral and medial temporal lobe regions with neocortical regions are compromised in schizophrenia. Disruptions of connectivity within these pathways may potentially contribute to the disturbances of memory, language, and social cognitive processing that characterize the disorder.}
}
@article{BENDER2024156,
title = {Dimension results for extremal-generic polynomial systems over complete toric varieties},
journal = {Journal of Algebra},
volume = {646},
pages = {156-182},
year = {2024},
issn = {0021-8693},
doi = {https://doi.org/10.1016/j.jalgebra.2024.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0021869324000553},
author = {Matías Bender and Pierre-Jean Spaenlehauer},
keywords = {Sparse polynomial systems, Toric varieties},
abstract = {We study polynomial systems with prescribed monomial supports in the Cox ring of a toric variety built from a complete polyhedral fan. We present combinatorial formulas for the dimension of their associated subvarieties under genericity assumptions on the coefficients of the polynomials. Using these formulas, we identify at which degrees generic systems in polytopal algebras form regular sequences. Our motivation comes from sparse elimination theory, where knowing the expected dimension of these subvarieties leads to specialized algorithms and to large speed-ups for solving sparse polynomial systems. As a special case, we classify the degrees at which regular sequences defined by weighted homogeneous polynomials can be found, answering an open question in the Gröbner bases literature. We also show that deciding whether a sparse system is generically a regular sequence in a polytopal algebra is hard from the point of view of theoretical computational complexity.}
}
@article{ABDALLAH2024102341,
title = {An evaluation of the use of air cooling to enhance photovoltaic performance},
journal = {Thermal Science and Engineering Progress},
volume = {47},
pages = {102341},
year = {2024},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2023.102341},
url = {https://www.sciencedirect.com/science/article/pii/S2451904923006947},
author = {Ramez Abdallah and Tamer Haddad and Mohammad Zayed and Adel Juaidi and Tareq Salameh},
keywords = {Photovoltaic, ANSYS fluent, CFD, PV cooling, Heat sink},
abstract = {The rapid rise in global energy consumption and its consequences on climate change has made incorporating renewable energy sources like solar photovoltaics into the building envelope easier. However, in spite of extensive uses and significant technological advances, the lower solar panel efficiencies caused by high temperatures remain a significant barrier to the viability of deploying photovoltaic technology in regions with hot climates utilizing computational fluid dynamics (CFD). This research examines the cooling effectiveness of air-cooled photovoltaic (PV) under the climate of Nablus - Palestine. This study presents a numerical model designed to cool solar panels using various air-cooled channel configurations. Rectangular fins made of high thermal conductivity materials such as copper were used in this study. The parametric study was based on the changing baseplate thickness, fin spacing, height, and thickness through a stepwise optimization process to enhance the heat transfer mechanism. The results show that the optimum design of average volume temperatures for the PV cell models in air-cooled channel configurations with and without fins were 40.28 °C and 42.58 °C, respectively. The optimum design was obtained at 3, 110, 60, and 4 mm for baseplate thickness, fin spacing, height, and thickness, respectively. This optimum design was responsible for the average PV panel temperature drop by 1.6 %, 1.3 %, 5.9 %, and 6.2 % for baseplate thickness, fin spacing, height, and thickness, respectively. The optimum design of an air-cooled cooling channel for PV is an important insight provided by this work, and it may help in the future development of more effective and affordable cooling methods.}
}
@article{LIU2024115,
title = {Water Quality System Informatics: An Emerging Inter-Discipline of Environmental Engineering},
journal = {Engineering},
volume = {43},
pages = {115-124},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924002601},
author = {Hong Liu and Zhaoming Chen and Zhiwei Wang and Ming Xu and Yutao Wang and Jinju Geng and Fengjun Yin},
keywords = {Water quality system, Water quality system informatics, Environmental engineering, Emerging interdisciplinary, Research pattern},
abstract = {Water quality system informatics (WQSI) is an emerging field that employs cybernetics to collect and digitize data associated with water quality. It involves monitoring the physical, chemical, and biological processes that affect water quality and the ecological impacts and interconnections within water quality systems. WQSI integrates theories and methods from water quality engineering, information engineering, and system control theory, enabling the intelligent management and control of water quality. This integration revolutionizes the understanding and management of water quality systems with greater precision and higher resolution. WQSI is a new stage of development in environmental engineering that is driven by the digital age. This work explores the fundamental concepts, research topics, and methods of WQSI and its features and potential to promote disciplinary development. The innovation and development of WQSI are crucial for driving the digital and intelligent transformation of national industry patterns in China, positioning China at the forefront of environmental engineering and ecological environment research on a global scale.}
}
@article{ZHANG2024603,
title = {An intelligent management and decision model of operational research for edge computing aided planning},
journal = {Alexandria Engineering Journal},
volume = {109},
pages = {603-609},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824010962},
author = {Yuanshou Zhang},
keywords = {Intelligent Decision-making Model, Edge Algorithm, Task Offloading, Mobile Edge Computing Server},
abstract = {With the development of “Internet plus” and the deep integration of information and network based technology and network technology, Industry 4.0 has become a hot topic. Industrial production is faced with a large number of tasks, complex and changeable demands, and difficult to predict. In order to solve a series of objective problems such as task delay in actual production, this paper proposed a method based on edge computing (EC) to reduce the delay to meet the real-time requirements of industrial production. However, due to the limitation of its computing power and storage capacity, it is difficult to adapt to large-scale data decision-making. In terms of the lag rate of the algorithm in the experiment of the intelligent decision model, when the number of tasks of EC algorithm was 15 and 6, the lag rate was the highest and the lowest, and its values were 16 % and 2 % respectively. Therefore, it can be seen that EC algorithm can play a good role in the intelligent management and decision-making model of operational research.}
}
@article{CASAJUS202488,
title = {Random partitions, potential, value, and externalities},
journal = {Games and Economic Behavior},
volume = {147},
pages = {88-106},
year = {2024},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2024.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S089982562400085X},
author = {André Casajus and Yukihiko Funaki and Frank Huettner},
keywords = {Shapley value, Partition function form, Random partition, Restriction operator, Ewens distribution, Chinese restaurant process, Potential, Externalities, Null player, Expected accumulated worth},
abstract = {The Shapley value equals a player's contribution to the potential of a game. The potential is a most natural one-number summary of a game, which can be computed as the expected accumulated worth of a random partition of the players. This computation integrates the coalition formation of all players and readily extends to games with externalities. We investigate those potential functions for games with externalities that can be computed this way. It turns out that the potential that corresponds to the MPW solution introduced by Macho-Stadler et al. (2007, J. Econ. Theory 135, 339–356) is unique in the following sense. It is obtained as the expected accumulated worth of a random partition, it generalizes the potential for games without externalities, and it induces a solution that satisfies the null player property even in the presence of externalities.}
}
@article{PERRIN20227006,
title = {Malonic Anhydrides, Challenges from a Simple Structure},
journal = {The Journal of Organic Chemistry},
volume = {87},
number = {11},
pages = {7006-7012},
year = {2022},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.2c00453},
url = {https://www.sciencedirect.com/science/article/pii/S0022326322022642},
author = {Charles L. Perrin},
abstract = {ABSTRACT
After many years of unsuccessful attempts, monomeric malonic anhydrides were prepared by ozonolysis of ketene dimers, a procedure validated by model studies. The structure proof relied most heavily on IR absorption at 1820 cm–1 and a Raman band at 1947 cm–1. Malonic anhydrides are unstable, decomposing below room temperature to a ketene plus carbon dioxide. Surprisingly, according to kinetic studies, the dimethyl derivative is slightly less unstable than the parent, and the monomethyl is the fastest to decompose, with an enthalpy of activation of only 12.6 kcal/mol. Computations rationalize this behavior in terms of a concerted [2s + 2a] cycloreversion that requires a more highly organized transition state, as also manifested by a negative entropy of activation.}
}
@article{KIM2025119365,
title = {Being stuck on negatives isn't equally bad: A cross-cultural Bayesian meta-analysis of rumination and its relation to depression},
journal = {Journal of Affective Disorders},
volume = {385},
pages = {119365},
year = {2025},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2025.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S0165032725007815},
author = {Sooyeon Kim and Seojeong Kim and Sunkyung Yoon},
keywords = {Rumination, RoBMA, Culture, Emotion regulation, Dialecticism},
abstract = {Background
Our understanding of rumination, an emotion regulation strategy that plays a critical role in depression, is limited in its cultural generalisability. Recent cross-cultural studies suggest that higher levels of rumination may not entail the same degree of depression between Eastern dialectical and Western analytical thinkers. This study aims to clarify this by quantitatively summarising existing literature to test whether the level of rumination and its relation to depression differ across these two cultures.
Methods
A robust Bayesian meta-analysis was conducted to compare cultural differences in rumination levels across 15 studies with a total sample of 8505 participants. Additionally, 8 studies with a combined sample of 6815 participants were further analysed to examine culture's moderating effect on the relationship between rumination and depression.
Results
We found 4 times more evidence supporting cultural differences in the mean level of rumination (vs. no cultural difference), with Easterners ruminating more than Westerners. Additionally, evidence for cultural moderation of the relationship between rumination and depression was 2.8 times stronger than evidence for no moderation, with a weaker relationship observed among Easterners.
Limitations
Our study was limited by a small sample size, high heterogeneity, and a focus on only two cultures.
Conclusion
Our findings suggest that greater rumination may not correspond to the same level of depression across cultures, highlighting the importance of considering culture in understanding emotion regulation.}
}
@article{WARNIER201715,
title = {Distributed monitoring for the prevention of cascading failures in operational power grids},
journal = {International Journal of Critical Infrastructure Protection},
volume = {17},
pages = {15-27},
year = {2017},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1874548216300427},
author = {Martijn Warnier and Stefan Dulman and Yakup Koç and Eric Pauwels},
keywords = {Power Grids, Cascading Failures, Robustness, Real-Time Monitoring, Distributed Computation},
abstract = {Electrical power grids are vulnerable to cascading failures that can lead to large blackouts. The detection and prevention of cascading failures in power grids are important problems. Currently, grid operators mainly monitor the states (loading levels) of individual components in a power grid. The complex architecture of a power grid, with its many interdependencies, makes it difficult to aggregate the data provided by local components in a meaningful and timely manner. Indeed, monitoring the resilience of an operational power grid to cascading failures is a major challenge. This paper attempts to address this challenge. It presents a robustness metric based on the topology and operative state of a power grid to quantify the robustness of the grid. Also, it presents a distributed computation method with self-stabilizing properties that can be used for near real-time monitoring of grid robustness. The research thus provides insights into the resilience of a dynamic operational power grid to cascading failures during real-time in a manner that is both scalable and robust. Computations are pushed to the power grid network, making the results available at each node and enabling automated distributed control mechanisms to be implemented.}
}
@article{SMOLARCZYK2024100669,
title = {Let’s get them on board: Focus group discussions with adolescents on empowering leisure engagement in Fab Labs and makerspaces},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100669},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100669},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000370},
author = {Kathrin Smolarczyk and Marios Mouratidis and Sophie Uhing and Rolf Becker and Stephan Kröner},
keywords = {Maker activities, Leisure, Youth, Focus groups, Sustainability},
abstract = {Makerspaces and Fab Labs are growing in number all over the world, holding the potential to empower children and adolescents. They form an important pathway to provide young people with access to digital manufacturing technologies while fostering self-determination, collaboration, and creativity. We explore how the engagement in Fab Lab based leisure maker activities may be promoted, taking into account both the perspectives of adolescents and the potential of surrounding systems. For this, we conducted focus group discussions with N = 61 non-maker, adolescent girls and boys from 6th to 9th grade, to scrutinize hindering and promoting factors of their engagement in leisure maker activities, and to explore their preferences regarding the involvement of parents, teachers and peers while considering the ecological sustainability of the activities. A reflexive thematic analysis identified the hindering and promoting factors across different aspects of maker activities such as the purpose, location and setting, content, and learning processes. Implications for the promotion and design of maker activities, as well as implications for further research, are discussed.}
}
@article{KAZIEVA20242933,
title = {Unpacking Complex Concepts to Enhance Use of Dynamic Simulations},
journal = {Procedia Computer Science},
volume = {246},
pages = {2933-2942},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.377},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024098},
author = {Victoria Kazieva},
keywords = {simulation modeling, decision-making, complex systems, dynamic simulations, agent-based modeling, system dynamics},
abstract = {The paper suggests a research framework that can aid simulation model designers and users in understanding and modeling complex concepts. The aim is to enhance the role of simulation in supporting decision-making with agent-based modeling and system dynamics by investigating literature that outlines challenges in the field. This study advocates for involving decision-makers in unpacking the overarching concept into manageable components that are associates with the top concept. The research framework generates insights into the interpretation of complex concepts and guides model designers in formulating concrete variables for simulation modeling through successive iterations of the unpacking practices.}
}
@article{PETRELLA2024139,
title = {The AI Future of Emergency Medicine},
journal = {Annals of Emergency Medicine},
volume = {84},
number = {2},
pages = {139-153},
year = {2024},
issn = {0196-0644},
doi = {https://doi.org/10.1016/j.annemergmed.2024.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S019606442400043X},
author = {Robert J. Petrella},
abstract = {In the coming years, artificial intelligence (AI) and machine learning will likely give rise to profound changes in the field of emergency medicine, and medicine more broadly. This article discusses these anticipated changes in terms of 3 overlapping yet distinct stages of AI development. It reviews some fundamental concepts in AI and explores their relation to clinical practice, with a focus on emergency medicine. In addition, it describes some of the applications of AI in disease diagnosis, prognosis, and treatment, as well as some of the practical issues that they raise, the barriers to their implementation, and some of the legal and regulatory challenges they create.}
}
@article{DARROCH2023105989,
title = {The rangeomorph Pectinifrons abyssalis: Hydrodynamic function at the dawn of animal life},
journal = {iScience},
volume = {26},
number = {2},
pages = {105989},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.105989},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223000664},
author = {Simon A.F. Darroch and Susana Gutarra and Hale Masaki and Andrei Olaru and Brandt M. Gibson and Frances S. Dunn and Emily G. Mitchell and Rachel A. Racicot and Gregory Burzynski and Imran A. Rahman},
keywords = {Zoology, Evolutionary biology, Paleobiology},
abstract = {Summary
Rangeomorphs are among the oldest putative eumetazoans known from the fossil record. Establishing how they fed is thus key to understanding the structure and function of the earliest animal ecosystems. Here, we use computational fluid dynamics to test hypothesized feeding modes for the fence-like rangeomorph Pectinifrons abyssalis, comparing this to the morphologically similar extant carnivorous sponge Chondrocladia lyra. Our results reveal complex patterns of flow around P. abyssalis unlike those previously reconstructed for any other Ediacaran taxon. Comparisons with C. lyra reveal substantial differences between the two organisms, suggesting they converged on a similar fence-like morphology for different functions. We argue that the flow patterns recovered for P. abyssalis do not support either a suspension feeding or osmotrophic feeding habit. Instead, our results indicate that rangeomorph fronds may represent organs adapted for gas exchange. If correct, this interpretation could require a dramatic reinterpretation of the oldest macroscopic animals.}
}
@article{MARGINEANU2014131,
title = {Systems biology, complexity, and the impact on antiepileptic drug discovery},
journal = {Epilepsy & Behavior},
volume = {38},
pages = {131-142},
year = {2014},
note = {SI: NEWroscience 2013},
issn = {1525-5050},
doi = {https://doi.org/10.1016/j.yebeh.2013.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S1525505013004344},
author = {Doru Georg Margineanu},
keywords = {Systems biology, Systems/network pharmacology, Drug resistance in epilepsy, Antiepileptic drug, Polypharmacology, Multitarget drug, Phenotypic screening, Modeling, Drug discovery},
abstract = {The number of available anticonvulsant drugs increased in the period spanning over more than a century, amounting to the current panoply of nearly two dozen so-called antiepileptic drugs (AEDs). However, none of them actually prevents/reduces the post-brain insult development of epilepsy in man, and in no less than a third of patients with epilepsy, the seizures are not drug-controlled. Plausibly, the enduring limitation of AEDs' efficacy derives from the insufficient understanding of epileptic pathology. This review pinpoints the unbalanced reductionism of the analytic approaches that overlook the intrinsic complexity of epilepsy and of the drug resistance in epilepsy as the core conceptual flaw hampering the discovery of truly antiepileptogenic drugs. A rising awareness of the complexity of epileptic pathology is, however, brought about by the emergence of nonreductionist systems biology (SB) that considers the networks of interactions underlying the normal organismic functions and of SB-based systems (network) pharmacology that aims to restore pathological networks. By now, the systems pharmacology approaches of AED discovery are fairly meager, but their forthcoming development is both a necessity and a realistic prospect, explored in this review. This article is part of a Special Issue entitled “NEWroscience 2013”.}
}
@article{BAIDYA2024100680,
title = {Comprehensive survey on resource allocation for edge-computing-enabled metaverse},
journal = {Computer Science Review},
volume = {54},
pages = {100680},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100680},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000649},
author = {Tanmay Baidya and Sangman Moh},
keywords = {Augmented reality, Edge computing, Metaverse, Offloading, Resource allocation, Virtual reality},
abstract = {With the rapid evaluation of virtual and augmented reality, massive Internet of Things networks and upcoming 6 G communication give rise to an emerging concept termed the “metaverse,” which promises to revolutionize how we interact with the digital world by offering immersive experiences between reality and virtuality. Edge computing, another novel paradigm, propels the metaverse functionality by enhancing real-time interaction and reducing latency, providing a responsive and seamless virtual environment. However, realizing the full potential of the metaverse requires dynamic and efficient resource-allocation strategies to handle the immense demand for communicational, computational, and storage resources required by its diverse applications. This survey comprehensively explores resource-allocation strategies in the context of an edge-computing-enabled metaverse, investigating various challenges, existing techniques, and emerging trends in this rapidly expanding field. We first explore the underlying metaverse characteristics and pivotal role of edge computing, after which we investigate various types of resources and their key issues and challenges. We also provide a brief discussion on offloading and caching strategies, which are the most prominent research issues in this context. In this study, we compare and analyze 35 different resource-allocation strategies, benchmark 19 algorithms, and investigate their suitability across diverse metaverse scenarios, offering a broader scope than existing surveys. The survey aims to serve as a comprehensive guide for researchers and practitioners, helping them navigate the complexities of resource allocation in the metaverse and supporting the development of more efficient, scalable, and user-centric virtual environments.}
}
@article{BOCK2020100960,
title = {On the semantics for spreadsheets with sheet-defined functions},
journal = {Journal of Computer Languages},
volume = {57},
pages = {100960},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100960},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300204},
author = {Alexander Asp Bock and Thomas Bøgholm and Peter Sestoft and Bent Thomsen and Lone Leth Thomsen},
keywords = {Spreadsheet, Semantics, Funcalc, Sheet-defined function, Recalculation},
abstract = {We give an operational semantics for the evaluation of spreadsheets, including sheet-defined and built-in numeric functions in the Funcalc spreadsheet platform. The semantics allows for different implementations and we discuss sheet-defined functions implemented using both interpretation and run-time code generation. The semantics specifies the expected result of a computation, also considering non-deterministic functions, independently of an evaluation mechanism. It can be extended to include the cost of formula evaluation for a cost analysis e.g. for use in parallelization of computations. An interesting future direction is to investigate experimentally how close our semantics is to that of major spreadsheet implementations.}
}
@article{GRAYSON2022108844,
title = {R Markdown as a dynamic interface for teaching: Modules from math and biology classrooms},
journal = {Mathematical Biosciences},
volume = {349},
pages = {108844},
year = {2022},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2022.108844},
url = {https://www.sciencedirect.com/science/article/pii/S0025556422000499},
author = {Kristine L. Grayson and Angela K. Hilliker and Joanna R. Wares},
keywords = {R markdown, Data visualization, Pedagogy, Herd immunity, Teaching programming},
abstract = {Advancing technologies, including interactive tools, are changing classroom pedagogy across academia. Here, we discuss the R Markdown interface, which allows for the creation of partial or complete interactive classroom modules for courses using the R programming language. R Markdown files mix sections of R code with formatted text, including LaTeX, which are rendered together to form complete reports and documents. These features allow instructors to create classroom modules that guide students through concepts, while providing areas for coding and text response by students. Students can also learn to create their own reports for more independent assignments. After presenting the features and uses of R Markdown to enhance teaching and learning, we present examples of materials from two courses. In a Computational Modeling course for math students, we used R Markdown to guide students through exploring mathematical models to understand the principle of herd immunity. In a Data Visualization and Communication course for biology students, we used R Markdown for teaching the fundamentals of R programming and graphing, and for students to learn to create reproducible data investigations. Through these examples, we demonstrate the benefits of R Markdown as a dynamic teaching and learning tool.}
}
@incollection{LUDLOW2025,
title = {Competence/Performance},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00557-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005573},
author = {Peter Ludlow},
keywords = {Linguistic competence, Linguistic performance, Cognizing rules, Knowing rules, Rule following, Natural logic, Norms},
abstract = {The competence/performance distinction has played a role in linguistic theorizing since 1965. The key idea is that one can distinguish between the grammar that an agent has and the performance errors that violate the posited rules or principles of the grammar. The first question to consider is what counts as a performance error and whether it can be defined in a way that avoids bailing out defective theories. The next question concerns what competence consists in—in what sense do we have linguistic rules or principles? One idea is that the rule plays a normative (guiding) role. Another idea is that we merely “cognize” or represent the rule. These lead to different conceptions of performance errors. We then turn to versions of the competence/performance distinction in other fields, including ethics and logic.}
}
@article{OYELADE2025127455,
title = {SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127455},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127455},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010772},
author = {Olaide N. Oyelade and Hui Wang and Karen Rafferty},
keywords = {Knowledge graphs (KGs), Large language model (LLMs), Idea generation, Novelty, Analogical reasoning, Node importance estimation, Natural language processing (NLP), Isomorphic subgraphs},
abstract = {Idea generation describes a creative process involving reasoning over some knowledge to derive new information. Traditional approaches such as mind-map and brainstorming are limited and often fail due to lack of quality ideas and ineffective methods. The reasoning capability of large language models (LLMs) have been investigated for ideation tasks and have reported interesting performance. However, these models suffer from limited logical reasoning capability which hinders the use of structural and factual real-world knowledge in discovery of latent insight and predict possible outcome when applied to ideation. In addition, the possibility of LLMs regurgitating knowledge learnt from datasets might adversely impact the degree of novel ideas the models can generate. In this paper, a two-stage logical reasoning approach is applied to initiate the search for candidate idea pathways based on the knowledge graphs (KGs) to address the problem of reasoning, domain-specificity and novelty. The divergence stage this reasoning explores utilizes a new node importance estimation (NIE) technique over KGs to discover latent connections supporting idea generation. In the convergence stage of this reasoning, subgraph matching using analogical reasoning (SMAR) is applied to find matching patterns to describe a new idea. The use of SMAR + NIE and KGs helps to achieve an improvement in reasoning over KGs before transferring such reasoning to LLMs for translation of idea into natural language. To evaluate the degree of novelty of ideas generated, a relevance-to-novelty scoring metrics is proposed based on multiple premise entailment (MPE). We combined this metric with other popular metrics to evaluate the performance of SMAR + NIE on benchmark datasets, and as well on the quality of ideas generated. Findings from the study showed that this approach demonstrates competitive performance with mainstream LLMs in idea generation tasks.}
}
@article{POVALA2022114712,
title = {Variational Bayesian approximation of inverse problems using sparse precision matrices},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {393},
pages = {114712},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.114712},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522000822},
author = {Jan Povala and Ieva Kazlauskaite and Eky Febrianto and Fehmi Cirak and Mark Girolami},
keywords = {Inverse problems, Bayesian inference, Variational Bayes, Precision matrix, Uncertainty quantification},
abstract = {Inverse problems involving partial differential equations (PDEs) are widely used in science and engineering. Although such problems are generally ill-posed, different regularisation approaches have been developed to ameliorate this problem. Among them is the Bayesian formulation, where a prior probability measure is placed on the quantity of interest. The resulting posterior probability measure is usually analytically intractable. The Markov Chain Monte Carlo (MCMC) method has been the go-to method for sampling from those posterior measures. MCMC is computationally infeasible for large-scale problems that arise in engineering practice. Lately, Variational Bayes (VB) has been recognised as a more computationally tractable method for Bayesian inference, approximating a Bayesian posterior distribution with a simpler trial distribution by solving an optimisation problem. In this work, we argue, through an empirical assessment, that VB methods are a flexible and efficient alternative to MCMC for this class of problems. We propose a natural choice of a family of Gaussian trial distributions parametrised by precision matrices, thus taking advantage of the inherent sparsity of the inverse problem encoded in its finite element discretisation. We utilise stochastic optimisation to efficiently estimate the variational objective and assess not only the error in the solution mean but also the ability to quantify the uncertainty of the estimate. We test this on PDEs based on the Poisson equation in 1D and 2D. A Tensorflow implementation is made publicly available on GitHub.}
}
@article{KRISHNANNAIR202437,
title = {Empowering Tomorrow's Scientists: ‘Girls In Control’ Workshop Promotes STEM Education For Young Girls},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {25},
pages = {37-42},
year = {2024},
note = {3rd Control Conference Africa CCA 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.10.234},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324020093},
author = {S. Krishnannair and A. Krishnannair},
keywords = {STEM education, Control Engineering, Girls in Control, Empowering Girls},
abstract = {Enhancement of female students’ access to and success in STEM-related subjects and courses has acquired an unprecedented level of traction in academia. Considering female students’ increased participation in STEM education as a moral imperative, the University of Zululand (South Africa) in partnership with SACAC, South Africa hosted two Girls in Control Workshops at its Science Centre during 2022 and 2023. This article reports on the workshop's purpose and the experiences of the participants. It also makes references to the workshop's implications on various aspects of girls’ participation in STEM education in general and control engineering in particular. The article thus places the need for instilling a high degree of receptiveness to STEM-related careers among girls from previously marginalized communities.}
}
@article{LI20231485,
title = {A Data Driven Security Correction Method for Power Systems with UPFC},
journal = {Energy Engineering},
volume = {120},
number = {6},
pages = {1485-1502},
year = {2023},
issn = {0199-8595},
doi = {https://doi.org/10.32604/ee.2023.022856},
url = {https://www.sciencedirect.com/science/article/pii/S0199859523000519},
author = {Qun Li and Ningyu Zhang and Jianhua Zhou and Xinyao Zhu and Peng Li},
keywords = {Manuscript, security correction, data-driven, deep neural network (DNN), unified power flow controller (UPFC), overload of transmission lines},
abstract = {The access of unified power flow controllers (UPFC) has changed the structure and operation mode of power grids all across the world, and it has brought severe challenges to the traditional real-time calculation of security correction based on traditional models. Considering the limitation of computational efficiency regarding complex, physical models, a data-driven power system security correction method with UPFC is, in this paper, proposed. Based on the complex mapping relationship between the operation state data and the security correction strategy, a two-stage deep neural network (DNN) learning framework is proposed, which divides the offline training task of security correction into two stages: in the first stage, the stacked auto-encoder (SAE) classification model is established, and the node correction state (0/1) output based on the fault information; in the second stage, the DNN learning model is established, and the correction amount of each action node is obtained based on the action nodes output in the previous stage. In this paper, the UPFC demonstration project of Nanjing West Ring Network is taken as a case study to validate the proposed method. The results show that the proposed method can fully meet the real-time security correction time requirements of power grids, and avoid the inherent defects of the traditional model method without an iterative solution and can also provide reasonable security correction strategies for N-1 and N-2 faults.}
}
@article{BASOV2020101433,
title = {Socio-semantic and other dualities},
journal = {Poetics},
volume = {78},
pages = {101433},
year = {2020},
note = {Discourse, Meaning, and Networks: Advances in Socio-Semantic Analysis},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2020.101433},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X19304073},
author = {Nikita Basov and Ronald Breiger and Iina Hellsten},
keywords = {Social network, Semantic network, Socio-semantic network, Duality, Culture, Special Issue},
abstract = {The social and the cultural orders are dual – that is, they constitute each other. To understand either we need to account for both. Socio-semantic network analysis brings together the study of relations among actors (social networks), relations among elements of actors’ cultural structures (their semantic networks), and relations among these two orders of networks. In this introductory essay, we describe how the duality of the social and semantic networks that constitute each other, as well as other related dualities (including material / symbolic, micro / macro, computational / qualitative, in-presence contexts / online contexts, ‘Big’ data / ‘thick’ data), have evolved in recent decades to mold socio-semantic network analysis into its present form. In doing so, we delineate the current state of the art and the main features of socio-semantic network analysis as highlighted by the papers included in this Special Issue. These articles range from in-depth analysis of ‘thick’ data on small group interactions to automated analysis of ‘Big’ online data in contexts extending from Renaissance parliamentary discussions to cutting-edge global scientific fields of the 21st century. We conclude by delineating current problems of and future prospects for socio-semantic network analysis.}
}
@article{KHACHATURIAN20253,
title = {Perspective on “Brain Network Disorders”},
journal = {Brain Network Disorders},
volume = {1},
number = {1},
pages = {3-6},
year = {2025},
issn = {3050-6239},
doi = {https://doi.org/10.1016/j.bnd.2024.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S3050623924000129},
author = {Zaven Khachaturian and Jean-Marie C. Boutellier and Jiri Damborsky and Ara S. Khachaturian}
}
@incollection{KALNOOR2021575,
title = {Chapter 24 - The brain-machine interface, nanosensor technology, and artificial intelligence: Their convergence with a novel frontier},
editor = {Chaudhery Mustansar Hussain and Suresh Kumar Kailasa},
booktitle = {Handbook of Nanomaterials for Sensing Applications},
publisher = {Elsevier},
pages = {575-587},
year = {2021},
series = {Micro and Nano Technologies},
isbn = {978-0-12-820783-3},
doi = {https://doi.org/10.1016/B978-0-12-820783-3.00013-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207833000130},
author = {Gauri Kalnoor},
keywords = {Neuroscience, Machine learning, Nanotechnology, Artificial intelligence (AI), Brain-computer interface, Brain-machine interface (BMI), Computational neuroscience},
abstract = {A confluence of technological capabilities is creating an opportunity for machine learning and artificial intelligence (AI) to enable “smart” nanoengineered brain-machine interfaces (BMI). This new generation of technologies will be able to communicate with the brain in ways that support contextual learning and adaptation to change functional requirements. This applies to both invasive technologies aimed at restoring neurological function, as in the case of neural prosthesis, as well as noninvasive technologies enabled by signals such as electroencephalograph (EEG). Advances in computation, hardware, and algorithms that learn and adapt in a contextually dependent way will be able to leverage the capabilities that nanoengineering offers the design and functionality of BMI. We explore the enabling capabilities that these devices may exhibit, why they matter, and the state of the technologies necessary to build them. We also discuss a number of open technical challenges and problems that will need to be solved to achieve this.}
}
@article{BOWER2024455,
title = {Model-Based Analysis of Pathway Recruitment During Subthalamic Deep Brain Stimulation},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {27},
number = {3},
pages = {455-463},
year = {2024},
issn = {1094-7159},
doi = {https://doi.org/10.1016/j.neurom.2023.02.084},
url = {https://www.sciencedirect.com/science/article/pii/S109471592300140X},
author = {Kelsey L. Bower and Angela M. Noecker and Anneke M. Frankemolle-Gilbert and Cameron C. McIntyre},
keywords = {Axons, electrode, Parkinson’s disease, subthalamic nucleus},
abstract = {Background
Subthalamic deep brain stimulation (DBS) is an established clinical therapy, but an anatomically clear definition of the underlying neural target(s) of the stimulation remains elusive. Patient-specific models of DBS are commonly used tools in the search for stimulation targets, and recent iterations of those models are focused on characterizing the brain connections that are activated by DBS.
Objective
The goal of this study was to quantify axonal pathway activation in the subthalamic region from DBS at different electrode locations and stimulation settings.
Materials and Methods
We used an anatomically and electrically detailed computational model of subthalamic DBS to generate recruitment curves for eight different axonal pathways of interest, at three generalized DBS electrode locations in the subthalamic nucleus (STN) (ie, central STN, dorsal STN, posterior STN). These simulations were performed with three levels of DBS electrode localization uncertainty (ie, 0.5 mm, 1.0 mm, 1.5 mm).
Results
The recruitment curves highlight the diversity of pathways that are theoretically activated with subthalamic DBS, in addition to the dependence of the stimulation location and parameter settings on the pathway activation estimates. The three generalized DBS locations exhibited distinct pathway recruitment curve profiles, suggesting that each stimulation location would have a different effect on network activity patterns. We also found that the use of anodic stimuli could help limit activation of the internal capsule relative to other pathways. However, incorporating realistic levels of DBS electrode localization uncertainty in the models substantially limits their predictive capabilities.
Conclusions
Subtle differences in stimulation location and/or parameter settings can impact the collection of pathways that are activated during subthalamic DBS.}
}
@article{GIACHETTI2025103229,
title = {Preface for “Selected papers from the 26th Ibero-American Conference on Software Engineering (CIbSE 2023)”},
journal = {Science of Computer Programming},
volume = {243},
pages = {103229},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2024.103229},
url = {https://www.sciencedirect.com/science/article/pii/S0167642324001527},
author = {Giovanni Giachetti and Breno {de França} and Marcela Genero and Renata Guizzardi}
}
@article{LARSON201129,
title = {Interdisciplinary research training in a school of nursing},
journal = {Nursing Outlook},
volume = {59},
number = {1},
pages = {29-36},
year = {2011},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2010.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0029655410004379},
author = {Elaine L. Larson and Bevin Cohen and Kristine Gebbie and Sarah Clock and Lisa Saiman},
abstract = {Although interdisciplinarity has become a favored model of scholarly inquiry, the assumption that interdisciplinary work is intuitive and can be performed without training is short-sighted. This article describes the implementation of an interdisciplinary research training program within a school of nursing. We describe the key elements of the program and the challenges we encountered. From 2007-2010, eleven trainees from 6 disciplines have been accepted into the program and 7 have completed the program; the trainees have published 12 manuscripts and presented at 10 regional or national meetings. The major challenge has been to sustain and “push the envelope” toward interdisciplinary thinking among the trainees and their mentors, and to assure that they do not revert to their “safer” disciplinary silos. This training program, funded by National Institute of Nursing Research (NINR), has become well-established within the school of nursing and across the entire University campus, and is recognized as a high quality research training program across disciplines, as exemplified by excellent applicants from a number of disciplines.}
}
@article{SHIPLEY201948,
title = {Collaboration, cyberinfrastructure, and cognitive science: The role of databases and dataguides in 21st century structural geology},
journal = {Journal of Structural Geology},
volume = {125},
pages = {48-54},
year = {2019},
note = {Back to the future},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0191814117303164},
author = {Thomas F. Shipley and Basil Tikoff},
keywords = {Spatial cognition, Cyberinfrastructure, Expert training},
abstract = {Structural geologists support their mind with tools, and these tools are increasingly computer based. The advent of Intelligent Systems will allow creation of research teams that combine the strengths of the human mind and computer processing to produce new research results. The efficacy of these approaches will require a solid grounding in cognitive science. Critical to this approach are databases, which are potentially transformative solely in their ability to allow access to data, in a primary form. Emerging more recently, however, is the concept of a dataguide, in which computer-aided analysis informs ongoing decisions about where and what data to collect. The creation of human and computer teams can expand the types of questions that can be addressed in structural geology and tectonics research, but it will take a community-based effort to understand the value of data to experts and how computers might aid an expert in the field.}
}
@article{BURKE2024102382,
title = {A chance for models to show their quality: Stochastic process model-log dimensions},
journal = {Information Systems},
volume = {124},
pages = {102382},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102382},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924000401},
author = {Adam T. Burke and Sander J.J. Leemans and Moe T. Wynn and Wil M.P. {van der Aalst} and Arthur H.M. {ter Hofstede}},
keywords = {Stochastic process mining, Process conformance, Stochastic Petri nets, Adhesion, Relevance, Simplicity},
abstract = {Process models describe the desired or observed behaviour of organisations. In stochastic process mining, computational analysis of trace data yields process models which describe process paths and their probability of execution. To understand the quality of these models, and to compare them, quantitative quality measures are used. This research investigates model comparison empirically, using stochastic process models built from real-life logs. The experimental design collects a large number of models generated randomly and using process discovery techniques. Twenty-five different metrics are taken on these models, using both existing process model metrics and new, exploratory ones. The results are analysed quantitatively, making particular use of principal component analysis. Based on this analysis, we suggest three stochastic process model dimensions: adhesion, relevance and simplicity. We also suggest possible metrics for these dimensions, and demonstrate their use on example models.}
}
@article{PAPAGIANNOPOULOS2025100397,
title = {Cosmological phase-space analysis of f(G)-theories of gravity},
journal = {Journal of High Energy Astrophysics},
volume = {47},
pages = {100397},
year = {2025},
issn = {2214-4048},
doi = {https://doi.org/10.1016/j.jheap.2025.100397},
url = {https://www.sciencedirect.com/science/article/pii/S2214404825000783},
author = {Giannis Papagiannopoulos and Orlando Luongo and Genly Leon and Andronikos Paliathanasis},
keywords = {Gauss-Bonnet, ()-gravity, Equilibrium points},
abstract = {The impact of topological terms that modify the Hilbert-Einstein action is here explored by virtue of a further f(G) contribution. In particular, we investigate the phase-space stability and critical points of an equivalent scalar field representation that makes use of a massive field, whose potential is function of the topological correction. To do so, we introduce to the gravitational Action Integral a Lagrange multiplier and model the modified Friedmann equations by virtue of new non-dimensional variables. We single out dimensionless variables that permit a priori the Hubble rate change of sign, enabling regions in which the Hubble parameter either vanishes or becomes negative. In this respect, we thus analyze the various possibilities associated with a universe characterized by such topological contributions and find the attractors, saddle points and regions of stability, in general. The overall analysis is carried out considering the exponential potential first and then shifting to more complicated cases, where the underlying variables do not simplify. We compute the eigenvalues of our coupled differential equations and, accordingly, the stability of the system, in both a spatially-flat and non-flat universe. Quite remarkably, regardless of the spatial curvature, we show that a stable de Sitter-like phase that can model current time appears only a small fraction of the entire phase-space, suggesting that the model under exam is unlikely in describing the whole universe dynamics, i.e., the topological terms appear disfavored in framing the entire evolution of the universe.}
}
@article{LIU2014330,
title = {Large scale two sample multinomial inferences and its applications in genome-wide association studies},
journal = {International Journal of Approximate Reasoning},
volume = {55},
number = {1, Part 3},
pages = {330-340},
year = {2014},
note = {Theory and applications of belief functions – Belief 2012},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2013.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X13000881},
author = {Chuanhai Liu and Jun Xie},
keywords = {Belief functions, Inference model},
abstract = {Statistical analysis of multinomial counts with a large number K of categories and a small number n of sample size is challenging to both frequentist and Bayesian methods and requires thinking about statistical inference at a very fundamental level. Following the framework of Dempster–Shafer theory of belief functions, a probabilistic inferential model is proposed for this “large K and small n” problem. The inferential model produces a probability triplet (p,q,r) for an assertion conditional on observed data. The probabilities p and q are for and against the truth of the assertion, whereas r=1−p−q is the remaining probability called the probability of “donʼt know”. The new inference method is applied in a genome-wide association study with very high dimensional count data, to identify association between genetic variants to the disease Rheumatoid Arthritis.}
}
@article{LEGLEITER20131,
title = {Introduction to the special issue: The field tradition in geomorphology},
journal = {Geomorphology},
volume = {200},
pages = {1-8},
year = {2013},
note = {The Field Tradition in Geomorphology 43rd Annual Binghamton Geomorphology Symposium, held 21-23 September 2012 in Jackson, Wyoming USA},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2013.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X13003140},
author = {Carl J. Legleiter and Richard A. Marston},
keywords = {Binghamton Geomorphology Symposium, Preface, Field work, Jackson Hole, Wyoming},
abstract = {In recognition of the critical role of field observations in the ongoing development of our discipline, the 43rd annual Binghamton Geomorphology Symposium (BGS) celebrated The Field Tradition in Geomorphology. By organizing a conference devoted to this theme, we sought to honor the contributions of pioneering, field-based geomorphologists and to encourage our community to contemplate how field work might continue to provide unique insight into a new, more technologically-driven era. For example, given recent advances in remote sensing methods such as LiDAR, what kind of added value can field work provide? Similarly, how can field-based studies contribute to societally relevant, large-scale questions related to climate change and sustainable management of the Earth system? Motivated by such questions, the 2012 BGS was convened in Jackson Hole, WY, a new, Western location that enabled participation by Rocky Mountain and west coast research groups underrepresented at previous Binghamton symposia. Also, in keeping with the field tradition theme, the 2012 BGS emphasized field trips, including a rafting excursion down the Snake River and an overview of the tectonic and glacial history of Jackson Hole. The on-site portion of the symposium consisted of invited oral and poster presentations and contributed posters, including many by graduate students. Topics ranged from an historical overview of the development of geomorphic thinking to long-term sediment tracer studies to a commentary on the synergy between LiDAR and field mapping. This special issue of Geomorphology consists of papers by invited authors from the 2012 BGS, and this overview provides some context for these contributions. Looking forward, we hope that the 43rd annual BGS will stimulate further discussion of the role of field work as the discipline of geomorphology continues to evolve, carrying on the field tradition into the future.}
}
@article{MOSS2013611,
title = {Senior Academic Physicians and Retirement Considerations},
journal = {Progress in Cardiovascular Diseases},
volume = {55},
number = {6},
pages = {611-615},
year = {2013},
note = {Symposium on Psychosocial Factors in Cardiovascular Disease},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2013.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S003306201300056X},
author = {Arthur J. Moss and Henry Greenberg and Edward M. Dwyer and Helmut Klein and Daniel Ryan and Charles Francis and Frank Marcus and Shirley Eberly and Jesaia Benhorin and Monty Bodenheimer and Mary Brown and Robert Case and John Gillespie and Robert Goldstein and Mark Haigney and Ronald Krone and Edgar Lichstein and Emanuela Locati and David Oakes and Poul Erik Bloch Thomsen and Wojciech Zareba},
keywords = {Academic physicians, Retirement issues, Retirement options},
abstract = {An increasing number of academic senior physicians are approaching their potential retirement in good health with accumulated clinical and research experience that can be a valuable asset to an academic institution. Considering the need to let the next generation ascend to leadership roles, when and how should a medical career be brought to a close? We explore the roles for academic medical faculty as they move into their senior years and approach various retirement options. The individual and institutional considerations require a frank dialogue among the interested parties to optimize the benefits while minimizing the risks for both. In the United States there is no fixed age for retirement as there is in Europe, but European physicians are initiating changes. What is certain is that careful planning, innovative thinking, and the incorporation of new patterns of medical practice are all part of this complex transition and timing of senior academic physicians into retirement.}
}
@article{JARRAHI2018577,
title = {Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making},
journal = {Business Horizons},
volume = {61},
number = {4},
pages = {577-586},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0007681318300387},
author = {Mohammad Hossein Jarrahi},
keywords = {Artificial intelligence, Organizational decision making, Human-machine symbiosis, Human augmentation, Analytical and intuitive decision making},
abstract = {Artificial intelligence (AI) has penetrated many organizational processes, resulting in a growing fear that smart machines will soon replace many humans in decision making. To provide a more proactive and pragmatic perspective, this article highlights the complementarity of humans and AI and examines how each can bring their own strength in organizational decision-making processes typically characterized by uncertainty, complexity, and equivocality. With a greater computational information processing capacity and an analytical approach, AI can extend humans’ cognition when addressing complexity, whereas humans can still offer a more holistic, intuitive approach in dealing with uncertainty and equivocality in organizational decision making. This premise mirrors the idea of intelligence augmentation, which states that AI systems should be designed with the intention of augmenting, not replacing, human contributions.}
}
@article{DECARVALHOBOTEGA2022109893,
title = {A data-driven Machine Learning approach to creativity and innovation techniques selection in solution development},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109893},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109893},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009868},
author = {Luiz Fernando {de Carvalho Botega} and Jonny Carlos {da Silva}},
keywords = {Decision support system, Creativity, Artificial intelligence, Design},
abstract = {The creation and refinement of new ideas is a strategic competence for teams and organization to innovate and prosper. This paper addresses the challenge of finding adequate creativity and innovation techniques (CITs) for improving individual or team creativity through the use of Machine Learning (ML). The process of choosing which CIT to use is complex and demanding, especially when taking into consideration the existence of hundreds of techniques and the plurality of different design contexts. This empiric knowledge, usually retained in an expert’s repertoire, can be extracted and implemented in a computational system, making it more available and permanent. This research focused on developing a Decision Support System embedded in an online application with a two-stage ML inference process able to evaluate users’ design scenario through an online form, and infer the most appropriate CITs from the database that would fit their needs. This paper presents two iterative development cycles of the prototype, first focused on core knowledge acquisition, representation, ML implementation, and verification; while second focused on system expansion, addition of web interface, and initial validation. After essaying 12 algorithms, the two-stage model achieved uses a Gradient Boosted Regression Trees algorithm using user provided information about the context to infer the required CITs characteristics; followed by a Logistic Regression classification-ranking algorithm that uses outputs from first model to define which CITs to present to users. To the best of our efforts, no other system was found to use ML approaches to address the problem of CIT selection.}
}
@article{SHUKLA2025128716,
title = {Deep belief network with fuzzy parameters and its membership function sensitivity analysis},
journal = {Neurocomputing},
volume = {614},
pages = {128716},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128716},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014875},
author = {Amit K. Shukla and Pranab K. Muhuri},
keywords = {Deep learning, Deep belief networks, Restricted Boltzmann machine, Fuzzy sets, Type-1 fuzzy sets, Contrastive divergence},
abstract = {Over the last few years, deep belief networks (DBNs) have been extensively utilized for efficient and reliable performance in several complex systems. One critical factor contributing to the enhanced learning of the DBN layers is the handling of network parameters, such as weights and biases. The efficient training of these parameters significantly influences the overall enhanced performance of the DBN. However, the initialization of these parameters is often random, and the data samples are normally corrupted by unwanted noise. This causes the uncertainty to arise among weights and biases of the DBNs, which ultimately hinders the performance of the network. To address this challenge, we propose a novel DBN model with weights and biases represented using fuzzy sets. The approach systematically handles inherent uncertainties in parameters resulting in a more robust and reliable training process. We show the working of the proposed algorithm considering four widely used benchmark datasets such as: MNSIT, n-MNIST (MNIST with additive white Gaussian noise (AWGN) and MNIST with motion blur) and CIFAR-10. The experimental results show superiority of the proposed approach as compared to classical DBN in terms of robustness and enhanced performance. Moreover, it has the capability to produce equivalent results with a smaller number of nodes in the hidden layer; thus, reducing the computational complexity of the network architecture. Additionally, we also study the sensitivity analysis for stability and consistency by considering different membership functions to model the uncertain weights and biases. Further, we establish the statistical significance of the obtained results by conducting both one-way and Kruskal-Wallis analyses of variance tests.}
}