@article{GENT202238,
title = {Making a mind},
journal = {New Scientist},
volume = {253},
number = {3374},
pages = {38-41},
year = {2022},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(22)00294-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407922002949},
author = {Edd Gent},
abstract = {In the push to make artificial intelligence that thinks like humans, many researchers are focused on fresh insights from neuroscience. Should they be looking to psychology instead, asks Edd Gent}
}
@article{PFOTENHAUER201638,
title = {Architecting complex international science, technology and innovation partnerships (CISTIPs): A study of four global MIT collaborations},
journal = {Technological Forecasting and Social Change},
volume = {104},
pages = {38-56},
year = {2016},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162515004102},
author = {Sebastian M. Pfotenhauer and Danielle Wood and Dan Roos and Dava Newman},
keywords = {Innovation policy, Research collaboration, Regional development, University partnerships, System architecture, Policy design, MIT, International partnerships},
abstract = {Complex international partnerships have emerged as a policy instrument of choice for many governments to build domestic capacity in science, technology and innovation with the help of foreign partners. At present, these flagship initiatives tend to be primarily practitioner-driven with limited systematic understanding of available design options and trade-offs. Here, we present an analysis of four such partnerships from the university sector between the Massachusetts Institute of Technology (MIT) and governments in the UK, Portugal, Abu Dhabi, and Singapore. Using a system architecture approach in conjunctions with in-depth case studies and elements of interpretive policy analysis, we map how in each country distinct capacity-building goals, activities, and political and institutional contexts translate into different partnership architectures: a bilateral hub-&-spokes architecture (UK), a consortium architecture (Portugal), an institution-building architecture (Abu Dhabi), and a functional expansion architecture (Singapore). Despite these differences in emergent macro-architectures, we show that each partnership draws on an identical, limited set of ‘forms’ that can by organized around four architectural views (education, research, innovation & entrepreneurship, institution-building) and four levels of interaction between partners (people, programs/projects, objects, organization/process). Based on our analysis, we derive a design matrix that can help guide the development future partnerships through a systematic understanding of available design choices. Our research underscores the utility and flexibility of complex international partnerships as systemic policy instruments. It suggests a greater role for global research universities in capacity-building and international development, and emphasizes the potential of targeted cross-border funding. Our research also demonstrates the analytic power of system architecture for policy analysis and design. We argue that architectural thinking provides a useful stepping stone for STS-type interpretive policy analysis into national innovation initiatives in different political cultures, as well as more custom-tailored approaches to program evaluation.}
}
@article{NOROOZI201279,
title = {Argumentation-Based Computer Supported Collaborative Learning (ABCSCL): A synthesis of 15 years of research},
journal = {Educational Research Review},
volume = {7},
number = {2},
pages = {79-106},
year = {2012},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2011.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X11000522},
author = {Omid Noroozi and Armin Weinberger and Harm J.A. Biemans and Martin Mulder and Mohammad Chizari},
keywords = {Argumentation, Argumentative knowledge construction, Collaborative argumentation, Computer-Supported Collaborative Learning, Argumentation-Based Computer Supported Collaborative Learning},
abstract = {Learning to argue is an essential objective in education; and online environments have been found to support the sharing, constructing, and representing of arguments in multiple formats for what has been termed Argumentation-Based Computer Supported Collaborative Learning (ABCSCL). The purpose of this review is to give an overview of research in the field of ABCSCL and to synthesize the findings. For this review, 108 publications (89 empirical studies and 19 conceptual papers) on ABCSCL research dating from 1995 through 2011 were studied to highlight the foci of the past 15 years. Building on Biggs’ (2003) model, the ABCSCL publications were systematically categorized with respect to student prerequisites, learning environment, processes, and outcomes. Based on the quantitative and qualitative findings, this paper concludes that ABCSCL environments should be designed in a systematic way that takes the variety of specific conditions for learning into account. It also offers suggestions for educational practice and future research.}
}
@article{BURGESS2011427,
title = {On system rollback and totalized fields: An algebraic approach to system change},
journal = {The Journal of Logic and Algebraic Programming},
volume = {80},
number = {8},
pages = {427-443},
year = {2011},
issn = {1567-8326},
doi = {https://doi.org/10.1016/j.jlap.2011.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1567832611000488},
author = {Mark Burgess and Alva Couch},
abstract = {In system operations the term rollback is often used to imply that arbitrary changes can be reversed i.e. ‘rolled back’ from an erroneous state to a previously known acceptable state. We show that this assumption is flawed and discuss error-correction schemes based on absolute rather than relative change. Insight may be gained by relating change management to the theory of computation. To this end, we reformulate previously-defined ‘convergent change operators’ of Burgess into the language of groups and rings. We show that, in this form, the problem of rollback from a convergent operation becomes equivalent to that of ‘division by zero’ in computation. Hence, we discuss how recent work by Bergstra and Tucker on zero-totalized fields helps to clear up long-standing confusion about the options for ‘rollback’ in change management.}
}
@article{FERNANDES202191,
title = {Pruning of generative adversarial neural networks for medical imaging diagnostics with evolution strategy},
journal = {Information Sciences},
volume = {558},
pages = {91-102},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.12.086},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521000189},
author = {Francisco Erivaldo Fernandes and Gary G. Yen},
keywords = {Deep Neural Networks, Convolutional Neural Networks, Generative Adversarial Networks, Medical Imaging Diagnostics, Evolution Strategy, Pruning},
abstract = {Deep Convolutional Neural Networks (DCNNs) have the potential to revolutionize the field of Medical Imaging Diagnostics due to their capabilities of learning by using only raw data. However, DCNNs can only learn when trained using thousands of data points, which is not always available when dealing with medical data. Moreover, due to patient privacy concerns and the small prevalence of certain diseases in the population, medical data often presents unbalanced classes and fewer data points than other data types. Researchers often rely on Generative Adversarial Networks (GANs) to synthesize more data from a given distribution to solve this problem. Nevertheless, GANs are computationally intensive models requiring the use of powerful hardware to run. In the present work, an algorithm for pruning GANs based on Evolution Strategy (ES) and Multi-Criteria Decision Making (MCDM) is proposed in which a model with the best trade-off between computational complexity and synthesis performance can be found without the use of any trade-off parameter. In the proposed algorithm, the model with the best trade-off is defined geometrically as the candidate solution with the minimum Manhattan distance (MMD) in a two-dimensional objective space established by the number of Floating-Point Operations (FLOPs) and the Wasserstein distance of all candidate solutions, also known as the knee solution. The results show that the pruned GAN model achieves similar performance compared with the original model with up to 70% fewer Floating-Point Operations.}
}
@article{TURNER2025100843,
title = {Meet the author: Tychele N. Turner, PhD},
journal = {Cell Genomics},
volume = {5},
number = {4},
pages = {100843},
year = {2025},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2025.100843},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X25000990},
author = {Tychele N. Turner},
abstract = {Dr. Tychele Turner is an assistant professor of genetics at the Washington University in St. Louis. This issue of Cell Genomics presents research from her lab in “Proteome-wide assessment of differential missense variant clustering in neurodevelopmental disorders and cancer” by Ng et al. This paper used their newly developed program, 3D-CLUMP, which can perform proteome-wide significant case-control analysis and clustering of protein-coding variants related to disease. Using 3D-CLUMP, they examine how patients with mostly cancer or neurodevelopmental disorders (NDDs) are observed to have the same gene, even if the specific mutations causing them are not shared.}
}
@article{HIEBERT1992439,
title = {Chapter 3 Reflection and communication: Cognitive considerations in school mathematics reform},
journal = {International Journal of Educational Research},
volume = {17},
number = {5},
pages = {439-456},
year = {1992},
issn = {0883-0355},
doi = {https://doi.org/10.1016/S0883-0355(05)80004-7},
url = {https://www.sciencedirect.com/science/article/pii/S0883035505800047},
author = {James Hiebert},
abstract = {The mathematics education reform efforts in the United States are shaped partially by our understanding of how students learn mathematics. Two traditions in psychology influence our current thinking most forcefully — cognitive psychology with its emphasis on individual mental operations and social cognition with its emphasis on context and group interaction. Reflection and communication, as cognitive processes and as representatives of these respective traditions, are used to establish the cognitive-based rationale for the reform and to analyze the nature of recommended changes. Issues addressed include the interdependence of reflection and communication and the way in which these processes can be used to analyze aspects of the school mathematics program, such as the way textbooks ordinarily treat written symbols. Although the theoretical arguments for reflection and communication are being increasingly well-articulated, the empirical data that address the claims are comparatively sparse. Future research efforts should aim to test theoretical claims for reflection and communication and to increase our understanding of the relationships between these cognitive processes and learning mathematics.}
}
@article{PAI201872,
title = {Assessing mobile health applications with twitter analytics},
journal = {International Journal of Medical Informatics},
volume = {113},
pages = {72-84},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618301199},
author = {Rajesh R. Pai and Sreejith Alathur},
keywords = {Mobile health, Sentiment analysis, Twitter analytics, Causal loop diagram, Technology adoption model},
abstract = {Introduction
Advancement in the field of information technology and rise in the use of Internet has changed the lives of people by enabling various services online. In recent times, healthcare sector which faces its service delivery challenges started promoting and using mobile health applications with the intention of cutting down the cost making it accessible and affordable to the people.
Objectives
The objective of the study is to perform sentiment analysis using the Twitter data which measures the perception and use of various mobile health applications among the citizens.
Methods
The methodology followed in this research is qualitative with the data extracted from a social networking site “Twitter” through a tool RStudio. This tool with the help of Twitter Application Programming Interface requested one thousand tweets each for four different phrases of mobile health applications (apps) such as “fitness app”, “diabetes app”, “meditation app”, and “cancer app”. Depending on the tweets, sentiment analysis was carried out, and its polarity and emotions were measured.
Results
Except for cancer app there exists a positive polarity towards the fitness, diabetes, and meditation apps among the users. Following a system thinking approach for our results, this paper also explains the causal relationships between the accessibility and acceptability of mobile health applications which helps the healthcare facility and the application developers in understanding and analyzing the dynamics involved the adopting a new system or modifying an existing one.}
}
@article{SCHIFFMAN20041079,
title = {Mainstream economics, heterodoxy and academic exclusion: a review essay},
journal = {European Journal of Political Economy},
volume = {20},
number = {4},
pages = {1079-1095},
year = {2004},
issn = {0176-2680},
doi = {https://doi.org/10.1016/j.ejpoleco.2004.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0176268004000643},
author = {Daniel A. Schiffman},
keywords = {Academic exclusion, Pluralism, Economics education, Historical specificity, Heterodoxy},
abstract = {Does the mainstream of economic thinking and analysis tend systematically to exclude ideas and approaches that could enrich the field, and, as a consequence, have important questions and issues been shunted aside for nonobjective reasons? Two recent volumes by heterodox economists that address these questions are Geoffrey Hodgson's How Economics Forgot History: The Problem of Historical Specificity in Social Science, and Steve Keen's Debunking Economics: The Naked Emperor of the Social Sciences. I evaluate their claims of academic exclusion and assess the current state of (selective) pluralism within mainstream economics.}
}
@article{WU2023101906,
title = {Human–machine hybrid intelligence for the generation of car frontal forms},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101906},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101906},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000344},
author = {Yu Wu and Lisha Ma and Xiaofang Yuan and Qingnan Li},
keywords = {Car frontal form, Creative generation, Human–machine hybrid intelligence, Human–machine shared knowledge base, generative adversarial network (GAN)},
abstract = {With the acceleration of the upgrading of the automobile consumption market, artificial intelligence has become an increasingly effective means of enhancing the creative design of automobile appearance modeling. However, when artificial intelligence processes specific design tasks, creativity is primarily based on data drive, resulting in machine-generated design schemes that do not match human-specific psychological intentions. Due to the absence of design knowledge in the process of machine design, there is a data gap between human cognitive thought and machine information processing. This paper aims to structure the human's complex cognitive knowledge of car frontal form, establish the consistency between human and machine cognitive structures, and reduce communication barriers in the process of human–machine hybrid creative design. To achieve this objective, a human–machine hybrid intelligence methodology – a combination of human cognitive mental model, human–machine shared knowledge base, and Generative Adversarial Networks (GAN) – was developed to generate a large number of car frontal forms that are consistent with the design intent. First, we constructeda mental model of human cognition based on three dimensions: design intent, drawing behavior, and functional structure. Second, we created a shared human–machine knowledge base with design Knowledge. This knowledge base contains 12,560 images of car frontal form designs with corresponding morphological semantic labels and 3,140 sketches of car frontal forms drawn by hand. Human–machine shared knowledge base datawasutilized in a machine learning training network. In addition, a conditional cross-domain generative adversarial network was developed to investigate the implicit relationship between sketch characteristics, morphological semantics, and image visual effects. Using the suggested method, a large number of images with the specified morphological semantic category and resembling the hand-drawn sketch of a car frontal form can be generated rapidly. In terms of the quality of car frontal form generation, our research is superior to the baseline model according to qualitative and quantitative assessments. In comparison to the designer's output, the human–machine hybrid intelligent generation also demonstrates excellent creative performance.}
}
@article{BAUERNEGRINI2022105785,
title = {Usability evaluation of circRNA identification tools: Development of a heuristic-based framework and analysis},
journal = {Computers in Biology and Medicine},
volume = {147},
pages = {105785},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105785},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522005522},
author = {Guilherme Bauer-Negrini and Guilherme {Cordenonsi da Fonseca} and Carmem Gottfried and Juliana Herbert},
keywords = {Usability, Bioinformatics, circRNA, Heuristic evaluation, Command-line interface},
abstract = {Background and objective
Circular RNAs (circRNAs) are endogenous molecules of non-coding RNA that form a covalently closed loop at the 3′ and 5′ ends. Recently, the role of these molecules in the regulation of gene expression and their involvement in several human pathologies has gained notoriety. The identification of circRNAs is highly dependent on computational methods for analyzing RNA sequencing data. However, bioinformatics software is known to be problematic in terms of usability. Evidence points out that tools for identifying circRNAs can have such problems, negatively impacting researchers in this field. Here we present a heuristic-based framework for evaluating the usability of command-line circRNA identification software.
Methods
We used heuristics evaluation to comprehensively identify the usability issues in a sample of circRNA identification tools.
Results
We identified 46 usability issues presented individually in four tools. Most of the issues had cosmetic or minor severity. These are unlikely to challenge experienced users but may cause inconvenience for novice users. We also identified severe issues with the potential to harm users regardless of their experience. The areas most affected were the documentation and the installability of the tools.
Conclusions
With the proposed framework, we formally describe, for the first time, the usability problems that can affect users in this area of circRNA research. We hope that our framework can help researchers evaluate their software's usability during development.}
}
@article{SPROULE2002412,
title = {Fuzzy pharmacology: theory and applications},
journal = {Trends in Pharmacological Sciences},
volume = {23},
number = {9},
pages = {412-417},
year = {2002},
issn = {0165-6147},
doi = {https://doi.org/10.1016/S0165-6147(02)02055-2},
url = {https://www.sciencedirect.com/science/article/pii/S0165614702020552},
author = {Beth A. Sproule and Claudio A. Naranjo and I.Burhan Türksen},
keywords = {fuzzy logic, pharmacodynamics, fuzzy sets, modelling, predictions, pharmacokinetics},
abstract = {Fuzzy pharmacology is a term coined to represent the application of fuzzy logic and fuzzy set theory to pharmacological problems. Fuzzy logic is the science of reasoning, thinking and inference that recognizes and uses the real world phenomenon that everything is a matter of degree. It is an extension of binary logic that is able to deal with complex systems because it does not require crisp definitions and distinctions for the system components. In pharmacology, fuzzy modeling has been used for the mechanical control of drug delivery in surgical settings, and work has begun evaluating its use in other pharmacokinetic and pharmacodynamic applications. Fuzzy pharmacology is an emerging field that, based on these initial explorations, warrants further investigation.}
}
@article{TILLS2025111783,
title = {Bioimaging and the future of whole-organismal developmental physiology},
journal = {Comparative Biochemistry and Physiology Part A: Molecular & Integrative Physiology},
volume = {300},
pages = {111783},
year = {2025},
issn = {1095-6433},
doi = {https://doi.org/10.1016/j.cbpa.2024.111783},
url = {https://www.sciencedirect.com/science/article/pii/S1095643324002101},
author = {Oliver Tills and Ziad Ibbini and John I. Spicer},
keywords = {Imaging, Phenomics, Deep learning, Embryonic development, Developmental physiology, Comparative developmental physiology, Computer vision},
abstract = {While omics has transformed the study of biology, concomitant advances made at the level of the whole organism, i.e. the phenome, have arguably not kept pace with lower levels of biological organisation. In this personal commentary we evaluate the importance of imaging as a means of measuring whole organismal developmental physiology. Image acquisition, while an important process itself, has become secondary to image analysis as a bottleneck to the use of imaging in research. Here, we explore the significant potential for increasingly sophisticated approaches to image analysis, including deep learning, to advance our understanding of how developing animals grow and function. Furthermore, unlike many species-specific methodologies, tools and technologies, we explore how computer vision has the potential to be transferable between species, life stages, experiments and even taxa in which embryonic development can be imaged. We identify what we consider are six of the key challenges and opportunities in the application of computer vision to developmental physiology carried out in our lab, and more generally. We reflect on the tangibility of transferrable computer vision models capable of measuring the integrative physiology of a broad range of developing organisms, and thereby driving the adoption of phenomics for developmental physiology. We are at an exciting time of witnessing the move from computer vision as a replacement for manual observation, or manual image analysis, to it enabling a fundamentally more powerful approach to exploring and understanding the complex biology of developing organisms, the quantification of which has long posed a challenge to researchers.}
}
@article{MAO2024101988,
title = {A survey on semantic processing techniques},
journal = {Information Fusion},
volume = {101},
pages = {101988},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101988},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003044},
author = {Rui Mao and Kai He and Xulang Zhang and Guanyi Chen and Jinjie Ni and Zonglin Yang and Erik Cambria},
keywords = {Semantic processing, Word sense disambiguation, Anaphora resolution, Named entity recognition, Concept extraction, Subjectivity detection},
abstract = {Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.}
}
@article{STOJANOVIC2021107270,
title = {Application of distance learning in mathematics through adaptive neuro-fuzzy learning method},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107270},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107270},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002536},
author = {Jelena Stojanović and Dalibor Petkovic and Ibrahim M Alarifi and Yan Cao and Nebojsa Denic and Jelena Ilic and Hamid Assilzadeh and Sead Resic and Biljana Petkovic and Afrasyab Khan and Milosav Milickovic},
keywords = {Pupils, E-learning, Distance learning, Moodle, Computational intelligent},
abstract = {The main aim of the study is analyzing of pupils’ knowledge in mathematics by adaptive neuro fuzzy inference system (ANFIS) after implementation of distance learning application or e-learning (electronic learning). Since a large number of faculties and other institutions are increasingly using e-learning, it can be stated that for this purpose the Modular object-oriented dynamic learning environment (Moodle) learning management system (LMS) is mostly used. This paper deals with the analysis of distance learning and the application of Moodle LMS in higher education institutions, taking into account the impact of such education on the quality of teaching and the acquisition of knowledge by students, and the methods teachers use in Serbia. The ANFIS is used to determine which factors are the most important for pupils’ performance in mathematics. The results show that the main influence on the pupils’ performance is their prior knowledge. The prior knowledge is more effective when it is combined with education software in the lectures of mathematics in elementary school. In secondary school, the prior knowledge is more effective if it is combined with motivation for learning mathematics.}
}
@article{ROLAND2002183,
title = {Dynamic depolarization fields in the cerebral cortex},
journal = {Trends in Neurosciences},
volume = {25},
number = {4},
pages = {183-190},
year = {2002},
issn = {0166-2236},
doi = {https://doi.org/10.1016/S0166-2236(00)02125-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166223600021251},
author = {Per E. Roland},
keywords = {general computational elements, voltage sensitive dyes, cortical dynamics, layer II-III neurons, memory, cognition},
abstract = {Recent physiological evidence shows that in response to stimuli and preceding motor activity, large fields of the upper layers of the cerebral cortex depolarize. It is argued that this finding is a general one and that these dynamic depolarization fields represent the computational elements of the cerebral cortex. Each depolarization field engages many more neurons than do columns and hyper-columns. These fields can be explained by cooperative neuronal computing in layers I–III of the cortex. In these layers, the computing modes might be general for all parts of the cerebral cortex and be sufficiently flexible to handle all sorts of cortical computations, including perception, memory storage, memory retrieval, thought and the production of behavior.}
}
@article{ARIFOVIC20071971,
title = {Call market book information and efficiency},
journal = {Journal of Economic Dynamics and Control},
volume = {31},
number = {6},
pages = {1971-2000},
year = {2007},
note = {Tenth Workshop on Economic Heterogeneous Interacting Agents},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2007.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0165188907000073},
author = {Jasmina Arifovic and John Ledyard},
keywords = {Computer testbeds, Call markets, Learning, Experiments with human subjects, Closed book, Market design},
abstract = {What are the consequences of making bids and offers in the book available to traders in a call market? This is a problem in market design. We employ a computational mechanism design methodology to attack this problem and find that allocative efficiencies are higher in a closed book design. We validate our computational approach by running a series of tests with human subjects in exactly the same environments.}
}
@article{BUAH2021103269,
title = {Augmenting the communication and engagement toolkit for CO2 capture and storage projects},
journal = {International Journal of Greenhouse Gas Control},
volume = {107},
pages = {103269},
year = {2021},
issn = {1750-5836},
doi = {https://doi.org/10.1016/j.ijggc.2021.103269},
url = {https://www.sciencedirect.com/science/article/pii/S1750583621000219},
author = {Eric Buah and Lassi Linnanen and Huapeng Wu},
keywords = {Artificial intelligence, CO capture and storage, CCS communication and engagement, CCS toolkit, CCS SWOT, Deep neural network algorithm, Fuzzy logic, Fuzzy deep learning},
abstract = {This paper revisits the Communication and Engagement Toolkit for CO2 Capture and Storage (CCS) projects proposed by Ashworth and colleagues in collaboration with the Global CCS Institute. The paper proposes a new method for understanding the social context where CCS will be deployed based on the toolkit. In practice, the proposed method can be used to harness social data collected on the CCS project. The outcome of this application is a development of a predictive tool for gaining insight into the future, to guide strategic decisions that may enhance deployment. Methodologically, the proposed predictive tool is an artificial intelligence (AI) tool. It uses fuzzy deep neural network to develop computational ability to reason about the social behavior. The hybridization of fuzzy logic and deep neural network algorithms make the predictive tool an explainable AI system. It means that the prediction of the algorithm is interpretable using fuzzy logical rules. The practical feasibility of the proposed system has been demonstrated using an experimental sample of 198 volunteers. Their perceptions, emotions and sentiments were tested using a standard questionnaire from the literature, on a hypothetical CCS project based on 26 predictors. The generalizability of the algorithm to predict future reactions was tested on, 84 out-of-sample respondents. In the simulation experiment, we observed an approximately 90 % performance. This performance was measured when the algorithm's predictions were compared to the self- reported reactions of the out of sample subjects. The implication of the proposed tool to enhance the predictive power of the conventional CCS Communication and Engagement tool is discussed © 2020 xx. Hosting by Elsevier B.V. All rights reserved.}
}
@incollection{BERGER20234,
title = {3.02 - Electronic structure of oxide and halide perovskites},
editor = {Jan Reedijk and Kenneth R. Poeppelmeier},
booktitle = {Comprehensive Inorganic Chemistry III (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {4-25},
year = {2023},
isbn = {978-0-12-823153-1},
doi = {https://doi.org/10.1016/B978-0-12-823144-9.00102-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128231449001023},
author = {Robert F. Berger},
keywords = {Band structure, Density functional theory, Perovskite, Photocatalyst, Photovoltaic},
abstract = {Compounds crystallizing in the ABX3 perovskite structure are studied for a remarkable variety of technologies. Particularly for applications such as photovoltaics and photocatalysis, it is crucial to understand the key features of perovskite electronic structure and how they can be tuned by modifying the composition and crystal structure. This chapter begins with an overview of the compositional and structural diversity of perovskites. Then, density functional theory-based computational methods that have been used to study perovskite compounds are described. Next, the electronic band structures of an undistorted oxide (SrTiO3) and halide (CsPbI3) perovskite are explained in detail, merging the viewpoints of crystal wavefunctions as both linear combinations of atomic orbitals and perturbed plane waves. Finally, routes toward the tunability of perovskite electronic structure and properties are reviewed for various modifications: changes in elemental composition, various modes of geometric distortion, the application of high pressure or strain, and the formation of superstructures with reduced dimensionality. While the concepts and discussion herein are relevant to all perovskite compounds, the examples described in this chapter are mainly d0 oxide perovskite photocatalysts and halide perovskite photovoltaics.}
}
@article{PALHARESDEMELO200121,
title = {Recommendation for fertilizer application for soils via qualitative reasoning},
journal = {Agricultural Systems},
volume = {67},
number = {1},
pages = {21-30},
year = {2001},
issn = {0308-521X},
doi = {https://doi.org/10.1016/S0308-521X(00)00044-5},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X00000445},
author = {L.A.M. {Palhares de Melo} and D.J. Bertioli and E.V.M. Cajueiro and R.C. Bastos},
keywords = {Fertilizer application, Qualitative reasoning, Approximate reasoning},
abstract = {In Brazil, liming and fertilization are essential practices in agriculture due to the soils being acidic and poor in nutrients. Distinct decision tables that serve as support for recommendation of fertilizer application are used, but one of their features is that they are based on a strictly quantitative analysis of input variables. This sometimes causes dificulties in their use when calculating the output (recommended fertilizer application). This work presents a model for the use and interpretation of decision tables for fertilizer application. It is based on a qualitative characterization for the rules and input variables used. The results have shown that this approach gives feasible results which more accurately reflect human thinking about the decision table.}
}
@incollection{RAHI2025597,
title = {Chapter 56 - Crowdsourcing and artificial intelligence based modeling framework for effective Public Healthcare Informatics and Smart eHealth System},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {597-608},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00056-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321870500056X},
author = {Pankaj Rahi and Monika Dandotiya and Santi Basa and Souvik Sen and Mayur D. Jakhete and P. Vijayakumar},
keywords = {Artificial intelligence, Crowdsourcing, IoT, Machine learning, Public health informatics, Smart eHealth system},
abstract = {With the help of a multiagent interactive healthcare plan, healthy and independent aging is possible. Testing one's fitness and keeping tabs on one's health can both benefit from an awareness of one's regular routine. The Healthcare Strategies Partnership is introduced here. Geographic and economic factors, including the prevalence of infectious tropical diseases and an increasing number of chronic illnesses, have contributed to a shift in the region's medical requirements. This takes place in a world that is not only difficult but also intricate. This system employs a smartphone's sensor, a machine learning algorithm, multiple agents (including a doctor, fitness instructor, guardian, and intelligent ranker agent), and a smartphone itself to increase a user's sense of independence. A group of health professionals collaborate to assess the patient's day-to-day activities and offer suggestions for improvement. The algorithm figures out a typical day in the life of an adult. A smart autonomous agent using crowdsourced data recommends the best possible treatment plan. In contrast to crowdsourcing, which places value on the abilities of people to generate, aggregate, or filter original data, automatic tools make use of information retrieval techniques to analyze publicly available information. Crowdsourcing, which facilitates collaboration among numerous individuals, is increasingly being used in a variety of industries. Methodical crowdsourcing of useful data and human computation of interchangeable knowledge will aid future advancements in public health informatics. The disease burden on any country can and will be decreased by these efforts the share of healthcare costs borne by individuals and their families. The work also aids in achieving the most crucial objectives along the path to the Sustainable Development Goals. To improve public health surveillance for the prevention and control of communicable and noncommunicable diseases, this chapter presents the fundamental modeling for its design and development. So that a smart autonomous agent can recommend the best course of treatment, lowering the disease burden in any country. Crowdsourcing is defined, and how it can be used to better public health, in this chapter of a book. They are better able to achieve a healthy lifestyle exit, and the statistics and indicators that are tracked primarily under the Sustainable Development Goals Framework are improved as a result. The goal of this chapter is to provide a high-level overview of recent developments in applying artificial intelligence techniques to public health surveillance and response, which is the single most important step toward preventing the spread of disease and saving the lives of humans and other sentient beings.}
}
@article{RAN2024102578,
title = {Spatiotemporal characteristics and influencing factors of airport service quality in China},
journal = {Journal of Air Transport Management},
volume = {117},
pages = {102578},
year = {2024},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2024.102578},
url = {https://www.sciencedirect.com/science/article/pii/S0969699724000437},
author = {Xinyue Ran and Lingling Li and Ruiling Han},
keywords = {Airport, Aviation complaint, Service quality, Influencing factors, Spatiotemporal differentiation characteristic},
abstract = {Airport service quality (ASQ) is essential for determining the quality of ground civil aviation services. In this study, ASQ was assessed using the monthly airport aviation complaint data from 2015 to 2019 of 196 airports in mainland China (except for airports in Hong Kong, Macao, and Taiwan, which are not included in the statistics). First, we constructed a seasonal index of aviation complaints to evaluate and compare the overall temporal characteristics of ASQ in China. Second, the spatial distribution pattern of ASQ in China was determined using the aviation complaint concentration index and hot spot analysis model. Finally, the major influencing factors and categories of ASQ in China were analyzed considering spatiotemporal dimensions using the correspondence analysis method. The results revealed that there were clear seasonal differences among ASQ in China, with a high–low–low–high distribution during all four seasons. The regional agglomeration trend of airport aviation complaints was obvious, and the spatial difference in ASQ was large. Northern and western China performed better than southern China. Spatiotemporal and influencing factors of ASQ were the most strongly correlated factors in each quarter and region. The predominant source of aviation complaints across all types of airports is related to fundamental services, with check-in services identified as the most impactful category affecting ASQ. This study, based on 60 months of statistical data, offers a comprehensive evaluation of ASQ throughout the entire airport network in mainland China, from the perspective of aviation complaints. Additionally, a systematic ASQ evaluation method and research system encompassing time, space, and elements were established. This framework not only stimulates the improvement and enhancement of ASQ but also provides a theoretical foundation for differentially enhancing ASQ in regional airports. Overall, our results contribute to breaking through the qualitative research thinking system in ASQ research from a theoretical perspective, paving the way for exploring broader research in enhancing the quality of ground civil aviation services.}
}
@article{FURIA2007164,
title = {Automated compositional proofs for real-time systems},
journal = {Theoretical Computer Science},
volume = {376},
number = {3},
pages = {164-184},
year = {2007},
note = {Fundamental Aspects of Software Engineering},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507000643},
author = {Carlo A. Furia and Matteo Rossi and Dino Mandrioli and Angelo Morzenti},
keywords = {Formal verification, Modular systems, Real-time, Compositionality, Rely/guarantee},
abstract = {We present a framework for formally proving that the composition of the behaviors of the different parts of a complex, real-time system ensures a desired global specification of the overall system. The framework is based on a simple compositional rely/guarantee circular inference rule, plus a methodology concerning the integration of the different parts into a whole system. The reference specification language is the TRIO metric linear temporal logic. The novelty of our approach with respect to existing compositional frameworks–most of which do not deal explicitly with real-time requirements–consists mainly in its generality and abstraction from any assumptions about the underlying computational model and from any semantic characterizations of the temporal logic language used in the specification. Moreover, the framework deals equally well with continuous and discrete time. It is supported by a tool, implemented on top of the proof-checker PVS, to perform deduction-based verification through theorem-proving of modular real-time axiom systems. As an example of application, we show the verification of a real-time version of the old-fashioned but still relevant “benchmark” of the dining philosophers problem.}
}
@article{YOUSIF20241342,
title = {Safety 4.0: Harnessing computer vision for advanced industrial protection},
journal = {Manufacturing Letters},
volume = {41},
pages = {1342-1356},
year = {2024},
note = {52nd SME North American Manufacturing Research Conference (NAMRC 52)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2024.09.161},
url = {https://www.sciencedirect.com/science/article/pii/S2213846324002451},
author = {Ibrahim Yousif and Jad Samaha and JuHyeong Ryu and Ramy Harik},
keywords = {Smart Manufacturing, Safety 4.0, Computer Vision, Industrial Protection, Musculoskeletal disorders (MSD), Effective Functional Training},
abstract = {In the pursuit of enhanced productivity, reduced costs, and minimized lead times, manufacturers are transitioning from traditional systems to autonomous systems. This shift, driven by the emergence of smart manufacturing and technological advancements such as robotics, collaborative robots (Cobots), automation, and digitalization, necessitates a parallel evolution in safety protocols—termed Safety 4.0—to mitigate the risks associated with such dynamic environments. The integration of smart technologies within manufacturing significantly transforms traditional workflows and intensifies the need for comprehensive safety training and guidelines. Innovations like smart personal protective equipment (PPE) and wearable sensors are pivotal in this transition, yet they often prove financially burdensome for manufacturers due to high costs and the scale of workforce deployment. Moreover, the effective use of these technologies requires continuous monitoring and data analysis, further straining resources. To address these challenges, this paper proposes the adoption of computer vision technology to enhance safety measures within manufacturing facilities, focusing on human and PPE detection. It details a holistic methodology encompassing data collection, preprocessing, training, and execution. The discussion extends to the implementation framework of this technology, emphasizing its role in enabling autonomous decision-making—a crucial step beyond mere detection. Furthermore, the paper explores the utilization of the accumulated data to develop immersive training modules employing Mixed Reality, thereby reinforcing safety protocols and fostering an environment of continuous learning and adaptation. This approach not only contributes to safeguarding personnel but also aligns with the financial and reputational interests of forward-thinking manufacturers.}
}
@article{WANG2023e131,
title = {Interbody Fusion Cage Design Driven by Topology Optimization},
journal = {World Neurosurgery},
volume = {174},
pages = {e131-e143},
year = {2023},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2023.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1878875023003042},
author = {Zuowei Wang and Jun Jiang and Fengzeng Jian and Zan Chen and Xingwen Wang and Wanru Duan and Weisheng Zhang},
keywords = {Fusion cage design, Interbody fusion, Moving morphable void approach, Topology optimization},
abstract = {Objective
We used topology optimization technology to explore the new theory and method of interbody fusion cage design and realized an innovative design of interbody cages.
Methods
The lumbar spine of a normal healthy volunteer was scanned to perform reverse modeling. Based on the scan data for the L1-L2 segments of the lumbar spine, a three dimensional model was reconstructed to obtain the complete simulation model of the L1-L2 segment. The boundary inversion method was used to obtain approximately isotropic material parameters that can effectively characterize the mechanical behavior of vertebrae, thereby reducing the computational complexity. The topology description function was used to model the clinically used traditional fusion cage to obtain Cage A. The moving morphable void-based topology optimization method was used for the integrated design of size, shape, and topology to obtain the optimized fusion cage, Cage B.
Results
The volume fraction of the bone graft window in Cage B was 74.02%, which was 60.67% higher than that (46.07%) in Cage A. Additionally, the structural strain energy in the design domain of Cage B was 1.48 mJ, which was lower than that of Cage A (satisfying the constraints). The maximum stress in the design domain of Cage B was 5.336 Mpa, which was 35.6% lower than that (8.286 Mpa) of Cage A. In addition, the surface stress distribution of Cage B was more uniform than that of Cage A.
Conclusions
This study proposed a new innovative design method for interbody fusion cages, which not only provides new insights into the innovative design of interbody fusion cages but may also guide the customized design of interbody fusion cages in different pathological environments.}
}
@article{LIAO2025100388,
title = {Design and FPGA implementation of a zero IF I/Q blind calibration algorithm},
journal = {Next Research},
volume = {2},
number = {2},
pages = {100388},
year = {2025},
issn = {3050-4759},
doi = {https://doi.org/10.1016/j.nexres.2025.100388},
url = {https://www.sciencedirect.com/science/article/pii/S3050475925002581},
author = {Yongbo Liao and Lang Li and Linhan Li and Jiangshan Liang and Mengyou Li and Rui Chen and Xiongfei Chen and Menghao Wang and Wu Wen},
keywords = {Zero intermediate frequency, I/Q imbalance, FastICA algorithm, FPGA Implementation},
abstract = {Summary
The aim of this study is to validate the implementation of a digital domain correction method on hardware with basis of the FastICA algorithm. This algorithm compensates for I/Q imbalance by separating independent components from mixed signals, while introducing differential thinking to adjust correction parameters in real-time to adapt to the processing of streaming signals. The effectiveness of the proposed algorithm was verified through simulation and hardware testing. The results showed that, under 1 MHz single tone signal input and 100 MHz sampling frequency, after correction, the image suppression ratio increased from 13.5 dB to 55.8 dB, and in hardware testing, it increased to 51.4 dB. It can be seen that the image suppression module designed in this study can effectively suppress DC offset and image interference, improve the performance of zero IF transceivers, and confirm an effective I/Q imbalance correction method.}
}
@article{DANILOV2019108891,
title = {On the geometric origin of spurious waves in finite-volume discretizations of shallow water equations on triangular meshes},
journal = {Journal of Computational Physics},
volume = {398},
pages = {108891},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.108891},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119305893},
author = {S. Danilov and A. Kutsenko},
keywords = {Triangular meshes, Finite volume discretization, Computational dispersion branches},
abstract = {Computational wave branches are common to linearized shallow water equations discretized on triangular meshes. It is demonstrated that for standard finite-volume discretizations these branches can be traced back to the structure of the unit cell of triangular lattice, which includes two triangles with a common edge. Only subsets of similarly oriented triangles or edges possess the translational symmetry of unit cell. As a consequence, discrete degrees of freedom placed on triangles or edges are geometrically different, creating an internal structure inside unit cells. It implies a possibility of oscillations inside unit cells seen as computational branches in the framework of linearized shallow water equations, or as grid-scale noise generally. Adding dissipative operators based on smallest stencils to discretized equations is needed to control these oscillations in solutions. A review of several finite-volume discretization is presented with focus on computational branches and dissipative operators.}
}
@article{SUZUKI2008511,
title = {Research and development of fusion grid infrastructure based on atomic energy grid infrastructure (AEGIS)},
journal = {Fusion Engineering and Design},
volume = {83},
number = {2},
pages = {511-515},
year = {2008},
note = {Proceedings of the 6th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2007.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S092037960700498X},
author = {Y. Suzuki and K. Nakajima and N. Kushida and C. Kino and T. Aoyagi and N. Nakajima and K. Iba and N. Hayashi and T. Ozeki and T. Totsuka and H. Nakanishi and Y. Nagayama},
keywords = {Grid, AEGIS, JT-60, LHD, LABCOM},
abstract = {In collaboration with the Naka Fusion Institute of Japan Atomic Energy Agency (NFI/JAEA) and the National Institute for Fusion Science of National Institute of Natural Science (NIFS/NINS), Center for Computational Science and E-systems of Japan Atomic Energy Agency (CCSE/JAEA) aims at establishing an integrated framework for experiments and analyses in nuclear fusion research based on the atomic energy grid infrastructure (AEGIS). AEGIS has been being developed by CCSE/JAEA aiming at providing the infrastructure that enables atomic energy researchers in remote locations to carry out R&D efficiently and collaboratively through the Internet. Toward establishing the integrated framework, we have been applying AEGIS to pre-existing three systems: experiment system, remote data acquisition system, and integrated analysis system. For the experiment system, the secure remote experiment system with JT-60 has been successfully accomplished. For the remote data acquisition system, it will be possible to equivalently operate experimental data obtained from LHD data acquisition and management system (LABCOM system) and JT-60 Data System. The integrated analysis system has been extended to the system executable in heterogeneous computers among institutes.}
}
@article{FAVERO2023112755,
title = {Analysis of subjective thermal comfort data: A statistical point of view},
journal = {Energy and Buildings},
volume = {281},
pages = {112755},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112755},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822009264},
author = {Matteo Favero and Antonio Luparelli and Salvatore Carlucci},
keywords = {Subjective thermal comfort data, Rating scales, Level of measurement, Ordinal regression, Bayesian analysis, Statistical thinking},
abstract = {Thermal comfort research aims to determine the relationship between the thermal environment and the human sense of warmth. This is usually achieved by measuring the subjective human thermal response to different thermal environments. However, it is common practice to use simple linear regression to analyse data collected using ordinal scales. This practice may lead to severe errors in inference. This study first set the methodological foundations to analyse subjective thermal comfort data from a statistical perspective. Subsequently, we show the practical consequences of fallacious assumptions by utilising a Bayesian approach and show, through an illustrative example, that a linear regression model applied to ordinal data suggests results different from those obtained using ordinal regression. Specifically, linear regression found no difference in means and effect size between genders, while the ordinal regression model led to the opposite conclusion. In addition, the linear regression model distorts the estimated regression coefficient for air temperature compared to the ordinal model. Finally, the ordinal model shows that the distance between adjacent response categories of the ASHRAE 7-point thermal sensation scale is not equidistant. Given the abovementioned issues, we advocate utilising ordinal models instead of metric models to analyse ordinal data.}
}
@article{DUMONTHEIL20101574,
title = {Taking perspective into account in a communicative task},
journal = {NeuroImage},
volume = {52},
number = {4},
pages = {1574-1583},
year = {2010},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2010.05.056},
url = {https://www.sciencedirect.com/science/article/pii/S1053811910007895},
author = {Iroise Dumontheil and Olivia Küster and Ian A. Apperly and Sarah-Jayne Blakemore},
keywords = {Theory of mind, Perspective taking, Social brain, Social cognition, Decision making, Inhibition},
abstract = {Previous neuroimaging studies of spatial perspective taking have tended not to activate the brain's mentalising network. We predicted that a task that requires the use of perspective taking in a communicative context would lead to the activation of mentalising regions. In the current task, participants followed auditory instructions to move objects in a set of shelves. A 2×2 factorial design was employed. In the Director factor, two directors (one female and one male) either stood behind or next to the shelves, or were replaced by symbolic cues. In the Object factor, participants needed to use the cues (position of the directors or symbolic cues) to select one of three possible objects, or only one object could be selected. Mere presence of the Directors was associated with activity in the superior dorsal medial prefrontal cortex (MPFC) and the superior/middle temporal sulci, extending into the extrastriate body area and the posterior superior temporal sulcus (pSTS), regions previously found to be responsive to human bodies and faces respectively. The interaction between the Director and Object factors, which requires participants to take into account the perspective of the director, led to additional recruitment of the superior dorsal MPFC, a region activated when thinking about dissimilar others' mental states, and the middle temporal gyri, extending into the left temporal pole. Our results show that using perspective taking in a communicative context, which requires participants to think not only about what the other person sees but also about his/her intentions, leads to the recruitment of superior dorsal MPFC and parts of the social brain network.}
}
@article{OKAMURA2021,
title = {NMB4.0: development of integrated nuclear fuel cycle simulator from the front to back-end},
journal = {EPJ - Nuclear Sciences & Technologies},
volume = {7},
year = {2021},
issn = {2491-9292},
doi = {https://doi.org/10.1051/epjn/2021019},
url = {https://www.sciencedirect.com/science/article/pii/S2491929221000145},
author = {Tomohiro Okamura and Ryota Katano and Akito Oizumi and Kenji Nishihara and Masahiko Nakase and Hidekazu Asano and Kenji Takeshita},
abstract = {Nuclear Material Balance code version 4.0 (NMB4.0) has been developed through collaborative R&D between TokyoTech&JAEA. Conventional nuclear fuel cycle simulation codes mainly analyze actinides and are specialized for front-end mass balance analysis. However, quantitative back-end simulation has recently become necessary for considering R&D strategies and sustainable nuclear energy utilization. Therefore, NMB4.0 was developed to realize the integrated nuclear fuel cycle simulation from front- to back-end. There are three technical features in NMB4.0: 179 nuclides are tracked, more than any other code, throughout the nuclear fuel cycle; the Okamura explicit method is implemented, which contributes to reducing the numerical cost while maintaining the accuracy of depletion calculations on nuclides with a shorter half-life; and flexibility of back-end simulation is achieved. The main objective of this paper is to show the newly developed functions, made for integrated back-end simulation, and verify NMB4.0 through a benchmark study to show the computational performance.}
}
@article{COOPER20091351,
title = {Emergence as a computability-theoretic phenomenon},
journal = {Applied Mathematics and Computation},
volume = {215},
number = {4},
pages = {1351-1360},
year = {2009},
note = {Physics and Computation},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2009.04.050},
url = {https://www.sciencedirect.com/science/article/pii/S0096300309004159},
author = {S. Barry Cooper},
keywords = {Computability, Emergence, Definability, Turing invariance},
abstract = {In dealing with emergent phenomena, a common task is to identify useful descriptions of them in terms of the underlying atomic processes, and to extract enough computational content from these descriptions to enable predictions to be made. Generally, the underlying atomic processes are quite well understood, and (with important exceptions) captured by mathematics from which it is relatively easy to extract algorithmic content. A widespread view is that the difficulty in describing transitions from algorithmic activity to the emergence associated with chaotic situations is a simple case of complexity outstripping computational resources and human ingenuity. Or, on the other hand, that phenomena transcending the standard Turing model of computation, if they exist, must necessarily lie outside the domain of classical computability theory. In this talk we suggest that much of the current confusion arises from conceptual gaps and the lack of a suitably fundamental model within which to situate emergence. We examine the potential for placing emergent relations in a familiar context based on Turing’s 1939 model for interactive computation over structures described in terms of reals. The explanatory power of this model is explored, formalising informal descriptions in terms of mathematical definability and invariance, and relating a range of basic scientific puzzles to results and intractable problems in computability theory.}
}
@article{OLUBAMBI2025,
title = {Conceptualising a dynamic BIM-based waste management system in enabling net-zero cities},
journal = {Proceedings of the Institution of Civil Engineers - Waste and Resource Management},
year = {2025},
issn = {1747-6534},
doi = {https://doi.org/10.1680/jwarm.23.00043},
url = {https://www.sciencedirect.com/science/article/pii/S1747653425000026},
author = {Ademilade Olubambi and Clinton Aigbavboa and Bolanle Ikotun},
keywords = {building information modelling (BIM), life cycle assessment, life cycle analysis, LCA, sustainable cities and communities, sustainable development, waste disposal system},
abstract = {This study investigates the possibility of applying building information modelling as a tool for eliminating waste throughout the building life cycle toward achieving net-zero waste in the construction industry. To accomplish this goal, literature was reviewed to identify aspects that necessitate using mechanism in optimising a sustainable waste management system. A building information modelling-based conceptual framework with a high capacity for implementing net-zero waste management action throughout the building development stages was developed. The results indicate that the tool can generate an effective programme for ordering materials, assembling, and supplying all building components during the design phase. Any alterations to the building information model during the design phase are updated automatically. During the procurement phase, three-dimensional geometry can be used for project sequencing, take-offs, and integrating energy analyses. Virtual construction modelling, which is extremely cost-effective, can be used during the construction phase. Furthermore, the tools can be utilised as a waste estimation tool before any demolition or remodelling, as well as to help determine a reasonable price for waste disposal rates, throughout the construction phase. In conclusion, this study demonstrates how waste is minimised and consequently prevented in building construction by integrating dynamic building information modelling tools while enabling the possibility of a net-zero cities.}
}
@article{KELLEY2011228,
title = {Theoretical explorations of cognitive robotics using developmental psychology},
journal = {New Ideas in Psychology},
volume = {29},
number = {3},
pages = {228-234},
year = {2011},
note = {Special Issue: Cognitive Robotics and Reevaluation of Piaget Concept of Egocentrism},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2009.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X0900035X},
author = {Troy D. Kelley and Daniel N. Cassenti},
keywords = {Development, Robotics, Cognition, Cognitive modeling},
abstract = {How can cognitive robotics inform developmental psychology researchers and what can developmental psychology tell us about creating robots? More importantly, how can cognitive robotics and developmental psychology nourish each other to become a symbiotic relationship for future research? We address the theoretical underpinnings of developmental change using a cognitive architecture implemented on a robotic system and how our theories of knowledge representation relate to critical periods of infant development. Next, we will show how descriptive theories of cognitive development, specifically Zelazo's Levels of Consciousness (LOC; Zelazo, 2000, Zelazo, 2004, Zelazo and Jacques, 1996), can be mapped onto a computational cognitive architecture (ACT-R; Anderson & Lebiere, 1998). Following our discussion of Zelazo's theory, we will apply the ACT-R architecture specifically to the problem of object permanence. Finally, we will address how cognitive robotics can serve as a computational proving ground of developmental psychology for future research.}
}
@article{HE2023126590,
title = {Global priors guided modulation network for joint super-resolution and SDRTV-to-HDRTV},
journal = {Neurocomputing},
volume = {554},
pages = {126590},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126590},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007130},
author = {Gang He and Shaoyi Long and Li Xu and Chang Wu and Wenxin Yu and Jinjia Zhou},
keywords = {Convolutional neural network, Super-resolution, SDRTV-to-HDRTV, High dynamic range},
abstract = {Watching low resolution standard dynamic range (LR SDR) video on a 4K high dynamic range (HDR) TV is not the best viewing experience. Joint super-resolution (SR) and SDRTV-to-HDRTV aims to enhance the visual quality of LR SDR videos that have quality deficiencies in resolution and dynamic range. Previous methods that rely on learning local information typically cannot do well in preserving color conformity and long-range structural similarity, resulting in unnatural color transition and texture artifacts. In order to tackle these challenges, we propose a global priors guided modulation network (GPGMNet). In particular, we design a global priors extraction module (GPEM) to extract color conformity prior and structural similarity prior that are beneficial for SDRTV-to-HDRTV and SR tasks, respectively. To further exploit the global priors and preserve spatial information, we devise multiple global priors-guided spatial-wise modulation blocks (GSMBs) with a few parameters for intermediate feature modulation. In these GSMBs, the modulation parameters are generated by the shared global priors and the spatial features map from the spatial pyramid convolution block (SPCB). With these elaborate designs, the GPGMNet can achieve higher visual quality with lower computational complexity. Extensive experiments demonstrate that our proposed GPGMNet is superior to the state-of-the-art methods. Specifically, our proposed model exceeds the state-of-the-art by 0.64 dB in PSNR, with 69% fewer parameters and 3.1× speedup.}
}
@article{BECK2024545,
title = {Understanding the cell: Future views of structural biology},
journal = {Cell},
volume = {187},
number = {3},
pages = {545-562},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2023.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0092867423013491},
author = {Martin Beck and Roberto Covino and Inga Hänelt and Michaela Müller-McNicoll},
keywords = {structural biology, digital twin, computational modeling, cellular self-organization},
abstract = {Summary
Determining the structure and mechanisms of all individual functional modules of cells at high molecular detail has often been seen as equal to understanding how cells work. Recent technical advances have led to a flush of high-resolution structures of various macromolecular machines, but despite this wealth of detailed information, our understanding of cellular function remains incomplete. Here, we discuss present-day limitations of structural biology and highlight novel technologies that may enable us to analyze molecular functions directly inside cells. We predict that the progression toward structural cell biology will involve a shift toward conceptualizing a 4D virtual reality of cells using digital twins. These will capture cellular segments in a highly enriched molecular detail, include dynamic changes, and facilitate simulations of molecular processes, leading to novel and experimentally testable predictions. Transferring biological questions into algorithms that learn from the existing wealth of data and explore novel solutions may ultimately unveil how cells work.}
}
@article{LI2024101038,
title = {One-stop multi-sensor fusion and multimodal precise quantified traditional Chinese medicine imaging health examination technology},
journal = {Journal of Radiation Research and Applied Sciences},
volume = {17},
number = {4},
pages = {101038},
year = {2024},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2024.101038},
url = {https://www.sciencedirect.com/science/article/pii/S168785072400222X},
author = {Chuanxue Li and Ping Wang and Meifang Zheng and Wenxiang Li and Jun Zhou and Lin Fu},
keywords = {Traditional Chinese medicine imaging, Large language model, Knowledge graphs, Multimodal imaging, Health examination technology, Image fusion, Imaging agent, Deep learning},
abstract = {Except for single-mode traditional Chinese medicine imaging techniques such as infrared thermal imaging, the one-stop multimodal whole-body imaging health examination technology and device is still blank. We focus on infrared thermal imaging as the main modality, integrated various modalities of medical imaging intelligent sensing agents such as terahertz imaging. The upper and lower computer and virtual instrument architecture are used, and the imaging data are collected by the lower computers that each is an intelligent sensing agent. The upper computer is used for image reconstruction with intelligent algorithms. Based on the core theory of traditional Chinese medicine, intelligent fusion imaging is achieved through various modalities to achieve the ‘observation, hearing, questioning, and palpation’ four diagnostic integration. We use fractional Fourier transform to filter imaging data, Laplacian pyramid for image fusion. We have proposed an implementation method and process for combining traditional Chinese medicine imaging large language model with knowledge graph, and based on deep learning, we have studied the image and report generation algorithm that combines traditional Chinese medicine pathology and four diagnostic methods with knowledge graph fusion, as well as the traditional Chinese medicine human physiological and pathological interpretation and evaluation system. We have achieved some results, and through further research and development, we can achieve commercial applications.}
}
@article{BELL200163,
title = {Futures studies comes of age: twenty-five years after The limits to growth},
journal = {Futures},
volume = {33},
number = {1},
pages = {63-76},
year = {2001},
issn = {0016-3287},
doi = {https://doi.org/10.1016/S0016-3287(00)00054-9},
url = {https://www.sciencedirect.com/science/article/pii/S0016328700000549},
author = {Wendell Bell},
abstract = {Twenty-five years ago, the publication of The limits to growth marked a period of accomplishments in the futures field. Today, futures studies is experiencing another burst of development and is ready to move more fully into mainstream intellectual life and the standard educational curriculum. In addition to continued work on methods, theory, and empirical research, the resolution of three issues might help persuade established academic communities of the serious purposes and sound intellectual contributions of futurists. They are (1) the adoption of an adequate theory of knowledge (critical realism is proposed), (2) the recognition that prediction does play a role in futures studies (so we can deal explicitly with the philosophical challenges it poses), and (3) the formulation and justification of core values (so we have a valid basis by which to judge the desirability of alternative futures). I propose a critical discourse among futurists in order to resolve each issue. The desire to make futures thinking a part of everyone's education is not, of course, mere futurist chauvinism, but is based on the conviction that futures studies has important contributions to make to human well-being.}
}
@article{PICCININI2008311,
title = {Some neural networks compute, others don’t},
journal = {Neural Networks},
volume = {21},
number = {2},
pages = {311-321},
year = {2008},
note = {Advances in Neural Networks Research: IJCNN ’07},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2007.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S089360800700250X},
author = {Gualtiero Piccinini},
keywords = {Connectionism, Neural network, Computation, Mechanism, Cognition, Brain},
abstract = {I address whether neural networks perform computations in the sense of computability theory and computer science. I explicate and defend the following theses. (1) Many neural networks compute—they perform computations. (2) Some neural networks compute in a classical way. Ordinary digital computers, which are very large networks of logic gates, belong in this class of neural networks. (3) Other neural networks compute in a non-classical way. (4) Yet other neural networks do not perform computations. Brains may well fall into this last class.}
}
@article{OUYANG2024e29176,
title = {Unmasking the challenges in ideological and political education in China: A thematic review},
journal = {Heliyon},
volume = {10},
number = {8},
pages = {e29176},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29176},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024052071},
author = {Sha Ouyang and Wei Zhang and Jian Xu and Abdullah {Mat Rashid} and Shwu Pyng How and Aminuddin {Bin Hassan}},
keywords = {Ideological and political education, China, Thematic review},
abstract = {China's distinctive educational approach, particularly its emphasis on ideological and political education, has garnered considerable academic attention for its impact on shaping individual values, fostering citizenship, and maintaining social stability. Despite the Chinese government's prioritization of ideological and political education, academic research in this field appears constrained, with existing studies predominantly focusing on normative and descriptive aspects. Normative research delineates how ideological and political education should be executed, while descriptive research illustrates its practical implementation. The effectiveness of these approaches is significantly diminished if they are not adequately interconnected—when only the current reality is explained without providing tools for improvement or when prescribed steps for improvement lack a basis in specific contexts. This paper conducts a comprehensive review of research on ideological and political education using ATLAS. ti 9 for thematic analysis. The review aims to unveil the intricate landscape of current research in China and address key questions: What are the primary trends in the literature on ideological and political education between 2021 and July 2023? What challenges does ideological and political education face? Through a direct exploration of these issues, this paper seeks to optimize the ideological and political education system, elevate its adaptability and effectiveness, and open avenues for research, fostering a more dynamic, inclusive, and resilient development of ideological and political education.}
}
@article{SANZ2021103070,
title = {The entropic tongue: Disorganization of natural language under LSD},
journal = {Consciousness and Cognition},
volume = {87},
pages = {103070},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2020.103070},
url = {https://www.sciencedirect.com/science/article/pii/S1053810020305377},
author = {Camila Sanz and Carla Pallavicini and Facundo Carrillo and Federico Zamberlan and Mariano Sigman and Natalia Mota and Mauro Copelli and Sidarta Ribeiro and David Nutt and Robin Carhart-Harris and Enzo Tagliazucchi},
keywords = {LSD, Psychedelics, Natural language, Entropy, Psychosis},
abstract = {Serotonergic psychedelics have been suggested to mirror certain aspects of psychosis, and, more generally, elicit a state of consciousness underpinned by increased entropy of on-going neural activity. We investigated the hypothesis that language produced under the effects of lysergic acid diethylamide (LSD) should exhibit increased entropy and reduced semantic coherence. Computational analysis of interviews conducted at two different time points after 75 μg of intravenous LSD verified this prediction. Non-semantic analysis of speech organization revealed increased verbosity and a reduced lexicon, changes that are more similar to those observed during manic psychoses than in schizophrenia, which was confirmed by direct comparison with reference samples. Importantly, features related to language organization allowed machine learning classifiers to identify speech under LSD with accuracy comparable to that obtained by examining semantic content. These results constitute a quantitative and objective characterization of disorganized natural speech as a landmark feature of the psychedelic state.}
}
@article{HUANG2025105310,
title = {Modeling student teachers’ self-regulated learning of complex professional knowledge: A sequential and clustering analysis with think-aloud protocols},
journal = {Computers & Education},
volume = {233},
pages = {105310},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105310},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000788},
author = {Lingyun Huang and Ying Zhan and Shen Ba},
keywords = {Self-regulated learning, Technological Pedagogical Content Knowledge, Sequential clustering analysis, Tthink-aloud protocols},
abstract = {There has been much discussion regarding the positive relationship between self-regulated learning (SRL) and technological pedagogical content knowledge (TPACK) development for student teachers. This study continued this claim and adopted advanced analytical methods to explain how SRL influences TPACK learning. Think-aloud protocols from 39 participants were collected and transcribed when they were learning TPACK by designing technology-infused lessons with nBrowser, a computer-based learning environment. Based on models, nine critical SRL events were retrieved from participants‘ think-aloud protocols and analyzed through sequential clustering analysis. The results show two SRL groups indicating distinct self-regulatory sequential patterns. One group had a shorter sequence length and dominantly enacted elaboration activities (Low-SRL group), while the other had longer sequence lengths and engaged in diverse SRL activities (High-regulation group). Relating to TPACK performance indicated by the quality of lesson plans, the results reveal that the participants in the High-SRL group outperformed their counterparts in the Low-SRL group. The findings are consistent with previous evidence and provide implications for practitioners about the importance of student teachers’ self-regulation trajectories.}
}
@article{ESCAMILLA2021102697,
title = {Interaction designers’ perceptions of using motion-based full-body features},
journal = {International Journal of Human-Computer Studies},
volume = {155},
pages = {102697},
year = {2021},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2021.102697},
url = {https://www.sciencedirect.com/science/article/pii/S1071581921001154},
author = {Antonio Escamilla and Javier Melenchón and Carlos Monzo and Jose Antonio Morán},
keywords = {Motion-based feature extraction, Full-body interaction, Interaction designers' perception, Designer-interpretable feature},
abstract = {Movement-based full-body interactions are increasingly being used in the design of interactive spaces, computer-mediated environments, and virtual user experiences due to the development and availability of diverse sensing technologies. In this context, the role of interaction designers is to find systematic and predictable relationships between bodily actions and the corresponding responses from technology. Sensor-based interaction design relies on sensor data analysis and higher-level feature extraction to improve detection capabilities. However, understanding human movement to inform the design of motion-based interactions is not straightforward if the detection capabilities of interaction technologies are unknown. We aim at understanding the problems and opportunities that practitioners—regardless of their technical background—perceive in using different motion-based full-body features. To achieve this, we conducted four separate focus groups with experienced practitioners, with and without technical backgrounds. We used a framework for the analysis of focus group data in information systems research to identify content areas and draw conclusions. Our findings suggest that most interaction designers, regardless of their technical background, consider motion-based feature extraction to be challenging and time-consuming. However, participants acknowledge they might use designer-interpretable features as a potential tool to foster user behavior exploration. Understanding how practitioners link sensor-based interaction design with feature extraction technology is relevant to design computational tools and reduce the technical effort required from designers to characterize the user’s movement.}
}
@article{NEINHUIS2017394,
title = {Innovations from the “ivory tower”: Wilhelm Barthlott and the paradigm shift in surface science},
journal = {Beilstein Journal of Nanotechnology},
volume = {8},
pages = {394-402},
year = {2017},
issn = {2190-4286},
doi = {https://doi.org/10.3762/bjnano.8.41},
url = {https://www.sciencedirect.com/science/article/pii/S2190428617000363},
author = {Christoph Neinhuis},
keywords = {Wilhelm Barthlott, 70th birthday, self-cleaning surfaces, lotus-effect},
abstract = {This article is mainly about borders that have tremendous influence on our daily life, although many of them exist and act mostly unrecognized. In this article the first objective will be to address more generally the relation between university and society or industry, borders within universities, borders in thinking and the huge amount of misunderstandings and losses resulting from these obvious or hidden borders. In the second part and in more detail, the article will highlight the impact of the research conducted by Wilhelm Barthlott throughout his scientific career during which not only one border was removed, shifted or became more penetrable. Among the various fields of interest not mentioned here (e.g., systematics of Cactaceae, diversity and evolution of epiphytes, the unique natural history of isolated rocky outcrops called inselbergs, or the global distribution of biodiversity), plant surfaces and especially the tremendous diversity of minute structures on leaves, fruits, seeds and other parts of plants represent a common thread through 40 years of scientific career of Wilhelm Barthlott. Based on research that was regarded already old-fashioned in the 1970s and 1980s, systematic botany, results and knowledge were accumulated that, some 20 years later, initiated a fundamental turnover in how surfaces were recognized not only in biology, but even more evident in materials science.}
}
@article{VANRINSVELD201717,
title = {Mental arithmetic in the bilingual brain: Language matters},
journal = {Neuropsychologia},
volume = {101},
pages = {17-29},
year = {2017},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2017.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0028393217301756},
author = {Amandine {Van Rinsveld} and Laurence Dricot and Mathieu Guillaume and Bruno Rossion and Christine Schiltz},
keywords = {Mathematics, Neuroimaging, Bilingualism, Numerical cognition, Arithmetics},
abstract = {How do bilinguals solve arithmetic problems in each of their languages? We investigated this question by exploring the neural substrates of mental arithmetic in bilinguals. Critically, our population was composed of a homogeneous group of adults who were fluent in both of their instruction languages (i.e., German as first instruction language and French as second instruction language). Twenty bilinguals were scanned with fMRI (3T) while performing mental arithmetic. Both simple and complex problems were presented to disentangle memory retrieval occuring in very simple problems from arithmetic computation occuring in more complex problems. In simple additions, the left temporal regions were more activated in German than in French, whereas no brain regions showed additional activity in the reverse constrast. Complex additions revealed the reverse pattern, since the activations of regions for French surpassed the same computations in German and the extra regions were located predominantly in occipital regions. Our results thus highlight that highly proficient bilinguals rely on differential activation patterns to solve simple and complex additions in each of their languages, suggesting different solving procedures. The present study confirms the critical role of language in arithmetic problem solving and provides novel insights into how highly proficient bilinguals solve arithmetic problems.}
}
@incollection{BISHT2022277,
title = {Chapter Twelve - Perceiving the level of depression from web text},
editor = {Shikha Jain and Kavita Pandey and Princi Jain and Kah Phooi Seng},
booktitle = {Artificial Intelligence, Machine Learning, and Mental Health in Pandemics},
publisher = {Academic Press},
pages = {277-298},
year = {2022},
isbn = {978-0-323-91196-2},
doi = {https://doi.org/10.1016/B978-0-323-91196-2.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911962000089},
author = {Sankalp Singh Bisht and Herumb Shandilya and Vaibhav Gupta and Shriyansh Agrawal and Shikha Jain},
keywords = {Depression, Loneliness, Mental disorder, Solitude, Suicide},
abstract = {Depression is one of the deadliest diseases found in today's world, and unfortunately, it is also one of the most ignored problems. Depression is a fact that is very hard to accept for any individual and is always a multistep process. The initial stage of Depression is Loneliness, and thus the information about these emotions can be leveraged and can help in the early detection of Depression, which in turn leads to suicidal thoughts. Tweet data analysis is one of the most popular ways to determine the presence of depression and suicidal thoughts, through the concepts of Machine Learning. Twitter proves to be a very rich source of data, as their user base is potentially large enough, but is also increasing in a fast manner. For the scope of this paper, we predicted from a user's specific tweet, which is categorized for loneliness. These tweets are analyzed to check the level of depression as moderate or severe when people start thinking of suicide. The simulation is carried out using four different models for one level of classification and eight models are used at the second level of classification. It is observed that Gated Recurrent Unit with BERT outperformed all the models and showed the accuracy of 99% and 97%. However, for class-1 recall with XLNet gave the best result with class-1 recall being 0.99. This application can help the individual in early detection of depression without any human intervention and seek medical help. Moreover, it also provides an insight about the feelings of the individual to the medical practitioners, which, in turn, can help them provide better decision-making.}
}
@article{2024100670,
title = {Erratum regarding missing declaration of competing interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100670},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100670},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000382}
}
@article{KHAN2023110525,
title = {AAD-Net: Advanced end-to-end signal processing system for human emotion detection & recognition using attention-based deep echo state network},
journal = {Knowledge-Based Systems},
volume = {270},
pages = {110525},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110525},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123002757},
author = {Mustaqeem Khan and Abdulmotaleb {El Saddik} and Fahd Saleh Alotaibi and Nhat Truong Pham},
keywords = {Affective computing, Attention mechanism, Convolution neural network, Echo state networks, Emotion recognition, Human–computer interaction, Audio speech signals},
abstract = {Speech signals are the most convenient way of communication between human beings and the eventual method of Human–Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.}
}
@article{GALE2025101969,
title = {Are we sleepwalking into a fully automated medical imaging service?},
journal = {Journal of Medical Imaging and Radiation Sciences},
volume = {56},
number = {5},
pages = {101969},
year = {2025},
issn = {1939-8654},
doi = {https://doi.org/10.1016/j.jmir.2025.101969},
url = {https://www.sciencedirect.com/science/article/pii/S1939865425001195},
author = {Niamh Gale},
keywords = {Artificial intelligence, Medical imaging, Review, Impact, Higher education, Recommendations},
abstract = {Introduction
Artificial intelligence (AI) is already embedded in medical imaging services, but now that the National Institute for Health and Care Excellence (NICE) has released position statements looking favourably on AI use in healthcare, its use will embed even further.
Discussion
AI has brought many positives to medical imaging services and is far superior at making calculations using vast amounts of data. It can therefore help improve the speed and accuracy of diagnosis and treatment plans for many patients, but at what cost to the radiography profession? Surveys have shown that the majority of the workforce welcome AI, but admit that they don’t fully understand the principles behind it. AI developers are keen to improve patient output, and many are unconcerned about the possible negative effects on staff morale and expertise. As computers remove the autonomy and competency that radiographers have previously held with pride, staff may find that they become de-skilled and de-motivated, and it may eventually subsume the traditional role of the radiographer altogether. The profession needs to be aware of these potential impacts and prepare accordingly.
Conclusion
Higher education plays an important role in preparing radiographers of the future for the changing landscape of medical imaging and should include more engineering and data science modules in the curriculum to prevent radiographers from becoming irrelevant.
Résumé
Introduction
L'intelligence artificielle (IA) est déjà intégrée aux services d'imagerie médicale, mais son utilisation se généralisera encore davantage maintenant que le National Institute for Health and Care Excellence (NICE) a publié des prises de position favorables à l'utilisation de l'IA dans les soins de santé.
Discussion
L'IA a apporté de nombreux avantages aux services d'imagerie médicale et est bien plus performante pour effectuer des calculs à partir de grandes quantités de données. Elle peut donc contribuer à améliorer la rapidité et la précision des diagnostics et des plans de traitement pour de nombreux patients, mais à quel prix pour la profession de la radiographie? Des enquêtes ont montré que la majorité des travailleurs sont favorables à l'IA, mais admettent qu'ils n'en comprennent pas pleinement les principes. Les développeurs d'IA sont désireux d'améliorer le rendement au niveau des patients, et beaucoup ne se soucient pas des effets négatifs possibles sur le moral et l'expertise du personnel. Comme les ordinateurs suppriment l'autonomie et les compétences que les radiographes détenaient auparavant avec fierté, le personnel peut se retrouver déqualifié et démotivé, et cela peut finir par supplanter complètement le rôle traditionnel du radiographe. La profession doit être consciente de ces impacts potentiels et s'y préparer en conséquence.
Conclusion
L'enseignement supérieur joue un rôle important dans la préparation des futurs radiographes à l'évolution de l'imagerie médicale. Le programme d'études devrait donc inclure davantage de modules d'ingénierie et de science des données afin d'éviter que les radiographes ne deviennent obsolètes.}
}
@article{LANDETE2024103034,
title = {Uncapacitated single-allocation hub median location with edge upgrading: Models and exact solution algorithms},
journal = {Transportation Research Part B: Methodological},
volume = {187},
pages = {103034},
year = {2024},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2024.103034},
url = {https://www.sciencedirect.com/science/article/pii/S0191261524001589},
author = {Mercedes Landete and Juan M. Muñoz-Ocaña and Antonio M. Rodríguez-Chía and Francisco Saldanha-da-Gama},
keywords = {Hub location, Hub-network design, Single allocation, Edge upgrading, MILP models, Branch-and-cut},
abstract = {In this paper, a class of single-allocation hub location problems is investigated from the perspective of upgrading. The latter is understood as an improvement of a set of edges to increase their individual performance, e.g., a decreased unit transportation cost. The goal is to obtain an improved optimal solution to the problem compared to that obtained if upgrading was not done. A budget constraint is assumed to limit the upgrading operations. A flow-based formulation is initially proposed that extends a classical model for uncapacitated single-allocation hub location with complete hub networks. Nevertheless, the fact that the unit costs after upgrading may violate the triangle inequality needs to be accounted for. Since the proposed formulation has a high computing burden, different possibilities are discussed for enhancing it. This leads to devising an efficient branch-and-cut algorithm with different variants. Additionally, a formulation based on the discrete ordered median function is also introduced that is also enhanced and embedded into a branch-and-cut algorithm again with several variants. All models and algorithms are also adapted to problems embedding hub network design decisions. Extensive computational tests were conducted to assess the methodological contributions proposed.}
}
@article{UCAN2022104878,
title = {Advice hierarchies among finite automata},
journal = {Information and Computation},
volume = {288},
pages = {104878},
year = {2022},
note = {Special Issue: Selected Papers of the 14th International Conference on Language and Automata Theory and Applications, LATA 2020},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2022.104878},
url = {https://www.sciencedirect.com/science/article/pii/S0890540122000207},
author = {Ahmet Bilal Uçan and A.C. Cem Say},
keywords = {Formal languages, Automata theory, Advised computation},
abstract = {We examine the effects of supplying increasing amounts of trusted advice to a finite automaton. Previous work has shown that allowing such automata with a single advice tape to make a single pass over their input renders them unable to recognize the palindromes language, whereas both two-way machines reading advice from a single tape and one-way machines with multiple advice tapes can recognize all languages with exponentially bounded amounts of advice. We study several architectural variants and demonstrate the existence of language hierarchies based on increased advice length, runtime (measured in terms of the number of allowed left-to-right passes on the input), and number of advice tapes. We also prove some lower bounds for recognizing certain concrete languages.}
}
@article{ALBERT2008401,
title = {A formal framework for modelling the developmental course of competence and performance in the distance, speed, and time domain},
journal = {Developmental Review},
volume = {28},
number = {3},
pages = {401-420},
year = {2008},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2008.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0273229708000257},
author = {Dietrich Albert and Michael D. Kickmeier-Rust and Fumiko Matsuda},
keywords = {Distance–speed–time system, Cognitive development, Overgeneralization, Competence-based Knowledge Space Theory},
abstract = {The developmental course in the distance–speed–time domain is still a matter of debate. Traditional stage models are contested by theories of continuous development and adaptive thinking. In the present work, we introduce a formal framework for modelling the developmental course in this domain, grounding on Competence-based Knowledge Space Theory. This framework, as a more general case, widely includes assumptions and facets of previous models and covers empirical findings collected based on different experimental paradigms. By a distinction of latent competences and observable performance, model validation is not bound to a certain experimental paradigm and no one-to-one correspondence between competences and tasks is required. Therefore, the framework has the potential to bridge the gap between stage models and models of continuous development. The approach also precisely defines misconceptions, for example overgeneralization, and empirically investigates their occurrence. In the present work, we established a prototypical model for the development of understanding the distance–speed–time system. We extended this model with definitions based on different perspectives of overgeneralization. The assumptions of the model and its extensions were examined on the basis of the results of two empirical investigations using six judgment task types. The results yielded a reasonably good fit of model and data. No evidence was found for the occurrence of overgeneralization in this domain. The theoretical model and empirical results are discussed with respect to their relationship to other developmental models and theories.}
}
@article{JURKOVA2023105046,
title = {Turing and von Neumann machines: Completing the new mechanism},
journal = {Biosystems},
volume = {234},
pages = {105046},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.105046},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723002216},
author = {Barbora Jurková and Lukáš Zámečník},
keywords = {Turing machine, von Neumann probe, New mechanism, Code biology, Extended mechanism},
abstract = {Turing (1937) introduces a model of code that is followed by other pioneers of computing machines (such as Flowers 1983, Eckert, Mauchly, Brainerd 1945 and others). One of them is John von Neumann, who defines the concept of optimal code in the context of the conception of EDVAC. He later uses it to build on in his theoretical considerations of the universal constructor (von Neumann 1966). Von Neumann (1963) further presents one of the first neural network models, in relation to the work of McCulloch and Pitts (1943), for both theoretical purposes (von Neumann probe) and practical applications (computer architecture of EDVAC). The aim of this paper is (1) to describe the differences between Turingʼs and von Neumannʼs conceptualizations of code and the mechanical computing model. Between von Neumann's abstract technical conception (von Neumann 1963 and 1966) and Turingʼs more concrete biochemical conception (Turing 1952). Furthermore, (2) we want to answer the question why these influential models of mechanisms (predominantly in computer science) have so far been ignored by philosophers of the new mechanism (Machamer, Darden, Craver 2000, Glennan 2017). We will show that these classical models of machines are not only compatible with the new mechanism, but moreover complement it, since they represent a completely separate type of model of mechanism, alongside producing, maintaining and underlying (Zámečník 2021). The final (3) and main goal of our paper will be an attempt to relate von Neumannʼs and Turingʼs notion of mechanism to Barbieriʼs notion of extended mechanism (Barbieri 2015).}
}
@article{GROSS20173,
title = {Prospects and problems for standardizing model validation in systems biology},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {3-12},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300177},
author = {Fridolin Gross and Miles MacLeod},
keywords = {Systems biology, Modeling, Standardization, Validation, Model selection},
abstract = {There are currently no widely shared criteria by which to assess the validity of computational models in systems biology. Here we discuss the feasibility and desirability of implementing validation standards for modeling. Having such a standard would facilitate journal review, interdisciplinary collaboration, model exchange, and be especially relevant for applications close to medical practice. However, even though the production of predictively valid models is considered a central goal, in practice modeling in systems biology employs a variety of model structures and model-building practices. These serve a variety of purposes, many of which are heuristic and do not seem to require strict validation criteria and may even be restricted by them. Moreover, given the current situation in systems biology, implementing a validation standard would face serious technical obstacles mostly due to the quality of available empirical data. We advocate a cautious approach to standardization. However even though rigorous standardization seems premature at this point, raising the issue helps us develop better insights into the practices of systems biology and the technical problems modelers face validating models. Further it allows us to identify certain technical validation issues which hold regardless of modeling context and purpose. Informal guidelines could in fact play a role in the field by helping modelers handle these.}
}
@article{RAMAN2002135,
title = {Coordinating informal and formal aspects of mathematics: student behavior and textbook messages},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {2},
pages = {135-150},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00119-0},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001190},
author = {Manya Raman},
keywords = {Informal mathematics, Formal mathematics, Precalculus textbooks, Calculus textbooks},
abstract = {In this paper I illustrate difficulties students have coordinating informal and formal aspects of mathematics. I also discuss two ways in which precalculus and calculus textbooks treat mathematics that may make this coordination difficult: emphasizing the informal at the expense of the formal and emphasizing the formal at the expense of the informal. By looking at student difficulties in light of textbook treatments, we see evidence that student difficulties are not merely developmental. Students are not given many opportunities to make the kinds of connections which, while difficult, are an essential component of mathematical thinking.}
}
@article{DELIMANETO2018225,
title = {A semiotic-inspired machine for personalized multi-criteria intelligent decision support},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {225-238},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300757},
author = {Fernando Buarque {de Lima Neto} and Denis Mayr {Lima Martins} and Gottfried Vossen},
keywords = {Multi-criteria decision support, Computational intelligence, Computational semiotics, Intelligent semiotic machine},
abstract = {The need for appropriate decisions to tackle complex problems increases every day. Selecting destinations for vacation, comparing and optimizing resources to create valuable products, or purchasing a suitable car are just a few examples of puzzling situations in which there is no standard form to find an appropriate solution. Such scenarios become arduous when the number of possibilities, restrictions, and factors affecting the decision rise, thereby turning decision makers into almost mere spectators. In such circumstances, decision support systems (DSS) can play an important role in guiding people and organizations towards more accurate decision making. However, conventional DSS lack the necessary adaptability to account for dynamic changes and are frequently inadequate to tackle the subjectivity inherent in decision-maker's preferences and intention. We argue that these shortcomings can be addressed by a suitable combination of Semiotic Theory and Computational Intelligence algorithms, which together can make up a new generation of DSS. In this article, a formal description of an Intelligent Semiotic Machine is provided and tried out in practical decision contexts. The results obtained show that our approach can provide well-suited decisions based on user preferences, achieving appropriateness while fanning out subjective options without losing decision context, objectivity, or accuracy.}
}
@incollection{MADIAJAGAN2019245,
title = {Chapter 15 - Parallel Machine Learning and Deep Learning Approaches for Bioinformatics},
editor = {Arun Kumar Sangaiah},
booktitle = {Deep Learning and Parallel Computing Environment for Bioengineering Systems},
publisher = {Academic Press},
pages = {245-255},
year = {2019},
isbn = {978-0-12-816718-2},
doi = {https://doi.org/10.1016/B978-0-12-816718-2.00022-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128167182000221},
author = {M. Madiajagan and S. Sridhar Raj},
keywords = {Machine learning, Deep Learning, Parallel processing, Bioinformatics, Parallel deep neural networks},
abstract = {Deep learning uses multiple layers of artificial neurons for classification and pattern recognition. The biggest drawbacks of deep learning algorithms have been the high computation cost, inter-processor communication bottlenecks and parameters training time. Hence, incorporating parallel computing into deep learning decreases the computation time of complex deep learning algorithms. This chapter presents how parallelization is applied over many processors which are loosely coupled. Up to 4096 processes are scaled linearly with higher accuracy and zero loss percentage. This capacity of huge scaling helps in training billions of training examples in just a few hours. Various applications of Hessian-free parallelization mechanism on bioinformatics applications are in gene therapy, drug development, antibiotic resistance research, waste cleanup, climate change studies, bioweapon creation, improving nutritional quality and veterinary science.}
}
@article{NARASIMHAN197879,
title = {Modelling behaviour: the need for a computational approach},
journal = {Journal of Social and Biological Structures},
volume = {1},
number = {1},
pages = {79-94},
year = {1978},
issn = {0140-1750},
doi = {https://doi.org/10.1016/0140-1750(78)90020-9},
url = {https://www.sciencedirect.com/science/article/pii/0140175078900209},
author = {R. Narasimhan},
abstract = {The principal objective of this paper is to argue the thesis that a science concerned with the study of behaviour requires the computational approach in a serious way for its theoretical advancement. It is pointed out that modelling behaviour requires the articulation of explanations at three levels. The methodology of computational simulations is indispensable to articulating explanations at the first level which underlie explanations at the other two levels. The paper contrasts the currently fashionable approaches in artificial intelligence studies to the kind of constraints viable behavioural models must satisfy. Teachability and open-endedness are two of the essential characteristics of organisms that any satisfactory model must have. It is argued that analogy-based computational techniques and paradigmatic learning/teaching techniques are two modelling aspects that require imaginative study.}
}
@article{DAN2025100814,
title = {Social robot assisted music course based on speech sensing and deep learning algorithms},
journal = {Entertainment Computing},
volume = {52},
pages = {100814},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100814},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001824},
author = {Xiao Dan},
keywords = {Speech sensing, Deep learning algorithms, Social robots, Course in music},
abstract = {In the field of social robot teaching, research has focused on how to use technological means to provide better learning support and personalized interactive experiences. Social robots can interact with students and provide personalized learning support, thereby improving their learning effectiveness and engagement. The speech sensing model of social robots can perceive students’ emotions and feedback in real-time through technologies such as speech recognition and sentiment analysis, thereby providing intelligent responses and guidance. The deep learning recommendation model for music course resources extracts music features through deep learning techniques, and combines session interest extraction techniques to personalized recommend music resources suitable for students’ interests and abilities. By analyzing students’ interests and learning goals, robots can provide music learning resources that meet their needs based on recommendation algorithms, further stimulating their learning interest and enthusiasm. The experimental results show that the use of social robots in the learning environment significantly improves the learning effectiveness and participation of students. Through personalized interaction and intelligent response guidance, students are more likely to understand and master music knowledge, while experiencing joyful and positive learning emotions. The study validated the effectiveness of social robot assisted music courses based on speech sensing and deep learning algorithms, demonstrating its advantages in improving student learning effectiveness and engagement.}
}
@article{LIM2024127512,
title = {Progressive expansion: Cost-efficient medical image analysis model with reversed once-for-all network training paradigm},
journal = {Neurocomputing},
volume = {581},
pages = {127512},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127512},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224002832},
author = {Shin Wei Lim and Chee Seng Chan and Erma Rahayu {Mohd Faizal} and Kok Howg Ewe},
keywords = {Medical image analysis, Machine learning, Model optimization, Cost-effective model},
abstract = {Low computational cost artificial intelligence (AI) models are vital in promoting the accessibility of real-time medical services in underdeveloped areas. The recent Once-For-All (OFA) network (without retraining) can directly produce a set of sub-network designs with Progressive Shrinking (PS) algorithm; however, the training resource and time inefficiency downfalls are apparent in this method. In this paper, we propose a new OFA training algorithm, namely the Progressive Expansion (ProX) to train the medical image analysis model. It is a reversed paradigm to PS, where technically we train the OFA network from the minimum configuration and gradually expand the training to support larger configurations. Empirical results showed that the proposed paradigm could reduce training time up to 68%; while still being able to produce sub-networks that have either similar or better accuracy compared to those trained with OFA-PS on ROCT (classification), BRATS and Hippocampus (3D-segmentation) public medical datasets. The code implementation for this paper is accessible at: https://github.com/shin-wl/ProX-OFA.}
}
@article{LEVINSON2002155,
title = {Returning the tables: language affects spatial reasoning},
journal = {Cognition},
volume = {84},
number = {2},
pages = {155-188},
year = {2002},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(02)00045-8},
url = {https://www.sciencedirect.com/science/article/pii/S0010027702000458},
author = {Stephen C Levinson and Sotaro Kita and Daniel B.M Haun and Björn H Rasch},
keywords = {Language, Spatial reasoning, Linguistic relativity},
abstract = {Li and Gleitman (Turning the tables: language and spatial reasoning. Cognition, in press) seek to undermine a large-scale cross-cultural comparison of spatial language and cognition which claims to have demonstrated that language and conceptual coding in the spatial domain covary (see, for example, Space in language and cognition: explorations in linguistic diversity. Cambridge: Cambridge University Press, in press; Language 74 (1998) 557): the most plausible interpretation is that different languages induce distinct conceptual codings. Arguing against this, Li and Gleitman attempt to show that in an American student population they can obtain any of the relevant conceptual codings just by varying spatial cues, holding language constant. They then argue that our findings are better interpreted in terms of ecologically-induced distinct cognitive styles reflected in language. Linguistic coding, they argue, has no causal effects on non-linguistic thinking – it simply reflects antecedently existing conceptual distinctions. We here show that Li and Gleitman did not make a crucial distinction between frames of spatial reference relevant to our line of research. We report a series of experiments designed to show that they have, as a consequence, misinterpreted the results of their own experiments, which are in fact in line with our hypothesis. Their attempts to reinterpret the large cross-cultural study, and to enlist support from animal and infant studies, fail for the same reasons. We further try to discern exactly what theory drives their presumption that language can have no cognitive efficacy, and conclude that their position is undermined by a wide range of considerations.}
}
@article{VARAS2023101289,
title = {Teachers’ strategies and challenges in teaching 21st century skills: Little common understanding},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101289},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101289},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000597},
author = {Diego Varas and Macarena Santana and Miguel Nussbaum and Susana Claro and Patricia Imbarack},
keywords = {21st century skills, In-service teacher perceptions, Teacher education, Teaching practice, 6 Cs, 4 Cs},
abstract = {Faced with a world of accelerating change and rapidly-evolving technology, education systems must provide students with the skills they need to succeed in the 21st century. However, many countries have failed to incorporate the teaching of these skills within their schools. Our study therefore looks to portray teachers' understanding, strategies and obstacles in teaching these skills across Latin American classrooms. To do so, we analyzed the responses to an online survey from 1391 active teachers across 20 countries in the region. This revealed varying understandings of 21st century skills, with little common understanding. Most teachers failed to mention the skills included in the most popular framework (the 4 Cs); those who did reported using the same strategies, regardless of the skill being taught. These strategies included project-based learning, oracy activities, literacy strategies, and teamwork. We conclude that there is little or no common understanding around these skills, nor the best strategies for developing them. Our study helps understand the potential causes preventing the teaching of these skills in the classroom, a problem that extends beyond Latin America.}
}
@article{DAVID2022132522,
title = {Integrating fourth industrial revolution (4IR) technologies into the water, energy & food nexus for sustainable security: A bibliometric analysis},
journal = {Journal of Cleaner Production},
volume = {363},
pages = {132522},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.132522},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622021230},
author = {Love O. David and Nnamdi I. Nwulu and Clinton O. Aigbavboa and Omoseni O. Adepoju},
keywords = {WEF Nexus, Fourth industrial revolution, Industry 4.0, Cleaner production, Internet of things (IoT)},
abstract = {The technologies of the fourth Industrial Revolution (4IR/Industry 4.0) have been a technological catalyst for all fields of human endeavor, permeating the water, energy, and food (WEF) nexus. However, there is no empirical evidence of the extent of applications and the permeability level in ensuring the three resources’ security. This study explored the relationship of the fourth industrial revolution technologies and the water, energy, and food nexus by evaluating the applications of the various technologies of 4IR on WEF nexus and examined the effect of 4IR on WEF nexus. The objectives were achieved using the qualitative methodology and bibliometric analysis of content analysis. The result showed that most fourth industrial revolution technologies had not been integrated with the WEF nexus. The result showed that only the Internet of Things (IoT) and Big Data analytics had permeated the nexus, which shows that data of the resources will be the foundation of the nexus. The systematic collection, accuracy of data, and empirical analysis of data will determine the level of security of WEF nexus. The qualitative results show that there are applications of the fourth industrial revolution technologies to the individual sectors of the nexus, birthing Water 4.0, Energy 4.0, and Food 4.0. The Bibliometric analysis result shows that the integration of the fourth industrial revolution with the WEF nexus will lead to cleaner production practices relating to the technological processes of water, energy, and food resources. These practices will ensure the environment's safety from WEF wastes and the water, energy, and food security in production processes. The empirical research and bibliometric analysis result, rooted in the concept of cleaner production, shows that the fourth industrial revolution affected the WEF nexus. The effects are; the birth of clean technologies & industrial applications, the catalyst for sustainability security of WEF nexus leveraging on life cycle thinking, enablement of technological transfer, enhancement of economic growth, and urban planning. The study concludes that the fourth industrial revolution technologies affect WEF nexus, ensuring the popularization of cleaner production strategies and processes of the resources during trade-offs and synergies. The study recommends the integration of a cleaner production concept in WEF processing. It should follow the innovation diffusion theory (IDT) and Technology acceptance theory (TAM) when applying 4IR technologies to the nexus of water, energy, and food resources, for their sustainable security.}
}
@article{20241231,
title = {In This Issue},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {9},
number = {12},
pages = {1231},
year = {2024},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2024.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S2451902224003070}
}
@article{PAN2024102334,
title = {Novel blockchain deep learning framework to ensure video security and lightweight storage for construction safety management},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102334},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102334},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004627},
author = {Xing Pan and Luoxin Shen and Botao Zhong and Da Sheng and Fang Huang and Luhan Yang},
keywords = {Construction safety management, Video security storage, Blockchain, Deep learning, Video summarization, Pre-defined rule},
abstract = {In construction management, video data tampering behavior like manual forging and deletion can negatively impact on-site safety and accident accountability. Blockchain technology holds the potential to address this issue by leveraging distributed ledger characteristics. However, blockchain's limited storage capacity and block size make it difficult to upload large-sized construction data such as daily monitoring video. Furthermore, it is unnecessary to store all construction data in any case. Therefore, this study proposes a blockchain deep learning framework that focuses on how to efficiently extract and securely store key information (i.e., video summarization that involves worker’s unsafe behavior) on-blockchain for data traceability. The framework involves a novel data-driven and rule-based keyframe extraction (DRKE) model to lightweight large-sized construction video in the nascent field of deep learning and blockchain combination. To define parameters for the DRKE model, specific construction rules (e.g., people’s unsafe behavior-based and people-based rules) have been pre-defined. This framework has been evaluated, and the results demonstrate its capability for effective video security storage, facilitating practical needs in construction management. The study extends existing research and provides a practical solution for large-sized construction video storage with security and lightweight considerations. The proposed video security storage and data lightweight process offers substantial benefits to construction management, such as streamlined accident investigation and accountability and improved on-site work efficiency, contributing to the smooth progress of construction projects.}
}
@article{BARKE2023618,
title = {Linking life cycle sustainability assessment and the sustainable development goals – Calculation of goal achievement},
journal = {Procedia CIRP},
volume = {116},
pages = {618-623},
year = {2023},
note = {30th CIRP Life Cycle Engineering Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.104},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123000999},
author = {Alexander Barke and Manbir S. Sodhi and Christian Thies and Thomas S. Spengler},
keywords = {Sustainable development goals (SDGs), SDG quantification, Sustainable development, Life cycle sustainability assessment},
abstract = {In 2015, the United Nations General Assembly proposed seventeen Sustainable Development Goals (SDGs) intended to ensure sustainable development worldwide at the economic, environmental, and social levels. SDGs are now being used by some corporations in formulating and expressing business strategies. However, assessing the effects of corporate activities and products regarding their contribution to SDGs is difficult. In this paper, we have developed a method for linking life cycle sustainability assessment (LCSA) with the SDGs and calculating the contribution to SDG achievement. An essential part of this approach is the weighting of LCSA impact categories, which is typically done using equal weighting. This weighting method enables compensation of negative contributions by positive contributions in different impact categories but results in ambiguity in the results. This article identifies alternative weighting methods, integrates them into a computational approach, and determines their influence on the SDG contribution scores. The analysis shows that the use of alternative weights changes SDG contribution scores. However, the same product always has the highest SDG contribution score, regardless of the weighting method used. Nonetheless, the recommendations for action with regard to the total product alternatives would change depending on the weighting method.}
}
@article{RAMIREZPEDRAZA2020293,
title = {A bio-inspired model of behavior considering decision-making and planning, spatial attention and basic motor commands processes},
journal = {Cognitive Systems Research},
volume = {59},
pages = {293-303},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S138904171930508X},
author = {Raymundo Ramirez-Pedraza and Natividad Vargas and Carlos Sandoval and Juan Luis {del Valle-Padilla} and Félix Ramos},
keywords = {Brain model, Decision-making, Planning, Spatial attention, Motor system, Goal-driven},
abstract = {Cognitive architectures (CA) are an IA approach to implement computer systems with human-like behavior. Fundamental exhibited human capabilities include planning and decision-making. In that regard, numerous AI systems successfully exhibit human-like behavior but are limited to either achieving specific objectives or are restrained to too heavily constrained environments, which makes them unsuitable in the presence of unforeseen situations where autonomy is required. To try to alleviate the problem, we present a bio-inspired computational model to solve the autonomous navigation problem of a computational entity in a controlled context. This proposal is the result of the interaction between planning and decision-making, spatial attention and the motor cognitive functions. The proposed model is based on neuroscientific evidence concerning the involved cognitive functions and is part of a more general cognitive architecture. In the case study developed to validate our idea, we can see that the processes previously identified play an important role to accomplish spatial navigation. In the case study presented, an agent achieves the navigation over an unexplored maze from an initial to a final position successfully. The reunited results motivate us to continue improving our model considering attentional information to influence the agent’s motor behavior.}
}
@incollection{CLEEREMANS20012584,
title = {Conscious and Unconscious Processes in Cognition},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2584-2589},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/03560-9},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767035609},
author = {A. Cleeremans},
abstract = {Characterizing the relationships between conscious and unconscious processes is one of the most important and long-standing goals of cognitive psychology. Renewed interest in the nature of consciousness—long considered not to be scientifically explorable—as well as the increasingly widespread availability of functional brain-imaging techniques, now offer the possibility of detailed exploration of the neural, behavioral, and computational correlates of conscious and unconscious cognition. This article reviews some of the relevant experimental work, highlights the methodological challenges involved in establishing the extent to which cognition can occur unconsciously, and situates ongoing debates in the theoretical context provided by current thinking about consciousness.}
}
@article{MARCHAND1995179,
title = {Policy analysis as a tool for habitat restoration: A case study of a Danube river floodplain, Hungary},
journal = {Water Science and Technology},
volume = {31},
number = {8},
pages = {179-186},
year = {1995},
note = {Integrated Water Resources Management},
issn = {0273-1223},
doi = {https://doi.org/10.1016/0273-1223(95)00399-8},
url = {https://www.sciencedirect.com/science/article/pii/0273122395003998},
author = {M. Marchand* and E.C.L. Marteijn** and P. Bakonyi***},
keywords = {Floodplain rehabilitation, Danube, policy analysis, water quality modelling},
abstract = {This paper will elaborate a policy analysis approach especially designed for habitat restoration. It will be illustrated by a case study example of a floodplain area along the Danube river, Hungary. The case study used hydrodynamic and water quality models and expertise from a range of disciplines. This made it possible to unravel the complex relations between the environment and human interventions. Crucial was the participation of local experts in the design and screening of measures, as well as the feedback from local interest groups at several occasions during the project. This resulted in the formulation of rehabilitation ideas, most of which have hitherto not been discussed. The combination of creative thinking with practical possibilities and limitations has been worked out in a cyclic process from which three different alternatives emerged. These have been analyzed for their feasibility with regard to the goals to be achieved, their costs and their impacts on other interests.}
}
@article{OSCARIDO2023539,
title = {The impact of competitive FPS video games on human's decision-making skills},
journal = {Procedia Computer Science},
volume = {216},
pages = {539-546},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.167},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022451},
author = {Juan Oscarido and Zulfikar Airlangga Siswanto and Devin Akwila Maleke and Alexander Agung Santoso Gunawan},
keywords = {decision making, comparison strategy, video games, cognitive skill, influence-of-games, game-based learning},
abstract = {The problem we face today is that many people think that playing games only has a negative impact on a person's brain and behavior. but the fact is that playing games has a positive impact in many ways. The aim of this document is to prove whether video games can really influence human behavior on their decision-making skills. We will test 22 respondents directly who are teenagers and adults around 17 - 25 years old, and we will score them after they have finished playing games with the genre that we decided. The results proved that competitive First-person shooter (FPS) games increase human ability to make decisions quickly and correctly. Many of our participants agree that after playing competitive FPS games, they feel a positive impact on their cognitive skills. Our participants said that they can quickly compare the impact of the decisions they make and choose exactly which is the best course of action.}
}
@article{ALBEROLA20161,
title = {An artificial intelligence tool for heterogeneous team formation in the classroom},
journal = {Knowledge-Based Systems},
volume = {101},
pages = {1-14},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116000964},
author = {Juan M. Alberola and Elena {del Val} and Victor Sanchez-Anguix and Alberto Palomares and Maria {Dolores Teruel}},
keywords = {Team formation, Artificial intelligence, Belbin roles, Computational intelligence},
abstract = {Nowadays, there is increasing interest in the development of teamwork skills in the educational context. This growing interest is motivated by its pedagogical effectiveness and the fact that, in labour contexts, enterprises organise their employees in teams to carry out complex projects. Despite its crucial importance in the classroom and industry, there is a lack of support for the team formation process. Not only do many factors influence team performance, but the problem becomes exponentially costly if teams are to be optimised. In this article, we propose a tool whose aim it is to cover such a gap. It combines artificial intelligence techniques such as coalition structure generation, Bayesian learning, and Belbin’s role theory to facilitate the generation of working groups in an educational context. This tool improves current state of the art proposals in three ways: i) it takes into account the feedback of other teammates in order to establish the most predominant role of a student instead of self-perception questionnaires; ii) it handles uncertainty with regard to each student’s predominant team role; iii) it is iterative since it considers information from several interactions in order to improve the estimation of role assignments. We tested the performance of the proposed tool in an experiment involving students that took part in three different team activities. The experiments suggest that the proposed tool is able to improve different teamwork aspects such as team dynamics and student satisfaction.}
}
@article{AYDIN2013173,
title = {A swarm intelligence based sample average approximation algorithm for the capacitated reliable facility location problem},
journal = {International Journal of Production Economics},
volume = {145},
number = {1},
pages = {173-183},
year = {2013},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2012.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0925527312004604},
author = {Nezir Aydin and Alper Murat},
keywords = {Reliable, Facility location, Stochastic programming, Sample average approximation, Swarm intelligence},
abstract = {We present a novel hybrid method, swarm intelligence based sample average approximation (SIBSAA), for solving the capacitated reliable facility location problem (CRFLP). The CRFLP extends the well-known capacitated fixed-cost facility problem by accounting for the unreliability of facilities. The standard SAA procedure, while effectively used in many applications, can lead to poor solution quality if the selected sample sizes are not sufficiently large. With larger sample sizes, however, the SAA method is not practical due to the significant computational effort required. The proposed SIBSAA method addresses this limitation by using smaller samples and repetitively applying the SAA method while injecting social learning in the solution process inspired by the swarm intelligence of particle swarm optimization. We report on experimental study results showing that the SIBSAA improves the computational efficiency significantly while attaining same or better solution quality than the SAA method.}
}
@article{HAN2014106,
title = {Toward an understanding of the impact of production pressure on safety performance in construction operations},
journal = {Accident Analysis & Prevention},
volume = {68},
pages = {106-116},
year = {2014},
note = {Systems thinking in workplace safety and health},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2013.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0001457513004041},
author = {SangUk Han and Farzaneh Saba and SangHyun Lee and Yasser Mohamed and Feniosky Peña-Mora},
keywords = {Safety, Systems thinking, Accident prevention, Simulation, Causal loop analysis},
abstract = {It is not unusual to observe that actual schedule and quality performances are different from planned performances (e.g., schedule delay and rework) during a construction project. Such differences often result in production pressure (e.g., being pressed to work faster). Previous studies demonstrated that such production pressure negatively affects safety performance. However, the process by which production pressure influences safety performance, and to what extent, has not been fully investigated. As a result, the impact of production pressure has not been incorporated much into safety management in practice. In an effort to address this issue, this paper examines how production pressure relates to safety performance over time by identifying their feedback processes. A conceptual causal loop diagram is created to identify the relationship between schedule and quality performances (e.g., schedule delays and rework) and the components related to a safety program (e.g., workers’ perceptions of safety, safety training, safety supervision, and crew size). A case study is then experimentally undertaken to investigate this relationship with accident occurrence with the use of data collected from a construction site; the case study is used to build a System Dynamics (SD) model. The SD model, then, is validated through inequality statistics analysis. Sensitivity analysis and statistical screening techniques further permit an evaluation of the impact of the managerial components on accident occurrence. The results of the case study indicate that schedule delays and rework are the critical factors affecting accident occurrence for the monitored project.}
}
@article{JACOB2020102142,
title = {Neural correlates of rumination in major depressive disorder: A brain network analysis},
journal = {NeuroImage: Clinical},
volume = {25},
pages = {102142},
year = {2020},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2019.102142},
url = {https://www.sciencedirect.com/science/article/pii/S2213158219304887},
author = {Yael Jacob and Laurel S Morris and Kuang-Han Huang and Molly Schneider and Sarah Rutter and Gaurav Verma and James W Murrough and Priti Balchandani},
keywords = {Default mode network, Depression, Entropy, Graph Theory, High-field MRI, Precuneus},
abstract = {Patients with major depressive disorder (MDD) exhibit higher levels of rumination, i.e., repetitive thinking patterns and exaggerated focus on negative states. Rumination is known to be associated with the cortical midline structures / default mode network (DMN) region activity, although the brain network topological organization underlying rumination remains unclear. Implementing a graph theoretical analysis based on ultra-high field 7-Tesla functional MRI data, we tested whether whole brain network connectivity hierarchies during resting state are associated with rumination in a dimensional manner across 20 patients with MDD and 20 healthy controls. Applying this data-driven approach we found a significant correlation between rumination tendency and connectivity strength degree of the right precuneus, a key node of the DMN. In order to interrogate this region further, we then applied the Dependency Network Analysis (DEPNA), a recently developed method used to quantify the connectivity influence of network nodes. This revealed that rumination was associated with lower connectivity influence of the left medial orbito-frontal cortex (MOFC) cortex on the right precuneus. Lastly, we used an information theory entropy measure that quantifies the cohesion of a network's correlation matrix. We show that subjects with higher rumination scores exhibit higher entropy levels within the DMN i.e. decreased overall connectivity within the DMN. These results emphasize the general DMN involvement during self-reflective processing related to maladaptive rumination in MDD. This work specifically highlights the impact of the MOFC on the precuneus, which might serve as a target for clinical neuromodulation treatment.}
}
@article{LAWLER1996241,
title = {Thinkable models},
journal = {The Journal of Mathematical Behavior},
volume = {15},
number = {3},
pages = {241-259},
year = {1996},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(96)90004-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312396900048},
author = {Robert W. Lawler},
abstract = {A primary objective of technical education should be the development of thinkable models in the minds of students. Thinkable models are representations of things and processes simple enough that people can use them in thought experiments. The organization of cognitive structures for technical domains can be imagined to be a network of appropriately connected thinkable models. Artificial intelligence (AI), as the science of representations, has focused in the main on languagelike representations. If we can enrich our vision of representations to include a greater variety of ways of thinking that are useful to people, we may hope to broaden access to scientific ideas. To pursue these notions in some detail, a taxonomy of models is developed and the issue of how representations relate to human modes of perception and action is raised. The notions are explored first through contrasting of several approaches to the Pythagorean Theorem.}
}
@article{J2023105690,
title = {Deep learning based multi-labelled soil classification and empirical estimation toward sustainable agriculture},
journal = {Engineering Applications of Artificial Intelligence},
volume = {119},
pages = {105690},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105690},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622006807},
author = {Padmapriya J. and Sasilatha T.},
keywords = {Soil classification, Q-HOG, SVM, Deep Neural Network, VGG16},
abstract = {Agriculture is the underlying occupation of the vast people in India and it is a major economic contribution. Soil is prime for the vital nutrient supply to the crops and its yield. Determination of the type of soil which comprises of the clay, sand and silt particles in the respective proportion is indeed significant for the suitable crop selection and to identify the weeds growth. The most commonly utilized soil determination methods were International Pipette method and Pressure-plate apparatus method. In this research work, multiclass soil classification using machine learning and deep learning models for the appropriate determination of the soil type as Multi-Stacking ensemble model and a novel feature selection algorithm Q-HOG is proposed; since the Artificial Intelligence has led to furtherance in the smart agriculture. Besides, the images are collected from the exploration site vriddhachalam along with the soil datasets will increase the classification accuracy. The deep learning models Recurrent Neural Network(RNN), Long Short Term Memory(LSTM), Gated Recurrent Unit(GRU) and VGG16 are considered and the comprehensive evaluation of these different deep learning architectures and also the machine learning algorithms such as Naïve-bayes, KNN, SVM are carried out and the obtained results are tabulated. Multi-stacking ensemble model for multi-classification is proposed with the Machine learning and deep learning algorithms and evaluated the performance with increased computation time. Among these models the proposed model outperformed in soil classification in-terms of accuracy as 98.96 percent, achieved precision as 96.14 percent, recall as 99.65 percent and the achieved F1-Score is 97.87 percent.}
}
@article{SPIVEY2025149477,
title = {A linking hypothesis for eyetracking and mousetracking in the visual world paradigm},
journal = {Brain Research},
volume = {1851},
pages = {149477},
year = {2025},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2025.149477},
url = {https://www.sciencedirect.com/science/article/pii/S0006899325000356},
author = {Michael J. Spivey},
keywords = {Psycholinguistics, Eyetracking, Mousetracking, Spoken word recognition, Action-perception cycle, Perception–action cycle, Dynamical systems, Embodied cognition},
abstract = {For a linking hypothesis in the visual world paradigm to clearly accommodate existing findings and make unambiguous predictions, it needs to be computationally implemented in a fashion that transparently draws the causal connection between the activations of internal representations and the measured output of saccades and reaching movements. Quantitatively implemented linking hypotheses provide an opportunity to not only demonstrate an existence proof of that causal connection but also to test the fidelity of the measuring methods themselves. When a system of interest is measured one way (e.g., ballistic dichotomous outputs) or another way (e.g., smooth graded outputs), the apparent results can differ substantially. What is needed is one linking hypothesis that can produce both types of outputs. The localist attractor network simulation of spoken word recognition demonstrated here recreates eye and mouse movements that capture key findings in the visual world paradigm, and especially relies on one particularly powerful theoretical construct: feedback from the action-perception cycle. Visual feedback from the eye position enhancing the cognitive prominence of the fixated object allows the simulation to fit a wider range of findings, and points to predictions for new experiments. When that feedback is absent, the linking hypothesis simulation no longer fits human data as well. Future experiments, and improvements of this network simulation, are discussed.}
}
@incollection{DU202457,
title = {Chapter 3 - Data, machine learning, first-principles, and hybrid models in the petrochemical industry},
editor = {Masoud Soroush and Richard {D Braatz}},
booktitle = {Artificial Intelligence in Manufacturing},
publisher = {Academic Press},
pages = {57-96},
year = {2024},
isbn = {978-0-323-99135-3},
doi = {https://doi.org/10.1016/B978-0-323-99135-3.00011-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991353000117},
author = {Di Du and Johannes Pieter Schmal},
keywords = {Data, Data types, Data-driven models, First-principles models, Hybrid models, Machine learning},
abstract = {With the increase in computational power, memory, data storage, and data availability models have become more abundant and powerful, leading to many process improvements and benefits in the petrochemical industry. In this chapter, we will first discuss the data types based on dimensions. We then discuss different machine-learning approaches and other data-driven approaches with applications in the petrochemical industry. First-principles and hybrid modeling approaches are also discussed and compared with machine-learning approaches.}
}
@article{PATTEE20025,
title = {The origins of Michael Conrad's research programs (1964–1979)},
journal = {Biosystems},
volume = {64},
number = {1},
pages = {5-11},
year = {2002},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00169-1},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001691},
author = {H.H Pattee},
keywords = {Artificial ecosystems, Adaptability, Evolvability, Non-programmable computation, Brain-computer disanalogy, Enzymatic neurons},
abstract = {This paper summarizes Michael Conrad's academic and professional career from the time he began his Ph.D. studies in 1964 to his appointment at Wayne State University in 1979. It describes the origins of several of his major research interests and presents a personal evaluation of how this early work continues to be of fundamental importance.}
}
@article{HUANG2011183,
title = {On the intrinsic inevitability of cancer: From foetal to fatal attraction},
journal = {Seminars in Cancer Biology},
volume = {21},
number = {3},
pages = {183-199},
year = {2011},
note = {Why Systems Biology and Cancer?},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2011.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X11000320},
author = {Sui Huang},
keywords = {Tumorigenesis, Tumour progression, Epigenetic landscape, Gene regulatory network, Attractor, State space, Somatic mutation, Oncogene},
abstract = {The cracks in the paradigm of oncogenic mutations and somatic evolution as driving force of tumorigenesis, lucidly exposed by the dynamic heterogeneity of “cancer stem cells” or the diffuse results of cancer genome sequencing projects, indicate the need for a more encompassing theory of cancer that reaches beyond the current proximate explanations based on individual genetic pathways. One such integrative concept, derived from first principles of the dynamics of gene regulatory networks, is that cancerous cell states are attractor states, just like normal cell types are. Here we extend the concept of cancer attractors to illuminate a more profound property of cancer initiation: its inherent inevitability in the light of metazoan evolution. Using Waddington's Epigenetic Landscape as a conceptual aid, for which we present a mathematical and evolutionary foundation, we propose that cancer is intrinsically linked to ontogenesis and phylogenesis. This explanatory rather than enumerating review uses a formal argumentation structure that is atypical in modern experimental biology but may hopefully offer a new coherent perspective to reconcile many conflicts between new findings and the old thinking in the categories of linear oncogenic pathways.}
}
@article{ZHANG2024308,
title = {Empathetic Language in LLMs under Prompt Engineering: A Comparative Study in the Legal Field},
journal = {Procedia Computer Science},
volume = {244},
pages = {308-317},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030059},
author = {Yifan Zhang and Christopher Radishian and Sabine Brunswicker and Dan Whitenack and Daniel W. Linna},
keywords = {LLM, Human-AI Interaction, Empathetic Response},
abstract = {The demand for empathetic conversations increases with conversational AIs’ rise and exponentially spreading applications. In areas like law and healthcare, where professional and empathetic conversations are essential, conversational AIs must strive to retain the correctness of information and logic while improving on empathetic language use. When addressing such an issue, we focus on linguistic empathy, relating only to syntactic and rhetoric choices in language while disregarding the emotional aspect of influence. By performing this study, we are interested in finding whether current open-sourced Large Language Models (LLMs) can match human experts in the legal field by using empathetic language while not compromising facts and logic in responses. We compare responses from three open-sourced LLMs under four prompting strategies with the expert responses. In the comparison, we use metrics from three aspects: text and semantic similarity, factual consistency, and ten rules of linguistic empathy from previous research literature. After statistical tests, the comparison results show that language models can use empathetic language without compromising the default knowledge base of LLMs when properly prompt-engineered. To accomplish this, additional domain knowledge is still needed to match factually. The data supporting this study is publicly available at huggingface.co/datasets/RCODI/empathy-prompt and code is available at github.com/RCODI-ConversationalAI/Empathy-Prompt.}
}
@incollection{ROSCHELLE20071,
title = {Designing Networked Handheld Devices to Enhance School Learning},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {70},
pages = {1-60},
year = {2007},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(06)70001-8},
url = {https://www.sciencedirect.com/science/article/pii/S0065245806700018},
author = {Jeremy Roschelle and Charles Patton and Deborah Tatar},
abstract = {Handheld devices, especially networked handheld devices, are growing in importance in education, largely because their affordability and accessibility create an opportunity for educators to transition from occasional, supplemental use of computers, to frequent and integral use of portable computational technology. Why and how might these new devices enhance school learning? We begin by discussing a simple but important factor: networked handhelds can allow a 1:1 student:device ratio for the first time, enabling ready-at-hand access to technology throughout the school day and throughout the learner's personal life. We argue that designers need to understand the capabilities of the new generation of handheld computers and wireless networks that are most relevant for learning. We follow this with a discussion of Learning Science theories that connect those capabilities to enhanced learning. The capabilities and features feed into design practices. We describe a set of example applications that are arising from the capabilities, theories and design practices previously described. Finally, we close with a discussion of the challenge of scale.}
}
@article{VANLAAR2025106259,
title = {Towards desirable futures for the circular adaptive reuse of buildings: A participatory approach},
journal = {Sustainable Cities and Society},
volume = {122},
pages = {106259},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2025.106259},
url = {https://www.sciencedirect.com/science/article/pii/S2210670725001362},
author = {Brian {van Laar} and Angela Greco and Hilde Remøy and Vincent Gruis and Mohammad B. Hamida},
keywords = {Adaptive reuse, Scenario development, Cross-impact balance analysis, Participatory scenario workshops, Normative narrative scenarios, Circularity},
abstract = {Adaptive reuse of buildings offers a sustainable strategy for reducing global CO2 emissions by repurposing existing structures, conserving resources, reducing the need to extract new materials, and minimizing waste. However, the decision-making process in adaptive reuse projects is often complex, involving conflicting criteria and diverse stakeholders. Current approaches tend to polarize alternatives, focusing either on broad functional use or specific design options, which can limit decision effectiveness and quality. This study addresses these challenges by developing a participatory mixed-methods approach that integrates Cross-Impact Balance (CIB) analysis with creative scenario-building techniques, including generative AI and participatory workshops. This approach balances the extremes of current decision-making processes, offering a more comprehensive overview of desirable futures for decision-makers. The methodology was applied to create 15 “big picture” circular adaptive reuse scenarios, each incorporating circular building adaptability (CBA) strategies, and enriched with AI generated narratives and visualizations. These scenarios provide stakeholders with a nuanced understanding of potential future pathways, enhancing decision-making processes. This mixed-method approach demonstrates the potential of participatory CIB scenario development in advancing circularity, offering a valuable tool for navigating the complexities of adaptive reuse decision-making.}
}
@incollection{SAHU2022127,
title = {Artificial Intelligence and Machine Learning: New Age Tools for Augmenting Plastic Materials Designing, Processing, and Manufacturing},
editor = {M.S.J. Hashmi},
booktitle = {Encyclopedia of Materials: Plastics and Polymers},
publisher = {Elsevier},
address = {Oxford},
pages = {127-152},
year = {2022},
isbn = {978-0-12-823291-0},
doi = {https://doi.org/10.1016/B978-0-12-820352-1.00108-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128203521001085},
author = {Kisor Kumar Sahu and Shibu Meher and Abhilash M. Menon and M.K. Sridhar and Gangala V. {Harsha Vardhan} and Saurabh Pandey and Ashutosh Kumar and Shreeja Das},
keywords = {Artificial intelligence (AI), Artificial neural network (ANN), Autoencoders, Deep learning, Machine learning (ML), Principal component analysis (PCA)},
abstract = {Plastic and polymers are late entrants in the repository of engineering materials, compared to bronze and iron (early phases of human civilizations are named after them). However, the extent of usage of the former is growing at an exponential rate because of the near-infinite combinatorial possibilities. In fact, there are hardly few engineering disciplines that can potentially offer so large and endless unexploited possibilities. Plastic industries are widespread across the world due to their easy scalability, favorable economics and extremely diverse applications. Plastic manufacturing and recycling are especially important for the economy of a country and provide livelihood for a large population. It is imperative that the current processes in plastic be improved upon by the advantages offered by computational tools and digital technologies. At this stage, we desperately need new age tools that can properly guide the human enterprise of innovation in designing, perfection in processing while maintaining stringent quality requirements in manufacturing. Artificial intelligence (AI) and machine learning (ML) perfectly fits this bill for the new age tools. We are at a very nascent stage of this AI/ML revolution. This article samples some of the pioneering works from the very discreet space of AI/ML applications in the field of plastic and polymer designing, processing and manufacturing and attempts to tie them up in a cohesive narrative. For the sake of completeness, applications of AI/ML for limiting the adverse environmental impact and future outlook have also been covered.}
}
@article{CAHILL20172131,
title = {Building a Community of Practice to Prepare the HPC Workforce},
journal = {Procedia Computer Science},
volume = {108},
pages = {2131-2140},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917305902},
author = {Katharine J. Cahill and Scott Lathrop and Steven Gordon},
keywords = {HPC workforce, petascale computing, on-line education, graduate education, SPOC course},
abstract = {It has been well documented for more than 30 years, that significantly more effort is needed to prepare the HPC workforce needed today and well into the future. The Blue Waters Virtual School of Computational Science (VSCSE) provides an innovative model for addressing this critical need. The VSCSE uses a Small Private Online Course (SPOC) approach to providing graduate level credit courses to students at multiple institutions. In this paper, we describe the rationale for this approach, a description of the implementation, findings from external evaluations, and lessons learned. The paper concludes with recommendations for future strategies to build on this work to address the workforce needs of our global society.}
}
@article{HOLZINGER2025103032,
title = {Enhancing trust in automated 3D point cloud data interpretation through explainable counterfactuals},
journal = {Information Fusion},
volume = {119},
pages = {103032},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103032},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001058},
author = {Andreas Holzinger and Niko Lukač and Dzemail Rozajac and Emile Johnston and Veljka Kocic and Bernhard Hoerl and Christoph Gollob and Arne Nothdurft and Karl Stampfer and Stefan Schweng and Javier {Del Ser}},
keywords = {Explainable AI, Point cloud data, Counterfactual reasoning, Information fusion, Interpretability, Human-centered AI},
abstract = {This paper introduces a novel framework for augmenting explainability in the interpretation of point cloud data by fusing expert knowledge with counterfactual reasoning. Given the complexity and voluminous nature of point cloud datasets, derived predominantly from LiDAR and 3D scanning technologies, achieving interpretability remains a significant challenge, particularly in smart cities, smart agriculture, and smart forestry. This research posits that integrating expert knowledge with counterfactual explanations – speculative scenarios illustrating how altering input data points could lead to different outcomes – can significantly reduce the opacity of deep learning models processing point cloud data. The proposed optimization-driven framework utilizes expert-informed ad-hoc perturbation techniques to generate meaningful counterfactual scenarios when employing state-of-the-art deep learning architectures. The optimization process minimizes a multi-criteria objective comprising counterfactual metrics such as similarity, validity, and sparsity, which are specifically tailored for point cloud datasets. These metrics provide a quantitative lens for evaluating the interpretability of the counterfactuals. Furthermore, the proposed framework allows for the definition of explicit interpretable counterfactual perturbations at its core, thereby involving the audience of the model in the counterfactual generation pipeline and ultimately, improving their overall trust in the process. Results demonstrate a notable improvement in both the interpretability of the model’s decisions and the actionable insights delivered to end-users. Additionally, the study explores the role of counterfactual reasoning, coupled with expert input, in enhancing trustworthiness and enabling human-in-the-loop decision-making processes. By bridging the gap between complex data interpretations and user comprehension, this research advances the field of explainable AI, contributing to the development of transparent, accountable, and human-centered artificial intelligence systems.}
}
@article{MOHIT20221713,
title = {Approach of artificial intelligence for analysing properties of concrete},
journal = {Materials Today: Proceedings},
volume = {48},
pages = {1713-1717},
year = {2022},
note = {SCPINM-2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S221478532106507X},
author = { Mohit and Balwinder Lallotra},
keywords = {Concrete, Artificial intelligence, Artificial neural network, Workability, Compressive strength},
abstract = {Technological progress is often measured by computation power. At the moment we are in the golden age where we are blessed with a perfect trio, machine learning algorithms, huge datasets across disciplines, and processing hardware. The constant desire to understand the human brain has led us to try mimicking it, thus forming the basis of neural networks creating a way for deep learning algorithms. Such algorithms have proven to work on non-linear data sets effectively, generating results that could find patterns just like our brains. In this paper, we explore a recently rising application for Neural Network frameworks; in particular, concrete in basic designing. We design and implement tests to analyze various properties of concrete of different concrete mixes. Customarily, the ability of concrete to perform is influenced by numerous non-straight factors, and testing its quality includes the destructive procedure of concrete samples.}
}
@incollection{SADEGHI2024457,
title = {Chapter Thirteen - Dynamic framework for large-scale modeling of membranes and peripheral proteins},
editor = {Markus Deserno and Tobias Baumgart},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {701},
pages = {457-514},
year = {2024},
booktitle = {Biophysical Approaches for the Study of Membrane Structure—Part B: Theory and Simulations},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0076687924001137},
author = {Mohsen Sadeghi and David Rosenberger},
keywords = {Membranes, Mesoscopic modeling, Particle-based modeling, Hydrodynamics, Membrane-protein interaction},
abstract = {In this chapter, we present a novel computational framework to study the dynamic behavior of extensive membrane systems, potentially in interaction with peripheral proteins, as an alternative to conventional simulation methods. The framework effectively describes the complex dynamics in protein-membrane systems in a mesoscopic particle-based setup. Furthermore, leveraging the hydrodynamic coupling between the membrane and its surrounding solvent, the coarse-grained model grounds its dynamics in macroscopic kinetic properties such as viscosity and diffusion coefficients, marrying the advantages of continuum- and particle-based approaches. We introduce the theoretical background and the parameter-space optimization method in a step-by-step fashion, present the hydrodynamic coupling method in detail, and demonstrate the application of the model at each stage through illuminating examples. We believe this modeling framework to hold great potential for simulating membrane and protein systems at biological spatiotemporal scales, and offer substantial flexibility for further development and parametrization.}
}
@article{WANG2023458,
title = {The Hutong neighbourhood grammar: A procedural modelling approach to unravel the rationale of historical Beijing urban structure},
journal = {Frontiers of Architectural Research},
volume = {12},
number = {3},
pages = {458-476},
year = {2023},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095263523000031},
author = {Yuyang Wang and Andrew Crompton and Asterios Agkathidis},
keywords = {Urban morphology, Siheyuan, Hutong neighbourhood, Procedural modelling, Shape grammar},
abstract = {Hutong neighbourhoods, composed of Chinese courtyard dwellings (Siheyuan), are historically and socially significant urban spaces that embody the traditional Chinese way of life and philosophy. As part of the national heritage, there is an increasing research interest in Hutong neighbourhoods, many of which are facing oblivion. This study presents a formal grammar for Hutong neighbourhood generation. This research investigates traditional principles of urban planning of ancient Beijing, based on examples on the historical map Qianlong Jingcheng Quantu, to derive the lost design rules. These rules are used to build up a procedural modelling framework, which reveals the development of Beijing's urban structure from the Yuan (1271–1368) to the Qing (1644–1911) dynasty. Our findings present a grammar incorporated into the procedural modelling framework to parametrically generate Hutong neighbourhoods, which replicates the morphological characteristics of historic cases. It contributes to the understanding of the generation of Hutong neighbourhoods. In support of heritage sustainability, this grammar can be implemented in a computational environment by visual scripting that enables the generation of new instances of Hutong neighbourhoods, both real and virtual.}
}
@article{CORDASCO201815,
title = {Distributed MASON: A scalable distributed multi-agent simulation environment},
journal = {Simulation Modelling Practice and Theory},
volume = {89},
pages = {15-34},
year = {2018},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X18301230},
author = {Gennaro Cordasco and Vittorio Scarano and Carmine Spagnuolo},
keywords = {Agent-based simulation, Parallel computing, Distributed computing, Scalable computational science, Cloud computing},
abstract = {Computational Social Science (CSS) involves interdisciplinary fields and exploits computational methods, such as social network analysis as well as computer simulation with the goal of better understanding social phenomena. Agent-Based Models (ABMs) represent an effective research tool for CSS and consist of a class of models, which, aim to emulate or predict complex phenomena through a set of simple rules (i.e., independent actions, interactions and adaptation), performed by multiple agents. The efficiency and scalability of ABMs systems are typically obtained distributing the overall computation on several machines, which interact with each other in order to simulate a specific model. Unfortunately, the design of a distributed simulation model is particularly challenging, especially for domain experts who sporadically are computer scientists and are not used to developing parallel code. D-MASON framework is a distributed version of the MASON library for designing and executing ABMs in a distributed environment ensuring scalability and easiness. D-MASON enable the developer to exploit the computing power of distributed environment in a transparent manner; the developer has to do simple incremental modifications to existing MASON models, without re-designing them. This paper presents several novel features and architectural improvements introduced in the D-MASON framework: an improved space partitioning strategy, a distributed 3D field, a distributed network field, a decentralized communication layer, a novel memory consistency mechanism and the integration to cloud environments. Full documentation, additional tutorials, and other material can be found at https://github.com/isislab-unisa/dmason where the framework can be downloaded.}
}
@article{COLOM2017385,
title = {Collaborative building of behavioural models based on internet of things},
journal = {Computers & Electrical Engineering},
volume = {58},
pages = {385-396},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302191},
author = {José Francisco Colom and Higinio Mora and David Gil and María Teresa Signes-Pont},
keywords = {Social internet of things, Big data, Embedded systems, Healthcare, Distributed system framework},
abstract = {This paper proposes a new framework that takes advantage of the computing capabilities provided by the Internet of Thing (IoT) paradigm in order to support collaborative applications. It looks at the requirements needed to run a wide range of computing tasks on a set of devices in the user environment with limited computing resources. This approach contributes to building the social dimension of the IoT by enabling the addition of computing resources accessible to the user without harming the other activities for which the IoT devices are intended. The framework mainly includes a model of the computing load, a scheduling mechanism and a handover procedure for transferring tasks between available devices. The experiments show the feasibility of the approach and compare different implementation alternatives.}
}
@article{COHEN2017208,
title = {Where Does EEG Come From and What Does It Mean?},
journal = {Trends in Neurosciences},
volume = {40},
number = {4},
pages = {208-218},
year = {2017},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2017.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166223617300243},
author = {Michael X Cohen},
keywords = {EEG, neural microcircuit, oscillations, electrophysiology, computation},
abstract = {Electroencephalography (EEG) has been instrumental in making discoveries about cognition, brain function, and dysfunction. However, where do EEG signals come from and what do they mean? The purpose of this paper is to argue that we know shockingly little about the answer to this question, to highlight what we do know, how important the answers are, and how modern neuroscience technologies that allow us to measure and manipulate neural circuits with high spatiotemporal accuracy might finally bring us some answers. Neural oscillations are perhaps the best feature of EEG to use as anchors because oscillations are observed and are studied at multiple spatiotemporal scales of the brain, in multiple species, and are widely implicated in cognition and in neural computations.}
}
@article{STRACHANREGAN2024e28340,
title = {The impact of room shape on affective states, heartrate, and creative output},
journal = {Heliyon},
volume = {10},
number = {6},
pages = {e28340},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e28340},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024043718},
author = {K. Strachan-Regan and O. Baumann},
keywords = {Built environment, Neuroarchitecture, Environmental psychology, Emotion, Creativity},
abstract = {The architectural design of space can deeply impact an individuals' mood, physiology, and mental health. While previous research has predominantly focused on elements like nature and lighting within architectural spaces, there is a growing literature base that also investigates the psychological and neurophysiological impacts of geometrical properties of architectural spaces. Employing virtual reality technology, the study sought to investigate the effects of curved and rectangular architectural spaces on affective states, heart rate, and creativity. A total of 35 participants were exposed to two distinct virtual environments: a curved room and a rectangular room. Participants' self-reported mood was assessed using the Positive and Negative Affect Schedule (PANAS-Long Form). Heart rate was monitored using a pulse oximeter, and creative output was evaluated using the Guilford Alternative Uses Task (GAUT). Statistical comparisons between the two room types indicated that participants experienced higher positive affect and lower negative affect in the curved room condition compared to the rectangular room condition. Furthermore, heart rate measurements revealed lower physiological arousal in the curved room. Additionally, participants exhibited higher creative output in the curved room as opposed to the rectangular room. These findings align with previous literature on the influence of geometric factors on affective responses. The implications of this study are significant as they pertain to individuals' daily environments and their impact on health and well-being. The positive influence of curved room geometry on mood, arousal, and creativity emphasises the importance of considering room layout and design in various settings, such as workplaces and educational environments. Architects and designers can utilise these findings to inform their decisions and promote neuroarchitecture that enhances positive emotional experiences and productivity.}
}
@article{PANG2024121485,
title = {A concept lattice-based expert opinion aggregation method for multi-attribute group decision-making with linguistic information},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121485},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121485},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019875},
author = {Kuo Pang and Luis Martínez and Nan Li and Jun Liu and Li Zou and Mingyu Lu},
keywords = {Concept lattice, Linguistic truth-valued lattice implication algebra, Linguistic information processing, Multi-attribute group decision-making},
abstract = {During the multi-attribute group decision-making (MAGDM) processing, the individuals often hold different opinions about the alternatives. It is necessary to aggregate the different individual opinions into a unified group opinion. In the real world, experts sometimes use linguistic expressions to evaluate attributes in uncertain environments. To address the problem of reducing the information loss of expert opinion aggregation in MAGDM, this paper proposes a MAGDM approach based on linguistic concept lattices in the context of uncertain linguistic expression. A linguistic concept lattice for multi-expert linguistic formal context is first constructed based on linguistic truth-valued lattice implication algebra, which can express both comparable and incomparable linguistic information in the decision-making process. Different expert opinions are aggregated via the extent of fuzzy linguistic concepts, which can reduce information loss in the aggregation process. Second, meet-irreducible elements in the linguistic concept lattice are introduced to reduce the computational complexity of obtaining all fuzzy linguistic concepts in the decision-making process. the distance between the intents of different fuzzy linguistic concepts is considered to enhance the rationality of linguistic decision results. In addition, the expert’s decision-making process for each alternative is visualized via linguistic concept lattices. Finally, the case study and comparative analysis illustrate the validity and rationality of the proposed approach in MAGDM with linguistic information.}
}
@article{TERZOPOULOU2024104133,
title = {Iterative voting with partial preferences},
journal = {Artificial Intelligence},
volume = {332},
pages = {104133},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104133},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224000699},
author = {Zoi Terzopoulou and Panagiotis Terzopoulos and Ulle Endriss},
keywords = {Social choice theory, Iterative voting, Partial preferences},
abstract = {Voting platforms can offer participants the option to sequentially modify their preferences, whenever they have a reason to do so. But such iterative voting may never converge, meaning that a state where all agents are happy with their submitted preferences may never be reached. This problem has received increasing attention within the area of computational social choice. Yet, the relevant literature hinges on the rather stringent assumption that the agents are able to rank all alternatives they are presented with, i.e., that they hold preferences that are linear orders. We relax this assumption and investigate iterative voting under partial preferences. To that end, we define and study two families of rules that extend the well-known k-approval rules in the standard voting framework. Although we show that for none of these rules convergence is guaranteed in general, we also are able to identify natural conditions under which such guarantees can be given. Finally, we conduct simulation experiments to test the practical implications of our results.}
}
@article{WU20183,
title = {The five key questions of human performance modeling},
journal = {International Journal of Industrial Ergonomics},
volume = {63},
pages = {3-6},
year = {2018},
note = {Human Performance Modeling},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2016.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169814116300427},
author = {Changxu Wu},
keywords = {Human performance modeling},
abstract = {Via building computational (typically mathematical and computer simulation) models, human performance modeling (HPM) quantifies, predicts, and maximizes human performance, human-machine system productivity and safety. This paper describes and summarizes the five key questions of human performance modeling: 1) Why we build models of human performance; 2) What the expectations of a good human performance model are; 3) What the procedures and requirements in building and verifying a human performance model are; 4) How we integrate a human performance model with system design; and 5) What the possible future directions of human performance modeling research are. Recent and classic HPM findings are addressed in the five questions to provide new thinking in HPM's motivations, expectations, procedures, system integration and future directions.}
}
@article{KALLEYA2024147,
title = {An innovative artificial intelligence-based visualization and rendering for e-commerce startup booth design},
journal = {Procedia Computer Science},
volume = {245},
pages = {147-154},
year = {2024},
note = {9th International Conference on Computer Science and Computational Intelligence 2024 (ICCSCI 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030461},
author = {Calista Kalleya and Andi Pramono and Chelsea Putri Angelina and Wilbert Alvin Cuarista and Riefky Prabowo and Fairuz Iqbal Maulana},
keywords = {Interior Design, Artificial Intelligence, Prototype, Render, Start-up},
abstract = {Marketplace development is increasing, as is the number of people with active internet access. Unive is a startup that runs an e-commerce platform connecting Student Activity Units with consumers. Focuses on fostering student entrepreneurship through practical business experience. For its development, this startup needs an interior for business escalation. Designers need software integrated with artificial intelligence (AI) in interior design. This study explores the integration of AI-powered rendering tools in the design of e-commerce platforms, focusing on how these tools enhance the user experience and satisfaction. The research employs a mixed-method approach, combining qualitative and quantitative analyses. The author collected data through user surveys, expert interviews, and performance metrics of AI rendering tools. The author based the evaluation criteria on adherence to interior design principles and the quality of rendering results. Findings indicate that selecting furniture and supporting components that adhere to interior design principles results in optimal rendering outcomes. These outcomes are close to user expectations, significantly improving user engagement and satisfaction. Detailed analysis shows the effectiveness of AI rendering in various design scenarios. The discussion highlights the implications of integrating AI rendering tools in e-commerce platforms. It addresses the potential of these tools to revolutionize user interaction and provides insights into future developments and applications in the field. This study demonstrates the value of AI-powered rendering tools in enhancing e-commerce platform design. By adhering to interior design principles, these tools can achieve rendering results that meet user expectations, ultimately leading to improved user experiences.}
}
@article{CHEN2022105882,
title = {Neural connectome features of procrastination: Current progress and future direction},
journal = {Brain and Cognition},
volume = {161},
pages = {105882},
year = {2022},
issn = {0278-2626},
doi = {https://doi.org/10.1016/j.bandc.2022.105882},
url = {https://www.sciencedirect.com/science/article/pii/S0278262622000409},
author = {Zhiyi Chen and Tingyong Feng},
keywords = {Procrastination, Neural connectome, Self-control network, DLPFC},
abstract = {Procrastination refers to an irrationally delay for intended courses of action despite of anticipating a negative consequence due to this delay. Previous studies tried to reveal the neural substrates of procrastination in terms of connectome-based biomarkers. Based on this, we proposed a unified triple brain network model for procrastination and pinpointed out what challenges we are facing in understanding neural mechanism of procrastination. Specifically, based on neuroanatomical features, the unified triple brain network model proposed that connectome-based underpinning of procrastination could be ascribed to the abnormalities of self-control network (i.e., dorsolateral prefrontal cortex, DLPFC), emotion-regulation network (i.e., orbital frontal cortex, OFC), and episodic prospection network (i.e., para-hippocampus cortex, PHC). Moreover, based on the brain functional features, procrastination had been attributed to disruptive neural circuits on FPN (frontoparietal network)-SCN (subcortical network) and FPN-SAN (salience network), which led us to hypothesize the crucial roles of interplay between these networks on procrastination in unified triple brain network model. Despite of these findings, poor interpretability and computational model limited further understanding for procrastination from theoretical and neural perspectives. On balance, the current study provided an overview to show current progress on the connectome-based biomarkers for procrastination, and proposed the integrative neurocognitive model of procrastination.}
}
@article{NAPIER2014331,
title = {Insight into the numerical challenges of implementing 2-dimensional SOA models in atmospheric chemical transport models},
journal = {Atmospheric Environment},
volume = {96},
pages = {331-344},
year = {2014},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2014.07.048},
url = {https://www.sciencedirect.com/science/article/pii/S1352231014005780},
author = {W.J. Napier and J.J. Ensberg and J.H. Seinfeld},
keywords = {Secondary organic aerosol, 2-Dimensional SOA model, Chemical transport model, Probability distribution, Computational efficiency},
abstract = {The new generation of secondary organic aerosol (SOA) models that represent gas- and particle-phase chemistry and thermodynamic partitioning using discrete two-dimensional grids (e.g. SOM, 2D-VBS) cannot be efficiently implemented into three-dimensional atmospheric chemical transport models (CTMs) due to the large number of bins (tracers) required. In this study, we introduce a novel mathematical framework, termed the Oxidation State/Volatility Moment Method, that is designed to address these computational burdens so as to allow the new generation of SOA models to be implemented into CTMs. This is accomplished by mapping the two-dimensional grids onto probability distributions that conserve carbon and oxygen mass. Assessment of the Moment Method strengths (speed, carbon and oxygen conservation) and weaknesses (numerical drift) provide valuable insight that can guide future development of SOA modules for atmospheric CTMs.}
}
@incollection{BUCHANAN2014183,
title = {Chapter Seven - Edge Replacement and Minimality as Models of Causal Inference in Children},
editor = {Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {46},
pages = {183-213},
year = {2014},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-800285-8.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002858000078},
author = {David W. Buchanan and David M. Sobel},
keywords = {Causal reasoning, Causal graphical models, Edge replacement, Cognitive Development, Computational Models},
abstract = {Recently, much research has focused on causal graphical models (CGMs) as a computational-level description of how children represent cause and effect. While this research program has shown promise, there are aspects of causal reasoning that CGMs have difficulty accommodating. We propose a new formalism that amends CGMs. This edge replacement grammar formalizes one existing and one novel theoretical commitment. The existing idea is that children are determinists, in the sense that they believe that apparent randomness comes from hidden complexity, rather than inherent nondeterminism in the world. The new idea is that children think of causation as a branching process: causal relations grow not directly from the cause, but from existing relations between the cause and other effects. We have shown elsewhere that these two commitments together, when formalized, can explain and quantitatively fit the otherwise puzzling effect of nonindependence observed in the adult causal reasoning literature. We then test the qualitative predictions of this new formalism on children in a series of three experiments.}
}