@article{MOHAMED2010317,
title = {Investigating Number Sense Among Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {317-324},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S187704281002149X},
author = {Mohini Mohamed and Jacinta Johnny},
keywords = {Number sense, Mental computation, Number sense test, Number sense framework},
abstract = {Number sense can be described as good intuition about numbers and their relationships. Individuals with good number sense tend to exhibit the following characteristics when performing mental computations; sense-making approach, planning and control, flexibility and appropriateness sense of reasonableness. This is a very important skill to be mastered by every individual to enable them to handle numerical problems in their daily life. Students rarely face problems with algorithms. Unfortunately, many studies have showed that students have poor understanding in making sense on numbers when tested on their competency in number sense component. This study aims to investigate if there is a relationship between student performance in number sense and mathematics achievement and to explore the components of number sense that students are weak in.}
}
@article{ZHOU2019244,
title = {Lightweight IoT-based authentication scheme in cloud computing circumstance},
journal = {Future Generation Computer Systems},
volume = {91},
pages = {244-251},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18307878},
author = {Lu Zhou and Xiong Li and Kuo-Hui Yeh and Chunhua Su and Wayne Chiu},
keywords = {Internet-of-things (IoT), Cloud computing, Authentication, Proverif, User tracking},
abstract = {Recently, authentication technologies integrated with the Internet of Things (IoT) and cloud computing have been promptly investigated for secure data retrieval and robust access control on large-scale IoT networks. However, it does not have a best practice for simultaneously deploying IoT and cloud computing with robust security. In this study, we present a novel authentication scheme for IoT-based architectures combined with cloud servers. To pursue the best efficiency, lightweight crypto-modules, such as one-way hash function and exclusive-or operation, are adopted in our authentication scheme. It not only removes the computation burden but also makes our proposed scheme suitable for resource-limited objects, such as sensors or IoT devices. Through the formal verification delivered by Proverif, the security robustness of the proposed authentication scheme is guaranteed. Furthermore, the performance evaluation presents the practicability of our proposed scheme in which a user-acceptable computation cost is achieved.}
}
@article{UMEREZ2001159,
title = {Howard Pattee's theoretical biology — a radical epistemological stance to approach life, evolution and complexity},
journal = {Biosystems},
volume = {60},
number = {1},
pages = {159-177},
year = {2001},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00114-9},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001149},
author = {Jon Umerez},
keywords = {Epistemological stance, Epistemic cut, Semantic closure, Code, Symbol},
abstract = {This paper offers a short review of Pattee's main contributions to science and philosophy. With no intention of being exhaustive, an account of Pattee's work is presented which discusses some of his ideas and their reception. This is done through an analysis centered in what is thought to be his main contribution: the elaboration of an internal epistemic stance to better understand life, evolution and complexity. Having introduced this core idea as a sort of a posteriori cohesive element of a complex but highly coherent and complete system of thinking, further specific elements are also reviewed.}
}
@incollection{DAVIS2024,
title = {Ideational Theories of Meaning},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00208-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041002088},
author = {Wayne A. Davis},
keywords = {Communication, Convention, Constituency, Expression, Extensions, Ideas, Intensions, Meanings, Privacy, Reference, Representation, Thoughts},
abstract = {Locke and Hobbes held that people use words as conventional signs of their thoughts and ideas. This was a predecessor of the ideational theory, which holds that for an expression to have a meaning is principally for it to express an idea (thought or thought part), and that what it means is determined by the idea it expresses. A major advantage of the theory is that it provides a solution to Frege's and Russell's problems for referentialist theories, as well as a robust account of semantic compositionality. The theory is briefly clarified, and replies to major objections are sketched.}
}
@article{BACK202423,
title = {Accelerated chemical science with AI},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {23-33},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00213f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000858},
author = {Seoin Back and Alán Aspuru-Guzik and Michele Ceriotti and Ganna Gryn'ova and Bartosz Grzybowski and Geun Ho Gu and Jason Hein and Kedar Hippalgaonkar and Rodrigo Hormázabal and Yousung Jung and Seonah Kim and Woo Youn Kim and Seyed Mohamad Moosavi and Juhwan Noh and Changyoung Park and Joshua Schrier and Philippe Schwaller and Koji Tsuda and Tejs Vegge and O. Anatole {von Lilienfeld} and Aron Walsh},
abstract = {In light of the pressing need for practical materials and molecular solutions to renewable energy and health problems, to name just two examples, one wonders how to accelerate research and development in the chemical sciences, so as to address the time it takes to bring materials from initial discovery to commercialization. Artificial intelligence (AI)-based techniques, in particular, are having a transformative and accelerating impact on many if not most, technological domains. To shed light on these questions, the authors and participants gathered in person for the ASLLA Symposium on the theme of ‘Accelerated Chemical Science with AI’ at Gangneung, Republic of Korea. We present the findings, ideas, comments, and often contentious opinions expressed during four panel discussions related to the respective general topics: ‘Data’, ‘New applications’, ‘Machine learning algorithms’, and ‘Education’. All discussions were recorded, transcribed into text using Open AI's Whisper, and summarized using LG AI Research's EXAONE LLM, followed by revision by all authors. For the broader benefit of current researchers, educators in higher education, and academic bodies such as associations, publishers, librarians, and companies, we provide chemistry-specific recommendations and summarize the resulting conclusions.}
}
@article{UTHAMACUMARAN2020759,
title = {Cancer: A turbulence problem},
journal = {Neoplasia},
volume = {22},
number = {12},
pages = {759-769},
year = {2020},
issn = {1476-5586},
doi = {https://doi.org/10.1016/j.neo.2020.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1476558620301548},
author = {Abicumaran Uthamacumaran},
keywords = {Cancer, Complexity, Chaos, Nonlinear dynamics, Fractals, Chemical turbulence},
abstract = {Cancers are complex, adaptive ecosystems. They remain the leading cause of disease-related death among children in North America. As we approach computational oncology and Deep Learning Healthcare, our mathematical models of cancer dynamics must be revised. Recent findings support the perspective that cancer-microenvironment interactions may consist of chaotic gene expressions and turbulent protein flows during pattern formation. As such, cancer pattern formation, protein-folding and metastatic invasion are discussed herein as processes driven by chemical turbulence within the framework of complex systems theory. To conclude, cancer stem cells are presented as strange attractors of the Waddington landscape.}
}
@article{NYBERG2022394,
title = {Spatial goal coding in the hippocampal formation},
journal = {Neuron},
volume = {110},
number = {3},
pages = {394-422},
year = {2022},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321010291},
author = {Nils Nyberg and Éléonore Duvelle and Caswell Barry and Hugo J. Spiers},
keywords = {hippocampus, entorhinal cortex, navigation, goal, wayfinding, spatial memory, reinforcement learning, rodent, human},
abstract = {Summary
The mammalian hippocampal formation contains several distinct populations of neurons involved in representing self-position and orientation. These neurons, which include place, grid, head direction, and boundary cells, are thought to collectively instantiate cognitive maps supporting flexible navigation. However, to flexibly navigate, it is necessary to also maintain internal representations of goal locations, such that goal-directed routes can be planned and executed. Although it has remained unclear how the mammalian brain represents goal locations, multiple neural candidates have recently been uncovered during different phases of navigation. For example, during planning, sequential activation of spatial cells may enable simulation of future routes toward the goal. During travel, modulation of spatial cells by the prospective route, or by distance and direction to the goal, may allow maintenance of route and goal-location information, supporting navigation on an ongoing basis. As the goal is approached, an increased activation of spatial cells may enable the goal location to become distinctly represented within cognitive maps, aiding goal localization. Lastly, after arrival at the goal, sequential activation of spatial cells may represent the just-taken route, enabling route learning and evaluation. Here, we review and synthesize these and other evidence for goal coding in mammalian brains, relate the experimental findings to predictions from computational models, and discuss outstanding questions and future challenges.}
}
@article{CRAIG20233427,
title = {FEFOS: a method to derive oxide formation energies from oxidation states††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3cy00107e},
journal = {Catalysis Science & Technology},
volume = {13},
number = {11},
pages = {3427-3435},
year = {2023},
issn = {2044-4753},
doi = {https://doi.org/10.1039/d3cy00107e},
url = {https://www.sciencedirect.com/science/article/pii/S2044475323006822},
author = {Michael John Craig and Felix Kleuker and Michal Bajdich and Max García-Melchor},
abstract = {ABSTRACT
Herein we report a method to extract formation energies from oxidation states, which we call FEFOS. This new scheme predicts the formation energies of binary oxides through analyzing unary oxide formation energies as a function of their oxidation states. Taking averages of fitted quadratic equations that represent how elements respond to oxidation and reduction, the weights of these averages are determined by constraining the compound to be neutral. The application of FEFOS results in mean absolute errors of ca. 0.10 eV per atom when tested against Materials Project data for oxides with general formulas A1−zBzO, A1−zBzO1.5, and A1−zBzO2 with specific coordinations. Our FEFOS method not only allows for the prediction of binary oxide formation energies with low variance and high interpretability, but also compares well with state-of-the-art deep learning methods without being biased by training data and the need for large resources to compute it. Finally, we discuss the potential applications of the FEFOS method in tackling the problem of inverse catalyst design.}
}
@article{POOBALAN2025101667,
title = {A novel and secured email classification using deep neural network with bidirectional long short-term memory},
journal = {Computer Speech & Language},
volume = {89},
pages = {101667},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101667},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000500},
author = {A. Poobalan and K. Ganapriya and K. Kalaivani and K. Parthiban},
keywords = {Email classification, DNN-BiLSTM, AES algorithm, Rabit algorithm, Random forests (RF)},
abstract = {Email data has some characteristics that are different from other social media data, such as a large range of answers, formal language, notable length variations, high degrees of anomalies, and indirect relationships. The main goal in this research is to develop a robust and computationally efficient classifier that can distinguish between spam and regular email content. The benchmark Enron dataset, which is accessible to the public, was used for the tests. The six distinct Enron data sets we acquired were combined to generate the final seven Enron data sets. The dataset undergoes early preprocessing to remove superfluous sentences. The proposed model Bidirectional Long Short-Term Memory (BiLSTM) apply spam labels and to examine email documents for spam. On seven Enron datasets, DNN-BiLSTM performs better than other classifiers in the performance comparison in terms of accuracy. DNN-BiLSTM and convolutional neural networks demonstrated that they can classify spam with 96.39 % and 98.69 % accuracy, respectively, in comparison to other machine learning classifiers. The risks associated with cloud data management and potential security flaws are also covered in the paper. This research presents hybrid encryption as a means of protecting cloud data while preserving privacy by using the hybrid AES-Rabit encryption algorithm which is based on symmetric session key exchange.}
}
@article{HALL20156607,
title = {Understanding Sector Dependencies in the Stabilization and Reconstruction of Nation-states},
journal = {Procedia Manufacturing},
volume = {3},
pages = {6607-6614},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915009932},
author = {Steven Hall and Curtis Blais},
keywords = {Multi-agent, system dynamics, modeling and simulation, panarchy, resiliency, stabilitiy, reconstruction and humanitarian operations.},
abstract = {The United States Army is undergoing a re-definition of its Civil Affairs officer positions. A recent project to define the educational requirements for an Army Civil Affairs Officer (38G) identified an educational requirement to help officers understand the complex ways in which the operations that advance the achievement of one stabilization objective often hinder the achievement of other objectives. The system level thinking was seen to be frequently insufficiently ingrained amongst Civil Affairs Officers (and the leaders they advised), who were both often perceived to be inclined, in the face of the complexities of the situation on the ground, to become too narrowly focused on achieving their specific assigned responsibilities, limiting their ability to see how the mission effectiveness of what they were recommending would be influenced by the state and trajectory of other Sectors and how, in turn, their recommendations would influence the mission effectiveness of other Sector stewards. While system dynamics modeling has proven itself to be effective in capturing and effectively communicating feedback loops that define such non-linear (and non-intuitive) systems they do not, in themselves, provide sufficient modeling richness to comprehensively capture the critical spatial (geographical) determinants of a successful state reconstruction process. For these purposes, a multi-agent cellular automata model is recommended both as a vehicle for introducing students to the complex nature of the state reconstruction process and, eventually, for use in the field by deployed Civilian Affairs Officers at all levels. This paper describes the problem and the modeling approach to address it.}
}
@article{MILIK201022,
title = {On Efficient Implementation of Search Algorithm for Genome Patterns},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {24},
pages = {22-27},
year = {2010},
note = {10th IFAC Workshop on Programmable Devices and Embedded Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20101006-2-PL-4019.00006},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015309812},
author = {Adam Milik and Andrzej Pulka},
keywords = {Dynamic programming, Computational methods, Pattern identification, Pattern recognition, Parallel processing, Pipeline processing},
abstract = {The presented paper describes the implementation of the computation algorithm on modern, complex programmable hardware devices. The presented algorithm originates from computation biology and works on very long chains of symbols, which come from reference patterns of the genome. The software solutions in the field are very limited and need large time and space resources. Main research efforts have been done to investigate the properties of the searching algorithm. Especially the influence of the penalty values assigned for the mismatch, the insertion and the deletion on the algorithm has been analyzed. This allows obtaining completely new algorithm that offers extremely efficient implementation and exhibits outstanding performance. The different FPGA generations have been considered as target families for the searching algorithm based on the dynamic programming idea. The obtained results are very promising and show the dominance of the dedicated platforms over the general purpose PC-based systems.}
}
@article{ASAHIRO202016,
title = {Graph orientation with splits},
journal = {Theoretical Computer Science},
volume = {844},
pages = {16-25},
year = {2020},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S030439752030387X},
author = {Yuichi Asahiro and Jesper Jansson and Eiji Miyano and Hesam Nikpey and Hirotaka Ono},
keywords = {Graph orientation, Maximum flow, Vertex cover, Partition, Algorithm, Computational complexity},
abstract = {The Minimum Maximum Outdegree Problem (MMO) is to assign a direction to every edge in an input undirected, edge-weighted graph so that the maximum weighted outdegree taken over all vertices becomes as small as possible. In this paper, we introduce a new variant of MMO called the p-Split Minimum Maximum Outdegree Problem (p-Split-MMO) in which one is allowed to perform a sequence of p split operations on the vertices before orienting the edges, for some specified non-negative integer p, and study its computational complexity.}
}
@article{KASTELLAKIS201519,
title = {Synaptic clustering within dendrites: An emerging theory of memory formation},
journal = {Progress in Neurobiology},
volume = {126},
pages = {19-35},
year = {2015},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0301008214001373},
author = {George Kastellakis and Denise J. Cai and Sara C. Mednick and Alcino J. Silva and Panayiota Poirazi},
keywords = {Plasticity, Active dendrites, Associative memory, Synapse clustering, Synaptic tagging and capture},
abstract = {It is generally accepted that complex memories are stored in distributed representations throughout the brain, however the mechanisms underlying these representations are not understood. Here, we review recent findings regarding the subcellular mechanisms implicated in memory formation, which provide evidence for a dendrite-centered theory of memory. Plasticity-related phenomena which affect synaptic properties, such as synaptic tagging and capture, synaptic clustering, branch strength potentiation and spinogenesis provide the foundation for a model of memory storage that relies heavily on processes operating at the dendrite level. The emerging picture suggests that clusters of functionally related synapses may serve as key computational and memory storage units in the brain. We discuss both experimental evidence and theoretical models that support this hypothesis and explore its advantages for neuronal function.}
}
@incollection{ASHBY2016211,
title = {Chapter 14 - The Vision: A Circular Materials Economy},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development},
publisher = {Butterworth-Heinemann},
address = {Boston},
pages = {211-239},
year = {2016},
isbn = {978-0-08-100176-9},
doi = {https://doi.org/10.1016/B978-0-08-100176-9.00014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081001769000141},
author = {Michael F. Ashby},
keywords = {Active stock, Circularity metrics, Material efficiency, Natural and industrial ecology, Product life extension, Product–service systems, Reuse, repair and recycling, Take-back schemes},
abstract = {We live at present with a largely linear materials economy. Our use of natural resources is characterized by the sequence “take – make – use – dispose” as materials progress from mine, through product, to landfill. Increasing population, rising affluence and the limited capacity for the planet to provide resources and absorb waste argue for a transition towards a more circular way of using materials. When products come to the end of their lives the materials they contain are still there. Repair, reuse and recycling (the three “Rs”) can return these to active use. Repair, reuse and recycling are not new ideas; they have been used for centuries to recirculate materials and, in less-developed economies, they still are. But in developed nations they have dwindled as the cost of materials fell and that of labor rose over time, making all three Rs uneconomic. So what is novel about the contemporary idea of a circular materials economy? Haven’t we been there before? The “circularity” concept is a way thinking that looks not just for efficiencies but also for new ways of providing the functions we need. In the last decade momentum has gathered about this transition. The idea of deploying rather than consuming materials, of using them not once but many times, and of redesign to make this a reality has economic as well as environmental appeal. Governments now sign up to programs to foster circular economic ideas and mechanisms begin to appear to advance them. This chapter examines the background, the successes and the difficulties of implementing a circular materials economy.}
}
@article{KLEEBARILLAS2015455,
title = {A comparative study and validation of state estimation algorithms for Li-ion batteries in battery management systems},
journal = {Applied Energy},
volume = {155},
pages = {455-462},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2015.05.102},
url = {https://www.sciencedirect.com/science/article/pii/S0306261915007357},
author = {Joaquín {Klee Barillas} and Jiahao Li and Clemens Günther and Michael A. Danzer},
keywords = {Lithium-ion battery, Battery management system, State of charge estimation, Robustness analysis, Sliding-mode observer, Kalman-based SOC estimation},
abstract = {To increase lifetime, safety, and energy usage battery management systems (BMS) for Li-ion batteries have to be capable of estimating the state of charge (SOC) of the battery cells with a very low estimation error. The accurate SOC estimation and the real time reliability are critical issues for a BMS. In general an increasing complexity of the estimation methods leads to higher accuracy. On the other hand it also leads to a higher computational load and may exceed the BMS limitations or increase its costs. An approach to evaluate and verify estimation algorithms is presented as a requisite prior the release of the battery system. The approach consists of an analysis concerning the SOC estimation accuracy, the code properties, complexity, the computation time, and the memory usage. Furthermore, a study for estimation methods is proposed for their evaluation and validation with respect to convergence behavior, parameter sensitivity, initialization error, and performance. In this work, the introduced analysis is demonstrated with four of the most published model-based estimation algorithms including Luenberger observer, sliding-mode observer, Extended Kalman Filter and Sigma-point Kalman Filter. The experiments under dynamic current conditions are used to verify the real time functionality of the BMS. The results show that a simple estimation method like the sliding-mode observer can compete with the Kalman-based methods presenting less computational time and memory usage. Depending on the battery system’s application the estimation algorithm has to be selected to fulfill the specific requirements of the BMS.}
}
@incollection{HEILMAN201319,
title = {Chapter 2 - Visual artistic creativity and the brain},
editor = {Stanley Finger and Dahlia W. Zaidel and François Boller and Julien Bogousslavsky},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {204},
pages = {19-43},
year = {2013},
booktitle = {The Fine Arts, Neurology, and Neuroscience},
issn = {0079-6123},
doi = {https://doi.org/10.1016/B978-0-444-63287-6.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444632876000026},
author = {Kenneth M. Heilman and Lealani Mae Acosta},
keywords = {artistic creativity, hemispheric functions, visuospatial skills, creative innovation, divergent thinking, imagery, global and focal attention},
abstract = {Creativity is the development of a new or novel understanding—insight that leads to the expression of orderly relationships (e.g., finding and revealing the thread that unites). Visual artistic creativity plays an important role in the quality of human lives, and the goal of this chapter is to describe some of the brain mechanisms that may be important in visual artistic creativity. The initial major means of learning how the brain mediates any activity is to understand the anatomy and physiology that may support these processes. A further understanding of specific cognitive activities and behaviors may be gained by studying patients who have diseases of the brain and how these diseases influence these functions. Physiological recording such as electroencephalography and brain imaging techniques such as PET and fMRI have also allowed us to gain a better understanding of the brain mechanisms important in visual creativity. In this chapter, we discuss anatomic and physiological studies, as well as neuropsychological studies of healthy artists and patients with neurological disease that have helped us gain some insight into the brain mechanisms that mediate artistic creativity.}
}
@article{CALDEIRA2025102657,
title = {Model compression techniques in biometrics applications: A survey},
journal = {Information Fusion},
volume = {114},
pages = {102657},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102657},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004354},
author = {Eduarda Caldeira and Pedro C. Neto and Marco Huber and Naser Damer and Ana F. Sequeira},
keywords = {Compression, Knowledge distillation, Quantization, Pruning, Biometrics, Bias},
abstract = {The development of deep learning algorithms has extensively empowered humanity’s task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. These compressed models are especially essential when implementing multi-model fusion solutions where multiple models are required to operate simultaneously. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.}
}
@article{ARTEMOV20093884,
title = {A tribute to D.B. Spalding and his contributions in science and engineering},
journal = {International Journal of Heat and Mass Transfer},
volume = {52},
number = {17},
pages = {3884-3905},
year = {2009},
note = {Special Issue Honoring Professor D. Brian Spalding},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2009.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S0017931009002026},
author = {V. Artemov and S.B. Beale and G. {de Vahl Davis} and M.P. Escudier and N. Fueyo and B.E. Launder and E. Leonardi and M.R. Malin and W.J. Minkowycz and S.V. Patankar and A. Pollard and W. Rodi and A. Runchal and S.P. Vanka},
keywords = {D.B. Spalding, Fluid dynamics, Heat transfer, Mass transfer, Combustion},
abstract = {This paper presents a summary of some of the scientific and engineering contributions of Prof. D.B. Spalding up to the present time. Starting from early work on combustion, and his unique work in mass transfer theory, Spalding’s unpublished “unified theory” is described briefly. Subsequent to this, developments in algorithms by the Imperial College group led to the birth of modern computational fluid dynamics, including the well-known SIMPLE algorithm. Developments in combustion, multi-phase flow and turbulence modelling are also described. Finally, a number of academic and industrial applications of computational fluid dynamics and heat transfer applications considered in subsequent years are mentioned.}
}
@article{ABDELAZIZ2024100615,
title = {A scoping review of artificial intelligence within pharmacy education},
journal = {American Journal of Pharmaceutical Education},
volume = {88},
number = {1},
pages = {100615},
year = {2024},
issn = {0002-9459},
doi = {https://doi.org/10.1016/j.ajpe.2023.100615},
url = {https://www.sciencedirect.com/science/article/pii/S0002945923045539},
author = {May H. {Abdel Aziz} and Casey Rowe and Robin Southwood and Anna Nogid and Sarah Berman and Kyle Gustafson},
keywords = {Pharmacy education, Artificial intelligence, Deep learning, Machine learning},
abstract = {Objectives
This scoping review aimed to summarize the available literature on the use of artificial intelligence (AI) in pharmacy education and identify gaps where additional research is needed.
Findings
Seven studies specifically addressing the use of AI in pharmacy education were identified. Of these 7 studies, 5 focused on AI use in the context of teaching and learning, 1 on the prediction of academic performance for admissions, and the final study focused on using AI text generation to elucidate the benefits and limitations of ChatGPT use in pharmacy education.
Summary
There are currently a limited number of available publications that describe AI use in pharmacy education. Several challenges exist regarding the use of AI in pharmacy education, including the need for faculty expertise and time, limited generalizability of tools, limited outcomes data, and several legal and ethical concerns. As AI use increases and implementation becomes more standardized, opportunities will be created for the inclusion of AI in pharmacy education.}
}
@article{HE2025103404,
title = {Thermal imaging monitoring based on image texture feature analysis for simulating gymnastics teaching images in universities},
journal = {Thermal Science and Engineering Progress},
volume = {60},
pages = {103404},
year = {2025},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2025.103404},
url = {https://www.sciencedirect.com/science/article/pii/S2451904925001945},
author = {Yuxin He and Dongcheng Ma},
keywords = {Image texture feature analysis, Thermal imaging monitoring, College gymnastics teaching, Image simulation},
abstract = {With the development of science and technology, thermal imaging technology is used as a non-contact monitoring means. This study designed and developed a gymnastics teaching image simulation system based on thermal imaging monitoring technology. The system aims to extract key image texture features by analyzing the thermal imaging images of gymnasts in the training process, so as to realize real-time monitoring and evaluation of athletes’ physical conditions. In this paper, image processing technology is used to extract key texture features from thermal imaging images, and the quantitative index reflecting athletes’ physical condition is extracted from thermal imaging images, and it is applied to the simulation of gymnastics teaching images. Through the analysis of image texture features, researchers successfully extracted the key indicators reflecting the athlete’s physical condition, and applied these indicators to the simulation of gymnastics teaching images. The experimental results show that the image simulation system based on thermal imaging monitoring can provide more accurate and objective evaluation results, which is helpful for coaches to better understand the training status of athletes and adjust the training plan in time.}
}
@article{YUAN2023107911,
title = {Surface profile evolution model for titanium alloy machined using abrasive waterjet},
journal = {International Journal of Mechanical Sciences},
volume = {240},
pages = {107911},
year = {2023},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2022.107911},
url = {https://www.sciencedirect.com/science/article/pii/S0020740322007895},
author = {Yemin Yuan and Jianfeng Chen and Hang Gao},
keywords = {Abrasive waterjet, Ti-6Al-4V alloy, Computational fluid dynamics (CFD), Stagnation zone, Surface profile evolution},
abstract = {The surface profile evolution model, which was initially developed for glass and polymers, can accurately predict a channel profile cross-section produced by abrasive jet (AJ) machining. In this study, the model is modified and applied for estimating the profiles of a Ti-6Al-4V alloy eroded by an abrasive waterjet (AWJ). First, the velocity and mass fraction distributions of the gas–liquid–solid phases in the AWJ at the nozzle exit were derived and compared, and several improvements were proposed, such as considering the divergence angle of the jet and particles, as well as the length of the jet core area, to precisely construct a theoretical connection of the erosion efficiency distribution before impacting the workpiece. Computational fluid dynamics (CFD) simulations were then performed to investigate the behaviour of the erosion jets during surface evolution. The results revealed that the jet diffusion provoked by the stagnation zone effect became more pronounced as the surface profile depth deepened, which led to jet directional deflection and suppressed the erosion capacities of the AWJ. Therefore, a central erosion depth function was introduced to correct this detrimental effect with the intention of obtaining an accurate channel profile. In addition, a second-order single-step fitting function was suggested to eliminate the fluctuations caused by uneven abrasive particles and the problem of reduced erosion efficiency due to channel depth variation. Finally, based on the determination of the parameters affecting the channel profile, a normalised centre erosion rate function, which only depends on the channel depth and is isolated from the material properties and the standoff distance, was recommended to simplify the calculation. The erosion function conforming to a Gaussian surface was fitted using MATLAB (R2019b, MathWorks, USA). The results demonstrated that the channel profiles predicted by the surface evolution model were consistent with the measured profiles, with an average error of 11.4%.}
}
@article{GAO2023101631,
title = {Key nodes identification in complex networks based on subnetwork feature extraction},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {7},
pages = {101631},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101631},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823001854},
author = {Luyuan Gao and Xiaoyang Liu and Chao Liu and Yihao Zhang and Giacomo Fiumara and Pasquale De Meo},
keywords = {Key nodes identification, Complex network, Subnetwork feature extraction, Graph convolutional networks},
abstract = {The problem of detecting key nodes in a network (i.e. nodes with the greatest ability to spread an infection) has been studied extensively in the past. Some approaches to key node detection compute node centrality, but there is no formal proof that central nodes also have the greatest spreading capacity. Other methods use epidemiological models (e.g., the SIR model) to describe the spread of an infection and rely on numerical simulations to find out key nodes; these methods are highly accurate but computationally expensive. To efficiently but accurately detect key nodes, we propose a novel deep learning method called Rank by Graph Convolutional Network, RGCN. Our method constructs a subnetwork around each node to estimate its spreading power; then RGCN applies a graph convolutional network to each subnetwork and the adjacency matrix of the network to learn node embeddings. Finally, a neural network is applied to the node embeddings to detect key nodes. Our RGCN method outperforms state-of-the-art approaches such as RCNN and MRCNN by 11.84% and 13.99%, respectively, when we compare the Kendall’s τ coefficient between the node ranking produced by each method with the true ranking obtained by SIR simulations.}
}
@article{LOVE2017113,
title = {On languaging and languages},
journal = {Language Sciences},
volume = {61},
pages = {113-147},
year = {2017},
note = {Orders of Language: A festschrift for Nigel Love},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117301080},
author = {Nigel Love},
keywords = {Languaging, Linguistic reflexivity, Metalanguage, Ontology of languages, Verbatim repetition, Writing},
abstract = {I consider the ontology of languages and the linguistic units said to constitute them, in the light of a speculative sketch of how languaging about language might give rise to the idea of a language. The focus is principally on the role of reflexivity and the development of writing in facilitating the decontextualisation, abstraction and reification of linguistic units and languages themselves. The main trend in modern linguistics has been to take the products of these processes as realia, and to retroject them on to languagers as the basis for their languaging activities: I touch on some of the deleterious effects of this on theorising about the acquisition, storage and production of language. Finally, I consider how in thinking about these matters the concept of different ‘orders’ of language has been and might be interpreted and deployed. Whether or not this concept has a useful role to play in formulating them, the ideas assembled here are offered in the hope that they might serve as a platform from which to debate the significance and implications of the stultifying effect our modes of metalanguaging have so far had on inquiry into our engagement with language.}
}
@article{OSWALD2025100552,
title = {Understanding individual differences in non-ordinary state of consciousness: Relationship between phenomenological experiences and autonomic nervous system},
journal = {International Journal of Clinical and Health Psychology},
volume = {25},
number = {1},
pages = {100552},
year = {2025},
issn = {1697-2600},
doi = {https://doi.org/10.1016/j.ijchp.2025.100552},
url = {https://www.sciencedirect.com/science/article/pii/S1697260025000109},
author = {Victor Oswald and Karim Jerbi and Corine Sombrun and Annen Jitka and Charlotte Martial and Olivia Gosseries and Audrey Vanhaudenhuyse},
keywords = {Non-ordinary states of consciousness, Auto-induced cognitive trance, Heart rate variability, Phenomenological experiences, Machine learning, Inter-individual differences},
abstract = {Non-ordinary states of consciousness offer a unique opportunity to explore the interplay between phenomenological experiences and physiological processes. This study investigated individual differences in phenomenological and autonomic nervous system changes between a resting state condition and a non-ordinary state of consciousness (auto-induced cognitive trance, AICT). Specifically, it examined the relationship between self-reported experiences (e.g., absorption, visual representations) and heart rate variability (HRV). Twenty-seven participants underwent electrocardiography recordings and completed self-report questionnaires during rest and AICT. A machine learning framework distinguished the rest and AICT states based on self-reported measures and HRV metrics. A linear mixed-effects model assessed inter-individual differences in HRV and self-reported phenomenology between the two states. Finally, the relationship between relative change in HRV and self-reported experiences was explored. Results showed changes in self-reported phenomenology (accuracy=86 %; p<.001) and HRV (accuracy=73 %; p<.001) characterizing the AICT state compared to rest. The baseline level in phenomenology or HRV was associated with change amplitude during AICT. Moreover, relative change in HRV was associated with change in phenomenology. The findings suggest that inter-individual differences at rest revealed a functional mechanism between phenomenology and the autonomic nervous system during non-ordinary states of consciousness, offering a novel perspective on how physiological mechanisms shape subjective experiences.}
}
@article{LOPEZ2025109150,
title = {A graph-theoretic framework for analyzing and designing chemical engineering curricula},
journal = {Computers & Chemical Engineering},
volume = {200},
pages = {109150},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109150},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425001541},
author = {Blake Lopez and Yue Shao and Victor M. Zavala},
keywords = {Education, Curricula, Chemical engineering, Graph theory},
abstract = {Topics and courses that compose chemical engineering curricula are interconnected in a complex manner. The organization/structure of chemical engineering curricula closely matches the practice of breaking down chemical processes into fundamental phenomena (e.g., thermo, balances, and transport) and unit operations (e.g., reactors, separators, and heat exchangers). Emergence of modern topics (e.g., sustainability and molecular engineering) and advances in pedagogy call for the analysis and potential re-organization of curricula (e.g., use of case studies to foster integration of courses and include new topics/courses in a synergistic manner). In this work, we propose a graph-theoretic abstraction to represent, analyze, and reorganize the structure of curricula. In this abstraction, nodes represent topics/concepts, edges represent connectivity/dependencies between topics, and courses can be interpreted as collections of topics that are tightly interconnected (also known as clusters or modules). The abstraction enables the use of algorithms and software tools of graph theory and optimization to formalize the visualization and evaluation of curricula (e.g., identify key topics) and to identify re-organization strategies (e.g., defining strategic modules/courses that maximize topic cohesiveness/connectivity). Additionally, the abstraction can help formalize and facilitate discussions between instructors that might have different priorities/perspectives on curriculum content and organization. We provide case studies that analyze real curricula at the University of Wisconsin–Madison to highlight the benefits of the proposed framework.}
}
@article{BRAITHWAITE201640,
title = {Non-formal mechanisms in mathematical cognitive development: The case of arithmetic},
journal = {Cognition},
volume = {149},
pages = {40-55},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S001002771630004X},
author = {David W. Braithwaite and Robert L. Goldstone and Han L.J. {van der Maas} and David H. Landy},
keywords = {Mathematical cognitive development, Concrete to abstract shift, Arithmetic, Syntax, Perception, Mathematics education},
abstract = {The idea that cognitive development involves a shift towards abstraction has a long history in psychology. One incarnation of this idea holds that development in the domain of mathematics involves a shift from non-formal mechanisms to formal rules and axioms. Contrary to this view, the present study provides evidence that reliance on non-formal mechanisms may actually increase with age. Participants – Dutch primary school children – evaluated three-term arithmetic expressions in which violation of formally correct order of evaluation led to errors, termed foil errors. Participants solved the problems as part of their regular mathematics practice through an online study platform, and data were collected from over 50,000 children representing approximately 10% of all primary schools in the Netherlands, suggesting that the results have high external validity. Foil errors were more common for problems in which formally lower-priority sub-expressions were spaced close together, and also for problems in which such sub-expressions were relatively easy to calculate. We interpret these effects as resulting from reliance on two non-formal mechanisms, perceptual grouping and opportunistic selection, to determine order of evaluation. Critically, these effects reliably increased with participants’ grade level, suggesting that these mechanisms are not phased out but actually become more important over development, even when they cause systematic violations of formal rules. This conclusion presents a challenge for the shift towards abstraction view as a description of cognitive development in arithmetic. Implications of this result for educational practice are discussed.}
}
@article{ASTLE2024105539,
title = {Understanding divergence: Placing developmental neuroscience in its dynamic context},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {157},
pages = {105539},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105539},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424000071},
author = {Duncan E. Astle and Dani S. Bassett and Essi Viding},
keywords = {Development, Systems neuroscience, Neurodevelopmental condition, Mental health, Computational neuroscience},
abstract = {Neurodevelopment is not merely a process of brain maturation, but an adaptation to constraints unique to each individual and to the environments we co-create. However, our theoretical and methodological toolkits often ignore this reality. There is growing awareness that a shift is needed that allows us to study divergence of brain and behaviour across conventional categorical boundaries. However, we argue that in future our study of divergence must also incorporate the developmental dynamics that capture the emergence of those neurodevelopmental differences. This crucial step will require adjustments in study design and methodology. If our ultimate aim is to incorporate the developmental dynamics that capture how, and ultimately when, divergence takes place then we will need an analytic toolkit equal to these ambitions. We argue that the over reliance on group averages has been a conceptual dead-end with regard to the neurodevelopmental differences. This is in part because any individual differences and developmental dynamics are inevitably lost within the group average. Instead, analytic approaches which are themselves new, or simply newly applied within this context, may allow us to shift our theoretical and methodological frameworks from groups to individuals. Likewise, methods capable of modelling complex dynamic systems may allow us to understand the emergent dynamics only possible at the level of an interacting neural system.}
}
@article{DRABECK202591,
title = {Disability in ecology and evolution},
journal = {Trends in Ecology & Evolution},
volume = {40},
number = {2},
pages = {91-95},
year = {2025},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2024.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169534724003173},
author = {Danielle Drabeck and Chris Rensing and Kat {Van der Poorten}}
}
@article{COSTABILE2021126306,
title = {A 2D-SWEs framework for efficient catchment-scale simulations: Hydrodynamic scaling properties of river networks and implications for non-uniform grids generation},
journal = {Journal of Hydrology},
volume = {599},
pages = {126306},
year = {2021},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2021.126306},
url = {https://www.sciencedirect.com/science/article/pii/S002216942100353X},
author = {Pierfranco Costabile and Carmelina Costanzo},
keywords = {2D shallow water equations, Surface runoff, River networks, Non-uniform grids, Channel heads, Scaling laws},
abstract = {The application of two-dimensional shallow-water equations models (2D-SWEs) for the description of hydrodynamic-based surface runoff computations is becoming a reference approach in rainfall-runoff simulations at the catchment scale. Due to their ability in generation of flow patterns throughout the basin, they can be used not only as an advanced method for flood mapping studies and hazard assessment but also as an innovative tool for the analysis of river drainage networks, opening new perspectives for several environmental processes. In particular, in this work we put the river networks in a 2D-SWEs framework, meaning that the traditional tree-like fluvial structure, represented by a skeleton composed of a set of lines, is replaced by a collection of points discretizing the 2-D geometry of the river structure itself, for which the values of the hydrodynamic values are provided by the numerical simulations. This approach is used here to derive a new scaling property that relates the specific discharge threshold, used to identify the river network cells, to the total areas of the network cells themselves. The hydrodynamic and geomorphological interpretation of this power law function and the influence of grid resolution, on some relevant parameters of this curve, have inspired the development of a heuristic procedure for non-uniform grid generation, able to detect the most hydrodynamically active areas of the basins for which the grid refinement process makes sense. Moreover, information related to how much grid refinement is needed is provided as well. The performances of this procedure are very promising in terms of accuracy of simulated discharges, hydrodynamic behaviour of the river network and flooded areas, reducing significantly the computational times in respect to the use of fine uniform grids.}
}
@article{ROLISON2022105401,
title = {Developmental differences in description-based versus experience-based decision making under risk in children},
journal = {Journal of Experimental Child Psychology},
volume = {219},
pages = {105401},
year = {2022},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2022.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0022096522000303},
author = {Jonathan J. Rolison and Thorsten Pachur and Teresa McCormack and Aidan Feeney},
keywords = {Decision making under risk, Children, Computational modeling, Description-based decision making, Experience-based decision making, Risk taking},
abstract = {The willingness to take a risk is shaped by temperaments and cognitive abilities, both of which develop rapidly during childhood. In the adult developmental literature, a distinction is drawn between description-based tasks, which provide explicit choice–reward information, and experience-based tasks, which require decisions from past experience, each emphasizing different cognitive demands. Although developmental trends have been investigated for both types of decisions, few studies have compared description-based and experience-based decision making in the same sample of children. In the current study, children (N = 112; 5–9 years of age) completed both description-based and experience-based decision tasks tailored for use with young children. Child temperament was reported by the children’s primary teacher. Behavioral measures suggested that the willingness to take a risk in a description-based task increased with age, whereas it decreased in an experience-based task. However, computational modeling alongside further inspection of the behavioral data suggested that these opposite developmental trends across the two types of tasks both were associated with related capacities: older (vs. younger) children’s higher sensitivity to experienced losses and higher outcome sensitivity to described rewards and losses. From the temperamental characteristics, higher attentional focusing was linked with a higher learning rate on the experience-based task and a bias to accept gambles in the gain domain on the description-based task. Our findings demonstrate the importance of comparing children’s behavior across qualitatively different tasks rather than studying a single behavior in isolation.}
}
@article{SAEED2022122012,
title = {A simple approach for short-term wind speed interval prediction based on independently recurrent neural networks and error probability distribution},
journal = {Energy},
volume = {238},
pages = {122012},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122012},
url = {https://www.sciencedirect.com/science/article/pii/S036054422102260X},
author = {Adnan Saeed and Chaoshun Li and Zhenhao Gan and Yuying Xie and Fangjie Liu},
keywords = {Wind speed interval prediction, Independently recurrent neural networks, Quantile regression, Error prediction, Distribution estimation},
abstract = {Improving the quality of Wind Speed Interval prediction is important to maximize the usage of integrated wind energy as well as to reduce the adverse effects of the uncertainties, introduced by the random fluctuations of wind, to the power systems. This paper utilizes independently recurrent neural network to propose two new interval prediction frameworks. This network possesses the ability to retain memory at different lengths, which is helpful in capturing temporal features, especially for multi-horizon forecasts where the local dynamics get quite involved. In the first approach, we integrated a quantile regression loss function into this network to generate the intervals. This framework however, require to train different regressors to generate the conditional quantiles. Removing this limitation, a new simple and intuitive approach, is proposed which estimates the prediction intervals using a Gaussian function centered on the prediction and estimated error by a point prediction model and an error prediction model respectively. In our computational experiments, which involve two different wind fields contributing to eight different cases, an improvement of 43% and 12%, in average coverage width criterion index, over traditional models and LSTM based model respectively is remarkable. Thus, the proposed framework is able to produce high quality PIs while simultaneously reducing the computational cost.}
}
@article{ZHU2024100138,
title = {Exploring the impact of ChatGPT on art creation and collaboration: Benefits, challenges and ethical implications},
journal = {Telematics and Informatics Reports},
volume = {14},
pages = {100138},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100138},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000240},
author = {Sijin Zhu and Zheng Wang and Yuan Zhuang and Yuyang Jiang and Mengyao Guo and Xiaolin Zhang and Ze Gao},
keywords = {Creative AI, HumanAI collaboration, Language models, Interactive AI literacy},
abstract = {This paper examines the chaos caused by introducing advanced language models, specifically ChatGPT, to art. Our focus is on the potential impact of ChatGPT on art creation and collaboration. We explore how it has been utilized to generate art and assist in creative writing and how it facilitates collaboration between artists. This exploration includes an investigation into the use of AI in creating art, music, and literature, emphasizing ChatGPT’s role in generating poetry and prose and its ability to provide valuable suggestions for sentence structure and word choice in creative writing. We conduct case studies and interviews with diverse artists and AI experts to understand the benefits and challenges of using ChatGPT in the creative process. Our findings reveal that artists find ChatGPT helpful in generating new ideas, overcoming creative blocks, and improving the quality of their work. It enables remote collaboration between artists by providing a real-time communication and idea-sharing platform. However, ethical concerns relating to authorship ownership and authenticity have emerged. Artists fear using ChatGPT may lead to losing their artistic identity and ownership of their work. While our data suggests that ChatGPT holds the potential to transform the art world, careful consideration must be given to the ethical implications of AI in art. We recommend future research to focus on developing guidelines for the responsible use of AI in art, safeguarding artists’ rights, and preserving artistic authenticity.}
}
@incollection{VODOVOTZ201563,
title = {Chapter 3.2 - Dynamic Knowledge Representation and the Power of Model Making},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {63-68},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000094},
author = {Yoram Vodovotz and Gary An},
keywords = {Systems biology, mathematical modeling, computational biology, computational modeling, knowledge representation, conceptual model},
abstract = {This chapter focuses on describing the primary tool used in Translational Systems Biology: dynamic computational modeling. This chapter discusses the conceptual basis and rationale for modeling, with particular emphasis on the role of dynamic computational and mathematical models in biomedical research. We introduce the concept of using models as means of Dynamic Knowledge Representation, with the scientific target of facilitating the visualization, instantiation, evaluation, and falsification of biological hypotheses. We compare and contrast the use of modeling and simulation for this purpose versus the development and use of “engineering grade” quantitative models, noting specifically that given the state of biological knowledge, biomedical Dynamic Knowledge Representation is aimed at facilitating discovery, as opposed to the engineering goal of optimizing solutions. We discuss the fundamental step in model construction, mapping, and explain its role in the use and potential interpretation of both biological proxy models and computational models. We introduce the concept of Conceptual Model Verification, and its role as a means of accelerating the Scientific Cycle.}
}
@article{BOULGAKOV2020154,
title = {Bringing Microscopy-By-Sequencing into View},
journal = {Trends in Biotechnology},
volume = {38},
number = {2},
pages = {154-162},
year = {2020},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167779919301349},
author = {Alexander A. Boulgakov and Andrew D. Ellington and Edward M. Marcotte},
keywords = {DNA microscopy, next-generation sequencing, barcoding, localization, oligonucleotides},
abstract = {The spatial distribution of molecules and cells is fundamental to understanding biological systems. Traditionally, microscopies based on electromagnetic waves such as visible light have been used to localize cellular components by direct visualization. However, these techniques suffer from limitations of transmissibility and throughput. Complementary to optical approaches, biochemical techniques such as crosslinking can colocalize molecules without suffering the same limitations. However, biochemical approaches are often unable to combine individual colocalizations into a map across entire cells or tissues. Microscopy-by-sequencing techniques aim to biochemically colocalize DNA-barcoded molecules and, by tracking their thus unique identities, reconcile all colocalizations into a global spatial map. Here, we review this new field and discuss its enormous potential to answer a broad spectrum of questions.}
}
@article{WENZLAFF200127,
title = {Mental control after dysphoria: Evidence of a suppressed, depressive bias},
journal = {Behavior Therapy},
volume = {32},
number = {1},
pages = {27-45},
year = {2001},
issn = {0005-7894},
doi = {https://doi.org/10.1016/S0005-7894(01)80042-3},
url = {https://www.sciencedirect.com/science/article/pii/S0005789401800423},
author = {Richard M. Wenzlaff and Ann R. Eisenberg},
abstract = {Previous research has generally failed to find persistent negative thinking following a depressive episode, suggesting that negative thoughts may simply be by-products of the emotional disturbance. The present research examined the idea that a persistent depressive bias does exist, but it is obscured by thought suppression. Mental control theory suggests that suppressed thoughts can be detected by assessing cognition before the effortful process of distraction is implemented. To test this prediction, formerly dysphoric, chronically dysphoric, and nondysphoric control groups interpreted audio recordings of words—some of which included homophones with emotional alternatives relevant to depression (e.g., weak/week). Participants wrote down each word either immediately or after a 10-sec delay. Although formerly dysphoric individuals did not display a depressive bias in the delayed condition, their immediate responses revealed a depressive bias. As predicted, the emergence of a negative bias was associated with high levels of chronic thought suppression.}
}
@article{COOKE2020138,
title = {Diverse perspectives on interdisciplinarity from Members of the College of the Royal Society of Canada},
journal = {FACETS},
volume = {5},
number = {1},
pages = {138-165},
year = {2020},
issn = {2371-1671},
doi = {https://doi.org/10.1139/facets-2019-0044},
url = {https://www.sciencedirect.com/science/article/pii/S2371167120000551},
author = {Steven J. Cooke and Vivian M. Nguyen and Dimitry Anastakis and Shannon D. Scott and Merritt R. Turetsky and Alidad Amirfazli and Alison Hearn and Cynthia E. Milton and Laura Loewen and Eric E. Smith and D. Ryan Norris and Kim L. Lavoie and Alice Aiken and Daniel Ansari and Alissa N. Antle and Molly Babel and Jane Bailey and Daniel M. Bernstein and Rachel Birnbaum and Carrie Bourassa and Antonio Calcagno and Aurélie Campana and Bing Chen and Karen Collins and Catherine E. Connelly and Myriam Denov and Benoît Dupont and Eric George and Irene Gregory-Eaves and Steven High and Josephine M. Hill and Philip L. Jackson and Nathalie Jette and Mark Jurdjevic and Anita Kothari and Paul Khairy and Sylvie A. Lamoureux and Kiera Ladner and Christian R. Landry and François Légaré and Nadia Lehoux and Christian Leuprecht and Angela R. Lieverse and Artur Luczak and Mark L. Mallory and Erin Manning and Ali Mazalek and Stuart J. Murray and Lenore L. Newman and Valerie Oosterveld and Patrice Potvin and Sheryl Reimer-Kirkham and Jennifer Rowsell and Dawn Stacey and Susan L. Tighe and David J. Vocadlo and Anne E. Wilson and Andrew Woolford and Jules M. Blais},
keywords = {interdisciplinarity, academic institutions, universities, funding, scholarly activity, boundary crossing, barriers},
abstract = {Various multiple-disciplinary terms and concepts (although most commonly “interdisciplinarity,” which is used herein) are used to frame education, scholarship, research, and interactions within and outside academia. In principle, the premise of interdisciplinarity may appear to have many strengths; yet, the extent to which interdisciplinarity is embraced by the current generation of academics, the benefits and risks for doing so, and the barriers and facilitators to achieving interdisciplinarity, represent inherent challenges. Much has been written on the topic of interdisciplinarity, but to our knowledge there have been few attempts to consider and present diverse perspectives from scholars, artists, and scientists in a cohesive manner. As a team of 57 members from the Canadian College of New Scholars, Artists, and Scientists of the Royal Society of Canada (the College) who self-identify as being engaged or interested in interdisciplinarity, we provide diverse intellectual, cultural, and social perspectives. The goal of this paper is to share our collective wisdom on this topic with the broader community and to stimulate discourse and debate on the merits and challenges associated with interdisciplinarity. Perhaps the clearest message emerging from this exercise is that working across established boundaries of scholarly communities is rewarding, necessary, and is more likely to result in impact. However, there are barriers that limit the ease with which this can occur (e.g., lack of institutional structures and funding to facilitate cross-disciplinary exploration). Occasionally, there can be significant risk associated with doing interdisciplinary work (e.g., lack of adequate measurement or recognition of work by disciplinary peers). Solving many of the world’s complex and pressing problems (e.g., climate change, sustainable agriculture, the burden of chronic disease, and aging populations) demands thinking and working across long-standing, but in some ways restrictive, academic boundaries. Academic institutions and key support structures, especially funding bodies, will play an important role in helping to realize what is readily apparent to all who contributed to this paper—that interdisciplinarity is essential for solving complex problems; it is the new norm. Failure to empower and encourage those doing this research will serve as a great impediment to training, knowledge, and addressing societal issues.}
}
@article{JANG2022103225,
title = {Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs},
journal = {Computer-Aided Design},
volume = {146},
pages = {103225},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2022.103225},
url = {https://www.sciencedirect.com/science/article/pii/S0010448522000239},
author = {Seowoo Jang and Soyoung Yoo and Namwoo Kang},
keywords = {Generative design, Topology optimization, Deep learning, Reinforcement learning, Design diversity},
abstract = {Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.}
}
@incollection{DALY20173,
title = {8.02 - Molecular Logic Gates as Fluorescent Sensors},
editor = {Jerry L. Atwood},
booktitle = {Comprehensive Supramolecular Chemistry II},
publisher = {Elsevier},
address = {Oxford},
pages = {3-19},
year = {2017},
isbn = {978-0-12-803199-5},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.12626-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472126265},
author = {B. Daly and V.A.D. Silverson and C.Y. Yao and Z.Q. Chen and A.P. {de Silva}},
keywords = {AND Logic, Fluorescent Sensors, IMPLICATION Logic, INHIBIT Logic, Intracellular AND Logic, Logic Gates, NAND Logic, XOR and XNOR Logic, YES Logic},
abstract = {Some recent developments in the use of molecular logic gates as fluorescent sensors are described. The discussion is classified in terms of the Boolean logical assignment of the sensor system. Even simple fluorescent sensors can be recognized as single-input logic gates. Several YES gates launch the analysis of examples. A consideration of various sensors driven by double inputs and higher multiple inputs then follows. Attention is particularly drawn to the appearance of double-input logical sensor molecules, which successfully operate within living cells—a milieu where conventional semiconductor-based logic devices would struggle on the grounds of compatibility and size. The value of molecular logical thinking in the understanding of fluorescent sensor behavior is emphasized throughout.}
}
@article{NAWAZ2024121481,
title = {CoffeeNet: A deep learning approach for coffee plant leaves diseases recognition},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121481},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121481},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019838},
author = {Marriam Nawaz and Tahira Nazir and Ali Javed and Sherif {Tawfik Amin} and Fathe Jeribi and Ali Tahir},
keywords = {CenterNet, Coffee plant disease, Classification, Deep learning, ResNet},
abstract = {Coffee is regarded as the highest consumed drink around the globe and has accounted as a major source of income in the regions where it is cultivated. To meet the coffee marketplace's requirements around the globe, cultivators must boost and analyze its cultivation and quality. Several factors like environmental changes and plant diseases are the major hindrance to increasing the yield of coffee. The development in the field of computer vision has facilitated the earliest diagnostic of diseased plant samples, however, the incidence of various image distortions i.e., color, light, size, orientation changes, and similarity in the healthy and diseased portions of examined samples are the major challenges in the effective recognition of various coffee plant leaf infections. The proposed work is focused to overwhelm the mentioned limitations by proposing a novel and effective DL model called the CoffeeNet. Explicitly, an improved CenterNet approach is proposed by introducing spatial-channel attention strategy-based ResNet-50 model for the computation of deep and disease-specific sample characteristics which are then classified by the 1-step detector of the CenterNet framework. We investigated the localization and cataloging outcomes of the suggested method on the Arabica coffee leaf repository which contains the images captured in the more realistic and complicated environmental constraints. The CoffeeNet model acquires a classification accuracy number of 98.54%, along with an mAP of 0.97 that is presenting the usefulness of our technique in localizing and categorizing various sorts of coffee plant leaf disorders.}
}
@article{LI2023106560,
title = {High energy capacity or high power rating: Which is the more important performance metric for battery energy storage systems at different penetrations of variable renewables?},
journal = {Journal of Energy Storage},
volume = {59},
pages = {106560},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.106560},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X2202549X},
author = {Mingquan Li and Rui Shan and Ahmed Abdulla and Jialin Tian and Shuo Gao},
keywords = {Energy storage, Energy-to-power ratio (EPR), Decarbonization, Carbon emissions, Renewable integration, Low-carbon transition},
abstract = {Studies exploring the role and value of energy storage in deep decarbonization often overlook the balance between the energy capacity and the power rating of storage systems—a key performance parameter that can affect every part of storage operation. Here, we quantitatively evaluate the system-wide impacts of battery storage systems with various energy-to-power ratios (EPRs) and at different levels of renewable penetration. We take Jiangsu province in China as our case study, due to its high electricity consumption and aggressive renewable energy targets. Our results show the evolving role of storage: as renewable penetration increases, higher EPRs are favored, as they lead to system-wide cost reductions, lower GHG emissions, and higher power system reliability. Whereas existing studies make exogenous assumptions about the lifetime of storage, we show that lifetimes across EPRs and renewable scenarios span 10 to 20 years. Existing research can thus send false signals to investors and grid planners, delaying the deployment of storage and retarding the energy transition. By showing how different EPRs yield different benefits at different stages of the energy transition, our results help investors, policy makers, and system planners design forward-thinking and dynamic policies that encourage prudent storage uptake.}
}
@incollection{POULTON20013,
title = {Chapter 1 A brief history},
editor = {Mary M. Poulton},
series = {Handbook of Geophysical Exploration: Seismic Exploration},
publisher = {Pergamon},
volume = {30},
pages = {3-18},
year = {2001},
booktitle = {Computational neural networks for geophysical data processing},
issn = {0950-1401},
doi = {https://doi.org/10.1016/S0950-1401(01)80015-X},
url = {https://www.sciencedirect.com/science/article/pii/S095014010180015X},
author = {Mary M. Poulton},
abstract = {Publisher Summary
Computational neural networks are not just the grist of science fiction writers anymore nor are they a temporary success that will soon fade from use. The field of computational neural networks has matured in the last decade and found so many industrial applications that the notion of using a neural network to solve a particular problem no longer needs a “sales pitch” to management in many companies. Neural networks are now being routinely used in process control, manufacturing, quality control, product design, financial analysis, fraud detection, loan approval, voice and handwriting recognition, and data mining to name just a few application areas. The resurgence of neural network research is often attributed to the publication of a nonlinear network algorithm that overcame many of the limitations of the Perceptron and ADALINE. Many industrial applications of neural networks can claim significant increases in productivity, reduced costs, improved quality, or new products. This chapter present neural networks to the geophysicists as a serious computational tool—a tool with great potential and great limitations.}
}
@article{BARTH201937,
title = {Progressive Circuit Changes during Learning and Disease},
journal = {Neuron},
volume = {104},
number = {1},
pages = {37-46},
year = {2019},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2019.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0896627319308347},
author = {Alison L. Barth and Ajit Ray},
abstract = {A critical step toward understanding cognition, learning, and brain dysfunction will be identification of the underlying cellular computations that occur in and across discrete brain areas, as well as how they are progressively altered by experience or disease. These computations will be revealed by targeted analyses of the neurons that perform these calculations, defined not only by their firing properties but also by their molecular identity and how they are wired within the local and broad-scale network of the brain. New studies that take advantage of sophisticated genetic tools for cell-type-specific identification and control are revealing how learning and neurological disorders initiate and successively change the properties of defined neural circuits. Understanding the temporal sequence of adaptive or pathological synaptic changes across multiple synapses within a network will shed light into how small-scale neural circuits contribute to higher cognitive functions during learning and disease.}
}
@article{COLOMBINI2022104631,
title = {Safety evaluations on unignited high-pressure methane jets impacting a spherical obstacle},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {74},
pages = {104631},
year = {2022},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2021.104631},
url = {https://www.sciencedirect.com/science/article/pii/S0950423021002400},
author = {Cristian Colombini and Edoardo Carminati and Andrea Parisi and Renato Rota and Valentina Busini},
keywords = {High-pressure release, Methane, Spherical obstacle influence, Risk assessment, CFD, Analytical correlation},
abstract = {Nowadays methane is a fossil fuel widely used both in industries and in civil appliances. From the safety point of view, due to its flammability, its use implies hazards for people and assets. The hazardous area related to a high-pressure jet of methane arising from an accidental loss of containment requires the estimation of the distance at which the methane concentration falls below the Lower Flammability limit. Such a topic is well covered in the literature when considering free jet conditions, i.e., jets that do not interact with any equipment or surface. The same cannot be said for high pressure jets impacting an obstacle. In this context, the present work focuses on studying high pressure methane jets impacting spherical obstacles by means of Computational Fluid Dynamics with the aim of giving some insights about such a jet-obstacle interaction, possibly providing a brief by-hand procedure that, only based on known scenario information, allows to estimate the maximum extent of the unignited high-pressure jet when interacting with a spherical obstacle.}
}
@article{MIENYE2025181,
title = {ChatGPT in Education: A Review of Ethical Challenges and Approaches to Enhancing Transparency and Privacy},
journal = {Procedia Computer Science},
volume = {254},
pages = {181-190},
year = {2025},
note = {International Conference on Digital Sovereignty (ICDS)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.077},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925004272},
author = {Ibomoiye Domor Mienye and Theo G. Swart},
keywords = {ChatGPT, Education, Ethics, LLMs, Privacy, Transparency},
abstract = {The integration of ChatGPT and large language models (LLMs) into education has created new possibilities for personalized learning, tutoring, and automation of administrative tasks. However, these advancements also present ethical challenges. This paper critically examines the ethical implications of deploying ChatGPT in educational settings, with a focus on data privacy, the opaque nature of AI decision-making, and the risks of biased outputs. To address these issues, we outline actionable approaches, including Explainable AI (XAI) techniques and privacy-preserving strategies, aimed at enabling transparency and protecting student data. We also outline frameworks that support human oversight and governance to maintain trust and accountability in Al-driven educational tools.}
}
@article{BENTO2025120579,
title = {Risk analysis in ocean and maritime engineering},
journal = {Ocean Engineering},
volume = {322},
pages = {120579},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2025.120579},
url = {https://www.sciencedirect.com/science/article/pii/S002980182500294X},
author = {Ana Margarida Bento and Tiago Fazeres-Ferradosa and Paulo Rosa-Santos and Francisco Taveira-Pinto}
}
@article{WANG2020223,
title = {Anonymous data collection scheme for cloud-aided mobile edge networks},
journal = {Digital Communications and Networks},
volume = {6},
number = {2},
pages = {223-228},
year = {2020},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864819300574},
author = {Anxi Wang and Jian Shen and Chen Wang and Huijie Yang and Dengzhi Liu},
keywords = {Cloud-aided mobile edge networks, Anonymous data collection, Communication model, Path selection},
abstract = {With the rapid spread of smart sensors, data collection is becoming more and more important in Mobile Edge Networks (MENs). The collected data can be used in many applications based on the analysis results of these data by cloud computing. Nowadays, data collection schemes have been widely studied by researchers. However, most of the researches take the amount of collected data into consideration without thinking about the problem of privacy leakage of the collected data. In this paper, we propose an energy-efficient and anonymous data collection scheme for MENs to keep a balance between energy consumption and data privacy, in which the privacy information of senors is hidden during data communication. In addition, the residual energy of nodes is taken into consideration in this scheme in particular when it comes to the selection of the relay node. The security analysis shows that no privacy information of the source node and relay node is leaked to attackers. Moreover, the simulation results demonstrate that the proposed scheme is better than other schemes in aspects of lifetime and energy consumption. At the end of the simulation part, we present a qualitative analysis for the proposed scheme and some conventional protocols. It is noteworthy that the proposed scheme outperforms the existing protocols in terms of the above indicators.}
}
@article{VARGA202491,
title = {Foundations of Programmable Process Structures for the unified modeling and simulation of agricultural and aquacultural systems},
journal = {Information Processing in Agriculture},
volume = {11},
number = {1},
pages = {91-108},
year = {2024},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2022.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214317322000737},
author = {Monika Varga and Bela Csukas},
keywords = {Unified process model, Meta-prototype-based architecture, Transition-based structure representation, Locally programmable functionality prototypes, Agricultural systems, Aquacultural systems},
abstract = {This research paper defines the theoretical foundations and computational implementation of a non-conventional modeling and simulation methodology, inspired by the needs of problem solving for biological, agricultural, aquacultural and environmental systems. The challenging practical problem is to develop a framework for automatic generation of causally right and balance-based, unified models that can also be applied for the effective coupling amongst the various (sophisticated field-specific, sensor data processing-based, upper level optimization-driven, etc.) models. The scientific problem addressed in this innovation is to develop Programmable Process Structures (PPS) by combining functional basis of systems theory, structural approach of net theory and computational principles of agent based modeling. PPS offers a novel framework for the automatic generation of easily extensible and connectible, unified models for the underlying complex systems. PPS models can be generated from one state and one transition meta-prototypes and from the transition oriented description of process structure. The models consist of unified state and transition elements. The local program containing prototype elements, derived also from the meta-prototypes, are responsible for the case-specific calculations. The integrity and consistency of PPS architecture are based on the meta-prototypes, prepared to distinguish between the conservation-laws-based measures and the signals. The simulation is based on data flows amongst the state and transition elements, as well as on the unification based data transfer between these elements and their calculating prototypes. This architecture and its AI language-based (Prolog) implementation support the integration of various field- and task-specific models, conveniently. The better understanding is helped by a simple example. The capabilities of the recently consolidated general methodology are discussed on the basis of some preliminary applications, focusing on the recently studied agricultural and aquacultural cases.}
}
@article{R2023100760,
title = {Stellar parameter estimation in O-type stars using artificial neural networks},
journal = {Astronomy and Computing},
volume = {45},
pages = {100760},
year = {2023},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2023.100760},
url = {https://www.sciencedirect.com/science/article/pii/S2213133723000756},
author = {M. Flores R. and L.J. Corral and C.R. Fierro-Santillán and S.G. Navarro},
keywords = {Methods: Data analysis, Deep learning, Stars: Fundamental parameters, Astronomical databases: Miscellaneous},
abstract = {This work presents the results of the implementation of a deep learning system capable of estimating the effective temperature and surface gravity of O-type stars. The proposed system was trained with a database of 5,557 synthetic spectra computed with the stellar atmosphere code CMFGEN that covers stars with Teff from ∼20,000 K to ∼58,000 K, log(L/L⊙) from 4.3 to 6.3 dex, logg from 2.4 to 4.2 dex, and mass from 9 to 120 M⊙. Important advantages proposed in this paper include using a set of equivalent width measurements over the optical region of the stellar spectra, which avoids processing the full spectra with the inherent computational cost and allows it to apply the same trained system over different spectra resolutions. The validation of the system was performed by processing a sample of twenty O-type stars taken from the IACOB database, and a subgroup of eleven stars of those twenty taken from The Galactic O-Star Spectroscopic Catalog (GOSC) with lower resolution. As complementary work, we show the results of a synthetic spectra fitting process with the aim of simplifying the comparison with other estimations and parameter fitting from the literature.}
}
@article{DEBRUIJNSMOLDERS2024e39439,
title = {Effective student engagement with blended learning: A systematic review},
journal = {Heliyon},
volume = {10},
number = {23},
pages = {e39439},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e39439},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024154709},
author = {M. {De Bruijn-Smolders} and F.R. Prinsen},
keywords = {Blended learning, Student engagement, Learning outcomes, Systematic review},
abstract = {Although student engagement is known to promote learning outcomes in higher education, what elements of blended learning designs impact effective student engagement and hereby learning outcomes, has not been clarified yet. Hence, it is unknown how to engage students with blended learning in an effective manner. The current study breaks down student engagement into four dimensions (academic, behavioral, cognitive, and affective), and reviews the evidence regarding blended learning that engages students effectively, whether this is academically, personally, socially, or with regard to citizenship. The studies reviewed (k = 15, N = 1,428) overall asserted that all blended learning interventions investigated had a moderate to high impact on student engagement and on learning outcomes. This review, a summary and insight into the evidence, is important for the field's understanding as well as for professionals in higher education: for lecturers and policy makers who want to introduce and monitor blended learning as a means to promote both student engagement and their learning outcomes in higher education. Further research is required to increase our knowledge of how blended learning impacts both multi-dimensional constructs: student engagement and learning outcomes.}
}
@article{MORISHITA2023102079,
title = {Data assimilation and control system for adaptive model predictive control},
journal = {Journal of Computational Science},
volume = {72},
pages = {102079},
year = {2023},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2023.102079},
url = {https://www.sciencedirect.com/science/article/pii/S1877750323001394},
author = {Y. Morishita and S. Murakami and M. Yokoyama and G. Ueno},
keywords = {Data assimilation, Model-based control, Fusion plasma, ASTI},
abstract = {Model-based control of complex systems is a challenging task, particularly when the system model involves many uncertain elements. To achieve model predictive control of complex systems, we require a method that sequentially reduces uncertainties in the system model using observations and estimates control inputs under the model uncertainties. In this work, we propose an extended data assimilation framework, named data assimilation and control system (DACS), to integrate data assimilation and optimal control-input estimation. The DACS framework comprises a prediction step and three filtering steps and provides adaptive model predictive control algorithms. Since the DACS framework does not require additional prediction steps, the framework can even be applied to a large system in which iterative model prediction is prohibitive due to computational burden. Through numerical experiments in controlling virtual (numerically created) fusion plasma, we demonstrate the effectiveness of DACS and reveal the characteristics of the control performance related to the choice of hyper parameters and the discrepancies between the system model and the real system.}
}
@article{ZU2023107200,
title = {Random walk numerical scheme for the steady-state of stochastic differential equations},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {121},
pages = {107200},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2023.107200},
url = {https://www.sciencedirect.com/science/article/pii/S1007570423001181},
author = {Jian Zu},
keywords = {Continuous-time random walk, Stochastic differential equation, Steady state, Invariant distribution},
abstract = {The continuous-time random walk (CTRW) scheme is a time-continuous and space-discretization method to obtain the numerical solution of stochastic differential equations (SDEs). Compared with the traditional time-discretization scheme, it has the advantages of numerical stability and can alleviate the curse of dimensionality. This paper proposes an improved version of the CTRW scheme for the numerical solution of SDEs. By compensating the artificial diffusion caused by the Poisson approximation of the drift term of the SDE, the improved CTRW scheme has significantly better performance in the weak noise case, especially in approximating the invariant probability measure. Numerical studies show that the improved CTRW scheme has more accuracy than the existing one but takes less computation time. In addition, it has better accuracy of the mean holding time. We also modify the hybrid Fokker–Planck solver proposed for the CTRW scheme to compute the invariant probability measure.}
}
@article{OVERLAN2017320,
title = {Learning abstract visual concepts via probabilistic program induction in a Language of Thought},
journal = {Cognition},
volume = {168},
pages = {320-334},
year = {2017},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717302020},
author = {Matthew C. Overlan and Robert A. Jacobs and Steven T. Piantadosi},
keywords = {Concept learning, Visual learning, Language of Thought, Computational modeling, Behavioral experiment},
abstract = {The ability to learn abstract concepts is a powerful component of human cognition. It has been argued that variable binding is the key element enabling this ability, but the computational aspects of variable binding remain poorly understood. Here, we address this shortcoming by formalizing the Hierarchical Language of Thought (HLOT) model of rule learning. Given a set of data items, the model uses Bayesian inference to infer a probability distribution over stochastic programs that implement variable binding. Because the model makes use of symbolic variables as well as Bayesian inference and programs with stochastic primitives, it combines many of the advantages of both symbolic and statistical approaches to cognitive modeling. To evaluate the model, we conducted an experiment in which human subjects viewed training items and then judged which test items belong to the same concept as the training items. We found that the HLOT model provides a close match to human generalization patterns, significantly outperforming two variants of the Generalized Context Model, one variant based on string similarity and the other based on visual similarity using features from a deep convolutional neural network. Additional results suggest that variable binding happens automatically, implying that binding operations do not add complexity to peoples’ hypothesized rules. Overall, this work demonstrates that a cognitive model combining symbolic variables with Bayesian inference and stochastic program primitives provides a new perspective for understanding people’s patterns of generalization.}
}
@incollection{SHERIDAN201023,
title = {Chapter 2 - The System Perspective on Human Factors in Aviation},
editor = {Eduardo Salas and Dan Maurino},
booktitle = {Human Factors in Aviation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {23-63},
year = {2010},
isbn = {978-0-12-374518-7},
doi = {https://doi.org/10.1016/B978-0-12-374518-7.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012374518700002X},
author = {Thomas B. Sheridan},
abstract = {Publisher Summary
This chapter reviews the system perspective in terms of its origins and fundamental quantitative ideas. An appreciation of these basic concepts adds rigor to analysis and synthesis of human-machine systems, and in particular to such systems in aviation. The chapter represents an effort to remind the reader of the meaning of “system,” where it comes from, and what it implies for research, design, construction, operation, and evaluation in aviation, especially with regard to the human role in aviation. Human factors professionals, pilots, and operational personnel in air traffic management and related practitioners who know about “systems” only as a general and often vague term for something complex can benefit from knowing a bit of the history, the people, and the quantitative substance that underlies the terminology. The chapter begins by defining what is meant by a system, then discusses the history of the idea, the major contributors and what they contributed, and what made the systems idea different from previous ideas in technology. It goes on to give examples of systems thinking applied to design, development, and manufacturing of aviation systems in consideration of the people involved. Salient system models such as control, decision, information, and reliability are then explicated.}
}
@article{GAO2019146333,
title = {Coactivations of barrel and piriform cortices induce their mutual synapse innervations and recruit associative memory cells},
journal = {Brain Research},
volume = {1721},
pages = {146333},
year = {2019},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2019.146333},
url = {https://www.sciencedirect.com/science/article/pii/S0006899319303877},
author = {Zilong Gao and Ruixiang Wu and Changfeng Chen and Bo Wen and Yahui Liu and Wei Lu and Na Chen and Jing Feng and Ruichen Fan and Dangui Wang and Shan Cui and Jin-Hui Wang},
keywords = {Associative learning, Memory cell, Neural circuit, Barrel cortex, Piriform cortex},
abstract = {After associative learning, a signal induces the recall of its associated signal, or the other way around. This reciprocal retrieval of associated signals is essential for associative thinking and logical reasoning. For the cellular mechanism underlying this associative memory, we hypothesized that the formation of synapse innervations among coactivated sensory cortices and the recruitment of associative memory cells were involved in the integrative storage and reciprocal retrieval of associated signals. Our study indicated that the paired whisker and olfaction stimulations led to an odorant-induced whisker motion and a whisker-induced olfaction response, a reciprocal form of associative memory retrieval. In mice that showed the reciprocal retrieval of associated signals, their barrel and piriform cortical neurons became mutually innervated through their axon projection and new synapse formation. These piriform and barrel cortical neurons gained the ability to encode both whisker and olfaction signals based on synapse innervations from the innate input and the newly formed input. Therefore, the associated activation of sensory cortices by pairing input signals initiates their mutual synapse innervations, and the neurons innervated by new and innate synapses are recruited to be associative memory cells that encode these associated signals. Mutual synapse innervations among sensory cortices to recruit associative memory cells may compose the primary foundation for the integrative storage and reciprocal retrieval of associated signals. Our study also reveals that new synapses onto the neurons enable these neurons to encode memories to new specific signals.}
}
@article{GILBOA202196,
title = {The complexity of the consumer problem},
journal = {Research in Economics},
volume = {75},
number = {1},
pages = {96-103},
year = {2021},
issn = {1090-9443},
doi = {https://doi.org/10.1016/j.rie.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1090944321000016},
author = {Itzhak Gilboa and Andrew Postlewaite and David Schmeidler},
keywords = {Consumer theory, Computational complexity, Mental accounting},
abstract = {A literal interpretation of neo-classical consumer theory suggests that the consumer solves a very complex problem. In the presence of indivisible goods, the consumer problem is NP-Hard, and it appears unlikely that it can be optimally solved by a human. Two implications of this observation are that (i) households may imitate each other’s choices; (ii) households may adopt heuristics that give rise to the phenomenon of mental accounting.}
}
@article{MOGLIA2017173,
title = {A review of Agent-Based Modelling of technology diffusion with special reference to residential energy efficiency},
journal = {Sustainable Cities and Society},
volume = {31},
pages = {173-182},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716306813},
author = {Magnus Moglia and Stephen Cook and James McGregor},
keywords = {Agent-Based Modelling, Diffusion of innovation, HVAC, Lighting, Appliances},
abstract = {Residential energy efficiency is an important strategy for reducing greenhouse gas emissions. There are many technologies that help improve residential energy efficiency, and in fact, increased energy efficiency has already helped reduce global greenhouse gas emissions significantly in the past. However, with greater innovation, further improvements can be made and improving energy efficiency is an ongoing activity. Policymakers around the world are putting strategies in place to speed up the adoption of energy efficient technologies and practices, but ultimately this process is based on choice by residents themselves. Human decision making and choice however is a very complex issue, and complex computational tools are required in order to analyse and/or predict the impact of various policies. Traditionally, equation-based models such as Bass and Choice models have been used to describe the diffusion of technologies in a population, but certain limitations have been identified. This article explores what these limitations are in the context of energy efficient residential technologies and how an alternative computational and empirical paradigm, Agent-Based Modelling (ABM), can help resolve some of these limitations. As such, this is a review article into how ABM can support analysis of strategies to catalyse greater uptake of energy efficiency in the residential sector.}
}
@article{BARTH2009441,
title = {Children’s multiplicative transformations of discrete and continuous quantities},
journal = {Journal of Experimental Child Psychology},
volume = {103},
number = {4},
pages = {441-454},
year = {2009},
note = {Special Issue: Typical Development of Numerical Cognition},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2009.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0022096509000289},
author = {Hilary Barth and Andrew Baron and Elizabeth Spelke and Susan Carey},
keywords = {Ratio sensitivity, Ratios, Multiplicative operations, Doubling, Halving, Numerical cognition},
abstract = {Recent studies have documented an evolutionarily primitive, early emerging cognitive system for the mental representation of numerical quantity (the analog magnitude system). Studies with nonhuman primates, human infants, and preschoolers have shown this system to support computations of numerical ordering, addition, and subtraction involving whole number concepts prior to arithmetic training. Here we report evidence that this system supports children’s predictions about the outcomes of halving and perhaps also doubling transformations. A total of 138 kindergartners and first graders were asked to reason about the quantity resulting from the doubling or halving of an initial numerosity (of a set of dots) or an initial length (of a bar). Controls for dot size, total dot area, and dot density ensured that children were responding to the number of dots in the arrays. Prior to formal instruction in symbolic multiplication, division, or rational number, halving (and perhaps doubling) computations appear to be deployed over discrete and possibly continuous quantities. The ability to apply simple multiplicative transformations to analog magnitude representations of quantity may form a part of the toolkit that children use to construct later concepts of rational number.}
}
@article{WANG2022103414,
title = {Cross-layer progressive attention bilinear fusion method for fine-grained visual classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103414},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103414},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002789},
author = {Chaoqing Wang and Yurong Qian and Weijun Gong and Junjong Cheng and Yongqiang Wang and Yuefei Wang},
keywords = {Fine-grained visual classification, Feature fusion, Attention, Progressive},
abstract = {Fine-grained visual classification (FGVC) is a critical task in the field of computer vision. However, FGVC is full of challenges due to the large intra-class variation and small inter-class variation of the classes to be classified on an image. The key in dealing with the problem is to capture subtle visual differences from the image and effectively represent the discriminative features. Existing methods are often limited by insufficient localization accuracy and insufficient feature representation capabilities. In this paper, we propose a cross-layer progressive attention bilinear fusion (CPABF in short) method, which can efficiently express the characteristics of discriminative regions. The CPABF method involves three components: 1) Cross-Layer Attention (CLA) locates and reinforces the discriminative region with low computational costs; 2) The Cross-Layer Bilinear Fusion Module (CBFM) effectively integrates the semantic information from the low-level to the high-level 3) Progressive Training optimizes the parameters in the network to the best state in a delicate way. The CPABF shows excellent performance on the four FGVC datasets and outperforms some state-of-the-art methods.}
}
@article{MIKITEN1995141,
title = {Intuition-based computing: A new kind of ‘virtual reality’},
journal = {Mathematics and Computers in Simulation},
volume = {40},
number = {1},
pages = {141-147},
year = {1995},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(95)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/0378475495000231},
author = {Terry M. Mikiten},
keywords = {Intuition, Mind, Problem-solving, Creativity, Cognition, Computing, Grand challenge},
abstract = {It is helpful to consider the mind and the computer as two separate information domains. Each has a separate system of rules that guide behavior. The interaction between the two is characterized as an interplay between rule systems. In this view, there should be interactions which are optimal and others which are not. To understand this, the first task is to identify the rules that operate in each domain. The next is to see how they interact. It is concluded that rules of the mind which give rise to what is generally termed ‘intuition’ is altogether compatible with rules of computation. This, in turn, suggests computational systems capable of independent ‘intuitive’ processing on the one hand, and other computational systems which can serve to augment human intuition.}
}
@article{HYLAND2007437,
title = {The Category Theoretic Understanding of Universal Algebra: Lawvere Theories and Monads},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {172},
pages = {437-458},
year = {2007},
note = {Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2007.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000874},
author = {Martin Hyland and John Power},
keywords = {Universal algebra, Lawvere theory, monad, computational effect},
abstract = {Lawvere theories and monads have been the two main category theoretic formulations of universal algebra, Lawvere theories arising in 1963 and the connection with monads being established a few years later. Monads, although mathematically the less direct and less malleable formulation, rapidly gained precedence. A generation later, the definition of monad began to appear extensively in theoretical computer science in order to model computational effects, without reference to universal algebra. But since then, the relevance of universal algebra to computational effects has been recognised, leading to renewed prominence of the notion of Lawvere theory, now in a computational setting. This development has formed a major part of Gordon Plotkin's mature work, and we study its history here, in particular asking why Lawvere theories were eclipsed by monads in the 1960's, and how the renewed interest in them in a computer science setting might develop in future.}
}
@article{BAUCELLS201329,
title = {Guided decisions processes},
journal = {EURO Journal on Decision Processes},
volume = {1},
number = {1},
pages = {29-44},
year = {2013},
issn = {2193-9438},
doi = {https://doi.org/10.1007/s40070-013-0003-8},
url = {https://www.sciencedirect.com/science/article/pii/S2193943821000108},
author = {Manel Baucells and Rakesh K. Sarin},
keywords = {Decision analysis, Behavioral decision making, Narrow bracket, Insurance, Multi-attribute decisions},
abstract = {The heuristics and bias research program has convincingly demonstrated that our judgments and choices are prone to systematic errors. Decision analysis requires coherent judgments about beliefs (probabilities) and tastes (utilities), and a rational procedure to combine them so that choices maximize subjective expected utility. A guided decision process is a middle-of-the-road between decision analysis and intuitive judgments in which the emphasis is on improving decisions through simple decision rules. These rules reduce cost of thinking, or decision effort, for the myriad decisions that one faces in daily life; but at the same time, they are personalized to the individual and produce near optimal choices. We discuss the principles behind the guided decision processes research program, and illustrate the approach using several examples.}
}
@article{DORR2017322,
title = {Common errors in reasoning about the future: Three informal fallacies},
journal = {Technological Forecasting and Social Change},
volume = {116},
pages = {322-330},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516301275},
author = {Adam Dorr},
keywords = {Technological progress, Accelerating change, Computing, Fallacy, Errors in reasoning},
abstract = {The continued exponential growth of the price-performance of computing is likely to effectuate technologies that radically transform both the global economy and the human condition over the course of this century. Conventional visions of the next 50years fail to realistically account for the full implications of accelerating technological change driven by the exponential growth of computing, and as a result are deeply flawed. These flawed visions are, in part, a consequence of three interrelated errors in reasoning: 1) the linear projection fallacy, 2) the ceteris paribus fallacy, and 3) the arrival fallacy. Each of these informal fallacies is likely a manifestation of shortcomings in our intuitions about complex dynamic systems. Recognizing these errors and identifying when and where they affect our own reasoning is an important first step toward thinking more realistically about the future.}
}
@article{ZHU2021118730,
title = {From gratitude to injustice: Neurocomputational mechanisms of gratitude-induced injustice},
journal = {NeuroImage},
volume = {245},
pages = {118730},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118730},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010028},
author = {Ruida Zhu and Zhenhua Xu and Song Su and Chunliang Feng and Yi Luo and Honghong Tang and Shen Zhang and Xiaoyan Wu and Xiaoqin Mai and Chao Liu},
keywords = {Gratitude, Protection tendency, Injustice, Mentalizing, Reward processing},
abstract = {Gratitude shapes individuals’ behaviours and impacts the harmony of society. Many previous studies focused on its association with prosocial behaviours. A possibility that gratitude can lead to moral violation has been overlooked until recently. Nevertheless, the neurocognitive mechanisms of gratitude-induced moral violation are still unclear. On the other hand, though neural correlates of the gratitude's formation have been examined, the neural underpinnings of gratitude-induced behaviour remain unknown. For addressing these two overlapped research gaps, we developed novel tasks to investigate how participants who had received voluntary (Gratitude group) or involuntary help (Control group) punished their benefactors’ unfairness with functional magnetic resonance imaging (fMRI). The Gratitude group punished their benefactors less than the Control group. The self-report and computational modelling results demonstrated a crucial role of the boosted protection tendency on behalf of benefactors in the gratitude-induced injustice. The fMRI results showed that activities in the regions associated with mentalizing (temporoparietal junction) and reward processing (ventral medial prefrontal cortex) differed between the groups and were related to the gratitude-induced injustice. They suggest that grateful individuals concern for benefactors’ benefits, value chances to interact with benefactors, and refrain from action that perturbs relationship-building (i.e., exert less punishment on benefactors’ unfairness), which reveal a dark side of gratitude and enrich the gratitude theory (i.e., the find-bind-remind theory). Our findings provide psychological, computational, and neural accounts of the gratitude-induced behaviour and further the understanding of the nature of gratitude.}
}
@article{VEZOLI2021117479,
title = {Cortical hierarchy, dual counterstream architecture and the importance of top-down generative networks},
journal = {NeuroImage},
volume = {225},
pages = {117479},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117479},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920309642},
author = {Julien Vezoli and Loïc Magrou and Rainer Goebel and Xiao-Jing Wang and Kenneth Knoblauch and Martin Vinck and Henry Kennedy},
keywords = {Non-human primate, Human, Brain, Electrophysiology, Anatomy, Modeling, Connectivity, Predictive coding, Perception, Consciousness},
abstract = {Hierarchy is a major organizational principle of the cortex and underscores modern computational theories of cortical function. The local microcircuit amplifies long-distance inter-areal input, which show distance-dependent changes in their laminar profiles. Statistical modeling of these changes in laminar profiles demonstrates that inputs from multiple hierarchical levels to their target areas show remarkable consistency, allowing the construction of a cortical hierarchy based on a principle of hierarchical distance. The statistical modeling that is applied to structure can also be applied to laminar differences in the oscillatory coherence between areas thereby determining a functional hierarchy of the cortex. Close examination of the anatomy of inter-areal connectivity reveals a dual counterstream architecture with well-defined distance-dependent feedback and feedforward pathways in both the supra- and infragranular layers, suggesting a multiplicity of feedback pathways with well-defined functional properties. These findings are consistent with feedback connections providing a generative network involved in a wide range of cognitive functions. A dynamical model constrained by connectivity data sheds insight into the experimentally observed signatures of frequency-dependent Granger causality for feedforward versus feedback signaling. Concerted experiments capitalizing on recent technical advances and combining tract-tracing, high-resolution fMRI, optogenetics and mathematical modeling hold the promise of a much improved understanding of lamina-constrained mechanisms of neural computation and cognition. However, because inter-areal interactions involve cortical layers that have been the target of important evolutionary changes in the primate lineage, these investigations will need to include human and non-human primate comparisons.}
}
@article{REN2025100774,
title = {Immersive E-learning mode application in Chinese language teaching system based on big data recommendation algorithm},
journal = {Entertainment Computing},
volume = {52},
pages = {100774},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100774},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001423},
author = {Chunjiao Ren},
keywords = {Big data, Interest recommendation algorithm, Immersive, E-Learning mode, Chinese teaching system},
abstract = {With the rapid development of information technology, E-Learning has become an innovative teaching method. However, in the field of Chinese teaching, how to provide effective learning resources and recommendation algorithms in E-Learning mode is still a challenge. This study aims to improve the effectiveness of Chinese teaching system and students’ learning outcomes through an immersive E-Learning model based on big data interest recommendation algorithm. This paper adopts an immersive E-Learning model based on big data interest recommendation algorithm, and constructs a Chinese teaching system. The web crawler is used to fully collect the experimental data and collate it in a targeted manner, and the required data is screened out by using a more efficient separation method. Adding big data recommendation algorithm to the system of this paper can not only record and analyze historical behaviors of users, but also recommend data information according to users’ interests, so that users can clarify their real information needs. By testing the system’s professional ability and recording the experimental data, this paper finds that the overall performance of this Chinese language teaching system is very good, and can achieve the original expected design purpose. In addition, the system largely solves the problem that the traditional system based on data recommendation algorithm is difficult to carry out effective recommendation smoothly when the total amount of data is too large.}
}
@article{CARVAJALRODRIGUEZ201576,
title = {Incorporación de la programación informática en el currículum de Biología},
journal = {Magister},
volume = {27},
number = {2},
pages = {76-82},
year = {2015},
issn = {0212-6796},
doi = {https://doi.org/10.1016/j.magis.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0212679615000286},
author = {Antonio Carvajal-Rodríguez},
keywords = {Docencia, Bioinformática, Biología computacional, Python, Teaching, Bioinformatics, Computational biology, Python},
abstract = {Resumen
La investigación en biología ha cambiado radicalmente debido al efecto combinado de los avances en biotecnología y ciencias de la computación. En consecuencia, la biología computacional y la bioinformática son tan esenciales para la biología del siglo xxi como la biología molecular lo fue en el anterior. Sin embargo, las competencias correspondientes a razonamiento matemático y computacional en el currículo de Biología apenas han cambiado en los últimos 25 años. La formación del biólogo debería ser tan sofisticada desde el punto de vista computacional como la del físico o la del ingeniero. La incorporación de estos cambios requiere tanto de un mayor esfuerzo de integración de las asignaturas cuantitativas existentes en el ámbito de los problemas biológicos como de la contextualización de las asignaturas propias de la biología desde un punto de vista más formal y de modelización. En este trabajo se revisan algunos de los esfuerzos que en este sentido se están haciendo en el panorama internacional y se presenta también la experiencia del autor en el diseño e impartición de un curso de iniciación a la programación para biólogos usando una metodología de aprendizaje basado en problemas.
The joint effect of biotechnology and computing has changed the research in biology. Consequently, computational biology is as essential for 21st-century biologists as molecular biology was in the 20th. However, Biology curricula have little emphasis in quantitative thinking and computation. The education for biologists should become as sophisticated as the computational education of physicists and engineers. The necessary changes to reach this goal require the connection of mathematics and quantitative subjects with real biological problems and at the same time, teaching some biological subjects from a modeling and computational perspective. In the present work, some of the current international effort in this path is reviewed and additionally, the author's experience when teaching an introduction to programming for biologists is presented.}
}
@article{WIECHA2024101129,
title = {Deep learning for nano-photonic materials – The solution to everything!?},
journal = {Current Opinion in Solid State and Materials Science},
volume = {28},
pages = {101129},
year = {2024},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2023.101129},
url = {https://www.sciencedirect.com/science/article/pii/S1359028623000748},
author = {Peter R. Wiecha},
abstract = {Deep learning is currently being hyped as an almost magical tool for solving all kinds of difficult problems that computers have not been able to solve in the past. Particularly in the fields of computer vision and natural language processing, spectacular results have been achieved. The hype has now infiltrated several scientific communities. In (nano-) photonics, researchers are trying to apply deep learning to all kinds of forward and inverse problems. A particularly challenging problem is for instance the rational design of nanophotonic materials and devices. In this opinion article, I will first discuss the public expectations of deep learning and give an overview of the quite different scales at which actors from industry and research are operating their deep learning models. I then examine the weaknesses and dangers associated with deep learning. Finally, I’ll discuss the key strengths that make this new set of statistical methods so attractive, and review a personal selection of opportunities that shouldn’t be missed in the current developments.}
}
@article{THACKER2023101782,
title = {Climate change by the numbers: Leveraging mathematical skills for science learning online},
journal = {Learning and Instruction},
volume = {86},
pages = {101782},
year = {2023},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2023.101782},
url = {https://www.sciencedirect.com/science/article/pii/S0959475223000518},
author = {Ian Thacker},
keywords = {Climate change, Conceptual change, Epistemic dispositions, Numerical estimation, Plausibility judgments, Learning technology},
abstract = {The purpose of this preregistered study was to test an online intervention that presents participants with novel numbers about climate change after they estimate those numbers. An experimental study design was used to investigate the impact of the intervention on undergraduate students’ climate change understanding and perceptions that human caused climate change is plausible. Findings revealed that posttest climate change knowledge and plausibility perceptions were higher among those randomly assigned to use the intervention compared with those assigned to a control condition, and that supplementing this experience with numeracy instruction was linked with the use of more explicit estimation strategies and greater learning gains for people with adaptive epistemic dispositions. Findings from this study replicate and extend prior research, support the idea that novel data can support knowledge revision, identify estimation strategies used in this context, and offer an open-source online intervention for sharing surprising data with students and teachers.}
}
@article{BENTLEY2000465,
title = {Statistics and archaeology in Israel},
journal = {Computational Statistics & Data Analysis},
volume = {32},
number = {3},
pages = {465-483},
year = {2000},
issn = {0167-9473},
doi = {https://doi.org/10.1016/S0167-9473(99)00094-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167947399000948},
author = {Jim Bentley and Tammi J Schneider},
keywords = {Variability, Density estimation, Kriging, Mapping: Surface survey, Archaeology Archaeometrics},
abstract = {While the field of statistics is fairly young, the field of archaeology is quite old. Modern archaeology prides itself on its ability to glean maximum information about the past from minimal information collected in the present. This paper attempts to show how the application of statistical thinking and techniques can aid the archaeologist in retrieving as much information as possible from artifacts; thus allowing the archaeologists to leave the majority of a site for future generations. In the past few years, archaeologists working in Israel have joined forces with statisticians in an attempt to generate more accurate recordings of archaeological information than is currently the standard in the Middle East. Careful application of statistical methods has reduced collection time and improved the display of archaeological information. An understanding of statistical concepts such as variability and density estimation has already been shown to be of use to archaeologists. Conversely, the use of examples from the field have proven to be of use in motivating humanities students to learn about statistical thinking. Archaeology has also provided a field in which students of statistics may apply their new found knowledge. The combination of statistics and archaeology is clearly of benefit to both disciplines.}
}
@article{RINGE2023101268,
title = {Cation effects on electrocatalytic reduction processes at the example of the hydrogen evolution reaction},
journal = {Current Opinion in Electrochemistry},
volume = {39},
pages = {101268},
year = {2023},
issn = {2451-9103},
doi = {https://doi.org/10.1016/j.coelec.2023.101268},
url = {https://www.sciencedirect.com/science/article/pii/S2451910323000613},
author = {Stefan Ringe},
keywords = {Cation effects, Hydrogen evolution reaction, Hydrogen underpotential deposition, CO reduction, Electric double layer, Solid-liquid interface},
abstract = {Cation effects provide invaluable insights into electrochemistry. In this review, I discuss them with a main focus on the hydrogen evolution reaction and a summary of recent in situ spectroscopic and electrochemical measurements as well as advanced computational simulation results conducted at varying cation identities, concentrations, and pH. According to these works, the interfacial cation concentration is the main descriptor to explain cation and pH effects. The detailed mechanism (such as e.g. water polarization, water structure changes, field-stabilization of intermediates) depends strongly on potential, pH, oxophilicity of the electrode, or the nature of the rate-limiting step and proton donor. With growing convergence in this field, cation effects remain a highly challenging and promising topic for research.}
}
@article{GUPTA2005267,
title = {Power-law distribution in a learning process: competition, learning and natural selection},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {345},
number = {1},
pages = {267-274},
year = {2005},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2004.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378437104009860},
author = {Hari M. Gupta and José R. Campanha},
keywords = {Power-law, Learning, Natural selection},
abstract = {In the present work, we propose a model for the statistical distribution of people versus number of steps acquired by them in a learning process, based on competition, learning and natural selection. We consider that learning ability is normally distributed. We found that the number of people versus step acquired by them in a learning process is given through a power law. As competition, learning and selection is also at the core of all economical and social systems, we consider that power-law scaling is a quantitative description of this process in social systems. This gives an alternative thinking in holistic properties of complex systems.}
}
@article{PELOROSSO2020101867,
title = {Modeling and urban planning: A systematic review of performance-based approaches},
journal = {Sustainable Cities and Society},
volume = {52},
pages = {101867},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101867},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719316968},
author = {Raffaele Pelorosso},
keywords = {Systems thinking, Thermodynamics of open systems, Standards, Spatial planning, Model classification},
abstract = {New planning approaches based on performance measures of the urban system are emerging to face the current challenges to the sustainability of cities. Through modelling, planners can understand the general behavior of the system and, consequently, decide the strategic allocation of land uses and human activities with respect to performances of the considered processes and the socio-ecological and economic uncertainties. Thus, model-based planning approaches present strong similarities with the performance-based planning (PBP) approaches and modelling can represent a valuable tool for the evolution and expansion of PBP. In this paper, a systematic review has explored a) the contribution of modelling within PBP approaches in moving cities towards sustainability; b) the applicability for modeling in PBP in urban contexts. Twelve operational examples of model-based urban planning and PBP have been identified in energy, water infrastructure, land use and ecological planning areas. A scoring system for potential model applicability in urban planning was tested in the sampled case studies. Moreover, several critical elements in the relation between modeling approaches and PBP have been identified. Finally, a discussion on the system performance concept as a new urban planning paradigm has been proposed.}
}
@article{HOU2025105329,
title = {Measuring undergraduate students' reliance on Generative AI during problem-solving: Scale development and validation},
journal = {Computers & Education},
volume = {234},
pages = {105329},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105329},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000971},
author = {Chenyu Hou and Gaoxia Zhu and Vidya Sudarshan and Fun Siong Lim and Yew Soon Ong},
keywords = {Human-AI collaboration, Problem-solving, Generative AI, Higher education, Reliance on AI, Scale development},
abstract = {Reliance on AI describes the behavioral patterns of when and how individuals depend on AI suggestions, and appropriate reliance patterns are necessary to achieve effective human-AI collaboration. Traditional measures often link reliance to decision-making outcomes, which may not be suitable for complex problem-solving tasks where outcomes are not binary (i.e., correct or incorrect) or immediately clear. Therefore, this study aims to develop a scale to measure undergraduate students' behaviors of using Generative AI during problem-solving tasks without directly linking them to specific outcomes. We conducted an exploratory factor analysis on 800 responses collected after students finished one problem-solving activity, which revealed four distinct factors: reflective use, cautious use, thoughtless use, and collaborative use. The overall scale has reached sufficient internal reliability (Cronbach's alpha = .84). Two confirmatory factor analyses (CFAs) were conducted to validate the factors using the remaining 730 responses from this activity and 1173 responses from another problem-solving activity. CFA indices showed adequate model fit for data from both problem-solving tasks, suggesting that the scale can be applied to various human-AI problem-solving tasks. This study offers a validated scale to measure students' reliance behaviors in different human-AI problem-solving activities and provides implications for educators to responsively integrate Generative AI in higher education.}
}
@article{MARZANO20231028,
title = {Manufacturing Ergonomics Improvements in Distillery Industry Using Digital Tools},
journal = {Procedia CIRP},
volume = {118},
pages = {1028-1032},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.176},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004031},
author = {Adelaide Marzano},
keywords = {Digital manufacturing system, ergonomics, design},
abstract = {This paper presents the steps taken by distilleries to uphold years old traditions and how new design tools can streamlined the current manufacturing processes. Different methods for bung removal are explored and how they are used today within warehouses and distilleries worldwide. The aim is to test new designs to replace the current tools used in distillery process to perform heavily manual tasks. Models of the current and new design are produced, and both are tested in a digital environment for ergonomics and time efficiency purposes.}
}
@article{WERNER200782,
title = {Perspectives on the Neuroscience of Cognition and Consciousness},
journal = {Biosystems},
volume = {87},
number = {1},
pages = {82-95},
year = {2007},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2006.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0303264706000608},
author = {Gerhard Werner},
keywords = {Cognition, Consciousness, Metastability, Phase space, Coordination dynamics, Computation, Representation, Information},
abstract = {The origin and current use of the concepts of computation, representation and information in Neuroscience are examined and conceptual flaws are identified which vitiate their usefulness for addressing the problem of the neural basis of Cognition and Consciousness. In contrast, a convergence of views is presented to support the characterization of the Nervous System as a complex dynamical system operating in a metastable regime, and capable of evolving to configurations and transitions in phase space with potential relevance for Cognition and Consciousness.}
}
@article{RODRIGUES2021406,
title = {Convolutional Neural Network for Respiratory Mechanics Estimation during Pressure Support Ventilation},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {15},
pages = {406-411},
year = {2021},
note = {11th IFAC Symposium on Biological and Medical Systems BMS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.290},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321016955},
author = {Adriano S. Rodrigues and Marcos R.O.A. Maximo and Marcus H. Victor},
keywords = {Mechanical Ventilation, Respiratory Mechanics, Respiratory Effort, Deep Learning, Convolutional Neural Networks},
abstract = {In mechanically ventilated patients, some lung injuries can be reduced or avoided with therapy individualization, while the lung function is evaluated continuously, breath by breath. However, obtaining information on respiratory mechanics (respiratory system resistance and compliance) in the presence of respiratory effort is challenging, even if using invasive and complex procedures. The contribution of this work is to predict both respiratory system resistance and compliance over time using a convolutional neural network (CNN) and estimate the respiratory effort profile using the respiratory dynamics. Therefore, the approach used in this work was to generate a large amount of simulated data to feed a CNN so it could learn how to predict the correct values of the respiratory system resistance and compliance. Then, the respiratory effort was estimated by solving a first-order linear model. The main results showed a normalized mean squared error of 5.7% for the respiratory system resistance and 11.56% for compliance from Bland-Altman plots derived from the computational simulator. Finally, the method was validated using real data from an active lung simulator within which respiratory mechanics varied, and some ventilator settings were adjusted to mimic actual patient situations. The active lung simulator effort profile was obtained with a normalized mean squared error of 8.31% considering the use of an active lung simulator. The results have shown that the simulated data were valuable for the CNN training, while the performance over the real data suggested that the network was generalized accordingly for estimating respiratory parameters and effort profile.}
}
@article{JANG2020107524,
title = {Hemispheric asymmetries in processing numerical meaning in arithmetic},
journal = {Neuropsychologia},
volume = {146},
pages = {107524},
year = {2020},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2020.107524},
url = {https://www.sciencedirect.com/science/article/pii/S0028393220301974},
author = {Selim Jang and Daniel C. Hyde},
keywords = {Arithmetic, Numerical cognition, Cerebral hemispheres, Late positivity, Distance effect},
abstract = {Hemispheric asymmetries in arithmetic have been hypothesized based on neuropsychological, developmental, and neuroimaging work. However, it has been challenging to separate asymmetries related to arithmetic specifically, from those associated general cognitive or linguistic processes. Here we attempt to experimentally isolate the processing of numerical meaning in arithmetic problems from language and memory retrieval by employing novel non-symbolic addition problems, where participants estimated the sum of two dot arrays and judged whether a probe dot array was the correct sum of the first two arrays. Furthermore, we experimentally manipulated which hemisphere receive the probe array first using a visual half-field paradigm while recording event-related potentials (ERP). We find that neural sensitivity to numerical meaning in arithmetic arises under left but not right visual field presentation during early and middle portions of the late positive complex (LPC, 400-800 ms). Furthermore, we find that subsequent accuracy for judgements of whether the probe is the correct sum is better under right visual field presentation than left, suggesting a left hemisphere advantage for integrating information for categorization or decision making related to arithmetic. Finally, neural signatures of operational momentum, or differential sensitivity to whether the probe was greater or less than the sum, occurred at a later portion of the LPC (800-1000 ms) and regardless of visual field of presentation, suggesting a temporal and functional dissociation between magnitude and ordinal processing in arithmetic. Together these results provide novel evidence for differences in timing and hemispheric lateralization for several cognitive processes involved in arithmetic thinking.}
}
@article{RAYAMORENO2024125385,
title = {Degradation of the ZT thermoelectric figure of merit in silicon when nanostructuring: From bulk to nanowires},
journal = {International Journal of Heat and Mass Transfer},
volume = {225},
pages = {125385},
year = {2024},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2024.125385},
url = {https://www.sciencedirect.com/science/article/pii/S0017931024002163},
author = {Martí Raya-Moreno and Riccardo Rurali and Xavier Cartoixà},
keywords = {Thermoelectrics, Nanowires, Phonon drag, , Coupled e-ph Boltzmann transport equation},
abstract = {Since the landmark paper by Hicks and Dresselhaus [Phys. Rev. B 47, 16631(R) (1993)], there has been a general consensus that one-dimensional nanoscale conductors, i.e. nanowires, provide the long sought paradigm to implement the so-called phonon-glass electron-crystal material, which results in large improvements in the thermoelectric figure of merit ZT. Despite some encouraging—though isolated—experimental results, this idea has never been subjected to a rigorous scrutiny and the effect of the coupled dynamics of electrons and phonons has usually been oversimplified. To bypass these limitations, we have calculated the effective thermoelectric parameters for silicon nanowires (SiNWs) by iteratively solving the coupled electron-phonon Boltzmann transport equation (EPBTE) supplied with first-principles data. This allows for an unprecedented precision in determining the correct dependence of the thermoelectric parameters with system size; including, but not limited to, the figure of merit and its enhancement or degradation due to nanostructuring. Indeed, we demonstrate that the commonly used relaxation time approximation (RTA), or the uncoupled beyond the RTA (iterative) solution fail to describe the correct effect of nanostructuring on the thermoelectric properties and efficiency in SiNWs due to the strong contribution of phonon drag to the Seebeck coefficient, so that the use of fully coupled solution of the EPBTE is essential to obtain the correct effect of nanostructuring. Most importantly, we show that, contrarily to what commonly argued, resorting to NWs is not necessarily beneficial for ZT. Indeed, in a wide range of diameters nanostructuring diminishes the Seebeck coefficient faster than the decrease in thermal conductivity, due to the suppression of very long wavelength phonons responsible for the largest contribution to the phonon drag component of the Seebeck coefficient. This penalty to ZT can be mitigated if the NWs have a very rough surface, providing additional reduction to the thermal conductivity. Additionally, we demonstrate that our methodology provides improved data sets for an accurate determination of doping concentration in NWs through electrical-based inference and excellent agreement with the available experimental data.}
}
@article{RENDONCASTRILLON2023104,
title = {Training strategies from the undergraduate degree in chemical engineering focused on bioprocesses using PBL in the last decade},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {104-116},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000258},
author = {Leidy Rendón-Castrillón and Margarita Ramírez-Carmona and Carlos Ocampo-López},
keywords = {Research hotbed, Biotechnology, Green chemistry, Circular economy, Sustainability, ABET, Engineering education},
abstract = {Global engineering education addresses the development of professional competencies in undergraduates to prepare professionals capable of solving complex technical problems under social, environmental, and economic challenges. In this work, training was carried out to incorporate the bioprocess research of the chemical engineering students at Universidad Pontificia Bolivariana in Medellin, Colombia, using a project-based learning methodology (PBL). An open call was made to the students, and they were challenged to build a prototype which they had to support together with a written report as evidence for their admission to the research hotbed and assign them research projects in bioprocesses. In the last decade, 276 students participated in the hotbed generating 21 conference presentations, four software, 14 research articles, and 16 academic awards. In parallel, a survey was conducted to analyze the perception of graduates participating in the hotbed according to a list of 17 competency criteria relevant to the chemical engineering program. It was found that the average perception is at the highest levels (4−5), which indicates that most of the graduates value the significant contribution made by the CIBIOT hotbed to the development of a professional in experimentation, communication, and acquisition of new knowledge.}
}
@article{WALLENTIN2017165,
title = {Dynamic hybrid modelling: Switching between AB and SD designs of a predator-prey model},
journal = {Ecological Modelling},
volume = {345},
pages = {165-175},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2016.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0304380016303714},
author = {Gudrun Wallentin and Christian Neuwirth},
keywords = {Hybrid model, Multi-paradigmatic modelling, Agent-based model, System-dynamics model, Predator-prey system},
abstract = {Entities and processes in complex systems are of diverse nature and operate at various spatial and temporal scales. Hybrid agent-based (AB) and system dynamics (SD) models have been suggested to capture the essence of these systems in a natural and computationally efficient way. However, the integration of the equation-based SD and individual-based AB models is not least challenged by considerable conceptual differences between these models. Examples of tightly integrated and dynamically switching hybrid models are rare. The aim of this paper is to expand on theoretical frameworks of hybrid agent-based and system dynamics models in ecology to support the model design process of dynamically switching hybrid models. We suggested six alternative model designs that switched between the two modelling paradigms. By the example of a fish-plankton lake ecosystem we demonstrated that a well-designed switching hybrid model can be a performant modelling approach that retains relevant spatial and attributive information. Important findings with respect to optimising computational versus predictive performance were (1) the most plausible results were produced by a spatially explicit design based on spatial plankton stocks and fish switching between individual agents and aggregate school-agents, (2) higher levels of aggregation did not necessarily result in higher computational performance, and (3) adaptive, emergence-based triggers for the paradigm switches minimised information loss and could connect hierarchical and spatial scales. In conclusion, we argue to reach beyond efficiency-oriented considerations and use emergent super-individuals as structural elements of dynamically switching hybrid models.}
}
@article{BANIK2022106232,
title = {Geometric systems of unbiased representatives},
journal = {Information Processing Letters},
volume = {176},
pages = {106232},
year = {2022},
issn = {0020-0190},
doi = {https://doi.org/10.1016/j.ipl.2021.106232},
url = {https://www.sciencedirect.com/science/article/pii/S0020019021001472},
author = {Aritra Banik and Bhaswar B. Bhattacharya and Sujoy Bhore and Leonardo Martínez-Sandoval},
keywords = {Computational geometry, Systems of unbiased representatives, Bicolorings, Np-Hard problems, Geometric ranges},
abstract = {Let P be a finite point set in Rd, B be a bicoloring of P and O be a family of geometric objects (that is, intervals, boxes, balls, etc). An object from O is called balanced with respect to B if it contains the same number of points from each color of B. For a collection B of bicolorings of P, a geometric system of unbiased representatives (G-SUR) is a subset O′⊆O such that for any bicoloring B of B there is an object in O′ that is balanced with respect to B. We pose and study problems on finding G-SURs. We obtain general bounds on the size of G-SURs consisting of intervals, size-restricted intervals, axis-parallel boxes and Euclidean balls. We show that the G-SUR problem is NP-Hard even in the simple case of points on a line and interval ranges. Furthermore, we study a related problem on determining the size of the largest and smallest balanced intervals for points on the real line with a random distribution and coloring. Our results are a natural extension to a geometric context of the work initiated by Balachandran et al. (Discrete Mathematics, 2018) on arbitrary systems of unbiased representatives.}
}
@article{CIMBUROVA2023127839,
title = {Making trees visible: A GIS method and tool for modelling visibility in the valuation of urban trees},
journal = {Urban Forestry & Urban Greening},
volume = {81},
pages = {127839},
year = {2023},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2023.127839},
url = {https://www.sciencedirect.com/science/article/pii/S1618866723000109},
author = {Zofie Cimburova and Stefan Blumentrath and David N. Barton},
keywords = {Cultural ecosystem services, GIS, Tree valuation, Urban trees, Visibility analysis},
abstract = {Tree visibility is a key determinant of cultural ecosystem services of urban trees. This paper develops a flexible, efficient and easy-to-use GIS method for modelling individual tree visibility to support tree valuation. The method is implemented as a GRASS GIS AddOn tool called v.viewshed.impact, making it available to a broad spectrum of users and purposes. Thanks to empirically validated underlying algorithms and parallel processing, the method is accurate and fast in analysing high-resolution datasets and large numbers of trees. We demonstrate the method in two use cases in Oslo, Norway, showing that it provides an alternative to field-based assessment of visibility indicators in tree valuation methods and facilitates the inclusion of complex visibility indicators not possible to assess in the field. We argue that the method could also be used for tree management and planning, urban ecosystem accounting and neighbour conflict resolution related to trees.}
}
@incollection{BOGLE20031,
title = {Computer aided biochemical process engineering},
editor = {Andrzej Kraslawski and Ilkka Turunen},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {14},
pages = {1-10},
year = {2003},
booktitle = {European Symposium on Computer Aided Process Engineering-13},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(03)80082-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794603800822},
author = {I.D.L. Bogle},
abstract = {The growth of the biochemical industries is heating up in Europe after not meeting the initial expectations. CAPE tools have made some impact and progress on computer aided synthesis and design of biochemical processes is demonstrated on a process for the production of a hormone. Systems thinking is being recognised by the life science community and to gain genuinely optimal process solutions it is necessary to design right through from product and function to metabolism and manufacturing process. The opportunities for CAPE experts to contribute in the explosion of interest in the Life Sciences is strong if we think of the ‘Process’ in CAPE as any process involving physical or (bio-)chemical change.}
}
@article{STABLER1984155,
title = {Berwick and Weinberg on linguistics and computational psychology},
journal = {Cognition},
volume = {17},
number = {2},
pages = {155-179},
year = {1984},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(84)90017-9},
url = {https://www.sciencedirect.com/science/article/pii/0010027784900179},
author = {Edward P. Stabler}
}
@article{MCDONALD201955,
title = {Cognitive bots and algorithmic humans: toward a shared understanding of social intelligence},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {55-62},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301979},
author = {Kelsey R McDonald and John M Pearson},
abstract = {Questions of social behavior are simultaneously among the most fundamental in neuroscience and the most challenging in artificial intelligence. Yet despite decades of work, a unified perspective from the cognitive and computational approaches to the problem has yet to emerge. Recently, however, excitement around the challenges posed to reinforcement learning by multiplayer video games, coupled with the adoption of more complex modeling strategies in social neuroscience, has broadened the interface between the two fields. Here, we review recent progress from both directions, arguing that advances in artificial intelligence provide neuroscientists with valuable tools for modeling social interactions. At the same time, the study of humans as efficient social learners can inform the design of new algorithms for multi-agent systems. We conclude by encouraging a joint approach that incorporates the best of both domains to advance a shared picture of social intelligence.}
}
@incollection{CLAUSER20231,
title = {Past, present, and future of educational measurement},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-14},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.10001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305100016},
author = {Brian E. Clauser and Melissa J. Margolis},
keywords = {Karl Pearson, Francis Galton, Classical test theory, Item response theory, Generalizability theory, Frederic Lord, Lee Cronbach, Coefficient alpha, Validity theory, Charles Spearman, Eugenics movement, Spearman-Brown formula, Alfred Binet, Army Alpha test, Georg Rasch, Intelligence testing},
abstract = {This article provides an overview of the past, present, and future of educational measurement. We begin by examining the historical events in the 1800s that led to the development of a coherent mathematical theory of test scores in the first half of the 20th century. In this section we describe the contributions of Francis Galton, Karl Pearson, Charles Spearman, Truman Kelley, and Lee Cronbach. In addition to outlining the theoretical contributions of these researchers, we describe the rise of large-scale testing beginning with the Army Alpha test in 1917 and the administration of IQ tests to millions of school children in the decade that followed. We continue by discussing the current state of educational measurement theory and practice including the development and widespread use of item response theory, generalizability theory, validity theory, and large-scale national and international achievement testing to evaluate educational systems. Finally, we consider directions and developments that are likely to define the future of the field. These directions include increased use of computational power in assessment, the use of new sources of data (referred to as process data), automated systems to create test materials, and an increased emphasis on fairness.}
}
@article{ZHANG20231815,
title = {An intention inference method for the space non-cooperative target based on BiGRU-Self Attention},
journal = {Advances in Space Research},
volume = {72},
number = {5},
pages = {1815-1828},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0273117723003101},
author = {Honglin Zhang and Jianjun Luo and Yuan Gao and Weihua Ma},
keywords = {Space non-cooperative target, Intention inference, Time series, BiGRU, Self-attention mechanism},
abstract = {Intention inference for space non-cooperative targets is the key to space situational awareness and assistant decision for collision avoidance. Given that the problem of target intention inference is essential to learn the dynamically changing time-series characteristics of space non-cooperative target intentions and infer their relative motion patterns for threat warning, this paper adopts a deep learning-based approach, introduces a bidirectional propagation mechanism and self-attention mechanism based on Gated Recurrent Unit (GRU) and proposes a bidirectional Gated Recurrent Unit (BiGRU)-Self Attention-based space non-cooperative target intention inference model. BiGRU is used to learn deep information in time-series characteristics of the space non-cooperative target, and self-attention mechanism is used to adaptively extract and assign weights to key characteristics to capture the internal correlations in time-series information, thus improving model performance. The line-of-sight measurements are used as the characteristics of target intention inference, and the typical target motion intentions are defined. Subsequently, the proposed model is trained and tested on the test set, with the accuracy reaching 97.1%. Besides, the effectiveness and advantages of the proposed model are verified by the simulation of a case study and comparison evaluations. The results demonstrate that our proposed model could significantly improve the accuracy, computational efficiency, and noise resistance for the space non-cooperative target intention inference compared with the existing intention inference models.}
}
@incollection{ZHANG2022363,
title = {Chapter 18 - KPF: A retrospective view on urban planning AI for 2020},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {363-380},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000044},
author = {Snoweria Zhang and Kate Ringo and Richard Chou and Brandon Pachuca and Eric Pietraszkiewicz and Luc Wilson},
keywords = {Computational design, Digital twin, Urban design, Future history, City planning},
abstract = {Architectural historians have been fascinated by the year 1000, as the expectation of an impending apocalypse drove the sharp contrast between a dearth of construction before and a booming market after. One thousand years later, residents of 2020 found themselves at the crossroads again with the effects of climate change looming as a global threat. We constructed this chapter as a piece of a future, speculative, and historical document that examines the use of AI in urban planning and design in 2020. As historians from 2120, we study the evolution of tools at this critical junction with the backdrop of a confluence of crises. From explorative visual interfaces, open data initiatives, and computational design to AI that augments and collaborates with humans in the design and development of the city, we present case studies of both the technology and the projects that demonstrate some of the first applications of AI in negotiating the threat of climate change. Through these first examples, we trace the development of tools and corresponding trends in urban AI to the present year of 2120. The speculative narrative frame allows for an explication of the current urban design workflow using AI alongside an opportunity to conjecture where we believe AI development in design and planning ought to be. City makers in 2020 were not involved in the development of AI technologies. This work can act to inspire technologists who are envisioning the future of the city.}
}
@article{OSEIBRYSON20121156,
title = {A context-aware data mining process model based framework for supporting evaluation of data mining results},
journal = {Expert Systems with Applications},
volume = {39},
number = {1},
pages = {1156-1164},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.07.117},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411010797},
author = {Kweku-Muata Osei-Bryson},
keywords = {Context, Data mining process, KDDM, Evaluation, Decision analysis, Multi-criteria decision analysis, Post-processing},
abstract = {The knowledge discovery via data mining process (KDDM) is a multiple phase that aims to at a minimum semi-automatically extract new knowledge from existing datasets. For many data mining tasks, the evaluation phase is a challenging one for various reasons. Given this challenge several studies have presented techniques that could be used for the semi-automated evaluation of data mining results. When taken together, these studies suggest the possibility of a common multi-criteria evaluation framework. The use of such a multi-criteria evaluation framework, however, requires that relevant objectives, measures and preference function be identified. This implies that the context of the DM problem is particularly important for the evaluation phase of the KDDM process. Our framework utilizes and integrates a pair of established tightly coupled techniques (i.e. Value Focused Thinking (VFT) and the Goal–Question–Metric (GQM) methods) as well as established techniques from multi-criteria decision analysis in order to explicate and utilize context information in order to facilitate semi-automated evaluation.}
}
@article{DELEON2021281,
title = {Assessing the Efficacy of Tier 2 Mathematics Intervention for Spanish Primary School Students},
journal = {Early Childhood Research Quarterly},
volume = {56},
pages = {281-293},
year = {2021},
issn = {0885-2006},
doi = {https://doi.org/10.1016/j.ecresq.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0885200621000508},
author = {Sara C. {de León} and Juan E. Jiménez and Nuria Gutiérrez and Juan Andrés Hernández-Cabrera},
keywords = {RtI model, math, early grades, Tier 2, at-risk},
abstract = {This study explored the efficacy of a Tier 2 intervention within the context of the Response to Intervention (RtI) model implemented by Spanish first- to third-grade primary school teachers to improve at-risk students’ early math skills. Teachers were instructed in the administration of a math curriculum-based measure composed of 5 isolated measures (quantity discrimination, missing number, single-digit computation, multidigit computation, and place value) to identify at-risk students and to monitor their progress; and in the implementation of a systematic and explicit instructional program to improve basic math skills in at-risk students. Implementation fidelity was analyzed using direct observations and self-reports. The intervention was conducted with adequate fidelity and had a significant positive impact on all grades. Significant differences were found between experimental and control students at risk of math failure in the improvement rate of quantity discrimination, missing number, and place value in all grades. Experimental at-risk students showed a monthly improvement, assessed using a combination of screening and progress monitoring measures. In conclusion, Spanish first to third graders at risk of math failure benefited from a Tier 2 intervention based on basic math skills, implemented by in-service teachers.}
}
@article{WILKINS202440,
title = {We need to think differently about artificial intelligence},
journal = {New Scientist},
volume = {263},
number = {3509},
pages = {40-43},
year = {2024},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(24)01696-8},
url = {https://www.sciencedirect.com/science/article/pii/S0262407924016968},
author = {Alex Wilkins},
abstract = {Will AI ever emulate human intelligence? Professor of machine intelligence Neil Lawrence tells Alex Wilkins it is misleading to compare the two}
}
@article{BRAMSON2023105397,
title = {Emotion regulation from an action-control perspective},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {153},
pages = {105397},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105397},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423003664},
author = {Bob Bramson and Ivan Toni and Karin Roelofs},
keywords = {Emotion control, Emotion regulation, Emotional-action selection, Forward modelling},
abstract = {Despite increasing interest in emotional processes in cognitive science, theories on emotion regulation have remained rather isolated, predominantly focused on cognitive regulation strategies such as reappraisal. However, recent neurocognitive evidence suggests that early emotion regulation may involve sensorimotor control in addition to other emotion-regulation processes. We propose an action-oriented view of emotion regulation, in which feedforward predictions develop from action-selection mechanisms. Those can account for acute emotional-action control as well as more abstract instances of emotion regulation such as cognitive reappraisal. We argue the latter occurs in absence of overt motor output, yet in the presence of full-blown autonomic, visceral, and subjective changes. This provides an integrated framework with testable neuro-computational predictions and concrete starting points for intervention to improve emotion control in affective disorders.}
}
@incollection{DETALLE2017495,
title = {2.20 - Translational Aspects in Drug Discovery},
editor = {Samuel Chackalamannil and David Rotella and Simon E. Ward},
booktitle = {Comprehensive Medicinal Chemistry III},
publisher = {Elsevier},
address = {Oxford},
pages = {495-529},
year = {2017},
isbn = {978-0-12-803201-5},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.12335-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472123352},
author = {L. Detalle and K. Vanheusden and M.L. Sargentini-Maier and T. Stöhr},
keywords = {Animal model, Biomarker, Imaging, Modeling, Simulation, Translational medicine},
abstract = {The efficiency of drug development has seen a constant decline. This observation is somewhat paradoxical since during the same time there have been huge advancements in drug discovery and development technologies that made it much cheaper, faster, and easier to identify new drug targets and new drug molecules. Translational Science or Translational Medicine (TM) has arisen as an important discipline in modern drug discovery and development. It was triggered by the fact that many promising drugs failed in clinical trials. The challenge was thus to enhance the predictivity of the preclinical models and to design exploratory clinical trial designs and methodologies to test promising molecules earlier and faster. Despite some advancements, the number of drugs that finally receive regulatory approval is still at a low level. The main reason for this drug failure rate was a lack of efficacy observed in clinical trials of drug candidates that showed great promise in drug discovery. There may be two main factors responsible for this: (1) the industrialization of drug discovery and development led to huge specialized departments that operate in isolation. (2) Tools for successful translational research have only been developed in the last one or two decades. We will describe the tools used in translational research, that is, biomarkers, animal models, imaging, in silico modeling, and simulations. Their use will be illustrated with examples and tips of how to implement those into daily project work. We believe, however, that TM is more than these tools and technologies. It is not yet another discipline or department, it is a way of thinking that should become part of every discipline involved in drug development. Thus, in addition to describing the tools and how best to use them, we will elaborate how to design a translational research strategy and exemplify with some case studies as to how this has been successfully implemented in the past.}
}
@article{NAKHALAKEL20252288,
title = {System-theoretic analysis for the identification of emerging risks in the storage of dangerous substances},
journal = {Procedia Computer Science},
volume = {253},
pages = {2288-2295},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.289},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002972},
author = {Antonio Javier {Nakhal Akel} and Francesco Simone and Elena Stefana and Lorenzo Fedele and Riccardo Patriarca},
keywords = {STAMP, cyber attacks, socio-technical systems, operations management},
abstract = {The energy transition process lets novel risks emerge, impacting safety of modern industrial settings. The introduction of automation and digitalization fosters the collaboration and the interconnection between system agents (both humans and technologies) to comply with new sustainability objectives. Cyber-physical systems are increasingly present in industries, stressing the need to consider safety and security jointly. Systemic approaches, such as System-Theoretic Process Analysis (STPA), have been shown to be effective tools for dealing with such problems. This paper employs STPA to identify and analyse emergent risks within an energy transition scenario. Performing STPA permitted to identify control flaws and unsafe interactions when integrating renewable energy technologies. Results highlight critical agents and actions that may lead accidents. Specifically, a case study related to the storage of dangerous substances is presented in this paper, showing how tank’s automated controls may be susceptible to disruptions.}
}
@article{HAN2025101699,
title = {Understanding the role of virtual mobility on how and what people create in virtual reality},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101699},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101699},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002372},
author = {Eugy Han and Portia Wang and Cyan DeVeaux and Gabriella M. Harari and Jeremy N. Bailenson},
keywords = {Virtual reality, Creativity, Virtual mobility, Design creations},
abstract = {Virtual reality (VR) is considered a compelling tool to foster creativity by allowing its users to create in 3D space. However, the challenge lies in understanding how people use these tools and what they create, hindering the drawing of meaningful conclusions about VR as a viable tool for creativity. Furthermore, past research has shown that contextual factors shape how people create within VR, suggesting the existence of other factors. Here, we analyze the 3D creations of 137 participants responding to different creativity activities across seven sessions on a social VR platform. Specifically, we evaluate the role of virtual mobility, the capacity to move freely or have restricted movement in virtual space. We additionally present a VR-specific creativity coding scheme that follows recommendations from previous literature. Using dimensions derived from this coding scheme, we examine how these dimensions relate to behaviors and features of the creations in the context of virtual mobility. Results showed the significant role of virtual mobility on the design process, such that participants iterated and revised more by deleting more when their avatars were allowed to teleport and translate freely, compared to when their avatar’s movements were restricted to sitting down in virtual chairs. Furthermore, participants built shorter creations and took up less projection space with restricted virtual mobility. Results also showed that participants created more practical, unique, and well-implemented creations the more 3D models they used. Similarly, the more participants deleted, the more well-implemented the creations were. We discuss implications for designers of creation-oriented VR platforms and pedagogy for instructors facilitating activities in educational contexts.}
}
@article{BARON2022113861,
title = {Might pain be experienced in the brainstem rather than in the cerebral cortex?},
journal = {Behavioural Brain Research},
volume = {427},
pages = {113861},
year = {2022},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2022.113861},
url = {https://www.sciencedirect.com/science/article/pii/S0166432822001292},
author = {Mark Baron and Marshall Devor},
keywords = {Anesthesia, Brain evolution, Consciousness, Coma, Mesopontine tegmentum, MPTA},
abstract = {It is nearly axiomatic that pain, among other examples of conscious experience, is an outcome of still-uncertain forms of neural processing that occur in the cerebral cortex, and specifically within thalamo-cortical networks. This belief rests largely on the dramatic relative expansion of the cortex in the course of primate evolution, in humans in particular, and on the fact that direct activation of sensory representations in the cortex evokes a corresponding conscious percept. Here we assemble evidence, drawn from a number of sources, suggesting that pain experience is unlike the other senses and may not, in fact, be an expression of cortical processing. These include the virtual inability to evoke pain by cortical stimulation, the rarity of painful auras in epileptic patients and outcomes of cortical lesions. And yet, pain perception is clearly a function of a conscious brain. Indeed, it is perhaps the most archetypical example of conscious experience. This draws us to conclude that conscious experience, at least as realized in the pain system, is seated subcortically, perhaps even in the “primitive” brainstem. Our conjecture is that the massive expansion of the cortex over the course of evolution was not driven by the adaptive value of implementing consciousness. Rather, the cortex evolved because of the adaptive value of providing an already existing subcortical generator of consciousness with a feed of critical information that requires the computationally intensive capability of the cerebral cortex.}
}
@article{SHAWKY2023100547,
title = {Adaptive chaotic map-based key extraction for efficient cross-layer authentication in VANETs},
journal = {Vehicular Communications},
volume = {39},
pages = {100547},
year = {2023},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2022.100547},
url = {https://www.sciencedirect.com/science/article/pii/S2214209622000948},
author = {Mahmoud A. Shawky and Muhammad Usman and Muhammad Ali Imran and Qammer H. Abbasi and Shuja Ansari and Ahmad Taha},
keywords = {Chebyshev chaotic mapping, Cross-layer authentication, Doppler emulation, Physical-layer signatures, Secret key extraction, Vehicular ad-hoc networks},
abstract = {Vehicle-to-everything (V2X) communication is expected to offer users available and ultra-reliable transmission, particularly for critical applications related to safety and autonomy. In this context, establishing a secure and resilient authentication process with low latency and high functionality may not be achieved using conventional cryptographic methodologies due to their significant computation costs. Recent research has focused on employing the physical (PHY) characteristics of wireless channels to develop efficient discrimination techniques to overcome the shortcomings of crypto-based authentication. This paper presents a cross-layer authentication scheme for multicarrier communication, leveraging the spatially/temporally correlated wireless channel features to facilitate key verification without exposing its secrecy. By mapping the time-stamped hashed key and masking it with channel phase responses, we create a PHY-layer signature, allowing for verifying the sender's identity while employing the correlated channel responses between subcarriers to verify messages' integrity. Furthermore, we developed a Diffie-Hellman secret key extraction algorithm that employs the computationally intractable problems of the Chebyshev chaotic mapping for channel probing. Thus, terminals can extract high entropy shared keys that can be used to create dynamic PHY-layer signatures, supporting forward and backward secrecy. We evaluated the scheme's security strength against active/passive attacks. Besides theoretical analysis, we designed a 3-Dimensional (3D) scattering Doppler emulator to investigate the scheme's performance at different speeds of a moving vehicle and signal-to-noise ratios (SNRs) for a realistic vehicular channel. Theoretical and hardware implementation analyses proved the capability of the proposed scheme to support high detection probability at SNR ≥ 0 dB and speed ≤ 45 m/s.}
}
@article{OBRIEN2021104184,
title = {Misplaced trust: When trust in science fosters belief in pseudoscience and the benefits of critical evaluation},
journal = {Journal of Experimental Social Psychology},
volume = {96},
pages = {104184},
year = {2021},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2021.104184},
url = {https://www.sciencedirect.com/science/article/pii/S0022103121000871},
author = {Thomas C. O'Brien and Ryan Palmer and Dolores Albarracin},
keywords = {Misinformation, Trust in science, Critical thinking, Methodological literacy},
abstract = {At a time when pseudoscience threatens the survival of communities, understanding this vulnerability, and how to reduce it, is paramount. Four preregistered experiments (N = 532, N = 472, N = 605, N = 382) with online U.S. samples introduced false claims concerning a (fictional) virus created as a bioweapon, mirroring conspiracy theories about COVID-19, and carcinogenic effects of GMOs (Genetically Modified Organisms). We identify two critical determinants of vulnerability to pseudoscience. First, participants who trust science are more likely to believe and disseminate false claims that contain scientific references than false claims that do not. Second, reminding participants of the value of critical evaluation reduces belief in false claims, whereas reminders of the value of trusting science do not. We conclude that trust in science, although desirable in many ways, makes people vulnerable to pseudoscience. These findings have implications for science broadly and the application of psychological science to curbing misinformation during the COVID-19 pandemic.}
}
@article{TRAUSANMATU20231052,
title = {Identification of creativity in collaborative conversations based on the polyphonic model},
journal = {Procedia Computer Science},
volume = {221},
pages = {1052-1057},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.087},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923008451},
author = {Stefan Trausan-Matu},
keywords = {polyphonic model, creativity, brainstorming, collaboration, computer-supported collaborative learning, natural language processing, deep learning},
abstract = {The paper presents a theoretical approach and a set of experiments that operationalize it for the identification of creative moments in conversations. State-of-the-art artificial intelligence technology is used for the operationalization: natural language processing, machine learning, and deep neural networks The approach is based on the polyphonic model introduced by Trausan-Matu, which starts from Mikhail Bakhtin's analogy of discourse building in texts with polyphonic music. The divergent and convergent steps of creativity are related to the inter-animation of voices through dissonances and consonances in polyphonic, contrapuntal music.}
}
@article{LIU2020102176,
title = {Using a new approach for revealing the spatiotemporal patterns of functional urban polycentricity: A case study in the Tokyo metropolitan area},
journal = {Sustainable Cities and Society},
volume = {59},
pages = {102176},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102176},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720301633},
author = {Kai Liu and Yuji Murayama and Toshiaki Ichinose},
keywords = {Functional urban area detection, Functional urban polycentricity, Multi-step Decision-making Newman algorithm, Tokyo Master Plan, Tokyo metropolitan area},
abstract = {This research designs a new approach by modifying the Fast-Newman algorithm for better implementing the process of detecting functional urban areas (FUAs) and further revealing the spatiotemporal patterns of functional urban polycentricity through 20 view-windows of each FUA in the Tokyo metropolitan area (TMA) by using geo-tagged big data. Through the 20 view-windows of our GIS microscope, it is possible to uncover patterns of functional connections and daily urban rhythms under the same layout of the functional urban structure in the TMA. Furthermore, our findings can elucidate the double-sided thinking by combining the explanations of functional urban polycentricity with the policy effects of the Tokyo Master Plan (TMP) from the perspective of area-byarea analysis across the entire TMA. Our results imply that the functional urban structure of the TMA is a four-level, annular pattern. The TMP still has room for the improvement toward sustainable urban planning in the TMA. Based on the investigation on the patterns of functional urban polycentricity, this research has obtained many hints and clues for improving the TMP. Rethinking the effectiveness of the TMP can also provide trustworthy academic verification and provide suggestions about concrete amendments that can enlighten future urban planning.}
}