@article{BURRIS1992175,
title = {Discriminator varieties and symbolic computation},
journal = {Journal of Symbolic Computation},
volume = {13},
number = {2},
pages = {175-207},
year = {1992},
issn = {0747-7171},
doi = {https://doi.org/10.1016/S0747-7171(08)80089-2},
url = {https://www.sciencedirect.com/science/article/pii/S0747717108800892},
author = {Stanley Burris},
abstract = {We look at two aspects of discriminator varieties which could be of considerable interest in symbolic computation:1.discriminator varieties are unitary (i.e., there is always a most general unifier of two unifiable terms), and2.every mathematical problem can be routinely cast in the form†p1 ≈ q1, …, pk ≈ qk implies the equation x ≈ y. Item (l) offers possibilities for implementations in computational logic, and (2) shows that Birkhoff's five rules of inference for equational logic are all one needs to prove theorems in mathematics.}
}
@article{ZHANG2010S238,
title = {Biomimetic Construction of Category Mental Imagery Based on Recognition Mechanism of Visual Cortex of Human Brain},
journal = {Journal of Bionic Engineering},
volume = {7},
pages = {S238-S244},
year = {2010},
issn = {1672-6529},
doi = {https://doi.org/10.1016/S1672-6529(09)60241-9},
url = {https://www.sciencedirect.com/science/article/pii/S1672652909602419},
author = {Xianghe Zhang and Dan Wang and Luquan Ren and Pingping Liu},
keywords = {multi-dimensional space biomimetic informatics, artificial intelligence, cognitive science, mental imagery, visual cortex, object recognition},
abstract = {The principle of homology-continuity in Multi-Dimensional Biomimetic Informatics Space is applied to construct the identifying mechanism of category of deep representation of mental imagery. The model of each cerebral region involved in recognizing is established respectively and a feedforward method for establishing category mental imagery is proposed. First, the model of feature acquisition is developed based on Hubel-Wiesel model, and Gaussian function is used to simulate the simple cell receptive field to satisfy the specific function of visual cortex. Second, multiple input aggregation operation is employed to simulate the feature output of complex cells to get the invariance representation in feature space. Then, imagery basis is extracted by unsupervised learning algorithm based on the primary feature and category mental imagery is obtained by building Radial Basis Function (RBF) network. Finally, the system model is tested by training set and test set composed of real images. Experimental results show that the proposed method can establish valid deep representation of these samples, based on which the biomimetic construction of category mental imagery can be achieved. This method provides a new idea for solving imagery problem and studying imagery thinking.}
}
@article{SCHUMNY1993319,
title = {Nanosystems - molecular machinery, manufacturing, and computation: by K. Eric Drexler. John Wiley & Sons, Chichester, England, 1992. ISBN 0-471-57518-6. 556 pages. Illustrated, Appendices, Glossary with detailed explanations, 337 references, extended Index.},
journal = {Computer Standards & Interfaces},
volume = {15},
number = {2},
pages = {319-320},
year = {1993},
note = {Object-Oriented Reference Models},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(93)90019-N},
url = {https://www.sciencedirect.com/science/article/pii/092054899390019N},
author = {Harald Schumny}
}
@article{CUI2024101074,
title = {AI-enhanced collective intelligence},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101074},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101074},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002332},
author = {Hao Cui and Taha Yasseri},
keywords = {AI, collective intelligence, hybrid intelligence, multi-agent systems, human-machine networks, human-machine intelligence},
abstract = {Summary
Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents’ diversity and interactions influence the system’s collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.}
}
@article{VADIATI2025100120,
title = {(En) coding care into digital urbanism: Vignettes of collective practices},
journal = {Digital Geography and Society},
volume = {8},
pages = {100120},
year = {2025},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2025.100120},
url = {https://www.sciencedirect.com/science/article/pii/S2666378325000091},
author = {Niloufar Vadiati and Letizia Chiappini and Martin Bangratz},
keywords = {Care, Digital urbanism, Refusing, Commoning, Reappropriating},
abstract = {The tech-entrepreneurial model behind the computation of urban processes is (re) producing what has already been identified as a technocratic, solutionist, and commodifying model of urban planning. Within this model, not only is care no longer a prerequisite for urban production, but decades of smartification and platformization have been diminishing the spaces, infrastructures, and socio-economic relations that were co-produced to enable care. Through the lens of feminist geography, care is examined as a multidimensional concept encompassing socio-spatial dynamics, power relations, and ethical urban practices. Using empirical data from three research projects, the study showcases alternative digital urbanism practices, categorized into three vignettes: refusal, commoning, and reappropriation. These categories are illustrated with cases such as grassroots food cooperatives, feminist hack-spaces, digital sovereignty initiatives, platform-based welfare experiments and civil society initiatives such as Code for Germany. By situating care within the spatial and social fabric of urban life, the paper argues for its potential as a politic, practice, and epistemology that challenges the exploitative logic of contemporary digital infrastructures. The findings reveal the embeddedness of care practices within local contexts, highlighting the dual need for trans-local networks and territorial embeddedness. This study contributes to the discourse on caring digital urbanism, advancing a feminist theorisation of everyday digital urbanism.}
}
@article{CHEN2025107442,
title = {Exploring quantum neural networks for binary classification on MNIST dataset: A swap test approach},
journal = {Neural Networks},
volume = {188},
pages = {107442},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107442},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025003211},
author = {Kehan Chen and Jiaqi Liu and Fei Yan},
keywords = {Quantum neural networks, Swap test, Quantum activation function, Binary classification},
abstract = {In this study, we propose a novel modularized Quantum Neural Network (mQNN) model tailored to address the binary classification problem on the MNIST dataset. The mQNN organizes input information using quantum images and trainable quantum parameters encoded in superposition states. Leveraging quantum parallelism, the model efficiently processes inner product calculations of quantum neurons via the swap test, achieving constant complexity. To enhance the expressive capacity of the mQNN, nonlinear transformations, specifically quantum versions of activation functions, are integrated into the quantum network. The mQNN’s circuits are constructed from flexible quantum modules, allowing the model to adapt its structure based on varying input data types and scales for optimal performance. Furthermore, rigorous mathematical derivations are employed to validate the quantum state evolution during computation within a quantum neuron. Testing on the Pennylane platform simulates the quantum environment and confirms the mQNN’s effectiveness on the MNIST dataset. These findings highlight the potential of quantum computing in advancing image classification tasks.}
}
@article{YOSHIOKA2024114985,
title = {An escort replicator dynamic with a continuous action space and its application to resource management},
journal = {Chaos, Solitons & Fractals},
volume = {185},
pages = {114985},
year = {2024},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2024.114985},
url = {https://www.sciencedirect.com/science/article/pii/S096007792400537X},
author = {Hidekazu Yoshioka},
keywords = {Evolutionary game, Escort replicator dynamic, Kaniadakis escort function, Numerical computation, Application to sustainable resource management},
abstract = {The escort replicator dynamic (ERD) is a version of the replicator dynamic in evolutionary games where the utility-driven decision-making process is modulated due to the information costs to be paid by players. The escort function as a coefficient to distort the decision-making determines the behavior of solutions to the ERD, whereas its investigations are still not sufficient. Particularly, the ERD was investigated in finite-action settings in the previous studies, while that with a continuum of actions has not been studied well. In this paper, we formulate and analyze the ERD with a continuum of actions represented by a bounded interval. Our ERD is a partial integro-differential equation whose well-posedness is nontrivial because of specific nonlocal terms arising from the escort function. The Kaniadakis escort function is chosen as a major example of the escort function, with which we obtain the unique existence of solutions to the ERD. We also discuss cases with the other escort functions, such as the power and constant ones, and suggest that the growth and regularity behaviors of the escort function are crucial. Finally, we computationally apply the ERD to problems related to sustainable environmental and resource management.}
}
@article{BOURDAKOU2023102881,
title = {Drug repurposing on Alzheimer's disease through modulation of NRF2 neighborhood},
journal = {Redox Biology},
volume = {67},
pages = {102881},
year = {2023},
issn = {2213-2317},
doi = {https://doi.org/10.1016/j.redox.2023.102881},
url = {https://www.sciencedirect.com/science/article/pii/S2213231723002823},
author = {Marilena M. Bourdakou and Raquel Fernández-Ginés and Antonio Cuadrado and George M. Spyrou},
keywords = {Alzheimer's disease, NRF2, , Association network,  drug repurposing, Differentially expressed genes},
abstract = {Alzheimer's disease (AD) is an age-dependent neurodegenerative disorder and the most common cause of cognitive decline. The alarming epidemiological features of Alzheimer's disease, combined with the high failure rate of candidate drugs tested in the preclinical phase, impose more intense investigations for new curative treatments. NRF2 (Nuclear factor-erythroid factor 2-related factor 2) plays a critical role in the inflammatory response and in the cellular redox homeostasis and provides cytoprotection in several diseases including those in the neurodegeneration spectrum. These roles suggest that NRF2 and its directly associated proteins may be novel attractive therapeutic targets in the fight against AD. In this study, through a systemics perspective, we propose an in silico drug repurposing approach for AD, based on the NRF2 interactome and regulome, with the aim of highlighting possible repurposed drugs for AD. Using publicly available information based on differential expressions of the NRF2-neighborhood in AD and through a computational drug repurposing pipeline, we derived to a short list of candidate repurposed drugs and small molecules that affect the expression levels of the majority of NRF2-partners. The relevance of these findings was assessed in a four-step computational meta-analysis including i) structural similarity comparisons with currently ongoing NRF2-related drugs in clinical trials ii) evaluation based on the NRF2-diseasome iii) comparison of relevance between targeted pathways of shortlisted drugs and NRF2-related drugs in clinical trials and iv) further comparison with existing knowledge on AD and NRF2-related drugs in clinical trials based on their known modes of action. Overall, our analysis yielded in 5 candidate repurposed drugs for AD. In cell culture, these 5 candidates activated a luciferase reporter for NRF2 activity and in hippocampus derived TH22 cells they increased NRF2 protein levels and the NRF2 transcriptional signatures as determined by increased expression of its downstream target heme oxygenase 1. We expect that our proposed candidate repurposed drugs will be useful for further research and clinical translation for AD.}
}
@article{WORTMANN2017173,
title = {Differentiating parametric design: Digital workflows in contemporary architecture and construction},
journal = {Design Studies},
volume = {52},
pages = {173-197},
year = {2017},
note = {Parametric Design Thinking},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300352},
author = {Thomas Wortmann and Bige Tunçer},
keywords = {parametric design, design automation, architectural design, software design, parametric master model},
abstract = {This paper examines Parametric Design (PD) in contemporary architectural practice. It considers three case studies: The Future of Us pavilion, the Louvre Abu Dhabi and the Morpheus Hotel. The case studies illustrate how, compared to non-parametrically and older parametrically designed projects, PD is employed to generate, document and fabricate designs with a greater level of detail and differentiation, often at the level of individual building components. We argue that such differentiation cannot be achieved with conventional Building Information Modelling and without customizing existing software. We compare the case studies' PD approaches (objected-oriented programming, functional programming, visual programming and distributed visual programming) and decomposition, algorithms and data structures as crucial factors for the practical viability of complex parametric models and as key aspects of PD thinking.}
}
@article{CHRISTENSEN2016125,
title = {Towards a formal assessment of design literacy: Analyzing K-12 students' stance towards inquiry},
journal = {Design Studies},
volume = {46},
pages = {125-151},
year = {2016},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X1530140X},
author = {Kasper Skov Christensen and Mikkel Hjorth and Ole Sejer Iversen and Paulo Blikstein},
keywords = {design education, design research, reflective practices, evaluation},
abstract = {We present a tool for quantitative assessment of K-12 students' stance towards inquiry as an important part of students' development of design literacy. On a basis of design thinking literature, we position designerly stance towards inquiry as a prerequisite for engaging with wicked problems. The Design Literacy (DeL) assessment tool contains design of a qualitative survey question, a coding scheme for assessing aspects of a designerly stance towards inquiry, and a description of how, we have validated the results through a large-scale survey administration in K-12 education. Our DeL tool is meant to provide educators, leaders, and policy makers with strong arguments for introducing design literacy in K-12 schools, which, we posit, function within in an age of measurement.}
}
@article{TABACHNECKSCHIJF1997305,
title = {CaMeRa: A computational model of multiple representations},
journal = {Cognitive Science},
volume = {21},
number = {3},
pages = {305-350},
year = {1997},
note = {Advances in analogy research: Integration of theory and data from the cognitive, computational, and neural sciences},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(99)80026-3},
url = {https://www.sciencedirect.com/science/article/pii/S0364021399800263},
author = {Hermina J.M. Tabachneck-Schijf and Anthony M. Leonardo and Herbert A. Simon},
abstract = {This research aims to clarify, by constructing and testing a computer simulation, the use of multiple representations in problem solving, focusing on their role in visual reasoning. The model is motivated by extensive experimental evidence in the literature for the features it incorporates, but this article focuses on the system's structure. We illustrate the model's behavior by simulating the cognitive and perceptual processes of an economics expert as he teaches some well-learned economics principles while drawing a graph on a blackboard. Data in the experimental literature and concurrent verbal protocols were used to guide construction of a linked production system and parallel network, CaMeRa (Computation with Multiple Representations), that employs a “Mind's Eye” representation for pictorial information, consisting of a bitmap and associated node-link structures. Propositional list structures are used to represent verbal information and reasoning. Small individual pieces from the different representations are linked on a sequential and temporary basis to form a reasoning and inferencing chain, using visually encoded information recalled to the Mind's Eye from long-term memory and from cues recognized on an external display. CaMeRa, like the expert, uses the diagrammatic and verbal representations to complement one another, thus exploiting the unique advantages of each.}
}
@incollection{KIRWAN202047,
title = {Chapter 3 - Strategies, planning, and design},
editor = {Christopher Kirwan and Fu Zhiyong},
booktitle = {Smart Cities and Artificial Intelligence},
publisher = {Elsevier},
pages = {47-67},
year = {2020},
isbn = {978-0-12-817024-3},
doi = {https://doi.org/10.1016/B978-0-12-817024-3.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128170243000039},
author = {Christopher Kirwan and Fu Zhiyong},
keywords = {Citizen Engagement, Co-design, Data Visualization, Design Thinking, Generative Design, Information Architecture, Living Lab, Metadesign, Real-time Data, Simulation, Transdisciplinary Methods, User Experience},
abstract = {Traditional planning and design methodologies can now be augmented by new innovative tools and processes enabled by AI and smart technologies. These facilitate a more open-ended, multi-dimensional approach that incorporates diverse stakeholders to shape the potential of a collective intelligent operating system — one that best reflects the inherent nature of each unique city and urban condition. The design of smart cities must incorporate and adapt a combination of universal standards and localized policies through global civil society organizations and public-private-people partnerships established to serve the user and citizen as participants of the living city. New methods including co-design, co-creation, citizen participation and user experience (UX) feedback foster inclusive cities. Living labs and innovation hubs provide opportunities and spaces to prototype such initiatives. Transdisciplinary approaches are needed more than ever to expand our scope of inclusion to all life forms, including the rights of animals and nature as stakeholders. By applying a new combination of human and AI-enabled methods such as design thinking, machine learning and generative design, cities can now augment and improve their current state seamlessly, integrating technologies and management as an autopoietic smart city operating system.}
}
@incollection{ZHANG2021440,
title = {The Use and Value of Geographic Information Systems in Transportation Modeling},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {440-447},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10364-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717103641},
author = {Ming Zhang},
keywords = {Accessibility modeling, Big-data, Four-step models, Geographic information systems (GIS), Land use-transportation integrated (LUTI) models, New mobility, Relational database, Topological data structure, Traffic simulation, Transportation modeling, Visualization},
abstract = {Geographic information systems (GIS) and transportation modeling have been developing in their respective fields separately. However, the geographic focus of GIS and the geographic nature of transportation make the use of GIS in transportation modeling both a logical choice and a motivation for modeling enhancement. This chapter first explains how the topological data structure underlying the design of vector GIS fits well the needs of data handling for transportation modeling. It then discusses the use and value of GIS in transportation modeling in three areas: (1) GIS for the four-step transportation models and for specific transportation modeling interests such as accessibility modeling and land use-transportation integrated (LUTI) modeling; (2) GIS visualization capabilities for presenting transportation modeling output and for facilitating visual thinking and knowledge construction; and (3) the potential of and challenges to GIS in transportation modeling amid rapid development of big-data technologies and new mobility.}
}
@article{RODRIGUEZMENDEZ2024103804,
title = {UK net-zero policy design – from optimisation to robustness},
journal = {Environmental Science & Policy},
volume = {158},
pages = {103804},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103804},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124001382},
author = {Quirina {Rodriguez Mendez} and Mark Workman and Geoff Darch},
keywords = {Robust Decision Making, Deep Uncertainty, Greenhouse Gas Removal, Climate modelling, United Kingdom Net-Zero target},
abstract = {The need to deal with the deep uncertainty and system complexity associated to Net-Zero pathways, especially those relying on emergent greenhouse gas removal (GGR) technologies, has resulted in a growing body of literature on alternative decision-support approaches. Exploratory modelling, and specifically Robust Decision Making (RDM), are potential approaches capable of addressing these challenges: by exploring a wide range of conceivable futures, they explicitly embrace deep uncertainties while seeking to reduce system vulnerabilities. However, though RDM methods have been well documented, there is little insight as to how such approach might be integrated into Net-Zero policy design processes. By means of a workshop (n=17) and interviews (n=13) with the UK climate policy and energy modelling communities, this contribution provides insights into the role and potential of RDM in explicitly dealing with the deep uncertainties that pervade in the establishment of a 60–100 MtCO2 UK GGR sector within three decades. The consultation process revealed that there is an appetite from the decision-making and analytical communities in integrating exploratory modelling concepts into UK policy design processes. It is recommended that to bridge the gap between theoretical RDM constructs and their broader adoption, the analytical process should include a broader set of disciplines and expertise. Specifically for the modelling community, this work suggests that in-use computational models should be adapted, rather than new tools developed. Key challenges also arise from the time and resources required, suggesting small scale place-based pilots could promote the acceptability and foster the adoption of the RDM methodology.}
}
@incollection{AKAL202471,
title = {Chapter Four - AI methods in microbial metabolite determination},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {55},
pages = {71-85},
year = {2024},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 1},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000023},
author = {H. Ceren Akal and Rumeysa Nur Kara-Aktaş and Sebnem Ozturkoglu-Budak},
keywords = {Metabolite, Microorganism-derived, Computational, Artificial intelligence},
abstract = {The multitude of microorganism species and the amount of data requiring examination is increasing day by day, which has made it very difficult to make informative determinations and analysis to be conducted by human labour. Artificial intelligence (AI) applications are crucial in mitigating these difficulties. AI is a multidisciplinary field that tries to imitate human-like abilities through learning, analysing, problem-solving and interpretation via digital systems. It can take part in many fields where human labour is required. It is widely used in various scientific disciplines and industries, including biotechnology, microbiology, medicine, etc. Machine learning, a subbranch of AI, is one of the most frequently used auxiliary methods. Critical topics are examined rapidly and meaningfully via machine-learning such as drug production, microbial detection, antimicrobial resistance, vaccine predictions, and disease diagnoses. The aim of this chapter is to highlight the relevance of computational methods for the determination of microbial metabolites which are mainly described in literatures. These computational methods are related with the advanced AI tools of data/genome mining, multivariate data analysis, molecular networking, mathematical modelling, and optimization. These novel methods create new perspectives to the isolation and/or determination of microbial metabolites which are unwanted or essential to human health.}
}
@article{BRADLEY2016277,
title = {Pilot Testing the Debriefing for Meaningful Learning Evaluation Scale},
journal = {Clinical Simulation in Nursing},
volume = {12},
number = {7},
pages = {277-280},
year = {2016},
issn = {1876-1399},
doi = {https://doi.org/10.1016/j.ecns.2016.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1876139916000104},
author = {Cynthia Sherraden Bradley and Kristina Thomas Dreifuerst},
keywords = {DML, debriefing, effective briefing, debriefing evaluation, measurement},
abstract = {Background
Debriefing for Meaningful Learning (DML), an evidence-based debriefing method, promotes thinking like a nurse through reflective learning. Despite widespread adoption of DML, little is known about how well it is implemented. To assess the effectiveness of DML implementation, an evaluative rubric was developed and tested.
Sample
Three debriefers who had been trained to use DML at least 1 year previously, submitted five recorded debriefings each for evaluation.
Methods
Three raters who were experts in DML scored each of the 15 recorded debriefing session using DML Evaluation Scale (DMLES). Observable behaviors were scored with binary options. These raters also assessed the items in the DMLES for content validity.
Results
Cronbach's alpha, intraclass correlation coefficients, and Content Validity Index scores were calculated to determine reliability and validity.
Conclusion
Use of DMLES could support quality improvement, teacher preparation, and faculty development. Future testing is warranted to investigate the relationship between DML implementation and clinical reasoning.}
}
@article{LIN2024200448,
title = {Maximizing the spread of information through content optimization},
journal = {Intelligent Systems with Applications},
volume = {24},
pages = {200448},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200448},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001224},
author = {Lei Lin and Yihua Du and Shibo Zhao and Wenkang Jiang and Qirui Tang and Li Xu},
keywords = {Computational social science, Human-in-the-loop, System simulation and optimization, Computational journalism, Computational advertisement},
abstract = {As data-driven prediction models advance, an increasing number of people are enjoying news personalized to their interests. The primary problem such recommendation models solve is to precisely match information with users and, in so doing, ensure that news spreads with greater efficiency. However, these techniques only help the media platform; they do not help those who produce the news. Hence, we devised a propagation framework based on a human-in-the-loop simulation that helps content authors maximize the spread of their messages through social networks. The framework works by acting on feedback provided by the simulation model. Additionally, the spread of information is formulated as a multi-objective optimization problem in which propagation is data-driven and simulated with machine learning techniques that leverage data on the historical behaviors of users. We additionally describe an implementation for this framework as an example of how the framework might be used in real life. On the practical side, the implementation uses text data from a blog to simulate the message's propagation, while, from a technical point of view, the multi-objective optimization problem is divided into an information retrieval problem and an integer programming problem, the results of which are fed back into the content editor as content operation strategies. A case study with the Sina Weibo microblog site not only validates the framework but also provides practitioners with insights into how to maximize the spread of information through social networking platforms. The results show that the proposed propagation framework is capable of increasing retweets by 7.9575 %. As an interesting aside, our experiments also show that the Weibo retweet lottery is both popular and a highly effective mechanism for increasing reposts.}
}
@article{CAMARGO20181116,
title = {A method for integrated process simulation in the mining industry},
journal = {European Journal of Operational Research},
volume = {264},
number = {3},
pages = {1116-1129},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717306409},
author = {Luis Felipe Riehs Camargo and Luis Henrique Rodrigues and Daniel Pacheco Lacerda and Fabio Sartori Piran},
keywords = {O.R. in natural resources, Production, Simulation, Systems thinking},
abstract = {This paper proposes a method of Integrated Process Simulation (MIPS), which considers the dynamic, stochastic and systemic characteristics of mining operations to support investment decisions in this industry. This MIPS supports development of a Decision Support System (DSS) that considers product quality, process productivity and production costs. A case study is described that used the MIPS to make better investment decisions. The MIPS has proven, in practice, to be effective in several applications; for example, in defining the maintenance policy for critical equipment in an iron ore concentration plant; the process for removing impurities and simulating the company's budget to evaluate the viability of different business plans.}
}
@article{PATON200263,
title = {Process, structure and context in relation to integrative biology},
journal = {Biosystems},
volume = {64},
number = {1},
pages = {63-72},
year = {2002},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00176-9},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001769},
author = {Ray Paton},
keywords = {Ecology, Proteins, Category theory, Modelling, Function, Liver},
abstract = {This paper seeks to provide some integrative tools of thought regarding biological function related to ideas of process, structure, and context. The incorporation of linguistic and mathematical thinking is discussed within the context of managing thinking about natural systems as described by Robert Rosen. Examples from ecology, protein networks, and liver function are introduced to illustrate key ideas. It is hoped that these tools of thought, and the further work needed to mobilise such ideas, will continue to address a number of issues raised and pursued by Michael Conrad, such as the seed-germination model and vertical information processing.}
}
@article{FORREST19901,
title = {Emergent computation: Self-organizing, collective, and cooperative phenomena in natural and artificial computing networks: Introduction to the proceedings of the ninth annual CNLS conference},
journal = {Physica D: Nonlinear Phenomena},
volume = {42},
number = {1},
pages = {1-11},
year = {1990},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(90)90063-U},
url = {https://www.sciencedirect.com/science/article/pii/016727899090063U},
author = {Stephanie Forrest}
}
@incollection{OGRADY2020135,
title = {Cyber Security},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {135-141},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10532-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105323},
author = {Nathaniel O'Grady and Andrew C. Dwyer},
keywords = {Computing, Cybersecurity, Cyberspace, Digital, Infrastructure, Networks, Privacy, Software, Surveillance},
abstract = {As computation has become increasingly integrated into everyday life, critical infrastructure, state defense, and cybersecurity has become a new, crucial area of inquiry for geographers. This is due to the fast-changing, new securities that are being formed and enabled through, by and because of the growing role of computation. Geographers have studied cybersecurity as collectively constituted through a complex mixture of technologies, materialities, cultures, knowledges. In so doing, they have probed a range of phenomena crucial to cybersecurity; from technical processes such as encryption, malware infection, and threat detection, to the social arrangements and negotiations between various organizations and states, the implications of surveillance and big data on privacy, and how threats affect various infrastructure that support ways of life across the globe. Nevertheless, geographers do not simply consider cybersecurity as a mode of security imposed “online” or through digital technologies. Rather, in its practice, geographers have demonstrated how cybersecurity involves and invokes socio-political complications around criminality, protection, inequalities, privacy, surveillance, private enterprise, and the role of the state in the life of citizens.}
}
@article{BYLANDER1994165,
title = {The computational complexity of propositional STRIPS planning},
journal = {Artificial Intelligence},
volume = {69},
number = {1},
pages = {165-204},
year = {1994},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)90081-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370294900817},
author = {Tom Bylander},
abstract = {I present several computational complexity results for propositional STRIPS planning, i.e., STRIPS planning restricted to ground formulas. Different planning problems can be defined by restricting the type of formulas, placing limits on the number of pre-and postconditions, by restricting negation in pre- and postconditions, and by requiring optimal plans. For these types of restrictions, I show when planning is tractable (polynomial) and intractable (NP-hard). In general, it is PSPACE-complete to determine if a given planning instance has any solutions. Extremely severe restrictions on both the operators and the formulas are required to guarantee polynomial time or even NP-completeness. For example, when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. When definite Horn ground formulas are permitted, determining plan existence is PSPACE-complete even if operators are limited to zero preconditions and one postcondition. One of the interesting tractable problems is if each operator is restricted to positive preconditions and one postcondition (only ground literals). The blocks-world problem, slightly modified, is a subproblem of this restricted planning problem. These results in combination with previous analyses are not encouraging for domain-independent planning.}
}
@article{BAICANG2025129857,
title = {Multi-modal information fusion for multi-task end-to-end behavior prediction in autonomous driving},
journal = {Neurocomputing},
volume = {634},
pages = {129857},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129857},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005296},
author = {Guo Baicang and Liu Hao and Yang Xiao and Cao Yuan and Jin Lisheng and Wang Yinlin},
keywords = {Autonomous driving, Multi-modal fusion, Vehicle behavior prediction, End-to-End, Attention mechanism},
abstract = {Behavior prediction in autonomous driving is increasingly achieved through end-to-end frameworks that predict vehicle states from multi-modal information, streamlining decision-making and enhancing robustness in time-varying road conditions. This study proposes a novel multi-modal information fusion-based, multi-task end-to-end model that integrates RGB images, depth maps, and semantic segmentation data, enhancing situational awareness and predictive precision. Utilizing a Vision Transformer (ViT) for comprehensive spatial feature extraction and a Residual-CNN-BiGRU structure for capturing temporal dependencies, the model fuses spatiotemporal features to predict vehicle speed and steering angle with high precision. Through comparative, ablation, and generalization tests on the Udacity and self-collected datasets, the proposed model achieves steering angle prediction errors of MSE 0.012 rad, RMSE 0.109 rad, and MAE 0.074 rad, and speed prediction errors of MSE 0.321 km/h, RMSE 0.567 km/h, and MAE 0.373 km/h, outperforming existing driving behavior prediction models. Key contributions of this study include the development of a channel difference attention mechanism and advanced spatiotemporal feature fusion techniques, which improve predictive accuracy and robustness. These methods effectively balance computational efficiency and predictive performance, contributing to practical advancements in driving behavior prediction.}
}
@article{YUZGEC2025113169,
title = {Accelerated opposition learning based chaotic single candidate optimization algorithm: A new alternative to population-based heuristics},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113169},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113169},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002163},
author = {Ugur Yuzgec},
keywords = {Opposition learning, Chaotic, Single candidate, Engineering design, Benchmark},
abstract = {This study considers the Single Candidate Optimizer (SCO) as an alternative to population-based heuristics, that is faster than them. Although the SCO algorithm is a fast single-candidate-based heuristic, it has certain limitations. To overcome these limitations and enhance the search performance of SCO, several solutions were proposed in this study. First, owing to the single-candidate nature of the SCO, the initial solution position can play a critical role. To compensate for this, an accelerated opposition-learning mechanism was integrated into the SCO. In addition, instead of the equation that is active when the number of unsuccessful improvement attempts is reached in the SCO structure, a mutation operator including chaotic functions (Levy, Gauss, and Cauchy) has been incorporated into the algorithm. Again, equations based on new approaches were added to the SCO algorithm to update the position of the candidate solution during the exploration and exploitation phases. Finally, the standard boundary value control mechanism is replaced with a more effective one. The algorithm developed in this study is named Accelerated Opposition Learning based Chaotic Single Candidate Optimizer (AccOppCSCO), inspired by the accelerated opposition learning mechanism and the mutation operator involving chaotic behaviors. The search capability of the proposed AccOppCSCO algorithm was first analyzed using four different methods: convergence, search history, trajectory, and computational complexity. The effectiveness of the mechanisms used in the AccOppCSCO algorithm for four different two-dimensional benchmark problems from the IEEE Congress on Evolutionary Computation 2014 (CEC2014) package was demonstrated. Subsequently, the performance of the proposed AccOppCSCO algorithm was evaluated on the CEC2014 and IEEE Congress on Evolutionary Computation 2020 (CEC2020) benchmark problems with different dimensions. The results show that the AccOppCSCO algorithm works effectively in the CEC2014 and CEC2020 test sets and offers better optimization results than SCO. The AccOppCSCO algorithm ranked first in the overall evaluation of the 30-dimensional CEC2014 comparison results with State of the Art (SOTA) heuristics from the literature. Finally, for ten different engineering design problems, the AccOppCSCO algorithm was analyzed and compared with the original SCO and other SOTA heuristics. The results show that AccOppCSCO is effective for engineering design problems. This emphasizes that the algorithm can work effectively on a wide range of problems and can be used in various applications. The source code of the AccOppCSCO algorithm for the CEC2014 benchmark suite is publicly available at https://github.com/uguryuzgec/AccOppCSCO.}
}
@article{PIQUEIRA2016271,
title = {A comparison of LMC and SDL complexity measures on binomial distributions},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {271-275},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008882},
author = {José Roberto C. Piqueira},
keywords = {Binomial, Complexity, Information, Measure, Probability},
abstract = {The concept of complexity has been widely discussed in the last forty years, with a lot of thinking contributions coming from all areas of the human knowledge, including Philosophy, Linguistics, History, Biology, Physics, Chemistry and many others, with mathematicians trying to give a rigorous view of it. In this sense, thermodynamics meets information theory and, by using the entropy definition, López-Ruiz, Mancini and Calbet proposed a definition for complexity that is referred as LMC measure. Shiner, Davison and Landsberg, by slightly changing the LMC definition, proposed the SDL measure and the both, LMC and SDL, are satisfactory to measure complexity for a lot of problems. Here, SDL and LMC measures are applied to the case of a binomial probability distribution, trying to clarify how the length of the data set implies complexity and how the success probability of the repeated trials determines how complex the whole set is.}
}
@article{SAMPSON20052095,
title = {Comments on: “Pore network simulation of fluid inbibition into paper during coating: II. Characterization of paper's morphology and computation of its effective permeability tensor” by Ghassemzadeh and Sahimi [Chemical Engineering Science 59(2004) 2265–2280]},
journal = {Chemical Engineering Science},
volume = {60},
number = {7},
pages = {2095},
year = {2005},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2004.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0009250904009327},
author = {W.W. Sampson and C.T.J. Dodson}
}
@article{BORSTLER2023111592,
title = {Investigating acceptance behavior in software engineering—Theoretical perspectives},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111592},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111592},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002680},
author = {Jürgen Börstler and Nauman bin Ali and Martin Svensson and Kai Petersen},
keywords = {Acceptance behavior, Dual process theory, Technology acceptance, Theory, TAM, UTAUT, TPB},
abstract = {Background:
Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology.
Objective:
To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development.
Method:
We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking).
Results:
We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior.
Conclusions:
Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories.}
}
@article{RAMOS202335,
title = {An institutional modernization project in chemical engineering education in Brazil: Developing broader competencies for societal challenges},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {35-44},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000167},
author = {Bruno Ramos and Moisés Teles dos Santos and Ardson S. Vianna and Luiz Kulay},
keywords = {Process safety, Chemical reaction engineering, Education, Active-based learning},
abstract = {Contemporary societal challenges put in evidence the need to improve the hard and soft skills of chemical engineering students. To promote a more student-centered approach, active-based learning, and improved assessment strategies, the Brazilian government approved the so-called New National Curriculum Guidelines (NCG) for engineering courses. To comply with those guidelines, the Department of Chemical Engineering of the Polytechnic School of the University of São Paulo (USP) is currently developing an educational modernization process sponsored by the Fulbright Commission in Brazil, called Special Program for Modernization of Undergraduate Education (PMG). The project is based on three pillars of modernization: content (what), form (how), and infrastructure (where). This paper describes initiatives in each of those pillars: content and format changes in Chemical Reaction Engineering and Process Safety courses and the creation of new spaces for a student-centered approach (an innovative classroom layout and a makerspace). By gathering two concrete classroom experiences guided by a broader institutional educational policies (the PMG project and the NCG), this paper highlights that slight changes can lead to great improvements in the learning process, leading to more engagement in the development of hard skills while favoring improvements in soft skills, such as communication, team-based work, and critical thinking.}
}
@article{TALL1999223,
title = {What Is the Object of the Encapsulation of a Process?},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {2},
pages = {223-241},
year = {1999},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(99)00029-2},
url = {https://www.sciencedirect.com/science/article/pii/S0732312399000292},
author = {David Tall and Michael Thomas and Gary Davis and Eddie Gray and Adrian Simpson},
abstract = {Several theories have been proposed to describe the transition from process to object in mathematical thinking. Yet, what is the nature of this “object” produced by the “encapsulation” of a process? Here, we outline the development of some of the theories (including Piaget, Dienes, Davis, Greeno, Dubinsky, Sfard, Gray, and Tall) and consider the nature of the mental objects (apparently) produced through encapsulation and their role in the wider development of mathematical thinking. Does the same developmental route occur in geometry as in arithmetic and algebra? Is the same development used in axiomatic mathematics? What is the role played by imagery?}
}
@article{SHRYANE2020112806,
title = {Is cognitive behavioural therapy effective for individuals experiencing thought disorder?},
journal = {Psychiatry Research},
volume = {285},
pages = {112806},
year = {2020},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2020.112806},
url = {https://www.sciencedirect.com/science/article/pii/S0165178119302793},
author = {Nick Shryane and Richard Drake and Anthony P. Morrison and Jasper Palmier-Claus},
keywords = {Psychosis, Cognitive behavioural therapy, Thought disorder, Randomized Controlled Trial},
abstract = {Various clinical guidelines recommend cognitive behavioural therapy (CBT) to treat psychosis without reference to patients’ thought disorder. However, there is a risk that disorganized thinking hampers CBT. We tested the prediction that thought disorder would interfere with the effectiveness of CBT for hallucinations and delusions, compared to treatment as usual and supportive counselling, in secondary data from two large, single blind randomised controlled trials. We fitted latent growth curve models separately for the development of frequency and distress of symptoms. CBT was significantly more successful than counselling in reducing delusional frequency in the short term and hallucinatory distress at any point, even in those with relatively high thought disorder. We found little evidence that clinicians should restrict CBT in this subgroup of patients. Nevertheless, the findings highlight the importance of effective initial treatment of thought disorder in maximising the benefit of CBT for psychosis, particularly for reducing distress from hallucinations.}
}
@article{JOO2024226,
title = {Teaching and Learning Model for Artificial Intelligence Education},
journal = {Procedia Computer Science},
volume = {239},
pages = {226-233},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.166},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014091},
author = {Kil Hong Joo and Nam Hun Park},
keywords = {Artificial intelligence education, Lower grades in elementary school},
abstract = {AI education aims to nurture convergence talents equipped with various knowledge and AI capabilities. However, the educational programs developed so far are designed for less than 10 sessions. This is insufficient time for students to understand artificial intelligence algorithms, utilize the learned principles of artificial intelligence, and expand by converging with various knowledge. Since the developmental characteristics of students in the lower grades of elementary school are different, it is difficult to apply them as they are. Therefore, it is necessary to study the following AI education teaching and learning methods suitable for lower grade students. In this study, we develop a teaching system design model for artificial intelligence-based subject convergence education for elementary school students in the lower grades, and based on this, design and apply an artificial intelligence-based subject convergence education program. Experiments were conducted with 47 second-year elementary school students, and students’ responses were better in the AI-based convergence education program than in general subject classes in terms of interest, understanding, and expectations for classes.}
}
@article{HUANG200870,
title = {Investigating the cognitive behavior of generating idea sketches through neural network systems},
journal = {Design Studies},
volume = {29},
number = {1},
pages = {70-92},
year = {2008},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2007.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X07000750},
author = {Yinghsiu Huang},
keywords = {drawings, computer supported design, visual reasoning, neural network},
abstract = {Design can be regarded as a seeing–moving–seeing process, where designers repeatedly see and generate ideas that are based on what they have done. The crucial point of design thinking is how designers recognize ambiguous shapes from sketches and then transfer them into different shapes. This study attempts to conduct cognitive experiments to elucidate the sketching process and to simulate two types of sketching behavior used by neural network systems. When exhibiting the first type of sketching behavior, designers are able to transform their original sketches to satisfy requirements. Simulating this type of visual cognitive behavior by neural networks could help computers modify shapes to meet design requirements, as human designers do. When demonstrating the second type of sketching behavior, designers are able to see an ambiguous shape as different complete shapes so as to associate divergent design ideas. Another set of neural networks investigated in this study could also associate different shapes by adjusting the TSL and produce different idea sketches from the same shape.}
}
@article{BUTLER2021170,
title = {Expert performance and crowd wisdom: Evidence from English Premier League predictions},
journal = {European Journal of Operational Research},
volume = {288},
number = {1},
pages = {170-182},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722172030480X},
author = {David Butler and Robert Butler and John Eakins},
keywords = {OR in sports, Prediction, Experts},
abstract = {This paper analyses the forecasting accuracy of experts vis-à-vis laypeople over three seasons of English Premier League matches. We find that former professional football players have superior forecasting ability when compared to laypeople. The results give partial support to the view that a crowd forecast offers the greatest precision. Pundits generate a positive return while both the crowd and laypeople generate losses. As the prediction of multiple score outcomes represents a computationally difficult task, both groups display forecasting biases including a preference toward specific score forecasts. The results are relevant for those concerned with gambling behaviour if the forecasting strategies adopted here generalise to match betting markets.}
}
@article{LIU2022189,
title = {Granular cabin: An efficient solution to neighborhood learning in big data},
journal = {Information Sciences},
volume = {583},
pages = {189-201},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521011543},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Xin Yang and Dun Liu and Pengfei Zhang and Jie Wang},
keywords = {Computational efficiency, Granular computing, Neighborhood learning, Neighborhood rough set},
abstract = {Neighborhood Learning (NL) is a paradigm covering theories and techniques of neighborhood, which facilitates data organization, representation and generalization. While delivering impressive performances across various fields such as granular computing, cluster analysis, NL is argued to be computationally demanding, thereby limiting its utility and applicability. In this study, a simple and generic scheme named granular cabin is proposed for drastically speeding up the algorithmic implementation of NL. Specifically, this scheme is deployed to Neighborhood Rough Set (NRS) which is a typical NL methodology. And three major applications of NRS are concerned including approximation computation, neighborhood classification and feature selection. Extensive experiments demonstrate that NRS methodology enhanced by granular cabin consumes much less time. This study offers a promising solution that ensures the great potential of NL in big data.}
}
@article{FATH2005485,
title = {Elucidating public perceptions of environmental behavior: a case study of Lake Lanier},
journal = {Environmental Modelling & Software},
volume = {20},
number = {4},
pages = {485-498},
year = {2005},
note = {Vulnerability of Water Quality in Intensively Developing Urban Watersheds},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2004.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815204000611},
author = {Brian D. Fath and M.B. Beck},
keywords = {Cultural theory, Integrated environmental assessment, Stakeholder participation},
abstract = {Participation of stakeholders in stewardship of the aquatic environment, including participation from members of the general public, has become much more widespread than was the case a decade or so ago. With this shift, from a former predominantly technocratic stance to something of a democratic stance on the style of management, it becomes important to elucidate public perceptions of environmental behavior. The paper examines this issue: from a rather specific perspective, where the role of time is significant; with a specific purpose in mind—for defining illustrative stakeholder aspirations for the future, whose plausibility is to be assessed against a computational model of lake behavior; and for a specific case study, Lake Lanier in the Chattahoochee watershed of Georgia, USA. Perturbations and variation in the behavior of the aquatic environment span many time frames, from the very short-term response associated with storms, infrastructure failure, transient pollution events, and so on, to the much longer-term, for instance, the biogeochemical ‘ageing’ of a lake over many decades and more. Our analysis is devoted to data from a survey of stakeholder imagination and perceptions of how the future state of Lake Lanier may evolve in the relatively short term (2–5 years) and in the long term, defined as 25+ years (the span of a generation). Overall, stakeholders are pessimistic and fear that things will be worse in the longer term. Guided largely by thinking on the perspectives of the social solidarities of Cultural Theory, extraction and analysis of sub-samples of the survey responses show that this outlook over the two frames of time is persistent, irrespective of what are, in principle, rather different ‘global’ attitudes towards the man-environment relationship. Of interest inter alia to the foresight generating procedure, by which the ‘reachability’ of stakeholder-derived futures for the lake is to be assessed using a computational model of the relevant parts of the science base, is the question of whether the same small number of priorities for further research on lake behavior is robust in the face of the rich variety of aspirations for the future inevitable in a democratic community of stakeholders.}
}
@article{CHEN201217,
title = {Data-Brain driven systematic human brain data analysis: A case study in numerical inductive reasoning centric investigation},
journal = {Cognitive Systems Research},
volume = {15-16},
pages = {17-32},
year = {2012},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S138904171100012X},
author = {Jianhui Chen and Ning Zhong and Peipeng Liang},
keywords = {Data-Brain, Systematic human brain data analysis, Provenance, Brain Informatics},
abstract = {As a crucial step in understanding human intelligence, Brain Informatics (BI) focuses on thinking centric investigations of human cognitive functions with respect to multiple activated brain areas and neurobiological processes for a given task. Although it has been recognized that systematic human brain data analysis is an important issue of BI methodology, the existing expert-driven multi-aspect data analysis excessively depends on individual capabilities and cannot be widely adopted in BI community. In this paper, we propose a Data-Brain driven approach for systematic brain data analysis, which is implemented by using the Data-Brain, Data-Brain based BI provenances and Global Learning Scheme for BI. Furthermore, a human numerical inductive reasoning centric investigation is described to demonstrate significance and usefulness of the proposed approach. Such a Data-Brain driven approach reduces the dependency on individual capabilities and provides a practical way for realizing the systematic human brain data analysis of BI methodology.}
}
@article{CORDA2021100834,
title = {The secret of planets’ perihelion between Newton and Einstein},
journal = {Physics of the Dark Universe},
volume = {32},
pages = {100834},
year = {2021},
issn = {2212-6864},
doi = {https://doi.org/10.1016/j.dark.2021.100834},
url = {https://www.sciencedirect.com/science/article/pii/S2212686421000650},
author = {Christian Corda},
abstract = {Three different approaches show that, contrary to a longstanding conviction older than 160 years, the advance of Mercury’s perihelion can be achieved in Newtonian gravity with a very high precision by correctly analyzing the situation without neglecting Mercury’s mass. General relativity remains more precise than Newtonian physics, but Newtonian framework is more powerful than researchers and astronomers were thinking till now, at least for the case of Mercury. The Newtonian formula of the advance of planets’ perihelion breaks down for the other planets. The predicted Newtonian result is indeed too large for Venus and Earth. Therefore, it is also shown that corrections due to gravitational and rotational time dilation, in an intermediate framework which analyzes gravity between Newton and Einstein, solve the problem. By adding such corrections, a result consistent with the one of general relativity is indeed obtained. Thus, the most important results of this paper are two: (i) It is not correct that Newtonian theory cannot predict the anomalous rate of precession of the perihelion of planets’ orbit. The real problem is instead that a pure Newtonian prediction is too large. (ii) Perihelion’s precession can be achieved with the same precision of general relativity by extending Newtonian gravity through the inclusion of gravitational and rotational time dilation effects. This second result is in agreement with a couple of recent and interesting papers of Hansen, Hartong and Obers. Differently from such papers, in the present work the importance of rotational time dilation is also highlighted. Finally, it is important to stress that a better understanding of gravitational effects in an intermediate framework between Newtonian theory and general relativity, which is one of the goals of this paper, could, in principle, be crucial for a subsequent better understanding of the famous Dark Matter and Dark Energy problems.}
}
@article{WEI2021189,
title = {Multi-core-, multi-thread-based optimization algorithm for large-scale traveling salesman problem},
journal = {Alexandria Engineering Journal},
volume = {60},
number = {1},
pages = {189-197},
year = {2021},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2020.06.055},
url = {https://www.sciencedirect.com/science/article/pii/S1110016820303227},
author = {Xin Wei and Liang Ma and Huizhen Zhang and Yong Liu},
keywords = {Multi-core, Multi-thread, Traveling Salesman Problem, Optimization Algorithm},
abstract = {With the rapid development of general hardware technology, microcomputers with multi-core CPUs have been widely applied in commercial services and household usage in the last ten years. Multi-core chips could, theoretically, lead to much better performance and computational efficiency than single-core chips. But so far, they have not shown general advantages for users, other than for operating systems and some specialized software. It is not easy to transform traditional single-core-based algorithms into multi-core-, multi-thread-based algorithms that can greatly improve efficiency, because of difficulties in computation and scheduling of hardware kernels, and because some programming languages cannot support multi-core, multi-thread programming. Therefore, a kind of multi-core-, multi-thread-based fast algorithm was designed and coded with Delphi language for the medium- and large-scale traveling salesman problem instances from TSPLIB, which can fully speed up the searching process without loss of quality. Experimental results show that the algorithm proposed can, under the given hardware limitations, take full advantage of multi-core chips and effectively balance the conflict between increasing problem size and computational efficiency and thus acquire satisfactory solutions.}
}
@article{MURRAY2019928,
title = {Center Finding in E. coli and the Role of Mathematical Modeling: Past, Present and Future},
journal = {Journal of Molecular Biology},
volume = {431},
number = {5},
pages = {928-938},
year = {2019},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0022283619300269},
author = {Seán M. Murray and Martin Howard},
keywords = {bacterial cell division positioning, plasmid segregation, MinCDE system, ParABS system, mathematical modeling},
abstract = {We review the key role played by mathematical modeling in elucidating two center-finding patterning systems in Escherichia coli: midcell division positioning by the MinCDE system and DNA partitioning by the ParABS system. We focus particularly on how, despite much experimental effort, these systems were simply too complex to unravel by experiments alone, and instead required key injections of quantitative, mathematical thinking. We conclude the review by analyzing the frequency of modeling approaches in microbiology over time. We find that while such methods are increasing in popularity, they are still probably heavily under-utilized for optimal progress on complex biological questions.}
}
@incollection{POULSEN201543,
title = {Chapter 3 - Better Concurrency and SIMD on HBM},
editor = {James Reinders and Jim Jeffers},
booktitle = {High Performance Parallelism Pearls},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {43-67},
year = {2015},
isbn = {978-0-12-802118-7},
doi = {https://doi.org/10.1016/B978-0-12-802118-7.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021187000030},
author = {Jacob Weismann Poulsen and Per Berg and Karthik Raman},
keywords = {HIROMB-BOOS-Model, Danish Meteorological Institute, Operational ocean models, Parallelization, Validation, Verification, OpenMP, MPI, Intel® VTune™ Amplifier, Performance Monitoring Unit, Vectorization, Nesting, Scaling, Cache layout},
abstract = {This chapter describes some work that is being performed at the Danish Meteorological Institute for optimization of a 3D ocean circulation model code with roots back to the 1990s and which is known as the HIROMB-BOOS-Model. The optimization of this large code is instructive agreeing with the authors’ strong belief that the best performance only comes with a focus on architecting for it starting with appropriate data structures. The thinking process and techniques used in this chapter have wide applicability: focus on data locality and then apply threading and vectorization techniques. This way of thinking should be on the mind of every programmer working to design a high-performance application.}
}
@article{KIM2024110348,
title = {Hierarchical aerial offload computing algorithm based on the Stackelberg-evolutionary game model},
journal = {Computer Networks},
volume = {245},
pages = {110348},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110348},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624001804},
author = {Sungwook Kim},
keywords = {Aerial access networks, High-altitude platforms, Unmanned aerial vehicles, Stackelberg-evolutionary game, Multi-objective bargaining solution},
abstract = {Aerial access networks have been envisioned as a promising 6 G technology to enhance the service experience in underserved areas where terrestrial base stations do not exist. In such scenarios, a hierarchical model of high-altitude platforms (HAPs) and unmanned aerial vehicles (UAVs) is considered to provide aerial computing services for ground Internet of Things (IoT) devices. In this study, we investigate a hierarchical aerial computing system to optimally orchestrate the limited computation resources in both HAPs and UAVs. For offloading services, we formulate a joint resource allocation problem to maximize service satisfaction for terrestrial IoT devices. To solve this problem, we employ the ideas of game theory with centralized decision and decentralized execution. Through the Stackelberg-evolutionary game model, the HAP works as a leader, and selects its price strategy based on the evolutionary learning process. As followers, individual UAVs make decisions to partially offload their computing tasks by considering different objectives. According to the interactive control paradigm, our proposed method can get reciprocal advantages for HAPs, UAVs, and ground IoT devices while adaptively handling dynamic aerial network conditions. Finally, extensive simulation results verify the efficiency of our proposed algorithm to increase the usability of edge servers’ computational resources. Compared with other existing state-of-the-art aerial network offloading protocols, we can improve the profits of system throughput, resource usability and UAV fairness up to 10 %, 10 %, and 15 %, respectively.}
}
@article{SCHULTZ2010174,
title = {Models and methods in motion: Declining the dogma dance},
journal = {Futures},
volume = {42},
number = {2},
pages = {174-176},
year = {2010},
note = {Epistemological pluralism in futures studies},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2009.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016328709001736},
author = {Wendy Schultz},
abstract = {I take a communicative pragmatist and realist approach to futures studies. This implies a sensitivity to understanding what the audience can absorb and using futures methods effectively to create spaces for new futures. While Wilber's work affords us with new insights to engage with methodology, is not the only path. Indeed, it is intellectual bigotry to demand that everyone master the tools one personally deems most appropriate. Critical conversations about futures must remain open, where post-modernist and integral thinking widen our horizons, they are welcomed, where they straitjacket our thoughts, they are not.}
}
@article{WHEELER2020192,
title = {Ideology and predictive processing: coordination, bias, and polarization in socially constrained error minimization},
journal = {Current Opinion in Behavioral Sciences},
volume = {34},
pages = {192-198},
year = {2020},
note = {Political Ideologies},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620300632},
author = {Nathan E Wheeler and Suraiya Allidina and Elizabeth U Long and Stephen P Schneider and Ingrid J Haas and William A Cunningham},
abstract = {Recent models of cognition suggest that the brain may implement predictive processing, in which top-down expectations constrain incoming sensory data. In this perspective, expectations are updated (error minimization) only if sensory data sufficiently deviate from these expectations (prediction error). Although originally applied to perception, predictive processing is thought to generally characterize cognitive architecture, including the social cognitive processes involved in ideological thinking. Scaling up these simple computational principles to the social sphere outlines a path by which group members may adopt shared ideologies and beliefs to predict behavior and cooperate with each other. Because ideological judgments are of specific interest to others in our political groups, we may increasingly regulate each other’s thinking, sharing the process of error minimization. In this paper, we outline how this process of shared error minimization may lead to shared ideologies and beliefs that allow group members to predict and cooperate with each other, and how, as a consequence, political polarization and extremism may result.}
}
@article{PACE2023105433,
title = {Exploring future research and innovation directions for a sustainable blue economy},
journal = {Marine Policy},
volume = {148},
pages = {105433},
year = {2023},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2022.105433},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X22004808},
author = {Lisa A. Pace and Ozcan Saritas and Alan Deidun},
keywords = {Foresight, Blue economy, Interdisciplinary science, Marine science, Sustainable development, Stakeholder participation},
abstract = {The blue economy integrates commercial, research and innovation activities across diverse industrial sectors. Achieving a sustainable blue economy requires unlocking the potential of science and innovation to develop innovative ocean sustainability solutions. This study explores the role of foresight in co-creating alternative, preferred futures for a sustainable blue economy looking towards 2030 and in establishing an interdisciplinary dialogue about research and innovation opportunities to achieve these futures. To this end, a foresight exercise is conducted with marine scientists and researchers in 6 countries in Europe. The exercise is designed in three stages: scanning, scenario-building and strategic orientation, and uses a combination of foresight methods to encourage creative thinking and exploration. The scenarios developed in the study describe alternative future worlds built on the establishment of self-sustaining communities and engaged societies; the diffusion of digitalisation and growth of blue biotechnologies; booming ecosystem services and open and collaborative research infrastructures that impact different sectors of the blue economy. A portfolio of research and innovation areas is developed that aims to inspire new research directions in four domains: (i) integrated ocean management tools; (ii) closed loop, circular polyculture systems; (iii) co-creation of innovation and transdisciplinary research; and (iv) open access and collaborative databases supporting ecosystem services. The study highlights the role of foresight in bridging across disciplinary perspectives and industry sectors. Foresight can be used to complement Decision-Support Systems and other quantitative approaches for research agenda-setting and for decision-making on policies addressing sustainability in the marine sciences. The process contributes to futures skills-building at institutional level and helps establish a futures mindset for strategic planning.}
}
@article{MCGILL2021113697,
title = {Evaluation of public health interventions from a complex systems perspective: A research methods review},
journal = {Social Science & Medicine},
volume = {272},
pages = {113697},
year = {2021},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.113697},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621000290},
author = {Elizabeth McGill and Vanessa Er and Tarra Penney and Matt Egan and Martin White and Petra Meier and Margaret Whitehead and Karen Lock and Rachel {Anderson de Cuevas} and Richard Smith and Natalie Savona and Harry Rutter and Dalya Marks and Frank {de Vocht} and Steven Cummins and Jennie Popay and Mark Petticrew},
keywords = {Systems thinking, Complexity science, Evaluation methodologies, Public health, Practice},
abstract = {Introduction
Applying a complex systems perspective to public health evaluation may increase the relevance and strength of evidence to improve health and reduce health inequalities. In this review of methods, we aimed to: (i) classify and describe different complex systems methods in evaluation applied to public health; and (ii) examine the kinds of evaluative evidence generated by these different methods.
Methods
We adapted critical review methods to identify evaluations of public health interventions that used systems methods. We conducted expert consultation, searched electronic databases (Scopus, MEDLINE, Web of Science), and followed citations of relevant systematic reviews. Evaluations were included if they self-identified as using systems- or complexity-informed methods and if they evaluated existing or hypothetical public health interventions. Case studies were selected to illustrate different types of complex systems evaluation.
Findings
Seventy-four unique studies met our inclusion criteria. A framework was developed to map the included studies onto different stages of the evaluation process, which parallels the planning, delivery, assessment, and further delivery phases of the interventions they seek to inform; these stages include: 1) theorising; 2) prediction (simulation); 3) process evaluation; 4) impact evaluation; and 5) further prediction (simulation). Within this framework, we broadly categorised methodological approaches as mapping, modelling, network analysis and ‘system framing’ (the application of a complex systems perspective to a range of study designs). Studies frequently applied more than one type of systems method.
Conclusions
A range of complex systems methods can be utilised, adapted, or combined to produce different types of evaluative evidence. Further methodological innovation in systems evaluation may generate stronger evidence to improve health and reduce health inequalities in our complex world.}
}
@article{WANG2020256,
title = {Fine-grained neural decoding with distributed word representations},
journal = {Information Sciences},
volume = {507},
pages = {256-272},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519307820},
author = {Shaonan Wang and Jiajun Zhang and Haiyan Wang and Nan Lin and Chengqing Zong},
keywords = {Neural decoding, fMRI word decoding, Word class, Stimuli paradigm, Word embedding models, Informative voxels},
abstract = {fMRI word decoding refers to decode what the human brain is thinking by interpreting functional Magnetic Resonance Imaging (fMRI) scans from people watching or listening to words, representing a sort of mind-reading technology. Existing works decoding words from imaging data have been largely limited to concrete nouns from a relatively small number of semantic categories. Moreover, such studies use different word-stimulus presentation paradigms and different computational models, lacking a comprehensive understanding of the influence of different factors on fMRI word decoding. In this paper, we present a large-scale evaluation of eight word embedding models and their combinations for decoding fine-grained fMRI data associated with three classes of words recorded from three stimulus-presentation paradigms. Specifically, we investigate the following research questions: (1) How does the brain-image decoder perform on different classes of words? (2) How does the brain-image decoder perform in different stimulus-presentation paradigms? (3) How well does each word embedding model allow us to decode neural activation patterns in the human brain? Furthermore, we analyze the most informative voxels associated with different classes of words, stimulus-presentation paradigms and word embedding models to explore their neural basis. The results have shown the following: (1) Different word classes can be decoded most effectively with different word embedding models. Concrete nouns and verbs are more easily distinguished than abstract nouns and verbs. (2) Among the three stimulus-presentation paradigms (picture, sentence and word clouds), the picture paradigm achieves the highest decoding accuracy, followed by the sentence paradigm. (3) Among the eight word embedding models, the model that encodes visual information obtains the best performance, followed by models that encode textual and contextual information. (4) Compared to concrete nouns, which activate mostly vision-related brain regions, abstract nouns activate broader brain regions such as the visual, language and default-mode networks. Moreover, both the picture paradigm and the model that encodes visual information have stronger associations with vision-related brain regions than other paradigms and word embedding models, respectively.}
}
@article{PEYRACHE2024255,
title = {A homothetic data generated technology},
journal = {European Journal of Operational Research},
volume = {316},
number = {1},
pages = {255-267},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S037722172400050X},
author = {Antonio Peyrache},
keywords = {Data envelopment analysis, Input homotheticity, Free disposal hull, Efficiency},
abstract = {I propose a method for constructing an enlargement of a variable returns to scale production technology that will satisfy homotheticity. The method can be used both with DEA and FDH single output (or single input) technologies and it is computationally fast. The method is constructed by adding a restriction to the axiomatically delineated homothetic reference technologies which requires these reference technologies to be subsets of the minimal reference technology that satisfies constant returns to scale. Within this set it is possible to identify a homothetic technology that satisfies the property of minimum extrapolation.}
}
@article{HAAS2024110900,
title = {Models vetted against prediction error and parameter sensitivity standards can credibly evaluate ecosystem management options},
journal = {Ecological Modelling},
volume = {498},
pages = {110900},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110900},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002886},
author = {Timothy C. Haas},
keywords = {Model vetting, Model credibility, Ecosystem management, Parameter sensitivity, Robust statistical estimators, High performance computing},
abstract = {A new standard for assessing model credibility is developed. This standard consists of parameter estimation, prediction error assessment, and a parameter sensitivity analysis that is driven by outside individuals who are skeptical of the model’s credibility (hereafter, skeptics). Ecological/environmental models that have a one-step-ahead prediction error rate that is better than naive forecasting — and are not excessively sensitive to small changes in their parameter values are said here to be vetted. A procedure is described that can perform this assessment on any model being evaluated for possible participation in an ecosystem management decision. Uncertainty surrounding the model’s ability to predict future values of its output variables and in the estimates of all of its parameters should be part of any effort to vett a model. The vetting procedure described herein, Prediction Error Rate-Deterministic Sensitivity Analysis (PER-DSA), incorporates these two aspects of model uncertainty. DSA in particular, requires participation by skeptics and is the reason why a successful DSA gives a model sufficient credibility to have a voice in ecosystem management decision making. But these models need to be stochastic and represent the mechanistic processes of the system being modeled. For such models, performing a PER-DSA can be computationally expensive. A cluster computing algorithm to speed-up these computations is described as one way to answer this challenge. This new standard is illustrated through a PER-DSA of a population dynamics model of South African rhinoceros (Ceratotherium simum simum).}
}
@article{SENANAYAKE2024104705,
title = {Agent-based simulation for pedestrian evacuation: A systematic literature review},
journal = {International Journal of Disaster Risk Reduction},
volume = {111},
pages = {104705},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104705},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924004679},
author = {Gayani P.D.P. Senanayake and Minh Kieu and Yang Zou and Kim Dirks},
keywords = {Pedestrian behaviour modelling, Agent-based modelling, Behavioural decision-making, Emergency evacuation},
abstract = {Agent-based models (ABMs) offer promise for realistically simulating human behaviours and interactions during emergency evacuations. This review aims to systematically assess the state of the art in ABM-based evacuation modelling with respect to methodologies, validation practices, and the associated challenges over the past decade. The review critically examines 134 studies from 2013 to 2023 that have applied ABMs for pedestrian evacuation simulation to synthesise current capabilities, limitations, and advancement pathways. Findings identify persistent challenges related to modeller bias, computational complexity, data scarcity for calibration and validation, and the predominance of simplistic rule-based decision-making models, while promise exists with the adoption of flexible behavioural frameworks, high-performance computing architectures, machine learning techniques for adaptive agent behaviours and surrogate modelling, and evolutionary computation methods for transparent rule generation. The findings underscore the importance of interdisciplinary collaboration among behavioural scientists, modellers, and emergency planners to enhance the realism and reliability of ABMs. By providing a critical synthesis of the state-of-the-art and proposing future research directions, this review aims to accelerate the development and application of ABMs that can meaningfully enhance the safety and resilience of communities facing emergencies.}
}
@incollection{KARALIS2024215,
title = {Chapter 6 - Artificial intelligence in drug discovery and clinical practice},
editor = {Natassa Pippa and Costas Demetzos and Maria Chountoulesi},
booktitle = {From Current to Future Trends in Pharmaceutical Technology},
publisher = {Academic Press},
pages = {215-255},
year = {2024},
isbn = {978-0-323-91111-5},
doi = {https://doi.org/10.1016/B978-0-323-91111-5.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911115000068},
author = {Vangelis D. Karalis},
keywords = {Artificial intelligence, Drug discovery, Clinical practice, Machine learning},
abstract = {Artificial intelligence (AI) is the imitation of human intelligence by computers. It is the science of creating intelligent machines capable of performing tasks equivalent or superior to those performed by humans. The process involves collecting data, formulating rules for its use, making approximate or definitive determinations, and self-correcting. In particular, artificial intelligence and machine learning (ML) have attracted considerable attention in a variety of sectors, including pharmaceutical sciences, and have led to a rapid rise in new applications for machine learning in numerous areas of pharmaceutical sciences. In computational chemistry, deep learning models have been used to predict drug-target interactions, develop new compounds, and predict pharmacokinetics. AI, robotics, and advanced computing have applications in drug repurposing, quality-by-design, 3D printing, and nanomedicine. When used properly, AI techniques can improve patient treatment, detection and reduction of risk factors, and identification of complications.}
}
@incollection{NA2025259,
title = {Chapter Ten - Quantum computing research: An in-depth exploration},
editor = {Pethuru Raj and Kavita Saini and Brij B. Gupta},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {138},
pages = {259-292},
year = {2025},
booktitle = {Post-Quantum Cryptography Algorithms and Approaches for IoT and Blockchain Security},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2025.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0065245825000294},
author = {Natraj N.A. and Pethuru Raj Chelliah},
keywords = {Quantum, Cryptography, Machine learning, Applications, Quantum Mechanics, Qubits, Gates, Drug discovery, chemistry},
abstract = {The area of computation has undergone a paradigm change with the introduction of quantum computing, which makes use of the complexity of quantum mechanics to successfully tackle problems that were previously considered to be impossible for traditional computers to address. This chapter will give a comprehensive study of quantum computing by delving into its core principles, important algorithms, and future applications. The goal of this chapter is to provide a comprehensive analysis of quantum computing. Quantum computing encompasses not only fundamental concepts like qubits, superposition, entanglement, quantum gates, and algorithms, but it also expands its scope to include quantum hardware, software, programming languages, simulation, modelling, and the intersection of quantum computing and artificial intelligence. Quantum computing is a relatively new field that has been showing significant promise in recent years. The transformative potential of quantum computing reverberates throughout a broad variety of sectors, including materials science and chemistry, as well as finance, optimisation, drug discovery, healthcare, and national security. Quantum computing has the potential to drastically alter these fields. Researchers and enthusiasts interested in acquiring a more in-depth understanding of this dynamic and continuously growing subject matter will find this chapter to be an indispensable resource. In addition to providing a detailed understanding of the theoretical underpinnings of quantum computing, it also details the implications that this technology has in the real world.}
}
@article{AVEN201633,
title = {On the use of conservatism in risk assessments},
journal = {Reliability Engineering & System Safety},
volume = {146},
pages = {33-38},
year = {2016},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015002938},
author = {Terje Aven},
keywords = {Conservatism, Risk assessments, Knowledge},
abstract = {It is common to use conservatism in risk assessments, replacing uncertain quantities with values that lead to a higher level of risk. It is argued that the approach represents a practical method for dealing with uncertainties and lack of knowledge in risk assessment. If the computed probabilities meet the pre-defined criteria with the conservative quantities, there is strong support for the “real risk” to meet these criteria. In this paper we look more closely into this practice, the main aims being to clarify what it actually means and what the implications are, as well as providing some recommendations. The paper concludes that conservatism should be avoided in risk assessments – “best judgements” should be the ruling thinking, to allow for meaningful comparisons of options. By incorporating sensitivity analyses and strength of knowledge judgements for the background knowledge on which the assigned probabilities are based, the robustness of the conclusions can be more adequately assessed.}
}
@article{KLIGER20217,
title = {Dynamic Archeology or Distant Reading: Literary Study Between Two Formalisms},
journal = {Russian Literature},
volume = {122-123},
pages = {7-28},
year = {2021},
note = {Digital Humanities and Russian and East European Studies},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000429},
author = {Ilya Kliger},
keywords = {Computational Literary Studies, Distant Reading, Literary Form, Russian Formalism, OPOIAZ},
abstract = {Scholars working within computational literary studies often invoke Russian Formalism as a methodologically like-minded school of thought and a repository of useful insights, which can at last be tested with the help of recently developed digital techniques. Yet the two formalisms diverge starkly when it comes to three of their most fundamental categories of analysis: first, in their respective conceptions of literary form itself; next, in their notions of history and of what it means to tell the history of form; and finally, in the ways in which they construe the relationship between literature and society as a whole, or, in other words, in their corresponding sociologies of literary form. This paper, then, is a contribution to creating the conditions for the possibility of a genuine exchange between the two formalisms here at issue by focusing, first and foremost, on what divides them.}
}
@article{COWLEY2019104025,
title = {Wide coding: Tetris, Morse and, perhaps, language},
journal = {Biosystems},
volume = {185},
pages = {104025},
year = {2019},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104025},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719301820},
author = {S J Cowley},
keywords = {Organic codes, Distributed language, Adaptors, Wide cognition, Reading, Languaging},
abstract = {Code biology uses protein synthesis to pursue how living systems fabricate themselves. Weight falls on intermediary systems or adaptors that enable translated DNA to function within a cellular apparatus. Specifically, code intermediaries bridge between independent worlds (e.g. those of RNAs and proteins) to grant functional lee-way to the resulting products. Using this Organic Code (OC) model, the paper draws parallels with how people use artificial codes. As illustrated by Tetris and Morse, human players/signallers manage code functionality by using bodies as (or like) adaptors. They act as coding intermediaries who use lee-way alongside “a small set of arbitrary rules selected from a potentially unlimited number in order to ensure a specific correspondence between two independent worlds” (Barbieri, 2015). As with deep learning, networked bodily systems mesh inputs from a coded past with current inputs. Received models reduce ‘use’ of codes to a run-time or program like process. They overlook how molecular memory is extended by living apparatuses that link codes with functioning adaptors. In applying the OC model to humans, the paper connects Turing’s (1937) view of thinking to Wilson’s (2004) appeal to wide cognition. The approach opens up a new view of Kirsh and Maglio’s (1994) seminal studies on Tetris. As players use an interface that actualizes a code or program, their goal-directed (i.e. ‘pragmatic’) actions co-occur with adaptor-like ‘filling in’ (i.e. ‘epistemic’ moves). In terms of the OC model, flexible functions derive from, not actions, but epistemic dynamics that arise in the human-interface-computer system. Second, I pursue how a Morse radio operator uses dibs and dabs that enable the workings of an artificial code. While using knowledge (‘the rules’) to resemiotize by tapping on a transmission key, bodily dynamics are controlled by adaptor-like resources. Finally, turning to language, I sketch how the model applies to writing and reading. Like Morse operators, writers resemiotize a code-like domain of alphabets, spelling-systems etc. by acting as (or like) bodily adaptors. Further, in attending to a text-interface (symbolizations), a reader relies on filling-in that is (or feels) epistemic. Given that humans enact or mimic adaptor functions, it is likely that the OC model also applies to multi-modal language.}
}
@article{HAREL201758,
title = {Field-based hypotheses on advancing standards for mathematical practice},
journal = {The Journal of Mathematical Behavior},
volume = {46},
pages = {58-68},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2017.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317300457},
author = {Guershon Harel},
keywords = {Common Core State Standards in Mathematics (CCSSM), Standards for mathematical practice},
abstract = {The Common Core State Standards in Mathematics (CCSSM, 2010) are organized around two types of standards: the standards for mathematical content and standards for mathematical practice. The central goal of this paper is to present cognitive and instructional analyses of standards for mathematical practice through a discussion of field-based activities with inservice secondary mathematics teachers and students. A potential value of the study is that it provides researchers with specific field-based hypotheses on advancing standards for mathematical practice.}
}
@article{CROOCK2025102650,
title = {Uncharted territory: the role of mitochondrial DNA variation in macrophage-mediated host susceptibility to tuberculosis},
journal = {Tuberculosis},
volume = {153},
pages = {102650},
year = {2025},
issn = {1472-9792},
doi = {https://doi.org/10.1016/j.tube.2025.102650},
url = {https://www.sciencedirect.com/science/article/pii/S1472979225000459},
author = {Dayna Croock and Yolandi Swart and Tomasz Janusz Sanko and Vuyo Mavumengwana and Marlo Möller and Caitlin Uren and Desiree C. Petersen},
keywords = {Mitochondria, , Tuberculosis (TB) susceptibility, Mitochondrial DNA (mtDNA) variation},
abstract = {Mitochondria form an integral, yet frequently underappreciated, part of the immune response to Mycobacterium tuberculosis (M.tb), particularly within macrophages. Despite growing recognition for their role in infection and immunity, studies investigating how mitochondrial DNA (mtDNA) variation influences host susceptibility to tuberculosis (TB) are limited. Notably, there are no studies in African-based populations, although Africans possess unparalleled human genetic diversity, including the earliest diverged mitochondrial haplogroups, and a high TB burden. This underrepresentation limits the discovery of novel ancestry-specific genetic loci associated with TB. In this review article, we describe the unique characteristics of mtDNA, highlight key mitochondrial functions relevant to macrophage responses during M.tb infection, and summarise published studies that investigate the role of host mtDNA variation in TB susceptibility. We further advocate for the inclusion of African populations in future studies to identify novel TB susceptibility genetic risk loci and expand the current knowledgebase on host TB susceptibility.}
}
@article{WU2023100739,
title = {The development of teacher feedback literacy in situ: EFL writing teachers’ endeavor to human-computer-AWE integral feedback innovation},
journal = {Assessing Writing},
volume = {57},
pages = {100739},
year = {2023},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2023.100739},
url = {https://www.sciencedirect.com/science/article/pii/S1075293523000478},
author = {Peisha Wu and Shulin Yu and Yanqi Luo},
keywords = {Teacher feedback literacy, Second language writing, Teacher feedback, Computer-mediated feedback, Writing assessment},
abstract = {While recent years have witnessed increasing theoretical and empirical elaboration on the construct of teacher feedback literacy in higher education and second language education, little research has investigated the development of teacher feedback literacy, especially when teachers collaborate in an attempt to improve feedback strategies with technology. To fill this gap, the present study examined two L2 writing teachers taking the initiative to create, update, and implement a human-computer-automatic writing evaluation (AWE) integral feedback platform, and how such a feedback innovation process impacted their feedback literacy development. The analysis of multiple sources of data, including semi-structured interviews, stimulated recalls, class observation, and artifacts, revealed that the two teachers approached the innovation by orchestrating mediating tools, interacting dialogically with social agents, reflecting critically, and crossing boundaries. Through this process, the development of teacher feedback literacy occurred at varying rates across different aspects. Specifically, positive changes were effected in the teachers’ feedback thinking as well as feedback giving and sharing practices. However, the teachers’ feedback literacy in classroom practice did not seem to have generated as salient a positive outcome. Possible reasons are discussed regarding the scope of the feedback innovation and contextual constraints, and implications are offered. The study underscored L2 writing teacher feedback literacy as a developmental phenomenon molded by situated social practice.}
}
@article{GARBEY1990293,
title = {Massively parallel computation of conservation laws},
journal = {Parallel Computing},
volume = {16},
number = {2},
pages = {293-304},
year = {1990},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(90)90067-J},
url = {https://www.sciencedirect.com/science/article/pii/016781919090067J},
author = {Marc Garbey and David Levine},
keywords = {Cellular automata, Partial differential equations, Method of characteristics, Parallel algorithms, Conservation laws},
abstract = {We present a new method for computing solutions of conservation laws based on the use of cellular automata with the method of characteristics. The method exploits the high degree of parallelism available with cellular automata and retains important features of the method of characteristics. It yields high numerical accuracy and extends naturally to adaptive meshes and domain decomposition methods for perturbed conservation laws. We describe the method and its implementation for a Dirichlet problem with a single conservation law for the one-dimensional case. Numerical results for the one-dimensional law with the classical Burgers nonlinearity or the Buckley-Leverett equation show good numerical accuracy outside the neighborhood of the shocks. The error in the area of the shocks is of the order of the mesh size. The algorithm is well suited for execution on both massively parallel computers and vector machines. We present timing results for an Alliant FX/8, Connection Machine Model 2, and CRAY X-MP.}
}
@incollection{MAGGIONI2010255,
title = {Knowledge Domains and Domain Learning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {255-264},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00483-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004838},
author = {L. Maggioni and P.A. Alexander},
keywords = {Discipline, Domain, History, Knowledge, Learning, Mathematics, Reading, Science, Writing},
abstract = {The roots of current disciplines and domains of study reach well back in history. An exploration of their development shows that these areas of knowledge have not only reflected cultural changes, but have also influenced societies, especially through formal educational systems. Besides being characterized by their focus on a particular part of the world, disciplines are also distinguished by a specific way of thinking about their respective domains of study. Psychological research has identified several features of these pathways to knowledge (e.g., reading, writing, history, mathematics, and science) that generally define the landscape of academic practice.}
}
@article{MANZOLLI2022112211,
title = {A review of electric bus vehicles research topics – Methods and trends},
journal = {Renewable and Sustainable Energy Reviews},
volume = {159},
pages = {112211},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112211},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122001344},
author = {Jônatas Augusto Manzolli and João Pedro Trovão and Carlos Henggeler Antunes},
keywords = {Electric bus, Electric mobility, Research gaps, Sustainability, Fleet operation, Energy management},
abstract = {The transportation sector accounts for a significant share of greenhouse gas emissions. Hence, the electrification of this sector is a crucial contributor to the mitigation of global warming. Recent studies suggest that electric vehicles will be economically paired with internal combustion engine vehicles in the near future. However, relying on private vehicle decarbonization only cannot deliver comprehensive space management efficiency solutions in urban environments. Therefore, it is essential to invest in the technological development and deployment of electric buses for public transportation, directly enhancing the quality of life in large cities. From this perspective, this review examines a wide range of scientific literature on electric bus research using science mapping methods and content analysis to support critical thinking unveiling the main research streams, methods, and gaps of the field. The analysis indicates that future research on electric buses will be mainly devoted to sustainability (encompassing economic, environmental and quality of service dimensions), energy management strategies, and fleet operation.}
}
@article{RUTER2000519,
title = {Analysis, finite element computation and error estimation in transversely isotropic nearly incompressible finite elasticity},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {190},
number = {5},
pages = {519-541},
year = {2000},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(99)00286-8},
url = {https://www.sciencedirect.com/science/article/pii/S0045782599002868},
author = {Marcus Rüter and Erwin Stein},
abstract = {In this paper we present constitutive models for nearly incompressible, transversely isotropic materials in finite hyperelasticity, particularly for reinforced rubber-like materials, which are of essential engineering interest. The theory is developed using a convected curvilinear coordinate system based on a mixed two-field displacement–pressure energy functional. Furthermore, an a posteriori error estimator without multiplicative constants is derived for non-linear anisotropic problems, which measures the discretization error in the first Piola–Kirchhoff stresses in the L2-norm by solving local Neumann problems with equilibrated tractions. Illustrative numerical examples demonstrate the anisotropic material behaviour of reinforced materials and the efficiency of using adaptive finite element methods.}
}
@article{MCCOWN201233,
title = {Farmers use intuition to reinvent analytic decision support for managing seasonal climatic variability},
journal = {Agricultural Systems},
volume = {106},
number = {1},
pages = {33-45},
year = {2012},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2011.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X11001557},
author = {R.L. McCown and P.S. Carberry and N.P. Dalgliesh and M.A. Foale and Z. Hochman},
keywords = {Decision support, Simulation, Information system, Cognitive system, Intuition, Climatic risk},
abstract = {The FARMSCAPE Information System emerged in a long-running research program aimed at making simulation models useful to Australian farmers in managing climatic variability. This paper is about how well it has worked. This is reported in relation to two standards: (1) the value to thinking and action expressed by farmers and their consultants, (2) correspondence with theory about learning and judgement in uncertain external environments. The former utilises recorded narrative interviews with participants over many years. The latter uses a cognitive framework drawn from theory of judgment and decision making featuring the relationship between intuition and analysis (McCown, 2011). The cognitive theory framework makes sense of several evaluation surprises. The first was high enthusiasm by largely-intuitive farmers for an analytic approach to soil water in conjunction with a newly-appreciated “bucket” metaphor for water balance. The second surprise was the virtual absence of soil water measurement 10years later. This had been replaced by various intuitive estimates, calibrated to maintain a heuristic relationship with regard to the “bucket” as a resource. Farmers and their advisers were facilitated in using simulation for thought experiments and planning under climatic uncertainty. Benchmarking enabled problem solving in documented conditions. Scenario analysis using historical climate records supported thought experiments by providing probability distributions that were valued for shaping expectations as a “history of the future”. In retrospective evaluation interviews, researchers were surprised to find that yield forecasting and tactical decision making, anticipated to be analyses that were both site- and season-specific forecasts, had served farmers as “management gaming” simulations to aid formulating action rules for such conditions, thus reducing the need for an on-going decision-aiding service. Equipped with their soil monitoring techniques and with their heuristic rules, farmers still reserved a place for simulation “when you’ve got a planting situation out of the ordinary.”}
}
@article{DENHAM2022105526,
title = {Visualization and modeling of forest fire propagation in Patagonia},
journal = {Environmental Modelling & Software},
volume = {158},
pages = {105526},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105526},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002262},
author = {Mónica M. Denham and Sigfrido Waidelich and Karina Laneri},
keywords = {Simulation, Modeling, Forest fire behavior, High-performance computing, GPGPU},
abstract = {Fire propagation is a big concern all over the world. Visualization is a valuable tool to test possible different scenarios for fire spread, specially for designing strategies for fire control, mitigation and management. We present a parallel High-Performance Computing (HPC) forest fire simulator with an interactive and intuitive user interface that offers several functionalities to the user. The visualization interface allows to choose the propagation model of preference, the scenario of interest, as well as numerous simulation features including firebreaks and ignition points. We show some of the outputs for two different mathematical models for fire spreading. The simulator was developed with an open source philosophy in the framework of Faster Than Real Time (FTRT) applications thinking on its possible use in the field during a forest fire propagation. It can be run in Linux (Ubuntu) and Windows Operating Systems and for portability purposes the simulator was also implemented on a NVIDIA Jetson Nano.}
}
@article{HUSSAIN2025100149,
title = {A novel data extraction framework using Natural Language Processing (DEFNLP) techniques},
journal = {Natural Language Processing Journal},
pages = {100149},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100149},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000251},
author = {Tayyaba Hussain and Muhammad Usman Akram and Anum Abdul Salam},
keywords = {Artificial intelligence (AI), Big data, Data mining, Machine learning (ML), Natural Language Processing (NLP), Question answer (QA), Information retrieval},
abstract = {Evidence through data is critical if government has to address threats faced by the nation, such as pandemics or climate change. Yet several facts about data necessary to inform evidence and science are locked inside publications. We used scientific literature dataset, Coleridge Initiative - Show US the Data, to discover how the data can be used for the public good. In this research, we demonstrate a general Data Extraction Framework Using Natural Language Processing (DEFNLP) Techniques which challenge data scientists to show how publicly funded data has been used to serve science and society. The proposed framework uses NLP libraries and techniques like SpaCy and NER respectively and different huggingface Question Answering (QA) models to predict the datasets used in publications. DEFNLP findings can assist the government in immediate decisions making, accountability, transparent public investments, economic and public health benefits. Until now such an issue having large dataset which belongs to numerous research areas has not been addressed. This approach is domain independent and therefore can be applied to all kind of case studies and scenarios which require data extraction. Our methodology sets the state-of-the-art on Coleridge Initiative dataset, reaching the highest score of 0.554 using salti bert QA model with the less runtime i.e. 417.4 and output of 819 bytes than other QA models e.g., Longformer (runtime: 2710.2, output: 1780 bytes) and BigBird (runtime: 839.4, output: 177020 bytes) with 0.444 and 0.387 score respectively which impressively raised the leaderboard score with an outcome of 0.711. Its computation time to answer each query on CPU is far less i.e. 0.0696s (than 0.3556s and 0.8967s) and has suitable hyperparameters for our dataset as maximum answer length is 64, greater batch size as well as learning rate. In terms of timing and performance, each epoch took around 5 min on average on a computer with output size of 3.27kB which is again far better than other frameworks.}
}
@article{BRANICKY199567,
title = {Universal computation and other capabilities of hybrid and continuous dynamical systems},
journal = {Theoretical Computer Science},
volume = {138},
number = {1},
pages = {67-100},
year = {1995},
note = {Hybrid Systems},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(94)00147-B},
url = {https://www.sciencedirect.com/science/article/pii/030439759400147B},
author = {Michael S. Branicky},
abstract = {We explore the simulation and computational capabilities of hybrid and continuous dynamical systems. The continuous dynamical systems considered are ordinary differential equations (ODEs). For hybrid systems we concentrate on models that combine ODEs and discrete dynamics (e.g., finite automata). We review and compare four such models from the literature. Notions of simulation of a discrete dynamical system by a continuous one are developed. We show that hybrid systems whose equations can describe a precise binary timing pulse (exact clock) can simulate arbitrary reversible discrete dynamical systems defined on closed subsets of Rn. The simulations require continuous ODEs in R2n with the exact clock as input. All four hybrid systems models studied here can implement exact clocks. We also prove that any discrete dynamical system in Zn can be simulated by continuous ODEs in R2n + 1. We use this to show that smooth ODEs in R3 can simulate arbitrary Turing machines, and hence possess the power of universal computation. We use the famous asynchronous arbiter problem to distinguish between hybrid and continuous dynamical systems. We prove that one cannot build an arbiter with devices described by a system of Lipschitz ODEs. On the other hand, all four hybrid systems models considered can implement arbiters even if their ODEs are Lipschitz.}
}
@incollection{WANDELL2025360,
title = {Visual processing},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {360-381},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00116-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001169},
author = {Brian A. Wandell and Jonathan Winawer},
keywords = {Physiological optics, Retinal circuits, Eye movements, Lateral geniculate nucleus, V1, Visual cortex, Functional specialization, Neural signaling, Visual field maps, Retinotopy, Receptive fields, Sparse representations, Asynchronous representation, Redundancy, Bayesian inference},
abstract = {The human visual system is a network of neural components that combine to create our perception of the world and guide our behavior. Deciphering the computational principles of this system is an important scientific challenge. We review measurements of these components, from the retinal encoding to cortical circuitry, and from molecules to circuits, focusing on measurements that are relevant to visual processing. We then delve into principles proposed to explain how this diverse collection of visual components enables us to interpret our surroundings.}
}
@article{ZMIGROD202034,
title = {The role of cognitive rigidity in political ideologies: theory, evidence, and future directions},
journal = {Current Opinion in Behavioral Sciences},
volume = {34},
pages = {34-39},
year = {2020},
note = {Political Ideologies},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619301147},
author = {Leor Zmigrod},
abstract = {A contentious debate in political psychology has centred on the role of cognitive rigidity in shaping individuals’ political ideologies and worldviews. Early theories in the 1950s posited that strict ideological doctrines may tend to attract individuals with dispositions towards mental rigidity. This question has persisted: Does psychological rigidity foster a tendency towards ideological extremism? This review evaluates the empirical landscape with respect to the rigidity-of-the-extreme and the rigidity-of-the-right hypotheses and offers conceptual and methodological recommendations for future research avenues. The evidence suggests that cognitive rigidity is linked to ideological extremism, partisanship, and dogmatism across political and non-political ideologies. Advances in the measurement of ideological extremity and cognitive rigidity will facilitate further elucidation regarding how exactly the two hypotheses may be reconciled and why they have been historically placed in a potentially false competition. This synthesis suggests that a scientifically rigorous understanding of the cognitive roots of ideological thinking may be essential for developing effective antidotes to intolerance and intergroup hostility.}
}
@article{BULLEY20203457,
title = {Children Devise and Selectively Use Tools to Offload Cognition},
journal = {Current Biology},
volume = {30},
number = {17},
pages = {3457-3464.e3},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2020.06.035},
url = {https://www.sciencedirect.com/science/article/pii/S0960982220308514},
author = {Adam Bulley and Thomas McCarthy and Sam J. Gilbert and Thomas Suddendorf and Jonathan Redshaw},
keywords = {cognitive artifacts, cognitive offloading, cognitive development, extended mind, metacognition},
abstract = {Summary
From maps sketched in sand to supercomputing software, humans ubiquitously enhance cognitive performance by creating and using artifacts that bear mental load [1, 2, 3, 4, 5]. This extension of information processing into the environment has taken center stage in debates about the nature of cognition in humans and other animals [6, 7, 8, 9]. How does the human mind acquire such strategies? In two experiments, we investigated the developmental origins of cognitive offloading in 150 children aged between 4 and 11 years. We created a memory task in which children were required to recall the location of hidden targets. In one experiment, participants were provided with a pre-specified cognitive offloading opportunity: an option to mark the target locations with tokens during the hiding period. Even 4-year-old children quickly adopted this external strategy and, in line with a metacognitive account, children across ages offloaded more often when the task was more difficult. In a second experiment, we provided children with the means to devise their own cognitive offloading strategy. Very few younger children spontaneously devised a solution, but by ages 10 and 11, nearly all did so. In a follow-up test phase, a simple prompt greatly increased the rate at which the younger children devised an offloading strategy. These findings suggest that sensitivity to the difficulties of thinking arises early in development and improves throughout the early school years, with children learning to modify the world around them to compensate for their cognitive limits.}
}
@article{ZHANG2022101060,
title = {The neural encoding of productive phonological alternation in speech production: Evidence from Mandarin Tone 3 sandhi},
journal = {Journal of Neurolinguistics},
volume = {62},
pages = {101060},
year = {2022},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2022.101060},
url = {https://www.sciencedirect.com/science/article/pii/S0911604422000045},
author = {Jie Zhang and Caicai Zhang and Stephen Politzer-Ahles and Ziyi Pan and Xunan Huang and Chang Wang and Gang Peng and Yuyu Zeng},
keywords = {Tone sandhi, Mandarin Chinese, Speech production, Event-related potentials, Phonological alternation, Word frequency},
abstract = {The understanding of alternation is a key goal in phonological research. But little is known about how phonological alternations are implemented in speech production. The current study tested the hypothesis that the production of words that undergo a highly productive alternation, Mandarin Tone 3 sandhi, is supported by a computation mechanism, which predicts that this alternation is subserved by neural activity in a time-window associated with post-lexical phonological and phonetic encoding regardless of word frequency. ERPs were recorded while participants sub-vocally produced high- and low-frequency disyllabic words that do or do not require sandhi. Sandhi words elicited more positive ERPs than non-sandhi words over left anterior channels around 336–520 ms after participants saw the cue instructing them to initiate sub-vocal production, but this effect was not significantly modulated by word frequency. These findings are consistent with predictions of the computation mechanism and have implications for current psycholinguistic models of speech production. (150 words)}
}
@article{WANG2022269,
title = {Intelligent Attack Analysis for IRS Communications with Incomplete Information},
journal = {Procedia Computer Science},
volume = {202},
pages = {269-276},
year = {2022},
note = {International Conference on Identification, Information and Knowledge in the internet of Things, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005695},
author = {Han Wang and Tianlin Zhu and Dapeng Li and Rui Jiang and Xiaoming Wang and Youyun Xu},
keywords = {IRS system, monitoring, attacking tactic, greedy, robust attack},
abstract = {Intelligent Reflection Surface (IRS) will be widely used in future communication system construction to reduce construction costs and improve coverage. However, IRS systems are generally equipped with controllers to receive wireless signal instructions, this increases the vulnerability of future communications. In this paper, we present an attack tactic to provide a way of thinking for the future defense deployment. At the beginning, the hacker cannot know the whole communication system, then it continuously attacks the IRS system, monitor the communication system, and sequentially learns new information about the system in each attacking round in order to attack more effectively in the next round. A two-layer optimal mathematical model is presented to describe the BS and the hacker’s decision. And the two-layer optimization which is difficult to solve is transformed into a single layer linear optimization by using equivalent transformation and dual transformation. A series of mathematical experiments are used to test different scenarios applicable to different monitoring style, and verify that the tactic proposed in this paper can effectively interfere with the system.}
}
@article{SAFARZYNSKA20121011,
title = {Evolutionary theorizing and modeling of sustainability transitions},
journal = {Research Policy},
volume = {41},
number = {6},
pages = {1011-1024},
year = {2012},
note = {Special Section on Sustainability Transitions},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2011.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0048733312000595},
author = {Karolina Safarzyńska and Koen Frenken and Jeroen C.J.M. {van den Bergh}},
keywords = {Coevolution, Evolutionary economics, Group selection, Lock-in, Niche, Regime, Social learning, Transition, Transition management},
abstract = {This paper argues that evolutionary thinking and modeling can contribute to the emerging research on sustainability transitions and their management. Evolutionary theory provides a range of concepts and mechanisms that are useful in making existing theorizing about transitions more precise and complete. In particular, we will discuss how the multi-level, multi-phase, co-evolutionary, and social learning dynamics underlying transitions can be addressed in evolutionary models. In addition, evolutionary theorizing offers suggestions for extending current theoretical frameworks of transitions. Group selection provides a good example. We review the small set of formal evolutionary models of sustainability transitions, and show that existing formal evolutionary models of technological, social and institutional change can provide useful inputs to transition research and management.}
}
@article{VALLVERDU20146,
title = {What are Simulations? An Epistemological Approach},
journal = {Procedia Technology},
volume = {13},
pages = {6-15},
year = {2014},
note = {SLACTIONS 2013: Research conference on virtual worlds – Learning with simulations},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314000139},
author = {Jordi Vallverdú},
keywords = {model, computer, simulation, epistemology, representation},
abstract = {Contemporary sciences use a wide and diverse range of computational simulations, including in the areas of aeronautics, chemistry, bioinformatics, social sciences, AI, the physics of elementary particles and most other scientific fields. A simulation is a mathematical model that describes or creates computationally a system process. Simulations are our best cognitive representation of complex reality, that is, our deepest conception of what reality is. In this paper we defend that a simulation is equivalent epistemologically and ontologically with all other types of cognitive models of elements of reality. Therefore, simulations cannot be considered secondary nor weak instruments to approach to the reality analysis.}
}
@article{KOKIS200226,
title = {Heuristic and analytic processing: Age trends and associations with cognitive ability and cognitive styles},
journal = {Journal of Experimental Child Psychology},
volume = {83},
number = {1},
pages = {26-52},
year = {2002},
issn = {0022-0965},
doi = {https://doi.org/10.1016/S0022-0965(02)00121-2},
url = {https://www.sciencedirect.com/science/article/pii/S0022096502001212},
author = {Judite V. Kokis and Robyn Macpherson and Maggie E. Toplak and Richard F. West and Keith E. Stanovich},
abstract = {Developmental and individual differences in the tendency to favor analytic responses over heuristic responses were examined in children of two different ages (10- and 11-year-olds versus 13-year-olds), and of widely varying cognitive ability. Three tasks were examined that all required analytic processing to override heuristic processing: inductive reasoning, deductive reasoning under conditions of belief bias, and probabilistic reasoning. Significant increases in analytic responding with development were observed on the first two tasks. Cognitive ability was associated with analytic responding on all three tasks. Cognitive style measures such as actively open-minded thinking and need for cognition explained variance in analytic responding on the tasks after variance shared with cognitive ability had been controlled. The implications for dual-process theories of cognition and cognitive development are discussed.}
}
@article{VAITESSWAR2024210,
title = {Machine learning based feature engineering for thermoelectric materials by design††Electronic supplementary information (ESI) available: Details on methodology, Box–Cox transformations, machine learning models, and inverse design. See DOI: https://doi.org/10.1039/d3dd00131h},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {210-220},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00131h},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000305},
author = {U. S. Vaitesswar and Daniil Bash and Tan Huang and Jose Recatala-Gomez and Tianqi Deng and Shuo-Wang Yang and Xiaonan Wang and Kedar Hippalgaonkar},
abstract = {Availability of material datasets through high performance computing has enabled the use of machine learning to not only discover correlations and employ materials informatics to perform screening, but also to take the first steps towards materials by design. Computational materials databases are well-labelled and provide a fertile ground for predicting both ground-state and functional properties of materials. However, a clear design approach that allows prediction of materials with the desired functional performance does not yet exist. In this work, we train various machine learning models on a dataset curated from a combination of Materials Project as well as computationally calculated thermoelectric electronic power factor using a constant relaxation time Boltzmann transport equation (BoltzTrap). We show that simple random forest-based machine learning models outperform more complex neural network-based approaches on the moderately sized dataset and also allow for interpretability. In addition, when trained on only cubic material systems, the best performing machine learning model employs a perturbative scanning approach to find new candidates in Materials Project that it has never seen before, and automatically converges upon half-Heusler alloys as promising thermoelectric materials. We validate this prediction by performing density functional theory and BoltzTrap calculations to reveal accurate matching. One of those predicted to be a good material, NbFeSb, has been studied recently by the thermoelectric community; from this study, we propose four new half-Heusler compounds as promising thermoelectric materials – TiGePt, ZrInAu, ZrSiPd and ZrSiPt. Our approach is generalizable to extrapolate into previously unexplored material spaces and establishes an automated pipeline for the development of high-throughput functional materials.}
}
@article{NOURANI2015891,
title = {Predictive Control, Competitive Model Business Planning, and Innovation ERP},
journal = {Procedia Computer Science},
volume = {65},
pages = {891-900},
year = {2015},
note = {International Conference on Communications, management, and Information technology (ICCMIT'2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915028781},
author = {Cyrus F. Nourani and Codrina Lauth},
keywords = {Competitive Models, Innovation Management, ERP. Multiplayer Games, Game Trees Computing, Predictive Modeling, Planning, Competitive Models, Dynamic Programming},
abstract = {New optimality principles are put forth based on competitive model business planning. A Generalized MinMax local optimum dynamic programming algorithm is presented and applied to business model computing where predictive techniques can determine local optima. Based on a systems model an enterprise is not viewed as the sum of its component elements, but the product of their interactions. The paper starts with introducing a systems approach to business modeling. A competitive business modeling technique, based on the author's planning techniques is applied. Systemic decisions are based on common organizational goals, and as such business planning and resource assignments should strive to satisfy higher organizational goals. It is critical to understand how different decisions affect and influence one another. Here, a business planning example is presented where systems thinking technique, using Causal Loops, are applied to complex management decisions. Predictive modeling specifics are briefed. A preliminary optimal game modeling technique is presented in brief with applications to innovation and R&D management. Conducting gap and risk analysis can assist with this process. Example application areas to e-commerce with management simulation models are examined.}
}
@article{ZHANG2017123,
title = {Collective decision optimization algorithm: A new heuristic optimization method},
journal = {Neurocomputing},
volume = {221},
pages = {123-137},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216311183},
author = {Qingyang Zhang and Ronggui Wang and Juan Yang and Kai Ding and Yongfu Li and Jiangen Hu},
keywords = {Collective decision optimization algorithm, Artificial neural networks, Meta-heuristic, Decision-making},
abstract = {Recently, inspired by nature, diversiform successful and effective optimization methods have been proposed for solving many complex and challenging applications in different domains. This paper proposes a new meta-heuristic technique, collective decision optimization algorithm (CDOA), for training artificial neural networks. It simulates the social behavior of human based on their decision-making characteristics including experience-based phase, others'-based phase, group thinking-based phase, leader-based phase and innovation-based phase. Different corresponding operators are designed in the methodology. Experimental results carried out on a comprehensive set of benchmark functions and two nonlinear function approximation examples demonstrate that CDOA is competitive with respect to other state-of-art optimization algorithms.}
}
@article{BRENTDANIEL2023105630,
title = {Extremely rapid, Lagrangian modeling of 2D flooding: A rivulet-based approach},
journal = {Environmental Modelling & Software},
volume = {161},
pages = {105630},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105630},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000166},
author = {W. {Brent Daniel} and Corinne Roth and Xue Li and Cindy Rakowski and Tim McPherson and David Judi},
keywords = {Modeling, Fluid flow, Flood modeling, Hydrodynamic model, Rivulet, Lagrangian},
abstract = {Estimates of potential flood inundation areas and depths are critical to informing the preparedness, response, and investment decisions of many government agencies and private sector organizations, especially under a changing climate. The standard modeling approaches, however, are often either computationally intensive or constrained in their accuracy or applicability. A novel, rivulet-based, 2D model of flooding is described in this article that is 10,000 to 10 million times less computationally complex than the full solution of the shallow water equations, yet achieves inundation area hit rates of between 0.8 and 0.9, relative absolute mean errors of 10%–20% across a wide range of flow depths, and comparable accuracy at forecasting empirical high-water marks. This combination of accuracy and efficiency will significantly enhance real-time depth estimates during flood events, support detailed sensitivity analyses, and allow for the generation of large ensembles to enable complex uncertainty analyses.}
}
@article{CARVALHO2016169,
title = {Origins and evolution of enactive cognitive science: Toward an enactive cognitive architecture},
journal = {Biologically Inspired Cognitive Architectures},
volume = {16},
pages = {169-178},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2015.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X15000535},
author = {Leonardo Lana de Carvalho and Denis James Pereira and Sophia Andrade Coelho},
keywords = {Cognitive science, Enaction, Complex systems, Cognitive architecture},
abstract = {This paper presents a historical perspective on the origin of the enactive approach to cognitive science, starting chronologically from cybernetics, with the aim of clarifying its main concepts, such as enaction, autopoiesis, structural coupling and natural drift; thus showing their influences in computational approaches and models of cognitive architecture. Works of renowned authors, as well as some of their main commentators, were addressed to report the development of enactive approach. We indicate that the enactive approach transcends its original context within biology, and at a second moment within connectionism, changes the understanding of the relationships so far established between the body and the environment, and the ideas of conceptual relationships between the mind and the body. The influence on computational theories is of great importance, leading to new artificial intelligence systems as well as the proposition of complex, autopoietic and alive machines. Finally, the article stresses the importance of the enactive approach in the design of agents, understanding that previous approaches have very different cognitive architectures and that a prototypical model of enactive cognitive architecture is one of the largest challenges today.}
}
@incollection{WARE20081,
title = {Chapter 1 - Visual Queries},
editor = {Colin Ware},
booktitle = {Visual Thinking},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {1-22},
year = {2008},
isbn = {978-0-12-370896-0},
doi = {https://doi.org/10.1016/B978-0-12-370896-0.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123708960000019},
author = {Colin Ware},
abstract = {Publisher Summary
This book about graphic design provides a channel for clear communication that supports visual thinking and acts as an interface to the vast information resources of the modern world. Visual thinking is a process that has the allocation of attention as its very essence. Attention, however, is multifaceted. Making an eye movement is an act of attending. Eye movements are executed to satisfy the need for information and can be thought of as a sequence of visual queries on the visual world. The idea of the visual query is shorthand for what one does when obtaining information either from the world at large or from some kind of information display. Understanding what visual queries are easily executed is a critical skill for the designer. The special skill of designers is not so much skill with drawing or graphic design software, although these are undoubtedly useful, but the talent to analyze a design in terms of its ability to support the visual queries of others. One reason why design is difficult is that the designer already has the knowledge expressed in the design and has seen it develop from inception and therefore cannot see it with fresh eyes. The solution is to be analytic and this is where this book is intended to have value. Effective design should start with a visual task analysis, determine the set of visual queries to be supported by a design, and then use color, form, and space to efficiently serve those queries.}
}
@article{THANKACHAN2024101283,
title = {A mathematical formulation of learner cognition for personalised learning experiences},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101283},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101283},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000779},
author = {Jeena A. Thankachan and Bama Srinivasan},
keywords = {Virtual Learning Environment (VLE), Cognitive Evaluation Metrics (CEM), Multimode evaluation, Cognitive abilities, Learning tasks, Reinforcement learning},
abstract = {The paper focuses on the assessment of cognitive skills within Virtual Learning Environments (VLEs). In response to the global shift to remote learning amid the COVID-19 pandemic, VLEs, which include learning management systems (LMS) and online collaboration platforms, gained prominence. The proposed work leverages an established Cattell–Horn–Carroll (CHC) theory to propose eight metrics, which collectively form a part of Cognitive Evaluation Metrics (CEM). The proposed metrics introduce a novel computational approach for multimode evaluation of learners’ cognitive abilities for each learning task within a learning environment. The paper details the formalism for the evaluation of the metrics and makes a contribution towards the potential of the proposed methodology to evaluate cognitive abilities. Additionally, the work implements CEM integration into the learner module of a Game-Based Learning (GBL) environment. Analysis of simulations in the GBL environment, along with statistical analysis, provides insights into the normal distribution of cognitive metrics. This reveals diverse ranges in various abilities such as long or short term memory, working memory, reasoning, attention, and processing speed. The paper also explores the impact of virtual assistants, which highlights their limited relevance to enhance cognitive abilities but serve as valuable on-demand support resources.}
}
@article{VANCOUVER20081,
title = {Integrating self-regulation theories of work motivation into a dynamic process theory},
journal = {Human Resource Management Review},
volume = {18},
number = {1},
pages = {1-18},
year = {2008},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2008.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053482208000028},
author = {Jeffrey B. Vancouver},
keywords = {Self-regulation, Control theory, Goals, Computational modeling, Dynamic processes},
abstract = {Instead of merely combining theories of self-regulation, the current paper articulates a dynamic process theory of the underlying cognitive subsystems that explain relationships among long-used constructs like goals, expectancies, and valence. Formal elements of the theory are presented in an attempt to encourage the building of computational models of human actors, thinkers, and learners in organizational contexts. Discussion focuses on the application of these models for understanding the dynamics of individuals interacting in their organizations.}
}
@incollection{BRAME201915,
title = {Chapter 2 - Course Design: Making Choices About Constructing Your Course},
editor = {Cynthia J. Brame},
booktitle = {Science Teaching Essentials},
publisher = {Academic Press},
pages = {15-28},
year = {2019},
isbn = {978-0-12-814702-3},
doi = {https://doi.org/10.1016/B978-0-12-814702-3.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128147023000020},
author = {Cynthia J. Brame},
keywords = {Undergraduate, science, education, course design, learning goals, learning objectives, guiding questions, formative assessment},
abstract = {Designing or redesigning a course can be a creative and rewarding effort, but it is always a challenge. Science is characterized by continuous change and an ever-growing (and already large!) body of knowledge, and our courses often seek to help students understand the core knowledge, experimental tools, and ways of thinking in a field. It’s a big task. Further, a course may play a particular role in the curriculum, serving as a prerequisite, a capstone, or the course in which students learn a particular skill. How do you pick on what to focus, and how do you organize your course to help your students be able to transfer their knowledge to a new setting? How can you design the course to help your students build a conceptual framework that can expand and grow as their understanding grows? This chapter describes six principles to guide your course design and provides suggestions for more detailed resources.}
}
@article{HUANG2025135963,
title = {Dynamic performance optimization of a floating offshore wind turbine based on fractal-inspired design principles},
journal = {Energy},
volume = {324},
pages = {135963},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.135963},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225016056},
author = {Haoda Huang and Qingsong Liu and Musa Bashir and Sean Malkeson and Chun Li and Minnan Yue and Weipao Miao and Jin Wang},
keywords = {Floating offshore wind turbine, Leaf-vein structure, Computational fluid dynamics, Fractal dimension, Fully coupled dynamic response},
abstract = {As the development of onshore and fixed offshore wind turbines approaches saturation, floating offshore wind turbines (FOWTs) are increasingly gaining attention due to their ability to operate in deeper waters and harness more stable wind resources. However, the dynamic responses of FOWTs are amplified significantly under the complex sea conditions, posing challenges to the overall system stability. This study proposes a novel semi-submersible platform featuring fractal structure inspired by Victoria Amazonica as solutions to the overall system stability of FOWTs. The computational fluid dynamics method, integrated with dynamic fluid-body interaction and volume of fluid wave model, is used to examine the aero, hydro, and mooring dynamics of the FOWT. A parametric model of the fractal structure with different branch levels is constructed by recursive method. Firstly, the hydrodynamic performance of the novel platforms with multi-level branch structures is examined under single wave conditions. The results show that vortices in fractal structures present higher velocity gradients and greater viscous dissipation, thereby effectively absorbing wave energy. The stability of the platform improves progressively as the branch levels increase. Subsequently, the dynamic responses of the full-configuration FOWT mounted on the platform with 8-level fractal structure (8LFS-FOWT) are further evaluated under wind-wave coupling conditions. The results reveal that 8LFS-FOWT achieves superior hydrodynamic performance with the most notable improvement in pitch amplitude of 25.22 % decrease. This enhancement also brings a 12.75 % reduction in the standard deviation of power output, forming positive feedback to ensure safe and stable operation of the system. The findings provide a valuable reference for promoting the innovative platform design of FOWTs.}
}
@incollection{TIRAPELLE20233465,
title = {Practical learning activities to increase the interest of university applicants in STEM careers in the era of Industry 4.0},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {3465-3470},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50553-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740505539},
author = {Monica Tirapelle and Dian Ning Chia and Fanyi Duanmu and Konstantinos Katsoulas and Alberto Marchetto and Eva Sorensen},
keywords = {Engineering Education, Hands-on activities, Active learning, Orientation to university, PSE computational tools},
abstract = {Inspiring young students, especially young girls, about STEM disciplines is crucial to address the current shortage of engineers. Since the engineering skills that are required by graduates are evolving in line with technological progress, there is now an even stronger need for graduates with strong Process Systems Engineering skills. In this work, we describe an effective way to promote the chemical engineering curriculum, with particular emphasis on computational tools, to a group of Year 12 high school students during a one-week course in our department. The course was designed to engage students in active learning through interactive sessions and practical hands-on activities. Through the course, the students gained a better understanding of the importance of STEM subjects and, in particular, of the challenges and opportunities that engineers encounter in the era of Industry 4.0 with ever-increasing use of digitalization in process design and operation.}
}
@article{LIGABO2023102155,
title = {Practical way to apply fourth-generation assessment tools integrated into creating meaningful learning experiences in biology at high school},
journal = {Evaluation and Program Planning},
volume = {96},
pages = {102155},
year = {2023},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2022.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0149718922001094},
author = {Mateus Ligabo and Fabiana Carvalho Silva and Ana Carolina da S.A. Carvalho and Durval Rodrigues and Rita C.L.B. Rodrigues},
keywords = {Concept maps, Meaningful learning, Hermeneutic-dialectic circle, Hofstede's cultural dimensions, Fourth-generation assessment},
abstract = {The learning process for a Biology topic regarding organisms and animal kingdom diversity was investigated through an innovative Interactive Didactic Sequence (IDS) which integrated the idea of “concept maps” with the Hermeneutic-Dialectic Circle (HDC). HDC is a tool for data collection and a reference for pluralist-constructivist thinking, considered a form of fourth-generation evaluation. Hofstede's cultural dimensions were also integrated into the investigation in order to facilitate mediation in an evaluative context. Students' performances (N = 25) from a São Paulo-Brazil public school were statistically evaluated. Their cultural profile was determined via the Hofstede Value Survey Model 1994 questionnaire. The elaborative process of arranging concept maps was individual (CM-individual) and integrated with HDC in groups (CM-HDC). Concept map assessment methods were based off existing literature. An improvement in students' performances (p < 0.05) that presented concept maps integrated to HDC in a more complex structure when compared to individually-built maps was observed. Employment of HDC helped form motivational/interactive dialogues between students and teachers, which, in turn, assisted in achieving greater learning through the use of concept maps. The application of the fourth-generation evaluation was improved via knowledge regarding students' cultural profiles.}
}
@article{MACLEOD2019101201,
title = {Mesoscopic modeling as a cognitive strategy for handling complex biological systems},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {78},
pages = {101201},
year = {2019},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2019.101201},
url = {https://www.sciencedirect.com/science/article/pii/S1369848618300839},
author = {Miles MacLeod and Nancy J. Nersessian},
keywords = {Mesoscopic modeling, Middle-out strategy, Systems biology, Model building, Mental modeling, Distributed cognition, Bounded rationality},
abstract = {In this paper we aim to give an analysis and cognitive rationalization of a common practice or strategy of modeling in systems biology known as a middle-out modeling strategy. The strategy in the cases we look at is facilitated through the construction of what can be called mesoscopic models. Many models built in computational systems biology are mesoscopic (midsize) in scale. Such models lack the sufficient fidelity to serve as robust predictors of the behaviors of complex biological systems, one of the signature goals of the field. This puts some pressure on the field to provide reasons for why and how these practices are warranted despite not meeting the stated goals of the field. Using the results of ethnographic study of problem-solving practices in systems biology, we aim to examine the middle-out strategy and mesoscopic modeling in detail and to show that these practices are rational responses to complex problem solving tasks on cognitive grounds in particular. However making this claim requires us to update the standard notion of bounded rationality to take account of how human cognition is coupled to computation in these contexts. Our account fleshes out the idea that has been raised by some philosophers on the “hybrid” nature of computational modeling and simulation. What we call “coupling” both extends modelers’ capacities to handle complex systems, but also produces various cognitive and computational constraints which need to be taken into account in any computational problem solving strategy seeking to maintain insight and control over the models produced.}
}
@incollection{KLATT200719,
title = {Perspectives for process systems engineering – a personal view from academia and industry},
editor = {Valentin Pleşu and Paul Şerban Agachi},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {24},
pages = {19-32},
year = {2007},
booktitle = {17th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(07)80027-7},
url = {https://www.sciencedirect.com/science/article/pii/S1570794607800277},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, critical assessment, emerging fields, modeling, design, optimization, control, operations, numerical algorithms, software.},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Modeling, simulation and optimization technologies have been developed to a mature state. These technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. Systems thinking has been established in industrial practice largely through powerful commercial process simulation software and through mandatory courses in most chemical engineering programs. This contribution reflects on the past, present and future of PSE. Special emphasis will be on the perspectives of this field from an academic and industrial point of view.}
}
@incollection{WHITTEN201953,
title = {Chapter 4 - Guided Cognition Effects in Learning Mathematics},
editor = {William B. Whitten and Mitchell Rabinowitz and Sandra E. Whitten},
booktitle = {Guided Cognition for Learning},
publisher = {Academic Press},
pages = {53-108},
year = {2019},
isbn = {978-0-12-817538-5},
doi = {https://doi.org/10.1016/B978-0-12-817538-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128175385000043},
author = {William B. Whitten and Mitchell Rabinowitz and Sandra E. Whitten},
keywords = {Advance organizers, Consolidators, Effective homework, Efficient homework, Guided Cognition design, Homework, Long-term or long-lasting learning, Middle school mathematics learning},
abstract = {This chapter reports 11 experiments that were done to determine whether Guided Cognition-designed homework facilitates learning middle school mathematics, and if so, to determine how it helps and what is learned. Experiments were performed in two middle schools and included 8th graders in two experiments and 7th graders in nine experiments. Mathematics topics ranged from fractions to integers to geometry. As in the literature experiments, students were in their normal school environment following their regular curriculum and were unaware that their learning was being observed. Guided Cognition design was found to be effective for learning mathematics. Working story problems that were enriched with cognitive events such as role play, divergent thinking, visualizing and illustrating, and relating to prior experience raised scores on unexpected quizzes by about a letter grade. Another unexpected quiz found that the improvements in problem-solving performance persisted for 6months. Guided Cognition homework was also found to be efficient in that students who worked eight problems and then performed four cognitive events performed as well on unexpected quizzes as students who worked 24 problems in the same time interval. Another pair of experiments determined that modest gains could be made from merely reading completed examples of cognitive events, but that these gains were not long-lasting. Performing the cognitive events was found to be most effective for long-term performance. Another experiment found that experiencing cognitive events after working some mathematics problems can help consolidate knowledge of how to work such problems.}
}
@article{BRANDT20051578,
title = {Mental spaces and cognitive semantics: A critical comment},
journal = {Journal of Pragmatics},
volume = {37},
number = {10},
pages = {1578-1594},
year = {2005},
note = {Conceptual Blending Theory},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2004.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0378216605000603},
author = {Per Aage Brandt},
keywords = {Mental Space Theory, Truth conditions, Spinoza, Semantic domains, Mental architecture, Material anchors},
abstract = {The article criticizes the negative influence of modern analytic, anti-semantic and anti-phenomenological thinking on cognitive semantics, and the errors or weaknesses of analysis it induces in current Mental Space Theory (MST). It also shows how a less inhibited theory of meaning, mental spaces and blending could develop more useful analyses of empirical occurrences, such as the artifacts called ‘material anchors’ and works of art — here exemplified by a painting by Matisse.}
}
@incollection{FROEMER2025234,
title = {Belief updates, learning and adaptive decision making},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {234-251},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000590},
author = {Romy Froemer and Matthew R. Nassar},
keywords = {Reinforcement learning, Reward, Value, Action, Dopamine, Belief updating, Sequential sampling, Attention, Confidence, Context, Experience, Goal-directed behavior, Cost-benefit decision-making},
abstract = {People make decisions every day and the outcomes of those decisions often lead them to change their beliefs and in some cases shape their future behavior. How does the brain decide which meal to order at a restaurant, and how does it learn from the experience of eating that meal? Here we review work from neuroscience, psychology and economics that shapes our understanding of how the brain makes decisions and learns through experience. We focus on computational mechanisms that can explain core phenomena in learning and decision making as well as how such mechanisms are implemented in the brain. Our review highlights both the considerable progress made in the last decades elucidating mechanisms of learning and decision making as well as the vast territory of open questions that remain to be answered.}
}
@article{LITT1993459,
title = {Single neuron computation: T. McKenna, J. Davis and S.F. Zornetzer (Eds.) (Academic Press, San Diego, CA, 1992, 664 p., Price US $59.95)},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {87},
number = {6},
pages = {459-460},
year = {1993},
issn = {0013-4694},
doi = {https://doi.org/10.1016/0013-4694(93)90160-W},
url = {https://www.sciencedirect.com/science/article/pii/001346949390160W},
author = {Brian Litt}
}
@article{PHONAPICHAT20143169,
title = {An Analysis of Elementary School Students’ Difficulties in Mathematical Problem Solving},
journal = {Procedia - Social and Behavioral Sciences},
volume = {116},
pages = {3169-3174},
year = {2014},
note = {5th World Conference on Educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.01.728},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814007459},
author = {Prathana Phonapichat and Suwimon Wongwanich and Siridej Sujiva},
keywords = {Mathematical problem solving, mathematical difficulties, mathematical skills, elementary school students},
abstract = {The main purpose of mathematics teaching is to enable students to solve problems in daily life. Unfortunately, according to the latest national test results, most students lack mathematical problem solving skills. This proves to be one of the reasons why overall achievement in mathematics is considered quite low. It also reflects that students have difficulties in comprehending mathematical problems affecting the process of problem-solving. Therefore, in order to allow teachers to establish a proper teaching plan suitable for students’ learning process, this research aims to analyze the difficulties in mathematical problem solving among elementary school students. Samples are divided into two groups, elementary school students and mathematics teachers. Data collection was conducted by structured interview, documentary analysis, and survey tests. Data analysis was conducted by descriptive statistics, and content analysis. The results suggest that there are several difficulties in problem solving, namely 1) Students have difficulties in understanding the keywords appearing in problems, thus cannot interpret them in mathematical sentences. 2) Students are unable to figure out what to assume and what information from the problem is necessary to solving it, 3) Whenever students do not understand the problem, they tend to guess the answer without any thinking process, 4) Students are impatient and do not like to read mathematical problems, and 5) Students do not like to read long problems. Therefore, the results found in this research will lead to the creation and the development of mathematical problem solving diagnostic tests for teachers, in order to improve students’ mathematical problem solving skills.}
}
@article{WANG2025113691,
title = {Effect of interlayer spacing on the mechanical properties of the graphene oxide/thermoplastic polyurethane nanocomposite},
journal = {Computational Materials Science},
volume = {250},
pages = {113691},
year = {2025},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2025.113691},
url = {https://www.sciencedirect.com/science/article/pii/S0927025625000345},
author = {Yuyang Wang and Guofu Yin and Junpeng Liu and Jiao Li and Chunqiang Wei and Minjie Li},
keywords = {Thermoplastic Polyurethane, Graphene Oxide, Interlayer Spacing, Mechanical Properties, Molecular Dynamics},
abstract = {In this paper, in order to investigate the effect of graphene oxide (GO) interlayer spacing on the overall mechanical properties of GO/thermoplastic polyurethane (TPU) nanocomposite, the uniaxial tensile simulation on the computational model of GO/TPU nanocomposite with monolayer and bilayer graphene sheets were carried out. During the tensile process, the variation of potential energy, void percentage, tensile strain contour, and density distribution with tensile strain were applied to analyze the underlying microscopic mechanism of stress change of monolayer GO/TPU system. The results show that when the system was within the stress yield region, the micro rearrangement motion and relative sliding of TPU chains play a major role. Furthermore, when the system enters into the stress softening region until stress failure, the void formation and nuclear within the system dominates the stress change, which is attributed to the interfacial structure of GO/TPU. Subsequently, based on the above-mentioned results in the case of monolayer GO/TPU result, the effects of different layer spacings on the overall mechanical properties of GO/TPU nanocomposite system were discussed by varying the spacing of GO layers. The results show that with the increase of GO layer spacing, the elastic modulus and yield strength of the system show a tendency of increasing and then decreasing, and the failure strain is the opposite, and when the GO dispersion is better, it can also play the role of delayed damage failure of the system.}
}
@article{OLIVIER2024103956,
title = {DynBioSketch: A tool for sketching dynamic visual summaries in biology, and its application to infection phenomena},
journal = {Computers & Graphics},
volume = {122},
pages = {103956},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103956},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000918},
author = {Pauline Olivier and Tara Butler and Pascal Guehl and Jean-Luc Coll and Renaud Chabrier and Pooran Memari and Marie-Paule Cani},
keywords = {Sketch-based modeling, Interactive geometric modeling, Sketch-based animation, Narration},
abstract = {Having simple methods of illustration is essential to scientific thinking. To complement the abstract sketches regularly used in cell biology, we propose DynBioSketch, an easy-to-use digital modeling and animation tool, enabling biologists to resort to less simplified representations when necessary without having to call professional artists. DynBioSketch is an interactive sketching system dedicated to the design and communication of biological phenomena at the cellular scale that can be illustrated in a few minutes of animation. Our model integrates 3D modeling, pattern-based design of 3D shape distributions, and sketch-based animation. These elements can be combined to create complex scenarios such as the infection phenomenon on which we focus, allowing a narrative design adapted to communication between researchers or educational applications in biology. Our results, along with a user study conducted with biology researchers, highlight the potential of DynBioSketch in enabling the direct design of dynamic visual summaries that convey relevant information, as shown in our infection case study. By bridging the gap between abstract representations used by experts and more illustrative depictions, DynBioSketch opens a new avenue for communicating biological concepts.}
}
@article{CHEN2025121691,
title = {A novel attribute reduction algorithm based on granular sequential three-way decision},
journal = {Information Sciences},
volume = {694},
pages = {121691},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121691},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524016050},
author = {Yuliang Chen and Yunlong Cheng and Binbin Luo and Yabin Shao and Mingfu Zhao and Qinghua Zhang},
keywords = {Granular computing, Sequential three-way decision, Granular rough sets, Attribute reduction},
abstract = {Attribute reduction plays a crucial role in knowledge discovery, and sequential three-way decision (S3WD) provides a new method for attribute reduction. However, the three regions of the S3WD model are usually represented as three sets, which leads to two disadvantages. On one hand, it is difficult to obtain the condition of a decision rule when multiple equivalence classes are merged into a set because different equivalence classes have different descriptions. On the other hand, if the boundary region of the upper level of S3WD is a set, one has to partition the upper level with all the acquired attributes rather than the newly added attribute. That is, there is double counting. Therefore, this paper focuses on how to retain the topology of equivalence classes in S3WD, and how to use this topology to enhance semantic interpretation and improve computational efficiency. To this end, a granular version of S3WD, called granular sequential three-way decision (GS3WD), is first developed to retain the information structure of equivalence classes. And then, three acceleration strategies and an efficient granular sequential three-way reduction (GS3WR) are proposed. Finally, a concept tree can be generated simultaneously in the process of GS3WR, and the decision rules with multi-granularity can be extracted from this concept tree directly. Experimental results show that GS3WR can obtain the same core attributes and reducts as the representative attribute reduction algorithms in rough sets and the computational efficiency is improved by hundreds of times.}
}
@incollection{MAMATHA2024259,
title = {Chapter Eleven - Bio-intelligent computing and optimization techniques for developing computerized solutions},
editor = {Anupam Biswas and Alberto Paolo Tonda and Ripon Patgiri and Krishn Kumar Mishra},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {135},
pages = {259-288},
year = {2024},
booktitle = {Applications of Nature-Inspired Computing and Optimization Techniques},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S006524582300089X},
author = {G.S. Mamatha and Haripriya V. Joshi and R. Amith},
keywords = {Bio-intelligent, Bio-inspired, Computing, Optimization technique, Bio-engineering},
abstract = {Bio-inspired computing is a field of study that Lois lee knits together subfields related to the connectionism, social behavior and emergence. It is often closely related to the field of artificial intelligence as many of its pursuits can be linked to machine learning. It relies heavily on fields of biology, computer science and mathematics. Briefly it is the use of computers to model the living phenomena and simultaneously the study of life to improve the usage of computer. Biologically inspired computation is a major subset of natural computation areas of research. Some areas of study encompassed under the canon of biologically inspired computing and their biological counterparts are, genetic algorithms, evolution, biodegradability prediction, biodegradation, cellular automata, life emergent system ants, termites, bees, wasps, neural networks, artificial immune systems rendering patterning and animal skins, bird feathers, mollusk shells and bacterial colonies. Linder Mayer systems, plant structures, communication networks and protocol, epidemiology and the spared of disease, intra membrane molecular processes in living cells, excitable media forest fires the wave heart conditions axons and sensor networks sensory organs. Optimization techniques takes more bottom-up decentralized approach and often involves the methods of specifying a set of simple rules, a set of simple organisms which adhere to those rules and method of iteratively applying those rules for example, training virtual insect to investigate to an unknown terrain for finding food includes six simple rules which can be adopted. After several generations of rules application, it is usually the case where some forms of complex behavior get built upon complexity until the end results is something markedly complex and quite often completely counterintuitive from what the original rules would be expected to produce. For this reason, most technology-oriented solutions like neural network models, algorithms and other techniques came in to existence for accurate measurements and analysis that can be used to refine statistical inference and extrapolation as system complexity increases. The rules of nature inspired computing are the principle simple rules yet after being used for over millions of years have produced remarkably complex optimization techniques. All these techniques for developing software applications along with optimization techniques are discussed in the chapter.}
}
@article{SIGALA2018151,
title = {New technologies in tourism: From multi-disciplinary to anti-disciplinary advances and trajectories},
journal = {Tourism Management Perspectives},
volume = {25},
pages = {151-155},
year = {2018},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211973617301435},
author = {Marianna Sigala},
abstract = {Technologies transform tourism management and marketing from a static and utilitarian sense (whereby managers and tourists use technologies as tools) to a transformative conceptualization whereby tourism markets and actors both shape and are shaped by technology. This paper unravels the transformative power of technologies on: the tourism actors and resources (both the traditional but also new actors, i.e. the technology agents); the ways actors interact to (co-)create but also (co-)destruct tourism value; and the context in which tourism actors interact from a linear supply chain tourism ‘industry’ to a complex socio-technical smart tourism ecosystem. To study such complex phenomena and transformations, the paper emphasises that research should not only adopt a multi-disciplinary approach, but it also needs to follow an anti-disciplinary thinking whereby new knowledge and constructs do not simply fall within existing paradigms, disciplinary silos and mindsets once developed by studying the ‘pure’ humans and their behaviours.}
}
@article{FU2013729,
title = {Expert representation of design repository space: A comparison to and validation of algorithmic output},
journal = {Design Studies},
volume = {34},
number = {6},
pages = {729-762},
year = {2013},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000495},
author = {Katherine Fu and Joel Chan and Christian Schunn and Jonathan Cagan and Kenneth Kotovsky},
keywords = {computer supported design, design by analogy, design methods, engineering design},
abstract = {Development of design-by-analogy tools is a promising design innovation research avenue. Previously, a method for computationally structuring patent databases as a basis for an automated design-by-analogy tool was introduced. To demonstrate its strengths and weaknesses, a computationally-generated structure is compared to four expert designers' mental models of the domain. Results indicate that, compared to experts, the computationally-generated structure is sensible in clustering of patents and organization of clusters. The computationally-generated structure represents a space in which experts can find common ground/consensus – making it promising to be intuitive/accessible to broad cohorts of designers. The computational method offers a resource-efficient way of usefully conceptualizing the space that is sensible to expert designers, while maintaining an element of unexpected representation of the space.}
}
@article{WILSON1997575,
title = {Computation and controversy: Value conflicts and social choices: R. KLING (Ed.) 2nd ed. Academic Press, New York (1996). xxiv + 961 pp., ISBN 0-12-415040-3},
journal = {Information Processing & Management},
volume = {33},
number = {4},
pages = {575-577},
year = {1997},
issn = {0306-4573},
doi = {https://doi.org/10.1016/S0306-4573(97)82727-6},
url = {https://www.sciencedirect.com/science/article/pii/S0306457397827276},
author = {Tom Wilson}
}
@article{MOGILNER2019R915,
title = {Alex Mogilner},
journal = {Current Biology},
volume = {29},
number = {19},
pages = {R915-R917},
year = {2019},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2019.07.077},
url = {https://www.sciencedirect.com/science/article/pii/S0960982219309571},
author = {Alex Mogilner}
}