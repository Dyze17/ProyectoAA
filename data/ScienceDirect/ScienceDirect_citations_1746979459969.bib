@article{KOMPALLI2016534,
title = {Clusters of Genetic-based Attributes Selection of Cancer Data},
journal = {Procedia Computer Science},
volume = {89},
pages = {534-539},
year = {2016},
note = {Twelfth International Conference on Communication Networks, ICCN 2016, August 19– 21, 2016, Bangalore, India Twelfth International Conference on Data Mining and Warehousing, ICDMW 2016, August 19-21, 2016, Bangalore, India Twelfth International Conference on Image and Signal Processing, ICISP 2016, August 19-21, 2016, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.06.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916311632},
author = {Vijaya Sri Kompalli and K. Usha Rani},
keywords = {Cluster, Coupling, Cohesion, Genetic Algorithm, Fuzzy C-Means.},
abstract = {Clustering of data simplifies the task of data analysis and results in better disease diagnosis. Well-existing K-Means clustering hard computes clusters. Due to which the data may be centered to a specific cluster having less concentration on the effect of the coupling of clusters. Soft Computing methods are widely used in medical field as it contains fuzzy natured data. A Soft Computing approach of clustering called Fuzzy C-Means (FCM) deals with coupling. FCM clustering soft computes the clusters to determine the clusters based on the probability of having memberships in each of the clusters. The probability function used, determines the extent of coupling among the clusters. In order to achieve the computational efficiency and binding of features genetic evaluation is introduced. Genetic-based features are identified having more cohesion based on the fitness function values and then the coupling of the clusters is done using K-Means clustering in one trial and FCM in another trial. Analysis of coupling and cohesion is performed on Wisconsin Breast Cancer Dataset. Nature of clusters formations are observed with respect to coupling and cohesion.}
}
@article{PENG2025104791,
title = {From GPT to DeepSeek: Significant gaps remain in realizing AI in healthcare},
journal = {Journal of Biomedical Informatics},
volume = {163},
pages = {104791},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104791},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000206},
author = {Yifan Peng and Bradley A. Malin and Justin F. Rousseau and Yanshan Wang and Zihan Xu and Xuhai Xu and Chunhua Weng and Jiang Bian},
keywords = {DeepSeek, ChatGPT, AI in Healthcare}
}
@article{MAJID2004108,
title = {Can language restructure cognition? The case for space},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {3},
pages = {108-114},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304000208},
author = {Asifa Majid and Melissa Bowerman and Sotaro Kita and Daniel B.M. Haun and Stephen C. Levinson},
abstract = {Frames of reference are coordinate systems used to compute and specify the location of objects with respect to other objects. These have long been thought of as innate concepts, built into our neurocognition. However, recent work shows that the use of such frames in language, cognition and gesture varies cross-culturally, and that children can acquire different systems with comparable ease. We argue that language can play a significant role in structuring, or restructuring, a domain as fundamental as spatial cognition. This suggests we need to rethink the relation between the neurocognitive underpinnings of spatial cognition and the concepts we use in everyday thinking, and, more generally, to work out how to account for cross-cultural cognitive diversity in core cognitive domains.}
}
@article{PANETSOS2011314,
title = {Physical measurement of brain perception abilities. Foundations of a working methodology for the design of “intelligent” beings},
journal = {Procedia Computer Science},
volume = {7},
pages = {314-316},
year = {2011},
note = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.09.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911006120},
author = {F. Panetsos and S.L. Andino Gonzalez and P.C. Marijuan and C. Herrera-Rincon},
keywords = {Emergent properties, complexity, artificial brain, synthetic approach},
abstract = {Most of the important properties of the brain (thinking, consciousness, music, etc.) are severely ill-defined. They are not the direct output of biological sensors or their combinations but emerge from complex computations at the network level and are not necessarily represented in the sensory input or the activity of individual cells. They are emergent properties arising from dynamic interactions between neurons in the different relay stations of the sensory pathways where recognition of basic physical properties of incoming stimuli take place. Emergent properties and interactions between them range from physical properties of stimuli to cognitive operations as emotions or consciousness and gradually involve interactions between sensory pathways, associative cortexes, hippocampus, or the amygdala. Here we propose to build neural tissues from embryonic stem cells in “in vitro” controlled environments to determine the way physical inputs are transformed into what humans perceive and measure. We will start with “low complexity” tissues able to perform low level recognition of physical properties, to gradually increase the complexity of the tissue to investigate how the physical characteristics of the incoming stimuli correspond at higher levels to the emergent properties of the system. Mathematical methods based on networks theory, nonlinear dynamics, fractal theory and chaos among other will be used to determine and measure the emergent properties of the nervous tissue at different complexity levels. We expected to provide criteria and methodologies to measure human-like perception variables and use them for the design of future living artifacts (autonomous robots, intelligent sensors, hybrid systems, etc.).}
}
@incollection{BIR202197,
title = {Chapter Four - Generic quantum hardware accelerators for conventional systems},
editor = {Shiho Kim and Ganesh Chandra Deka},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {122},
pages = {97-133},
year = {2021},
booktitle = {Hardware Accelerator Systems for Artificial Intelligence and Machine Learning},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000322},
author = {Parth Bir},
keywords = {Quantum mechanics, Computational basis, State space, Deterministic model, Probabilistic model, QA, GQHA},
abstract = {Quantum mechanics proposes, universe is a sum of a generic building block. Different orientation (i.e., angle, phase, amplitude, etc.) and summation of blocks forms entities. When differentiated, building blocks used for formation of entity are termed as basis. Following computational theory, these basis are termed as computational basis. Classical computers possess binary basis. Quantum system possess exponential computational power because of infinite computational basis. When computing solution to a problem, it's found in state space. Deterministic model (Conventional) requires both correct and incorrect solution set. For problems of probabilistic nature with plenty of variables (NP and P problems), computing solution requires exponential time, as entire state space is scanned. Furthermore, if solution is incomputable, the computation will never complete as solution is missing from both sets. Probabilistic model (Quantum) conducts a guided state space search and possess greater information carrying capacity per bit. Therefore, Quantum Accelerators (QA) are ideal for solving such problems. Resulting implementation of a Generic Quantum Hardware Accelerator (GQHA) is described via algorithms, mathematical models and microarchitecture. Next, a competitive industrial analysis and virtual implementation in a cloud environment is defined. Finally, it's proven that GQHA can replace conventional accelerators to produce faster and reliable results.}
}
@incollection{ESFELD2001859,
title = {Atomism and Holism: Philosophical Aspects},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {859-864},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01005-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767010056},
author = {M. Esfeld},
abstract = {Social atomism is the thesis that an individual considered in isolation can have thoughts with a determinate conceptual content. Social holism, by contrast, is the thesis that social relations are essential for a human being in order to be a ‘thinking’ being. The discussion on atomism vs. holism extends to aspects of thoughts as well. Semantic atomism is the thesis that each thought has a meaning independently of other thoughts. Semantic holism, in reverse, is the thesis that the meaning of a thought consists in its inferential relations to other thoughts in a system of thoughts. Confirmation atomism is the thesis that thoughts can be empirically confirmed one by one. Confirmation holism, by contrast, is the thesis that only a whole system of thoughts or a whole theory can be confirmed by experience. Social atomism in modern philosophy goes back to Hobbes. Social holism comes up in romanticism and its predecessors; it is worked out by Hegel. In today's discussion, the rule-following considerations that are developed by Kripke on behalf of the later Wittgenstein are the main argument for social holism. Social atomists counter this argument by a naturalistic account of rule following in terms of certain dispositions to behavior.}
}
@article{LEE2025111886,
title = {Project symphony: Composing a masterpiece in a science laboratory},
journal = {iScience},
volume = {28},
number = {2},
pages = {111886},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111886},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225001464},
author = {Sunghee Lee and Jamie Gudyka and Marnie Skinner and Jasmin Ceja-Vega and Amani Rabadi and Christopher Poust and Caroline Scott and Micaela Panella and Elizabeth Andersen and Jessica Said},
abstract = {In the spirit of collaborative science, Prof. Sunghee Lee (Chemistry Professor at Iona University in New York, USA) embarked on her academic career with a vision to bring an interdisciplinary approach to undergraduate education. At a Predominantly Undergraduate Institution (PUI) such as Iona, she saw a unique opportunity to weave together teaching and research, creating a rich tapestry of learning experiences for students. Her goal was simple yet ambitious: to use research as a bridge connecting classroom theory to real-world interdisciplinary scientific practice. In this Backstory, Sunghee and her students and recent graduates reflect on the development and experiences that shaped their journey through Project Symphony and the resulting skills they’ve learned. The symphony they’ve created together is a testament to the transformative power of collaborative undergraduate research – a melody of discovery that continues to evolve and inspire.}
}
@article{ESSEX2018554,
title = {Model falsifiability and climate slow modes},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {502},
pages = {554-562},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.02.090},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118301766},
author = {Christopher Essex and Anastasios A. Tsonis},
keywords = {Climate complexity, Computer errors, Computational over-stabilization, Dynamical and thermodynamical sensitivity, Slow climate modes},
abstract = {The most advanced climate models are actually modified meteorological models attempting to capture climate in meteorological terms. This seems a straightforward matter of raw computing power applied to large enough sources of current data. Some believe that models have succeeded in capturing climate in this manner. But have they? This paper outlines difficulties with this picture that derive from the finite representation of our computers, and the fundamental unavailability of future data instead. It suggests that alternative windows onto the multi-decadal timescales are necessary in order to overcome the issues raised for practical problems of prediction.}
}
@article{DELLACQUA2021199,
title = {Increased functional connectivity within alpha and theta frequency bands in dysphoria: A resting-state EEG study},
journal = {Journal of Affective Disorders},
volume = {281},
pages = {199-207},
year = {2021},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0165032720331049},
author = {Carola Dell'Acqua and Shadi Ghiasi and Simone {Messerotti Benvenuti} and Alberto Greco and Claudio Gentili and Gaetano Valenza},
keywords = {depression, depressive symptoms, dysphoria, functional connectivity, EEG, vulnerability},
abstract = {Background: The understanding of neurophysiological correlates underlying the risk of developing depression may have a significant impact on its early and objective identification. Research has identified abnormal resting-state electroencephalography (EEG) power and functional connectivity patterns in major depression. However, the entity of dysfunctional EEG dynamics in dysphoria is yet unknown. Methods: 32-channel EEG was recorded in 26 female individuals with dysphoria and in 38 age-matched, female healthy controls. EEG power spectra and alpha asymmetry in frontal and posterior channels were calculated in a 4-minute resting condition. An EEG functional connectivity analysis was conducted through phase locking values, particularly mean phase coherence. Results: While individuals with dysphoria did not differ from controls in EEG spectra and asymmetry, they exhibited dysfunctional brain connectivity. Particularly, in the theta band (4-8 Hz), participants with dysphoria showed increased connectivity between right frontal and central areas and right temporal and left occipital areas. Moreover, in the alpha band (8-12 Hz), dysphoria was associated with increased connectivity between right and left prefrontal cortex and between frontal and central-occipital areas bilaterally. Limitations: All participants belonged to the female gender and were relatively young. Mean phase coherence did not allow to compute the causal and directional relation between brain areas. Conclusions: An increased EEG functional connectivity in the theta and alpha bands characterizes dysphoria. These patterns may be associated with the excessive self-focus and ruminative thinking that typifies depressive symptoms. EEG connectivity patterns may represent a promising measure to identify individuals with a higher risk of developing depression.}
}
@article{NOBRE2019132,
title = {Premembering Experience: A Hierarchy of Time-Scales for Proactive Attention},
journal = {Neuron},
volume = {104},
number = {1},
pages = {132-146},
year = {2019},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2019.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0896627319307366},
author = {Anna C. Nobre and Mark G. Stokes},
keywords = {memory, attention, decision-making, hippocampus, prefrontal cortex, priming, working memory, episodic memory, implicit memory},
abstract = {Memories are about the past, but they serve the future. Memory research often emphasizes the former aspect: focusing on the functions that re-constitute (re-member) experience and elucidating the various types of memories and their interrelations, timescales, and neural bases. Here we highlight the prospective nature of memory in guiding selective attention, focusing on functions that use previous experience to anticipate the relevant events about to unfold—to “premember” experience. Memories of various types and timescales play a fundamental role in guiding perception and performance adaptively, proactively, and dynamically. Consonant with this perspective, memories are often recorded according to expected future demands. Using working memory as an example, we consider how mnemonic content is selected and represented for future use. This perspective moves away from the traditional representational account of memory toward a functional account in which forward-looking memory traces are informationally and computationally tuned for interacting with incoming sensory signals to guide adaptive behavior.}
}
@incollection{DEDEOGLU2023251,
title = {Chapter Nine - Blockchain meets edge-AI for food supply chain traceability and provenance},
editor = {Joost Laurus Dinant Nelis and Aristeidis S. Tsagkaris},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {101},
pages = {251-275},
year = {2023},
booktitle = {Smartphones for Chemical Analysis: From Proof-of-concept to Analytical Applications},
issn = {0166-526X},
doi = {https://doi.org/10.1016/bs.coac.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X22001064},
author = {Volkan Dedeoglu and Sidra Malik and Gowri Ramachandran and Shantanu Pal and Raja Jurdak},
keywords = {Blockchain, Edge AI, Traceability, Provenance, Supply Chains},
abstract = {Food supply chains are increasingly digitised and automated through the use of technologies such as Internet-of-Things (IoT), blockchain and Artificial Intelligence (AI). Such digitization efforts often rely on cloud computing, which creates bandwidth overhead, high latency, security and privacy challenges. In this chapter, we propose the use of edge AI, which is a computing paradigm that combines edge computing and AI, to complete computing tasks close to the sensor data sources. Edge AI can promote greater scalability and avoid the security and privacy challenges of centralised cloud computing. The chapter introduces the provenance and traceability requirements of food supply chains and the digitization of these supply chains through blockchain, IoT, and AI. The chapter also proposes the use of smartphone integrated sensors to provide unique physical, chemical, or biological signatures of food supply chain products, and to conduct the necessary computations on the smartphone. The proposed Edge AI approach to supply chain digitization sets the scene for greater resilience in modern digital supply chains.}
}
@article{ALLGOWER2019147,
title = {Position paper on the challenges posed by modern applications to cyber-physical systems theory},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {34},
pages = {147-165},
year = {2019},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X19300603},
author = {Frank Allgöwer and João {Borges de Sousa} and James Kapinski and Pieter Mosterman and Jens Oehlerking and Patrick Panciatici and Maria Prandini and Akshay Rajhans and Paulo Tabuada and Philipp Wenzelburger},
keywords = {cyber–physical systems theory},
abstract = {Cyber-physical systems theory offers a powerful framework for modeling, analyzing, and designing real engineering systems integrating communication, control, and computation functionalities (the cyber part) within a natural and/or man-made system governed by the laws of physics (the physical part). New methodological developments in cyber-physical systems theory are required by traditional application domains such as manufacturing, transportation, and energy systems, which are currently experiencing significant and – to some extent – revolutionary changes to address the needs of our modern society. The goal of this position paper is to provide the cyber-physical systems community, and especially young researchers, a clear view on what are research directions worth pursuing motivated by the challenges posed by modern applications.}
}
@article{DELIGUORO2023114082,
title = {From semantics to types: The case of the imperative λ-calculus},
journal = {Theoretical Computer Science},
volume = {973},
pages = {114082},
year = {2023},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2023.114082},
url = {https://www.sciencedirect.com/science/article/pii/S030439752300395X},
author = {Ugo de'Liguoro and Riccardo Treglia},
keywords = {State monad, Imperative lambda calculus, Type assignment systems, Filter models},
abstract = {We study the logical semantics of an untyped λ-calculus equipped with operators representing read and write operations from and to a global store. Such a logic consists of an intersection type assignment system, which we derive from the denotational semantics of the calculus, based on the monadic approach to model computational λ-calculi. The system is obtained by constructing a filter model in the category of ω-algebraic lattices, such that the typing rules can be recovered out of the term interpretation. By construction, the so-obtained type system satisfies the “type-semantics” property and completeness.}
}
@article{JAIN2023119859,
title = {Optimized levy flight model for heart disease prediction using CNN framework in big data application},
journal = {Expert Systems with Applications},
volume = {223},
pages = {119859},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119859},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423003603},
author = {Arushi Jain and Annavarapu {Chandra Sekhara Rao} and Praphula {Kumar Jain} and Yu-Chen Hu},
keywords = {Optimization, Heart disease prediction, Convolution neural networks, Big data, Swarm intelligence algorithm},
abstract = {Cardiac disease is one of the most complex diseases globally. It affects the lives of humans critically. It is essential for accurate and timely diagnosis to treat heart failure and prevent the disease. In most aspects, it was not so successful with the traditional method, which uses past medical history. Many existing models had several types of the loss function in traditional CNN can lead to misidentification of the model. To solve this problem, so many scholars have used the swarm intelligence algorithm, but most of these techniques are stuck in the local minima and suffer from premature convergence. In the proposed method, we build up the Levy Flight – Convolutional Neural Network (LV-CNN) depending on the diagnostic system using heart disease image data set for heart disease assessment. Initially, the input Big Data images are resized to reduce the computational complexity of the system. Then, those resized images are subject to the proposed LV-CNN model. Therefore, the LV approach is integrated with the Sunflower Optimization Algorithm (SFO) to reduce loss function occurring in the CNN architecture. Such a combination helps the SFO algorithm avoid trapping in local minima due to the random walk of the levy flight. The proposed algorithm will be simulated using the MATLAB tool and tested experimentally in terms of accuracy is 95.74%, specificity is 0.96%, the error rate is 0.35, and time consumption is 9.71 s. This comparative analysis revealed that the excellence of the proposed model.}
}
@article{WANG2007254,
title = {An efficient algorithm for generalized discriminant analysis using incomplete Cholesky decomposition},
journal = {Pattern Recognition Letters},
volume = {28},
number = {2},
pages = {254-259},
year = {2007},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2006.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167865506001966},
author = {Haixian Wang and Zilan Hu and Yu’e Zhao},
keywords = {Generalized discriminant analysis, Nonlinear feature extraction, Eigenvalue decomposition, Gram–Schmidt orthonormalization, Incomplete Cholesky decomposition},
abstract = {Generalized discriminant analysis (GDA) has provided an extremely powerful approach to extracting nonlinear features via kernel trick. And it has been suggested for a number of applications, such as classification problem. Whereas the GDA could be solved by the utilization of Mercer kernels, a drawback of the standard GDA is that it may suffer from computational problem for large scale data set. Besides, there is still attendant problem of numerical accuracy when computing the eigenvalue problem of large matrices. Also, the GDA would occupy large memory (to store the kernel matrix). To overcome these deficiencies, we use Gram–Schmidt orthonormalization and incomplete Cholesky decomposition to find a basis for the entire training samples, and then formulate GDA as another eigenvalue problem of matrix whose size is much smaller than that of the kernel matrix by using the basis, while still working out the optimal discriminant vectors from all training samples. The theoretical analysis and experimental results on both artificial and real data set have shown the superiority of the proposed method for performing GDA in terms of computational efficiency and even the recognition accuracy, especially when the training samples size is large.}
}
@article{KOULADOUM2024200052,
title = {The role of institutional quality on the impact of Chinese foreign direct investments and human capital development on macroeconomic performance in the CEMAC zone},
journal = {Transnational Corporations Review},
volume = {16},
number = {2},
pages = {200052},
year = {2024},
issn = {1925-2099},
doi = {https://doi.org/10.1016/j.tncr.2024.200052},
url = {https://www.sciencedirect.com/science/article/pii/S1925209924005783},
author = {Jean-Claude Kouladoum},
keywords = {Institutional quality, Chinese foreign direct investments, Human capital development, Macroeconomic performance, CEMAC zone},
abstract = {This paper investigates the role of institutional quality in terms of governance on the impact of Chinese foreign direct investments and human capital development on the macroeconomic performance of the CEMAC zone between 2003 and 2020. The data were analyzed using descriptive statistics and the Correlated Panels Corrected Standard Errors (PCSEs) Approach. The findings indicated that poor governance performance in the CEMAC zone deteriorates the effects of Chinese foreign direct investments and human capital development on the macroeconomic performance of the CEMAC zone. At the same time, poor governance performance also deteriorates the impact of foreign aid and personal remittances received on the macroeconomic performance of the CEMAC zone. This study strongly recommends measures to improve institutional quality such as increased training on ethical thinking in all forms of education, meritocratic recruitment to the civil service, and auditing of public finances and services.}
}
@article{ZOU2024134011,
title = {Synthesis and mechanism of quaternary ammonium salts based on porphyrin as high-performance copper levelers},
journal = {Tetrahedron},
volume = {159},
pages = {134011},
year = {2024},
issn = {0040-4020},
doi = {https://doi.org/10.1016/j.tet.2024.134011},
url = {https://www.sciencedirect.com/science/article/pii/S0040402024001911},
author = {Peikun Zou and Xuyang Li and Xin Chen and Wenhao Zhou and Kexin Du and Limin Wang},
keywords = {Porphyrin, Porphyrin quaternary ammonium salts, Through-hole electroplating, Electroplating leveler, Quantum chemical calculations},
abstract = {The molecular structure and energy distribution of organic compounds have a great influence on their adsorption capacity on the metal surface. However, there are still insufficient researches on the influence of energy distribution on adsorption properties of organic molecules. Herein, a family of porphyrin derivatives (TPyP-Et, TPyP-Oct, TPyP-Bn and TPyP-Al) bearing quaternary ammonium groups were synthesized for the first time as promising levelers for through-hole copper electrodeposition. Electrochemical tests revealed that all four TPyP derivatives displayed enhanced electrochemical properties. Theoretical calculations and molecular dynamics simulations were carried out to investigate the physisorption capacity and chemical reaction activity of the TPyP molecules, as well as the adsorption capacity on the surface of the copper layer. Through optical and scanning electron microscopy as well as X-ray diffractometry, it was demonstrated that TPyP molecules are effective electroplating levelers. TPyP-Oct, with its longer carbon chain substituent, exhibited superior hole-filling performance in practical PCB experiments among the four compounds. This study expandes the application range of porphyrin compounds, analyzes the influence of organic molecular adsorption properties in copper electrodeposition, and provides theoretical guidance for the future study of organic compounds adsorbed on metal surfaces.}
}
@article{SCHNASE2017198,
title = {MERRA Analytic Services: Meeting the Big Data challenges of climate science through cloud-enabled Climate Analytics-as-a-Service},
journal = {Computers, Environment and Urban Systems},
volume = {61},
pages = {198-211},
year = {2017},
note = {Geospatial Cloud Computing and Big Data},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S019897151300118X},
author = {John L. Schnase and Daniel Q. Duffy and Glenn S. Tamkin and Denis Nadeau and John H. Thompson and Cristina M. Grieg and Mark A. McInerney and William P. Webster},
keywords = {MapReduce, Hadoop, Data analytics, Data services, Cloud Computing, Generativity, iRODS, MERRA, ESGF, BAER},
abstract = {Climate science is a Big Data domain that is experiencing unprecedented growth. In our efforts to address the Big Data challenges of climate science, we are moving toward a notion of Climate Analytics-as-a-Service (CAaaS). We focus on analytics, because it is the knowledge gained from our interactions with Big Data that ultimately produce societal benefits. We focus on CAaaS because we believe it provides a useful way of thinking about the problem: a specialization of the concept of business process-as-a-service, which is an evolving extension of IaaS, PaaS, and SaaS enabled by Cloud Computing. Within this framework, Cloud Computing plays an important role; however, we see it as only one element in a constellation of capabilities that are essential to delivering climate analytics as a service. These elements are essential because in the aggregate they lead to generativity, a capacity for self-assembly that we feel is the key to solving many of the Big Data challenges in this domain. MERRA Analytic Services (MERRA/AS) is an example of cloud-enabled CAaaS built on this principle. MERRA/AS enables MapReduce analytics over NASA’s Modern-Era Retrospective Analysis for Research and Applications (MERRA) data collection. The MERRA reanalysis integrates observational data with numerical models to produce a global temporally and spatially consistent synthesis of 26 key climate variables. It represents a type of data product that is of growing importance to scientists doing climate change research and a wide range of decision support applications. MERRA/AS brings together the following generative elements in a full, end-to-end demonstration of CAaaS capabilities: (1) high-performance, data proximal analytics, (2) scalable data management, (3) software appliance virtualization, (4) adaptive analytics, and (5) a domain-harmonized API. The effectiveness of MERRA/AS has been demonstrated in several applications. In our experience, Cloud Computing lowers the barriers and risk to organizational change, fosters innovation and experimentation, facilitates technology transfer, and provides the agility required to meet our customers’ increasing and changing needs. Cloud Computing is providing a new tier in the data services stack that helps connect earthbound, enterprise-level data and computational resources to new customers and new mobility-driven applications and modes of work. For climate science, Cloud Computing’s capacity to engage communities in the construction of new capabilities is perhaps the most important link between Cloud Computing and Big Data.}
}
@incollection{MURRAY202119,
title = {Chapter Two - The neurocognitive mechanisms of responsibility: A framework for normatively relevant neuroscience},
editor = {Martín Hevia},
series = {Developments in Neuroethics and Bioethics},
publisher = {Academic Press},
volume = {4},
pages = {19-40},
year = {2021},
booktitle = {Regulating Neuroscience: Transnational Legal Challenges},
issn = {2589-2959},
doi = {https://doi.org/10.1016/bs.dnb.2021.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2589295921000023},
author = {Samuel Murray and Felipe {De Brigard}},
keywords = {Moral responsibility, Autonomy of ethics, Moral neuroscience, Decision-making, Practical reasoning, Moral agency},
abstract = {We argue that research in cognitive neuroscience can contribute meaningfully to some normative theorizing. To make our case, we develop one instance where ethical inquiry progressed through empirical research into the computational basis of decision-making. From this, we draw some general considerations about the kinds of normative inquiry where research in cognitive neuroscience might be relevant.}
}
@article{YAMANE2021102520,
title = {Humor meets morality: Joke generation based on moral judgement},
journal = {Information Processing & Management},
volume = {58},
number = {3},
pages = {102520},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102520},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000297},
author = {Hiroaki Yamane and Yusuke Mori and Tatsuya Harada},
keywords = {Computational humor, Morality, Recurrent neural networks, Joke generation},
abstract = {Although humor enriches human lives, some jokes fail to amuse people because of a lack of morality. In this paper, we propose a mechanism capable of selecting humor based on moral criteria. To this end, we first construct a model based on an N-gram corpus and generate joke candidates using various template patterns. We then employ a moral judgement classifier based on a recurrent neural network and utilize the trained model for humor selection. The experimental results obtained from best–worst scaling demonstrate that this scheme is able to generate jokes with moral category labels. We confirmed that jokes about the classifier categorized as Loyalty and Authority, which are regarded as good in our study, are funnier than jokes about Fairness, Purity, Harm, Cheating, and Degradation. Although we did not confirm that there was a difference in the funny level between good and bad moral jokes, the results demonstrate that moral categories of humor can affect the funny level.}
}
@article{XU2024102292,
title = {Hierarchical spatio-temporal graph convolutional neural networks for traffic data imputation},
journal = {Information Fusion},
volume = {106},
pages = {102292},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102292},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000708},
author = {Dongwei Xu and Hang Peng and Yufu Tang and Haifeng Guo},
keywords = {Traffic data imputation, Hierarchical representation, Graph convolution network, Spatio-temporal features},
abstract = {The quality of traffic services depends on the accuracy and completeness of the collected traffic data. However,the existing traffic data imputation methods usually only rely on the predefined road network structure to capture the spatio-temporal features and only consider the imputation effect from a single perspective, which are very limited for imputation of different missing patterns of road traffic data. In this paper, we propose a novel deep learning framework called Hierarchical Spatio-temporal Graph Convolutional Neural Networks(HSTGCN) to impute traffic data,through the macro layer and the road layer. The model constructs macro graph of the road network based on the data temporal correlation clustering, which can mine the temporal dependencies of road traffic data from a hierarchical perspective. Besides, a temporal attention mechanism and adaptive adjacency matrix are introduced in the road layer to better extract the spatio-temporal information of the road traffic data. Finally, we use graph convolution neural networks to learn the spatio-temporal feature representations of the road layer and macro layer, which are then fused to achieve data imputation. To illustrate the efficient performance of the model, experiments are conducted on traffic data collected from California and Seattle. The proposed model performs better than the comparison model for traffic data imputation.}
}
@article{ADAMOVIC2024100604,
title = {Streamlined approach to 2nd/3rd graders learning basic programming concepts},
journal = {Entertainment Computing},
volume = {48},
pages = {100604},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100604},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000599},
author = {Milan Đ. Adamović and Dragan V. Ivetić},
keywords = {Video games, Edutainment, Programming, School},
abstract = {There is a growing need to teach schoolchildren programming at an increasingly younger age. The goal of this study is to determine if it is possible to teach schoolchildren basic programming concepts in a streamlined manner. In order to present the new knowledge in a way schoolchildren could understand easily, analogies between basic programming concepts and traffic were used. A simple video game was developed with this in mind and an effort was made to avoid design pitfalls commonly found in edutainment titles. The study involved 112 schoolchildren ages 7 to 9. Test group and control group were given a pre-test, a re-test and a post-test. The re-test and the post-test respectively showed 16% and 7% score difference in favor of the test group. Focusing on questions featuring content analogous to basic programming concepts showed 36% and 20% difference in scores.}
}
@article{RZHETSKY20089,
title = {Seeking a New Biology through Text Mining},
journal = {Cell},
volume = {134},
number = {1},
pages = {9-13},
year = {2008},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2008.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0092867408008167},
author = {Andrey Rzhetsky and Michael Seringhaus and Mark Gerstein},
abstract = {Tens of thousands of biomedical journals exist, and the deluge of new articles in the biomedical sciences is leading to information overload. Hence, there is much interest in text mining, the use of computational tools to enhance the human ability to parse and understand complex text.}
}
@incollection{ROSENBERG2023157,
title = {Chapter 9 - Machine learning and precision medicine},
editor = {Gary A. Rosenberg},
booktitle = {Neuroinflammation in Vascular Dementia},
publisher = {Academic Press},
pages = {157-173},
year = {2023},
isbn = {978-0-12-823455-6},
doi = {https://doi.org/10.1016/B978-0-12-823455-6.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234556000055},
author = {Gary A. Rosenberg},
keywords = {Principal component analysis (PCA), exploratory factor analysis (EFA), Binswanger’s disease score (BDS), The Alzheimer Disease Neuroimaging Initiative (ADNI), hierarchical clustering analysis (HCA)},
abstract = {Clinical medicine is experiencing a massive increase in the amount of information available to the physician caring for a patient. Certain medical fields have incorporated this deluge of information into patient care while others are lagging behind. Neurology has been slow to adopt the new methods to use the large amount of information, but it is rapidly learning from other fields. Cancer diagnosis and treatment has been at the forefront of this revolution; not only have there been an extensive number of genes associated with different cancers discovered, but this information has been used to formulate treatment plans. Other fields such as radiology and dermatology are using computer-aided imaging to diagnose illness by analysis of radiographs and to automate diagnoses of skin cancers. The concepts behind the use of machine learning in diagnosis originated from early work in the field of “cybernetics,” which is a transdisciplinary approach for exploring regulatory systems – their structures, constraints, and possibilities. Norbert Wiener defined cybernetics in 1948 as “the scientific study of control and communication in the animal and the machine.” From the early work on control theory by Wiener and others has slowly evolved the modern concepts of artificial intelligence (AI) and machine learning. There are various definitions of AI or machine learning. The term is used to describe computers that perform cognitive functions that we associate with the human mind; these “thinking machines” can beat experts in chess and the Chinese game of Go. In medicine, there are capable of analyzing large amounts of data to arrive at a diagnosis through pattern recognition. Antibiotic drugs have been designed by AI in ways that were unavailable to humans, pointing to the future of molecular discovery in medicine.}
}
@article{EVANS1989499,
title = {A review and synthesis of OR/MS and creative problem solving (Parts 1 and 2)},
journal = {Omega},
volume = {17},
number = {6},
pages = {499-524},
year = {1989},
issn = {0305-0483},
doi = {https://doi.org/10.1016/0305-0483(89)90055-8},
url = {https://www.sciencedirect.com/science/article/pii/0305048389900558},
author = {JR Evans},
keywords = {creativity, problem-solving, OR/MS methodology},
abstract = {Problem solving in operations research and management science is both a science and an art. While much has been written about the science of OR/MS, relatively little has been written about the art. Art, by its very nature, is a creative discipline. This implies that creative thinking should be an important component of OR/MS methodology. A rich literature on creative thinking exists, mostly in the domains of psychology and design. Creativity has been indirectly discussed in OR/MS research and practice, but seldom as a central theme. The purpose of this paper is to review the literature on creative thinking and problem solving that has special relevance to traditional OR/MS methodology. In this part we focus on problem solving, the need for creative thinking, and fundamental concepts of creativity. In Part 2 we synthesize the OR/MS literature that relates to creative thinking, and provide a framework for integrating structured creative thinking processes with OR/MS methodology. Finally, we discuss implications for education, research, and practice.}
}
@article{NESI2024184,
title = {Exploring enactivism: A scoping review of its key concepts and theorical approach},
journal = {Advances in Integrative Medicine},
volume = {11},
number = {4},
pages = {184-190},
year = {2024},
issn = {2212-9588},
doi = {https://doi.org/10.1016/j.aimed.2024.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S221295882400082X},
author = {Jacson Nesi and Roberta Lemos {dos Santos} and Michele Benites},
keywords = {Enactivism, Enaction, body, Anatomy, Embodiment, Scoping Review},
abstract = {Enactivism is a theoretical perspective in the fields of philosophy of mind and cognition that emphasizes the active role of the organism in constructing and giving meaning to the world around it. It highlights that the mind is not merely a passive receiver of information from the environment, but rather an active participant in the creation of meaning and experience. The idea for this article arises from the uncertainty surrounding the distinction of practice, principles, and osteopathic concepts, which have been raised by various regarding osteopathic principles: the anachronism of their distinction, whether the role of these principles could still be accepted as a guide for osteopathy in the contemporary world, whether the biopsychosocial model could be the basis for a proposal to redefine them and even whether the use of these principles could do more harm than good. Objectives: Facilitate access to essential definitions and concepts related to enactivism, and make the understanding of these elements more accessible, as they play a crucial role in the reconceptualization of osteopathy. Materials and methods: The work was elaborated as a scoping review, using the PRISMA-P 2020 Checklist.}
}
@article{PHILLIPS2009597,
title = {Advances in evolution and genetics: Implications for technology strategy},
journal = {Technological Forecasting and Social Change},
volume = {76},
number = {5},
pages = {597-607},
year = {2009},
note = {Two Special Sections: Advances in Evolution and Genetics: Implications for Technology Strategy The Digital Economy in Asia},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2008.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162508001522},
author = {Fred Phillips and Yu-Shan Su},
keywords = {Evolution, Selection, Genetics, Technology strategy, Technology forecasting},
abstract = {Genetic and evolutionary principles are of great importance to technology strategists, both directly (as in the forecasting of genetic engineering technologies) and as a source of metaphor and perspective on socio-technical change. Recent rapid progress in the molecular sciences have revealed new genetic mechanisms of evolution, and introduced new controversies of interpretation. How do these recent developments affect technology forecasting and our view of technological evolution? This paper provides a quick primer for TFSC readers on several new developments in evolution and genetics, comments upon a number of common misconceptions and pitfalls in evolutionary thinking, and critically describes some controversies and open questions, introducing key readings and sources. It relates genetic and evolutionary knowledge, analogies and metaphors to areas of interest to researchers in technology forecasting and assessment, noting possible future directions. The paper concludes with an overview of the other papers in this special section.}
}
@article{PATON1997245,
title = {The organisations of hereditary information},
journal = {Biosystems},
volume = {40},
number = {3},
pages = {245-255},
year = {1997},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(96)01652-8},
url = {https://www.sciencedirect.com/science/article/pii/S0303264796016528},
author = {Ray Paton},
keywords = {Gene, Syntax/semantics, Hierarchy, Epigenetic system, Talkback},
abstract = {The meaning of hereditary information is not simple. It includes not only what a system receives and transmits but particularly what it makes. The syntactic basis to hereditary information is also not straightforward. For example, is DNA instructions, or data, or both? The answer to this question requires an appreciation of the meaning of the information yet there are a number of possible semantic systems for describing hereditary information including proteins and development. The descriptive boundaries of hereditary information are examined by locating some general organising themes including hierarchy, ecology, regulation, epigenetic systems and talkback. Though metaphors have limits in terms of their explanatory power, a number have influenced the development of biological thinking and biosystems have variously been represented as chemical laboratories, computers, electromechanical machines and societies. In this article a further metaphor is discussed, that of life-as-a-play or dance in which the trio of script (genome), cast (metabolism) and stage (cellular structure) co-exist and pre-exist the phenotypic life history which inherits them. A fuller examination of this trio provides an important perspective on the study of the organisations of information processing in hereditary systems.}
}
@article{SUKHOBOKOV2024101279,
title = {A universal knowledge model and cognitive architectures for prototyping AGI},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101279},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000731},
author = {Artem Sukhobokov and Evgeny Belousov and Danila Gromozdov and Anna Zenger and Ilya Popov},
keywords = {Cognitive architecture, AGI, Metagraph, Archigraph, Universal knowledge model, Machine consciousness, Machine subconsciousness, Machine reflection, Machine worldview},
abstract = {The article identified 56 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a reference cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph model are used, constructed as a development of annotated metagraphs. As other components, the reference cognitive architecture being developed includes following modules: machine consciousness, machine subconsciousness, interaction with the external environment, a goal management, an emotional control, social interaction, reflection, ethics, worldview, learning, monitoring, statement problems, solving problems, self-organization and meta learning. Based on the composition of the proposed reference architecture modules, existing cognitive architectures containing the following modules were analyzed: machine consciousness, machine subconsciousness, reflection, worldview.}
}
@article{OMIZO2020102578,
title = {Machining Topoi: Tracking Premising in Online Discussion Forums with Automated Rhetorical Move Analysis},
journal = {Computers and Composition},
volume = {57},
pages = {102578},
year = {2020},
note = {Composing Algorithms: Writing (with) Rhetorical Machines},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2020.102578},
url = {https://www.sciencedirect.com/science/article/pii/S8755461520300396},
author = {Ryan M. Omizo},
keywords = {computational rhetoric, Docuscope, Faciloscope, discussion forums, topoi},
abstract = {This article interrogates recent computational work on discovering and analyzing topoi through the use of topic modeling in the discipline of the literary digital humanities against the long history of topical research and pedagogy in rhetoric and composition. While significant work has been done in the literary digital humanities to advance the study of texts through topic modeling, this article argues that the emphasis on the textuality of topoi in computational research neglects situated rhetorical actions and the dynamics of audience interaction. In response to this deemphasis, this article proposes an algorithmic alternative to the identification and explanation of the rhetorical topoi through the integrated use of a computational rhetorical move classifier called the Faciloscope (Omizo et al., 2016) and the pattern-matching program, Docuscope (Kaufer and Ishizaki, 1998).}
}
@article{WILLMANN201616,
title = {Robotic timber construction — Expanding additive fabrication to new dimensions},
journal = {Automation in Construction},
volume = {61},
pages = {16-23},
year = {2016},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0926580515002046},
author = {Jan Willmann and Michael Knauss and Tobias Bonwetsch and Anna Aleksandra Apolinarska and Fabio Gramazio and Matthias Kohler},
keywords = {Non-standard timber structures, Automated assembly, Computational design, Industrial full scale implementation, Additive digital fabrication, Robotic Timber Construction},
abstract = {This paper presents a novel approach to non-standard timber assembly – Robotic Timber Construction (RTC) – where robotic fabrication is used to expand additive digital fabrication techniques towards industrial full scale dimensions. Featuring robotic systems that grasp, manipulate, and finally position building components according to a precise digital blueprint, RTC combines robotic assembly procedures and advanced digital design of non-standard timber structures. The resulting architectural morphologies allow for a convergence of aesthetic and functional concerns, enabling structural optimisation through the locally differentiated aggregation of material. Initiated by the group of Gramazio Kohler Research at ETH Zurich, this approach offers a new perspective on automated timber construction, where the focus is shifted from the processing of single parts towards the assembly of generic members in space. As such, RTC promotes unique advantages over conventional approaches to timber construction, such as, for example, CNC joinery and cutting: through the automated placement of material exactly where it is needed, RTC combines additive and largely waste-free construction with economic assembly procedures, it does not require additional external building reference, and it offers digital control across the entire building process, even when the design and assembly information are highly complex. This paper considers 1) research parameters for the individual components of RTC (such as computational design processes, construction methods and fabrication strategies), and 2) the architectural implications of integrating these components into a systemic, unifying process at the earliest stages of design. Overall, RTC leads to profound changes in the design, performance and expressive language of architecture and thus fosters the creation of architecture that profoundly reinvents its constructive repertoire.}
}
@article{RAMIREZPEDRAZA2021122,
title = {Decision-making bioinspired model for target definition and “satisfactor” selection for physiological needs},
journal = {Cognitive Systems Research},
volume = {66},
pages = {122-133},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300942},
author = {Raymundo Ramirez-Pedraza and Felix Ramos},
keywords = {Decision-making, Brain model, Satisfactor selection, Physiological need, Goal-driven},
abstract = {Every person, from an early age, has to make decisions to resolve situations that arise in life. In general, different people make different decisions in the same situation, since decision-making takes into account different factors such as age, emotional state, experience, among others. We can make decisions about situations that we classify as: more important than others, routine, unexpected, or trivial. However, making the correct decision(s) in a timely manner for these situations is one of the most complex and delicate challenges that human beings face. This is due to the arduous mental process required to be carried out. Providing such behavior to a virtual entity is possible through the use of Cognitive Architectures (CAs). CAs are an approach for modeling human intelligence and behavior. This paper presents an functional bioinspired computational decision-making model to satisfy the physiological needs of hunger and thirst. Our proposal considers as black boxes other cognitive functions that are part of a general CA (named Cuäyöllötl or brain in Nahuatl). In the proposed case study, it is proved that the decision-making process plays an essential role in determining the objective and selecting the object that satisfies the established need.}
}
@article{NOLAN2025108094,
title = {Efficient Bayesian functional principal component analysis of irregularly-observed multivariate curves},
journal = {Computational Statistics & Data Analysis},
volume = {203},
pages = {108094},
year = {2025},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2024.108094},
url = {https://www.sciencedirect.com/science/article/pii/S0167947324001786},
author = {Tui H. Nolan and Sylvia Richardson and Hélène Ruffieux},
keywords = {Functional principal component analysis, Hierarchical modelling, Multivariate functional data, Variational message passing},
abstract = {The analysis of multivariate functional curves has the potential to yield important scientific discoveries in domains such as healthcare, medicine, economics and social sciences. However, it is common for real-world settings to present longitudinal data that are both irregularly and sparsely observed, which introduces important challenges for the current functional data methodology. A Bayesian hierarchical framework for multivariate functional principal component analysis is proposed, which accommodates the intricacies of such irregular observation settings by flexibly pooling information across subjects and correlated curves. The model represents common latent dynamics via shared functional principal component scores, thereby effectively borrowing strength across curves while circumventing the computationally challenging task of estimating covariance matrices. These scores also provide a parsimonious representation of the major modes of joint variation of the curves and constitute interpretable scalar summaries that can be employed in follow-up analyses. Estimation is conducted using variational inference, ensuring that accurate posterior approximation and robust uncertainty quantification are achieved. The algorithm also introduces a novel variational message passing fragment for multivariate functional principal component Gaussian likelihood that enables modularity and reuse across models. Detailed simulations assess the effectiveness of the approach in sharing information from sparse and irregularly sampled multivariate curves. The methodology is also exploited to estimate the molecular disease courses of individual patients with SARS-CoV-2 infection and characterise patient heterogeneity in recovery outcomes; this study reveals key coordinated dynamics across the immune, inflammatory and metabolic systems, which are associated with long-COVID symptoms up to one year post disease onset. The approach is implemented in the R package bayesFPCA.}
}
@article{RUBIN2023104955,
title = {Cartography of the multiple formal systems of molecular autopoiesis: from the biology of cognition and enaction to anticipation and active inference},
journal = {Biosystems},
volume = {230},
pages = {104955},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104955},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001302},
author = {Sergio Rubin},
keywords = {Self-fabrication, Operational closure, Closure to efficient causation, Calculus of self-reference, Non-algorithmic, Enaction, Final cause},
abstract = {A rich literature has grown up over the years that bears with autopoiesis, which tends to assume that it is a model, a theory, a principle, a definition of life, a property, refers to self-organization or even to hastily conclude that it is hylomorphic, hylozoist, in need of reformulation or to be overcome, making its status even more unclear. Maturana insists that autopoiesis is none of these and rather it is the causal organization of living systems as natural systems (NS) such that when it stops, they die. He calls this molecular autopoiesis (MA), which comprises two domains of existence: that of the self-producing organization (self-fabrication) and that of the structural coupling/enaction (cognition). Like all-NS in the universe, MA is amenable to be defined in theoretical terms, i.e. encoded in mathematical models and/or formal systems (FS). Framing the multiple formal systems of autopoiesis (FSA) into the Rosen's modeling relation (a process of bringing into equivalence the causality of NS and the inferential rules of FS), allows a classification of FSA into analytical categories, most importantly Turing machine (algorithmic) vs non-Turing machine (non-algorithmic) based, and FSA with a purely reactive mathematical image as cybernetic systems, i.e. feedbacks based, or conversely, as anticipatory systems making active inferences. It is thus the intent of the present work to advance the precision with which different FS may be observed to comply (preserve correspondence) with MA in its worldly state as a NS. The modeling relation between MA and the range of FS proposed as potentially illuminating their processes forecloses the applicability of Turing-based algorithmic computational models. This outcome indicates that MA, as modelled through Varela's calculus of self-reference or more especially through Rosen's (M,R)-system, is essentially anticipatory without violating structural determinism nor causality whatsoever, hence enaction may involve it. This quality may capture a fundamentally different mode of being in living systems as opposed to mechanical-computational systems. Implications in different fields of biology from the origin of life to planetary biology as well as in cognitive science and artificial intelligence are of interest.}
}
@article{HE2021117140,
title = {Understanding chemical short-range ordering/demixing coupled with lattice distortion in solid solution high entropy alloys},
journal = {Acta Materialia},
volume = {216},
pages = {117140},
year = {2021},
issn = {1359-6454},
doi = {https://doi.org/10.1016/j.actamat.2021.117140},
url = {https://www.sciencedirect.com/science/article/pii/S1359645421005206},
author = {Q.F. He and P.H. Tang and H.A. Chen and S. Lan and J.G. Wang and J.H. Luan and M. Du and Y. Liu and C.T. Liu and C.W. Pao and Y. Yang},
keywords = {Chemical Short Rang Order, High entropy alloy, Solid solution, Lattice distortion},
abstract = {Chemical short-range ordering (CSRO) or demixing in solid solution high entropy alloys (HEAs) is a fundamental issue yet to be fully understood. In this work, we first developed a generalized quasi-chemical solid solution model that enables quantitative computation of the local chemical ordering or demixing in solid solution HEAs. After that, we performed synchrotron diffraction experiments, extensive Reverse Monte Carlo (RMC) simulations, and first principles calculations on the CoCrFeNi model alloy to study the development of local chemical environments after long time thermal annealing. The outcome of the combined research demonstrates that the development of local chemical ordering or demixing in CoCrFeNi is not only affected by the heat of mixing between dislike atoms but also coupled with local lattice distortion.}
}
@article{TALAAT2021164,
title = {The validity of an artificial intelligence application for assessment of orthodontic treatment need from clinical images},
journal = {Seminars in Orthodontics},
volume = {27},
number = {2},
pages = {164-171},
year = {2021},
note = {Artificial Intelligence applications in Orthodontics -An update},
issn = {1073-8746},
doi = {https://doi.org/10.1053/j.sodo.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S1073874621000359},
author = {Sameh Talaat and Ahmed Kaboudan and Wael Talaat and Budi Kusnoto and Flavio Sanchez and Mohammed H. Elnagar and Christoph Bourauel and Ahmed Ghoneima},
abstract = {Aim: To assess the validity of a Convolutional Neural Network (CNN) digital model to detect and localize orthodontic malocclusions from intraoral clinical images. Materials and methods: The sample of this study consisted of the intraoral images of 700 Subjects. All images were intraoral clinical images, in one of the following views: Left Occlusion, Right Occlusion, Front Occlusion, Upper Occlusal, and Lower Occlusal. The following malocclusion conditions were localized: crowding, spacing, increased overjet, cross bite, open bite, deep bite. The images annotations were repeated by the same investigator (S.T) with a one week interval (ICC ≥ 0.9). The CNN model used for this research study was the “You Only Look Once” model. This model can detect and localize multiple objects or multiple instances of the same object in each image. It is a fully convolutional deep neural network; 24 convolutional layers followed by 2 fully connected layers. This model was implemented using the TensorFlow framework freely available from Google. Results: The created CNN model was able to detect and localize the malocclusions with an accuracy of 99.99%, precision of 99.79%, and a recall of 100%. Conclusions: The use of computational deep convolutional neural networks to identify and localize orthodontic problems from clinical images proved valid. The built AI engine accurately detected and localized malocclusion from different views of intra-oral clinical images.}
}
@article{CARLI2012119,
title = {Efficient algorithms for large scale linear system identification using stable spline estimators},
journal = {IFAC Proceedings Volumes},
volume = {45},
number = {16},
pages = {119-124},
year = {2012},
note = {16th IFAC Symposium on System Identification},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20120711-3-BE-2027.00394},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015379386},
author = {Francesca P. Carli and Alessandro Chiuso and Gianluigi Pillonetto},
keywords = {Parametric prediction error methods, output error models, model complexity, marginal likelihood, kernel eigenfunctions},
abstract = {A new nonparametric approach for system identification has been recently proposed where, in place of postulating parametric classes of impulse responses, the estimation process starts from an infinite-dimensional space. In particular, the impulse response is seen as the realization of a zero-mean Gaussian process. Its covariance, the so called stable spline kernel, encodes information on system stability and depends on few hyperparameters estimated from data via marginal likelihood optimization. This approach has been proved to compare much favorably with classical parametric methods but, in data rich situations, a possible drawback may be represented by its computational complexity which scales with the cube of the number of available samples. In this work we design a new computational strategy which may reduce significantly the computational load required by the stable spline estimator, thus extending its practical applicability also to large-scale scenarios.}
}
@incollection{GROSSBERG19873,
title = {The Qijantized Geometry of Visual Space: The Coherent Computation of Depth, Form and Lightness},
editor = {Stephen Grossberg},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {43},
pages = {3-79},
year = {1987},
booktitle = {The Adaptive Brain II},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)61756-2},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508617562},
author = {Stephen Grossberg},
keywords = {binocular vision, brightness perception, figure-ground, feature extraction, form perception, neural network, nonlinear resonance, receptive field, short-term memory, spatial scales, visual completion},
abstract = {A theory is presented of how global visual interactions between depth, length, lightness, and form percepts can occur. The theory suggests how quantized activity patterns which reflect these visual properties can coherently fill-in, or complete, visually ambiguous regions starting with visually informative data features. Phenomena such as the Cornsweet and Craik-O'Brien effects, phantoms and subjective contours, binocular brightness summation, the equidistance tendency, Emmert's law, allelotropia, multiple spatial frequency scaling and edge detection, figure-ground completion, coexistence of depth and binocular rivalry, reflectance rivalry, Fechner's paradox, decrease of threshold contrast with increased number of cycles in a grating pattern, hysteresis, adaptation level tuning, Weber law modulation, shift of sensitivity with background luminance, and the finite capacity of visual short term memory are discussed in terms of a small set of concepts and mechanisms. Limitations of alternative visual theories which depend upon Fourier analysis, Laplacians, zero-crossings, and cooperative depth planes are described. Relationships between monocular and binocular processing of the same visual patterns are noted, and a shift in emphasis from edge and disparity computations toward the characterization of resonant activity-scaling correlations across multiple spatial scales is recommended. This recommendation follows from the theory's distinction between the concept of a structural spatial scale, which is determined by local receptive field properties, and a functional spatial scale, which is defined by the interaction between global properties of a visual scene and the network as a whole. Functional spatial scales, but not structural spatial scales, embody the quantization of network activity that reflects a scene's global visual representation. A functional scale is generated by a filling-in resonant exchange, or FIRE, which can be ignited by an exchange of feedback signals among the binocular cells where monocular patterns are binocularly matched.}
}
@incollection{GRANGER1986137,
title = {The Computation Of Contingency In Classical Conditioning},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {20},
pages = {137-192},
year = {1986},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60018-3},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108600183},
author = {Richard H. Granger and Jeffrey C. Schlimmer},
abstract = {Publisher Summary
This chapter discusses a unified framework, which encompasses the computations, algorithms, and neurobiological implementations underlying classical conditioning. It presents an extensive mathematical analysis of the constraints on classical conditioning—that is, the precise contingency conditions under which mammals may and may not learn a particular association between two events in a classical conditioning situation. In classical conditioning, an unconditional stimulus (US)—that is, a cue, which is inherently biologically salient to an animal (such as an electric shock), is repeatedly paired with a conditional stimulus (CS), a cue that initially has no special significance to the animal over repeated trials, the animal can learn that the CS is predictive of or associated with the US. This phenomenon of associative learning is subject to laws and constraints: An association is learned to some extent in some conditions and to a lesser extent in others.}
}
@incollection{YANG202333,
title = {Chapter 2 - Machine learning for solid mechanics},
editor = {Yuebing Zheng and Zilong Wu},
booktitle = {Intelligent Nanotechnology},
publisher = {Elsevier},
pages = {33-45},
year = {2023},
series = {Materials Today},
isbn = {978-0-323-85796-3},
doi = {https://doi.org/10.1016/B978-0-323-85796-3.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857963000020},
author = {Charles Yang and Zhizhou Zhang and Grace X. Gu},
keywords = {Solid mechanics, Inverse design, Physics-informed deep learning, Graph neural networks},
abstract = {Solid mechanics is an important field responsible for the robust designs of humanity's greatest engineering accomplishments, from skyscrapers to airplanes to the space shuttle. A burst of new manufacturing techniques and novel next-generation materials is ushering in a new age of engineering revolving around sustainable development. In this chapter, we outline how artificial intelligence (AI) can help scientists and engineers manage the increasing complexity and computational requirements in solid mechanics fields. Two common problem-solving frameworks, forward and inverse design, as well as two promising new AI-based approaches, physics-informed deep learning and graph neural networks, are covered.}
}
@article{VIEIRA2020106268,
title = {Symmetry exploitation to reduce impedance evaluations in grounding grids},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {123},
pages = {106268},
year = {2020},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2020.106268},
url = {https://www.sciencedirect.com/science/article/pii/S0142061519342188},
author = {Pedro H.N. Vieira and Rodolfo A.R. Moura and Marco Aurélio O. Schroeder and Antonio C.S. Lima},
keywords = {Electromagnetic analysis, Frequency response, Grounding, Method of moments, Numerical methods, Symmetry},
abstract = {One main concern on wideband evaluation of grounding systems is the high computational burden related to the determination of the impedance matrices. Traditionally, one has to divide any given conductor in a large number of segments which leads to a rather time consuming procedure. However, there are a number of geometrical symmetries that if exploited can significantly reduce the overall computational time. This work aims at investigating the adequacy of using some existing symmetries to reduce computer burden in the assessment of a wideband grounding system in models based on the Method of Moments. An algorithmic approach is proposed to extend the symmetry exploitation to arbitrarily oriented uniform rectangular grounding systems. Several topologies are used to assess the performance of the proposed approach. According to results, the proposed methodology can be more than 12 times faster than the traditional approach without loss of accuracy because it is not a numerical approximation.}
}
@article{ARTHURS2013443,
title = {Efficient simulation of cardiac electrical propagation using high-order finite elements II: Adaptive p-version},
journal = {Journal of Computational Physics},
volume = {253},
pages = {443-470},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113004841},
author = {Christopher J. Arthurs and Martin J. Bishop and David Kay},
keywords = {Adaptive finite element method, -version, Monodomain simulation, Computational cardiology, Numerical efficiency},
abstract = {We present a computationally efficient method of simulating cardiac electrical propagation using an adaptive high-order finite element method to automatically concentrate computational effort where it is most needed in space on each time-step. We drive the adaptivity using a residual-based error indicator, and demonstrate using norms of the error that the indicator allows us to control it successfully. Our results using two-dimensional domains of varying complexity demonstrate that significant improvements in efficiency are possible over the standard linear FEM in our single-thread studies, and our preliminary three-dimensional results suggest that improvements are also possible in 3D. We do not work in parallel or investigate the challenges for adaptivity such as dynamic load-balancing which are associated with parallelisation. However, based upon recent work demonstrating that in some circumstances and with moderate processor counts parallel h-adaptive methods are efficient, and upon the claim that p-adaptivity will outperform h-adaptivity, we argue that p-adaptivity should be investigated for efficiency in parallel for simulation on moderate numbers of processors.}
}
@article{LI2025112016,
title = {The neural correlates of logical-mathematical symbol systems processing resemble those of spatial cognition more than language processing},
journal = {iScience},
volume = {28},
number = {4},
pages = {112016},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112016},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225002767},
author = {Yuannan Li and Shan Xu and Jia Liu},
keywords = {Neuroscience, Cognitive neuroscience},
abstract = {Summary
The ability to use logical-mathematical symbols (LMS), encompassing tasks such as calculation, reasoning, and programming, is special to humans with recent emergence. LMS processing was suggested to build upon fundamental cognitive systems through neuronal recycling, with natural language processing and spatial cognition as key candidates. This study used meta-analyses and synthesized neural maps of representative LMS tasks, including reasoning, calculation, and mental programming, to compare their neural correlates with those of the two systems. Our results revealed greater activation overlap and multivariate similarity between LMS and spatial cognition than with language processing. Hierarchical clustering further indicated that LMS tasks were indistinguishable from spatial tasks at the neural level, suggesting an inherent connection. Our findings support the hypothesis that spatial cognition is the basis of LMS processing, shedding light on the logical reasoning limitations of large language models, particularly those lacking explicit spatial representations.}
}
@article{TRONCOSOGARCIA2023108387,
title = {Explainable hybrid deep learning and Coronavirus Optimization Algorithm for improving evapotranspiration forecasting},
journal = {Computers and Electronics in Agriculture},
volume = {215},
pages = {108387},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108387},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923007755},
author = {A.R. Troncoso-García and I.S. Brito and A. Troncoso and F. Martínez-Álvarez},
keywords = {XAI, Deep learning, Evapotranspiration forecasting, Hyperparameter optimization},
abstract = {Reference evapotranspiration is a critical hydrological measurement closely associated with agriculture. Accurate forecasting is vital in effective water management and crop planning in sustainable agriculture. In this study, the future values of reference evapotranspiration are forecasted by applying a recurrent long short-term memory neural network optimized using the Coronavirus Optimization Algorithm, a novel bioinspired metaheuristic based on the spread of COVID-19. The input data is sourced from the Sistema Agrometeorológico para a Gestão da Rega no Alentejo, in Portugal, with meteorological data such as air temperature or wind speed. Several baseline models are applied to the same problem to facilitate comparisons, including support vector machines, multi-layer perceptron, Lasso and decision tree. The results demonstrate the successful forecasting performance of the proposed model and its potential in this field. In turn, to gain deeper insights into the model’s inner workings, the SHapley Additive exPlanation tool is applied for explainability. Consequently, the study identifies the most relevant variables for reference evapotranspiration forecasting, including previously measured evapotranspiration values. Additionally, a univariable model is tested using historic evapotranspiration values as input, offering a comparable performance with a considerable reduction of computational time.}
}
@article{MOTSA2023116912,
title = {A data-driven, machine learning scheme used to predict the structural response of masonry arches},
journal = {Engineering Structures},
volume = {296},
pages = {116912},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2023.116912},
url = {https://www.sciencedirect.com/science/article/pii/S0141029623013275},
author = {Siphesihle Mpho Motsa and Georgios Ε. Stavroulakis and Georgios Α. Drosopoulos},
keywords = {FEM, Machine Learning, Artificial Neural Network, Multi-hinge failure, Damage Prediction, Masonry Arches, Data-driven Mechanics, Digital Twin},
abstract = {A data-driven methodology is proposed, for the investigation of the ultimate response of masonry arches. Aiming to evaluate their structural response in a computationally efficient framework, machine learning metamodels, in the form of artificial neural networks, are adopted. Datasets are numerically built, integrating Matlab, Python and commercial finite element software. Heyman’s assumptions are adopted within non-linear finite element analysis, incorporating contact-friction laws between adjacent stones, to capture failure in the arch. The artificial neural networks are trained, validated, and tested using the least square minimization technique. It is shown that the proposed scheme can be used to provide a fast and accurate prediction of the deformed geometry, the collapse mechanism and the ultimate load. Cases studies demonstrate the efficiency of the method in random, new arch geometries. Relevant Matlab/Python scripts and datasets are provided. The method can be extended towards structural health monitoring and the concept of digital twin.}
}
@article{PRATO201888,
title = {Considering built environment and spatial correlation in modeling pedestrian injury severity},
journal = {Traffic Injury Prevention},
volume = {19},
number = {1},
pages = {88-93},
year = {2018},
issn = {1538-9588},
doi = {https://doi.org/10.1080/15389588.2017.1329535},
url = {https://www.sciencedirect.com/science/article/pii/S1538958822003630},
author = {Carlo G. Prato and Sigal Kaplan and Alexandre Patrier and Thomas K. Rasmussen},
keywords = {Pedestrian crashes, injury severity models, built environment, spatial correlation},
abstract = {ABSTRACT
Objective: This study looks at mitigating and aggravating factors that are associated with the injury severity of pedestrians when they have crashes with another road user and overcomes existing limitations in the literature by focusing attention on the built environment and considering spatial correlation across crashes. Method: Reports for 6,539 pedestrian crashes occurred in Denmark between 2006 and 2015 were merged with geographic information system resources containing detailed information about the built environment and exposure at the crash locations. A linearized spatial logit model estimated the probability of pedestrians sustaining a severe or fatal injury conditional on the occurrence of a crash with another road user. Results: This study confirms previous findings about older pedestrians and intoxicated pedestrians being the most vulnerable road users and crashes with heavy vehicles and in roads with higher speed limits being related to the most severe outcomes. This study provides novel perspectives by showing positive spatial correlations of crashes with the same severity outcomes and emphasizing the role of the built environment in the proximity of the crash. Conclusions: This study emphasizes the need for thinking about traffic calming measures, illumination solutions, road maintenance programs, and speed limit reductions. Moreover, this study emphasizes the role of the built environment, because shopping areas, residential areas, and walking traffic density are positively related to a reduction in pedestrian injury severity. Often, these areas have in common a larger pedestrian mass that is more likely to make other road users more aware and attentive, whereas the same does not seem to apply to areas with lower pedestrian density.}
}
@article{LOU2023102236,
title = {A function-behavior mapping approach for product conceptual design inspired by memory mechanism},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102236},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102236},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003646},
author = {Shanhe Lou and Yixiong Feng and Yicong Gao and Hao Zheng and Tao Peng and Jianrong Tan},
keywords = {Conceptual design, Situated function-behavior-structure, Memory mechanism, Reinforcement learning},
abstract = {Conceptual design is a pivotal stage for new product development that relies more on designers to solve open-ended and ill-defined problems. Situated function-behavior-structure ontology is an acknowledged method to facilitate conceptual design in a goal-oriented way. However, it depends on the subjective cognition abilities of designers, which are influenced by limited memory and reasoning capacities. Developing computer-aided methods grounded in this ontology holds significant promise in enhancing designers' cognitive abilities. This study delves into the function-behavior (F-B) mapping process. It explores the effect of working memory and long-term memory on design cognition and introduces a memory-inspired reinforcement learning framework for F-B mapping. The Markov decision process is then adopted to formalize F-B mapping while motivation-driven Q learning is employed by the design agent to learn knowledge from historical design cases. The learned state-action value matrix can be applied to guide the designer in selecting feasible behaviors for the specific function requirement. The proposed approach empowers design agents with self-learning and self-evolving capacities. A case study on the F-B mapping of a traction system is conducted to illustrate the feasibility and practicability of the proposed approach.}
}
@article{POLHEMUS2020,
title = {Human-Centered Design Strategies for Device Selection in mHealth Programs: Development of a Novel Framework and Case Study},
journal = {JMIR mHealth and uHealth},
volume = {8},
number = {5},
year = {2020},
issn = {2291-5222},
doi = {https://doi.org/10.2196/16043},
url = {https://www.sciencedirect.com/science/article/pii/S2291522220003046},
author = {Ashley Marie Polhemus and Jan Novák and Jose Ferrao and Sara Simblett and Marta Radaelli and Patrick Locatelli and Faith Matcham and Maximilian Kerz and Janice Weyer and Patrick Burke and Vincy Huang and Marissa Fallon Dockendorf and Gergely Temesi and Til Wykes and Giancarlo Comi and Inez Myin-Germeys and Amos Folarin and Richard Dobson and Nikolay V Manyakov and Vaibhav A Narayan and Matthew Hotopf},
keywords = {human-centric design, design thinking, patient centricity, device selection, technology selection, remote patient monitoring, remote measurement technologies},
abstract = {Background
Despite the increasing use of remote measurement technologies (RMT) such as wearables or biosensors in health care programs, challenges associated with selecting and implementing these technologies persist. Many health care programs that use RMT rely on commercially available, “off-the-shelf” devices to collect patient data. However, validation of these devices is sparse, the technology landscape is constantly changing, relative benefits between device options are often unclear, and research on patient and health care provider preferences is often lacking.
Objective
To address these common challenges, we propose a novel device selection framework extrapolated from human-centered design principles, which are commonly used in de novo digital health product design. We then present a case study in which we used the framework to identify, test, select, and implement off-the-shelf devices for the Remote Assessment of Disease and Relapse-Central Nervous System (RADAR-CNS) consortium, a research program using RMT to study central nervous system disease progression.
Methods
The RADAR-CNS device selection framework describes a human-centered approach to device selection for mobile health programs. The framework guides study designers through stakeholder engagement, technology landscaping, rapid proof of concept testing, and creative problem solving to develop device selection criteria and a robust implementation strategy. It also describes a method for considering compromises when tensions between stakeholder needs occur.
Results
The framework successfully guided device selection for the RADAR-CNS study on relapse in multiple sclerosis. In the initial stage, we engaged a multidisciplinary team of patients, health care professionals, researchers, and technologists to identify our primary device-related goals. We desired regular home-based measurements of gait, balance, fatigue, heart rate, and sleep over the course of the study. However, devices and measurement methods had to be user friendly, secure, and able to produce high quality data. In the second stage, we iteratively refined our strategy and selected devices based on technological and regulatory constraints, user feedback, and research goals. At several points, we used this method to devise compromises that addressed conflicting stakeholder needs. We then implemented a feedback mechanism into the study to gather lessons about devices to improve future versions of the RADAR-CNS program.
Conclusions
The RADAR device selection framework provides a structured yet flexible approach to device selection for health care programs and can be used to systematically approach complex decisions that require teams to consider patient experiences alongside scientific priorities and logistical, technical, or regulatory constraints.}
}
@article{FENG2022127434,
title = {Parallel cooperation search algorithm and artificial intelligence method for streamflow time series forecasting},
journal = {Journal of Hydrology},
volume = {606},
pages = {127434},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.127434},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422000099},
author = {Zhong-kai Feng and Peng-fei Shi and Tao Yang and Wen-jing Niu and Jian-zhong Zhou and Chun-tian Cheng},
keywords = {Hydrological time series forecasting, Artificial intelligence, Evolutionary computation, Parallel computing},
abstract = {Reliable streamflow prediction is an important productive information in the hydrology and water resources management fields. As used to forecast the nonlinear streamflow time series, the conventional artificial intelligence model may suffer from local convergence defect and fail to track the dynamic changes of the hydrological process when the model parameters and network structure are not well identified. Thus, this research develops a practical hydrological forecasting model based on parallel cooperation search algorithm (PCSA) and extreme learning machine (ELM), where the standard ELM method is chosen as the basic forecasting model, and then the PCSA method using several smaller and independent subswarms for parallel computation is used to determine satisfying input-hidden weights and hidden biases of the ELM model. The proposed model is used to forecast the nonlinear streamflow time series of several real-world hydrological stations in China. The results demonstrate that the proposed model outperforms the standard ELM model in various evaluation indicators. Thus, the key contributions of this study lie in two aspects: (1) for the first time, the parallel computing technique is developed to improve the global search ability and resources utilization efficiency of the emerging cooperation search algorithm; (2) an artificial intelligence model coupled with parallel evolutionary optimizer is proposed to improve the prediction accuracy of hydrological time series.}
}
@article{BAYER202380,
title = {The SPEAK study rationale and design: A linguistic corpus-based approach to understanding thought disorder},
journal = {Schizophrenia Research},
volume = {259},
pages = {80-87},
year = {2023},
note = {Language and Speech Analysis in Schizophrenia and Related Psychoses},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.12.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422004959},
author = {J.M.M. Bayer and J. Spark and M. Krcmar and M. Formica and K. Gwyther and A. Srivastava and A. Selloni and M. Cotter and J. Hartmann and A. Polari and Z.R. Bilgrami and C. Sarac and A. Lu and Alison R. Yung and A. McGowan and P. McGorry and J.L. Shah and G.A. Cecchi and R. Mizrahi and B. Nelson and C.M. Corcoran},
keywords = {Thought disorder, Ultra/clinical high risk, Natural language processing, Psychosis, Latent semantic analysis, Part-of-speech-tagging},
abstract = {Aim
Psychotic symptoms are typically measured using clinical ratings, but more objective and sensitive metrics are needed. Hence, we will assess thought disorder using the Research Domain Criteria (RDoC) heuristic for language production, and its recommended paradigm of “linguistic corpus-based analyses of language output”. Positive thought disorder (e.g., tangentiality and derailment) can be assessed using word-embedding approaches that assess semantic coherence, whereas negative thought disorder (e.g., concreteness, poverty of speech) can be assessed using part-of-speech (POS) tagging to assess syntactic complexity. We aim to establish convergent validity of automated linguistic metrics with clinical ratings, assess normative demographic variance, determine cognitive and functional correlates, and replicate their predictive power for psychosis transition among at-risk youths.
Methods
This study will assess language production in 450 English-speaking individuals in Australia and Canada, who have recent onset psychosis, are at clinical high risk (CHR) for psychosis, or who are healthy volunteers, all well-characterized for cognition, function and symptoms. Speech will be elicited using open-ended interviews. Audio files will be transcribed and preprocessed for automated natural language processing (NLP) analyses of coherence and complexity. Data analyses include canonical correlation, multivariate linear regression with regularization, and machine-learning classification of group status and psychosis outcome.
Conclusions
This prospective study aims to characterize language disturbance across stages of psychosis using computational approaches, including psychometric properties, normative variance and clinical correlates, important for biomarker development. SPEAK will create a large archive of language data available to other investigators, a rich resource for the field.}
}
@incollection{HORRIGAN2004317,
title = {A Study in the Process of Planning, Designing and Executing a Survey Program: The BLS American Time-Use Survey},
series = {Contributions to Economic Analysis},
publisher = {Elsevier},
volume = {271},
pages = {317-350},
year = {2004},
booktitle = {The Economics of Time Use},
issn = {0573-8555},
doi = {https://doi.org/10.1016/S0573-8555(04)71012-3},
url = {https://www.sciencedirect.com/science/article/pii/S0573855504710123},
author = {Michael Horrigan and Diane Herz},
keywords = {US, time use, survey, American time-use survey},
abstract = {In this study, we describe the evolution of the American time-use survey (ATUS) from its inception as an issue of statistical policy interest in 1991 to its implementation in January 2003 as an ongoing monthly survey sponsored by the US Bureau of Labor Statistics. This 12-year process included four developmental phases. Each successive phase represented a deeper level of agency commitment and outside statistical support. The resulting reports referenced in the text reflect an evolution in our thinking on survey estimation objectives, units of measurement, universe frame and sampling plan, and data collection and coding protocols.}
}
@article{BANDARAGODA2019104424,
title = {Enabling Collaborative Numerical Modeling in Earth Sciences using Knowledge Infrastructure},
journal = {Environmental Modelling & Software},
volume = {120},
pages = {104424},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219301562},
author = {C. Bandaragoda and A. Castronova and E. Istanbulluoglu and R. Strauch and S.S. Nudurupati and J. Phuong and J.M. Adams and N.M. Gasparini and K. Barnhart and E.W.H. Hutton and D.E.J. Hobley and N.J. Lyons and G.E. Tucker and D.G. Tarboton and R. Idaszak and S. Wang},
keywords = {Cyberinfrastructure, Knowledge infrastructure, Reproducible modeling, Landlab, HydroShare, Earth science education},
abstract = {Knowledge infrastructure is an intellectual framework for creating, sharing, and distributing knowledge. In this paper, we use knowledge infrastructure to address common barriers to entry into numerical modeling in Earth sciences as demonstrated in three computational narratives: physical process modeling education, replicating published model results, and reusing published models to extend research. We outline six critical functional requirements: 1) workflows designed for new users; 2) community-supported collaborative web platform; 3) distributed data storage; 4) software environment; 5) personalized cloud-based high-performance computing platform; and 6) a standardized open source modeling framework. Our methods meet these functional requirements by providing three interactive computational narratives for hands-on, problem-based research using Landlab on HydroShare. Landlab is an open-source toolkit for building, coupling, and exploring two-dimensional numerical models. HydroShare is an online collaborative environment for the sharing of data and models. We describe the methods we are using to accelerate knowledge development by providing a suite of modular and interoperable process components that allows students, domain experts, collaborators, researchers, and sponsors to learn by exploring shared data and modeling resources. The system is designed to support uses on the continuum from fully-developed modeling applications to prototyping research software tools. Landlab notebooks are available for interactive computing on HydroShare at https://doi.org/10.4211/hs.fdc3a06e6ad842abacfa5b896df73a76 and for further development on Github at https://zenodo.org/badge/latestdoi/187289993.}
}
@article{MAMAT201311,
title = {MAR: Maximum Attribute Relative of soft set for clustering attribute selection},
journal = {Knowledge-Based Systems},
volume = {52},
pages = {11-20},
year = {2013},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113001706},
author = {Rabiei Mamat and Tutut Herawan and Mustafa Mat Deris},
keywords = {Data mining, Soft set theory, Clustering attributes, Attribute relative, Complexity},
abstract = {Clustering, which is a set of categorical data into a homogenous class, is a fundamental operation in data mining. One of the techniques of data clustering was performed by introducing a clustering attribute. A number of algorithms have been proposed to address the problem of clustering attribute selection. However, the performance of these algorithms is still an issue due to high computational complexity. This paper proposes a new algorithm called Maximum Attribute Relative (MAR) for clustering attribute selection. It is based on a soft set theory by introducing the concept of the attribute relative in information systems. Based on the experiment on fourteen UCI datasets and a supplier dataset, the proposed algorithm achieved a lower computational time than the three rough set-based algorithms, i.e. TR, MMR, and MDA up to 62%, 64%, and 40% respectively and compared to a soft set-based algorithm, i.e. NSS up to 33%. Furthermore, MAR has a good scalability, i.e. the executing time of the algorithm tends to increase linearly as the number of instances and attributes are increased respectively.}
}
@article{MOHAN2020771,
title = {Spread Spectrum Hop Count analyzing technique based code-division multiple access for data frequencies examining in wireless network},
journal = {Computer Communications},
volume = {150},
pages = {771-776},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419304980},
author = {N. Mohan},
keywords = {Data transfer, Quality improvement, Spread Spectrum, Hop Count, CDMA},
abstract = {Code-division multiple access (CDMA) is a bandwidth access technique used by different radio waves and signal advancements. CDMA is a way of providing multiple access, where transmitters can send data at the same time, where a single clock channel can be completed. It enables us to share data frequencies with a few systems (refer to the data transfer and capacity). From the multiple backward spaces allow this CDMA uses a wide range of novelty innovation and exceptional coding scheme (where each transmitter is allocated code). In this work, the different application of the Spread Spectrum Hop Count Analyzing Technique (SSHCA–CDMA) is presented which organizes information testing techniques to create accessible assessment data, with the ultimate goal of providing the most efficient techniques for execution improvement thinking. The underlying area of eligibility testing and the evaluation of metadata inquiry are the expectation space, the data that select the most effective regulatory function. Similarly, in this work, master-based techniques have been demonstrated to validate and analyze SSHCA cells. Long, most recent developments have retained a perspective and the nature of customer correspondence management.}
}
@article{ASH1984412,
title = {Computations of cuspidal cohomology of congruence subgroups of SL(3, Z)},
journal = {Journal of Number Theory},
volume = {19},
number = {3},
pages = {412-436},
year = {1984},
issn = {0022-314X},
doi = {https://doi.org/10.1016/0022-314X(84)90081-7},
url = {https://www.sciencedirect.com/science/article/pii/0022314X84900817},
author = {Avner Ash and Daniel Grayson and Philip Green},
abstract = {Algorithms are presented which find a basis of the vector space of cuspidal cohomology of certain congruence subgroups of SL(3, Z) and which determine the action of the Hecke operators on this space. These algorithms were implemented on a computer. Four pairs of cuspidal classes were found with prime level less than 100. Tables are given of the eigenvalues of the first few Hecke operators on these classes.}
}
@article{YUCEL2019352,
title = {Battling gender stereotypes: A user study of a code-learning game, “Code Combat,” with middle school children},
journal = {Computers in Human Behavior},
volume = {99},
pages = {352-365},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302109},
author = {Yeliz Yücel and Kerem Rızvanoğlu},
keywords = {User experience (UX), Digital gender divide, Gender stereotypes, Stereotype threat, Games, Serious games},
abstract = {Abstract.
Gender has been consistently controlled as a variable in usability and playability tests. However, there is no consensus on whether and how gender differences should influence the design of digital environments. According to some research, digital environments may be unintentionally designed especially for males as a result of the existing gender biases which risks reproducing gender-polarized culture in a computational field. This study attempts to highlight that females are still being negatively affected by existing gender stereotypes and prescribed gender identities despite relatively equal access and use of computer technology. This qualitative study aims to provide insights about the first-time user experience in a home environment of 16 middle school children in Turkey (8 males - 8 females), aged between 11 and 14 years, with a code learning game named “Code Combat”. The analysis is supported with complementary quantitative findings. The present study investigates the participants' conceptualizations and opinions toward coding concept and this specific coding game. Further, it explores how existing gender stereotypes and gender biased expectations impact their behaviors and attitudes in the context of game experience. Our results indicated that perceived computer competence and perceived coding difficulty had important effects on the participants’ performance relatedly with their gender identity. According to our findings, there are important gender differences to be found in our 9 constructs, namely; perceived computer competence, perceived coding difficulty, identification, perceived game difficulty, perceived success, level of enjoyment, level of anxiety, the likelihood of playing it another time and the likelihood of trying new features.}
}
@article{BAUM1997195,
title = {A Bayesian approach to relevance in game playing},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {195-242},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00059-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000593},
author = {Eric B. Baum and Warren D. Smith},
keywords = {Relevance, Game tree search, Game theory, Computer game playing, Directed search, Utility guided search, Metareasoning, Computer chess, Computer Othello, Game trees, Graphical model, Decision theory, Utility, Bayesian model, Evaluation function, Rational search},
abstract = {The point of game tree search is to insulate oneself from errors in the evaluation function. The standard approach is to grow a full width tree as deep as time allows, and then value the tree as if the leaf evaluations were exact. The alpha-beta algorithm implements this with great computational efficiency. This approach has been effective in many games. Our approach is to form a Bayesian model of our uncertainty. We adopt an evaluation function that returns a probability distribution estimating the probability of various errors in valuing each position. These estimates are obtained by training from data. We thus use additional information at each leaf not available to the standard approach. We utilize this information in three ways: to evaluate which move is best after we are done expanding, to allocate additional thinking time to moves where additional time is most relevant to game outcome, and, perhaps most importantly, to expand the tree along the most relevant lines. Our measure of the relevance of expanding a given leaf provably approximates a measure of the impact of expanding the leaf on expected payoff, including the impact of the outcome of the leaf expansion on later expansion decisions. Our algorithms run (under reasonable assumptions) in time linear in the size of the final tree and hence except for a small constant factor, are as time efficient as alpha-beta. Our algorithm focuses on relevant lines, on which it can in principle grow a tree several times as deep as alpha-beta in a given amount of time. We have tested our approach on a variety of games, including Othello, Kalah, Warri, and others. Our probability independence approximations are seen to be significantly violated, but nonetheless our tree valuation scheme was found to play significantly better than minimax or the Probability Product rule when both competitors search the same tree. Our full search algorithm was found to outplay a highly ranked, directly comparable alpha-beta Othello program even when the alpha-beta program was given sizeable time odds, and also performed well against the three top Othello programs on the Internet Othello Server.}
}
@incollection{YANG2001291,
title = {Curved Beam Theories and Related Computational Aspects},
editor = {S. Valliappan and N. Khalili},
booktitle = {Computational Mechanics–New Frontiers for the New Millennium},
publisher = {Elsevier},
address = {Oxford},
pages = {291-298},
year = {2001},
isbn = {978-0-08-043981-5},
doi = {https://doi.org/10.1016/B978-0-08-043981-5.50046-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080439815500468},
author = {Yeong-Bin Yang},
keywords = {Buckling, curved beam, curved beam element, joint equilibrium, stability, straight beam element},
abstract = {ABSTRACT
The theories of buckling for horizontal curved beams presented by Timoshenko, Vlasov, Yoo, and Yang and Kuo are first reviewed, with their key features identified. The previous argument concerning the incapability of straight beam elements to predict the buckling loads of curved beams in incorrect, due to overlook of the conditions of equilibrium for structural joints connecting non-aligned members in the deformed position, as implied by conventional finite element approaches. If such conditions are duly taken into account, then the straight beam elements derived, which are referred to as the semitangential elements, can be used as a reliable tool for predicting the buckling loads of curved beams. Moreover, by simulating a curved beam in the limit as an infinite number of infinitesimal elements, the theory for straight beams can be manipulated through use of the concept of transfer matrix to yield the ones for curved beams for the cases of uniform bending and uniform compression. It is in this sense that the theories of straight beams and curved beams are unified.}
}
@incollection{NI2016239,
title = {Chapter 17 - More Intelligent Models},
editor = {Daiheng Ni},
booktitle = {Traffic Flow Theory},
publisher = {Butterworth-Heinemann},
pages = {239-251},
year = {2016},
isbn = {978-0-12-804134-5},
doi = {https://doi.org/10.1016/B978-0-12-804134-5.00017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128041345000179},
author = {Daiheng Ni},
keywords = {Car-following models, Psycho-physical model, Carsim model, Rule-based model, Neural network model},
abstract = {Along the lines of car-following models, single-regime models stand at one end and use one equation to handle all driving situations. Models can become increasingly intelligent if they include more and more equations to represent different regimes, such as start-up, speedup, free-flow, approaching, following, and stopping. Even more intelligent models can mimic the way of human thinking—for example, using rules and reasoning based on neural networks.}
}
@article{FANG2025101300,
title = {Generative AI-enhanced human-AI collaborative conceptual design: A systematic literature review},
journal = {Design Studies},
volume = {97},
pages = {101300},
year = {2025},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2025.101300},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X25000122},
author = {Cong Fang and Yujie Zhu and Le Fang and Yonghao Long and Huan Lin and Yangfan Cong and Stephen Jia Wang},
keywords = {Human-AI collaboration, AI-enhanced design, Design methodology, Design process, Conceptual design},
abstract = {Generative Artificial Intelligence (GenAI) has gained increasing attention, enhancing design productivity by elevating creativity within the conceptual design process. Despite these advancements, how GenAI will influence the conceptual design process and methods remains ambiguous, hindering its full potential. This study introduces a systematic literature review to explore GenAI's role in the conceptual design process, emphasizing the GenAI-human interactions and collaborations. We offer a critical evaluation of the current state of GenAI-human collaboration, identifying challenges, opportunities, and future research directions to leverage GenAI's design potential for enhancing creativity in conceptual design practice. Finally, a Generative AI Enhanced Conceptual Design framework was further proposed to clarify the potential collaborative design process, which can serve as a guideline for effective human-AI collaboration in the conceptual design process.}
}
@article{KHARE2022105028,
title = {A hybrid decision support system for automatic detection of Schizophrenia using EEG signals},
journal = {Computers in Biology and Medicine},
volume = {141},
pages = {105028},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105028},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521008222},
author = {Smith K. Khare and Varun Bajaj},
keywords = {Schizophrenia, Electroencephalography, Optimization, Robust variational mode decomposition, Optimized extreme learning machine classifier},
abstract = {Background
Schizophrenia (SCZ) is a serious neurological condition in which people suffer with distorted perception of reality. SCZ may result in a combination of delusions, hallucinations, disordered thinking, and behavior. This causes permanent disability and hampers routine functioning. Trained neurologists use interviewing and visual inspection techniques for the detection and diagnosis of SCZ. These techniques are manual, time-consuming, subjective, and error-prone. Therefore, there is a need to develop an automatic model for SCZ classification. The aim of this study is to develop an automated SCZ classification model using electroencephalogram (EEG) signals. The EEG signals can capture the changes in neural dynamics of human cognition during SCZ.
Method
Based on the nature of the SCZ condition, the EEG signals must be examined. For accurate interpretation of EEG signals during SCZ, an automated model integrating a robust variational mode decomposition (RVMD) and an optimized extreme learning machine (OELM) classifier is developed. Traditional VMD suffers from noisy mode generation, mode duplication, under segmentation, and mode discarding. These problems are suppressed in RVMD by automating the selection of quadratic penalty factor (α) and a number of modes (L). The hyperparameters (HPM) of the OELM classifier are automatically selected to ensure maximum accuracy for each mode without overfitting or underfitting. For the selection of α and L in RVMD and HPM in the OELM classifier, a whale optimization algorithm is used. The root mean square error is minimized for RVMD and classification accuracy of each mode is maximized for the OELM classifier. The EEG signals of three conditions performing basic sensory tasks have been analyzed to detect SCZ.
Results
The Kruskal Wallis test is used to select different features extracted from the modes produced by RVMD. An OELM classifier is tested using a ten-fold cross-validation technique. An accuracy, precision, specificity, F-1 measure, sensitivity, and Cohen's Kappa of 92.93%, 93.94%, 91.06% 94.07%, 97.15%, and 85.32% are obtained.
Conclusion
The third mode's chaotic features helped to capture the significant changes that occurred during the SCZ state. The findings of the RVMD-OELM-based hybrid decision support system can help neuro-experts for the accurate identification of SCZ in real-time scenarios.}
}
@article{RICH2018110,
title = {Participatory systems approaches for urban and peri-urban agriculture planning: The role of system dynamics and spatial group model building},
journal = {Agricultural Systems},
volume = {160},
pages = {110-123},
year = {2018},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2016.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16305959},
author = {Karl M. Rich and Magda Rich and Kanar Dizyee},
keywords = {Urban agriculture, System dynamics, Spatial group model building, Participatory processes, Planning, Christchurch},
abstract = {Urban agriculture has become an important research theme in recent years. Over the past decade, a number of different, diverse value chains have been established in the urban areas of developed and developing countries alike, with increasing convergence in their motivations related to food security and livelihoods development, particularly for poor and disadvantaged segments of society. However, for urban agriculture to be sustainable as a livelihoods and resilience strategy will require decision-support tools that allow planners and participants alike to jointly develop strategies and assess potential leverage points within urban food value chains. In this paper, we argue that system dynamics (SD) models combined with participatory approaches have important roles in bridging this gap, though these will need to be adapted to the spatial influences that exist in urban settings. We first review elements of urban agriculture and some of the policy challenges faced in this growing phenomenon. We follow this by motivating the role of SD models in the context of urban agriculture and note their potential utility in overlaying quantitative models of urban food value chains alongside their land-use characteristics, highlighting the dynamic feedbacks between intensive processes within changing urban food systems and extensive processes associated with land-use and planning. From this background, we introduce the concept of spatial group model building (SGMB), which adapts standard group model building concepts to account for both the spatial context of urban agriculture and enables a spatially sensitive, participatory approach to qualitative and quantitative model building. We provide a qualitative proof-of-concept of SGMB principles and techniques in the context of describing the setting and dynamic issues facing organic urban agriculture value chains in Christchurch, New Zealand. Our approach fills an important space between participatory GIS practices and the development of complex spatial system dynamics models, infusing systems thinking principles to participatory processes, while showing a way to enhance the future development of quantitative spatial system dynamics models more generally.}
}
@article{TSOTSOS2021305,
title = {On the control of attentional processes in vision},
journal = {Cortex},
volume = {137},
pages = {305-329},
year = {2021},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010945221000150},
author = {John K. Tsotsos and Omar Abid and Iuliia Kotseruba and Markus D. Solbach},
keywords = {Vision, Attention, Control, Cognitive program, Selective tuning},
abstract = {The study of attentional processing in vision has a long and deep history. Recently, several papers have presented insightful perspectives into how the coordination of multiple attentional functions in the brain might occur. These begin with experimental observations and the authors propose structures, processes, and computations that might explain those observations. Here, we consider a perspective that past works have not, as a complementary approach to the experimentally-grounded ones. We approach the same problem as past authors but from the other end of the computational spectrum, from the problem nature, as Marr's Computational Level would prescribe. What problem must the brain solve when orchestrating attentional processes in order to successfully complete one of the myriad possible visuospatial tasks at which we as humans excel? The hope, of course, is for the approaches to eventually meet and thus form a complete theory, but this is likely not soon. We make the first steps towards this by addressing the necessity of attentional control, examining the breadth and computational difficulty of the visuospatial and attentional tasks seen in human behavior, and suggesting a sketch of how attentional control might arise in the brain. The key conclusions of this paper are that an executive controller is necessary for human attentional function in vision, and that there is a 'first principles' computational approach to its understanding that is complementary to the previous approaches that focus on modelling or learning from experimental observations directly.}
}
@article{LAFORCADE2010347,
title = {A Domain-Specific Modeling approach for supporting the specification of Visual Instructional Design Languages and the building of dedicated editors},
journal = {Journal of Visual Languages & Computing},
volume = {21},
number = {6},
pages = {347-358},
year = {2010},
note = {Special Issue on Visual Instructional Design Languages},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2010.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X10000492},
author = {Pierre Laforcade},
keywords = {Visual Instructional Design Languages Domain Specific Modeling, Visual and executable models},
abstract = {This paper presents, illustrates and discusses theories and practices about the application of a domain-specific modeling (DSM) approach to facilitate the specification of Visual Instructional Design Languages (VIDLs) and the development of dedicated graphical editors. Although this approach still requires software engineering skills, it tackles the need of building VIDLs allowing both visual models for human-interpretation purposes (explicit designs, communication, thinking, etc.) and machine-readable notations for deployment or other instructional design activities. This article proposes a theoretical application and a categorization, based on a domain-oriented separation of concerns of instructional design. It also presents some practical illustrations from experiments of specific DSM tooling. Key lessons learned as well as observed obstacles and challenges to deal with are discussed in order to further develop such an approach.}
}
@incollection{PREISIG20121242,
title = {HAZOP - an automaton-inspired approach},
editor = {Ian David Lockhart Bogle and Michael Fairweather},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {30},
pages = {1242-1246},
year = {2012},
booktitle = {22nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-59520-1.50107-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044459520150107X},
author = {Heinz A Preisig and Flavio Manenti},
keywords = {dynamic system, safety, HAZOP},
abstract = {If there exists a gradient pointing outwards a save-operation region, then there exists a possible path for the plant to cross out of the save-operation region. A method is introduced that allows the identification of such surface pieces in the phase space of the state variable for the analysed equipment. The idea is based on splitting the phase space into subspaces separated by the zero-dynamic component surface. The computation requires only root solving of the right-hand side of the dynamic equations, one at the time.}
}
@article{HAFRI2021475,
title = {The Perception of Relations},
journal = {Trends in Cognitive Sciences},
volume = {25},
number = {6},
pages = {475-492},
year = {2021},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661321000085},
author = {Alon Hafri and Chaz Firestone},
keywords = {visual psychophysics, core cognition, intuitive physics, compositionality, structured representations, role-binding},
abstract = {The world contains not only objects and features (red apples, glass bowls, wooden tables), but also relations holding between them (apples contained in bowls, bowls supported by tables). Representations of these relations are often developmentally precocious and linguistically privileged; but how does the mind extract them in the first place? Although relations themselves cast no light onto our eyes, a growing body of work suggests that even very sophisticated relations display key signatures of automatic visual processing. Across physical, eventive, and social domains, relations such as support, fit, cause, chase, and even socially interact are extracted rapidly, are impossible to ignore, and influence other perceptual processes. Sophisticated and structured relations are not only judged and understood, but also seen — revealing surprisingly rich content in visual perception itself.}
}
@article{SELKER2005410,
title = {Fostering motivation and creativity for computer users},
journal = {International Journal of Human-Computer Studies},
volume = {63},
number = {4},
pages = {410-421},
year = {2005},
note = {Computer support for creativity},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2005.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1071581905000443},
author = {Ted Selker},
keywords = {Communicate, Communication, Community, Computers, Creative, Creativity, Creativity-enhancing, Design, Engineering, Filter, Graphics, Human–computer, Idea, Interaction, Motivation, Product, Programming, Social, Support, Text, User interface},
abstract = {Creativity might be viewed as any process which results in a novel and useful product. People use computers for creative tasks; they flesh out ideas for text, graphics, engineering solutions, etc. Computer programming is an especially creative activity, but few tools for programming aid creativity. Computers can be designed to foster creativity as well. As a start, all computer programs should help users enumerate ideas, remember alternatives and support various ways to compare them. More sophisticated thinking aids could implement other successful techniques as well. Most computers are used in solitude; however, people depend on social supports for creativity. User scenarios can provide the important social support and gracious cues normally offered by collaborators that keep people motivated and help them consider alternatives. People also use computers to build community and to communicate. Computers should also support and filter these potentially creativity-enhancing communication acts. User-interface designers are so busy exposing features and fighting bugs that they might ignore their users’ needs for motivation and creativity support. This paper develops the notion that creativity and motivation enhancement can easily be aligned with the design of high-quality human–computer interaction. User interface toolkits and evaluations should include support for motivation and creativity-enhancing approaches.}
}
@article{ZHENG2018214,
title = {An improved genetic approach for composing optimal collaborative learning groups},
journal = {Knowledge-Based Systems},
volume = {139},
pages = {214-225},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117304914},
author = {Yaqian Zheng and Chunrong Li and Shiyu Liu and Weigang Lu},
keywords = {Collaborative learning, Learner group formation, Genetic algorithm, Optimal solution},
abstract = {Collaborative learning is an effective strategy for promoting learning in both traditional face-to-face and online environments. When applying it, students should be assigned to best collaborative groups at the first step, which is called the learner group formation task. In previous studies, various approaches have been proposed to solve this problem. However, they failed to meet all the problem requirements. To address this problem, a generic group formation method that covers all aspects of the problem is proposed in this study. In this method, all requirements of the learner group formation problem are formulated into an integrated mathematical model and an improved genetic algorithm is proposed to solve the model and obtain optimal learning groups to meet various grouping requirements for different educational contexts. To analyse the performance of the proposed approach from a computational perspective, a series of computational experiments are conducted based on eight simulation datasets with different levels of complexity. The simulation results indicate that the proposed method is effective and stable for solving the learner group formation problem. An empirical study is also carried out to validate the proposed approach from a pedagogical view by comparing it with two traditional group formation strategies. The results show that groups formed through the proposed method produce better outcomes than others in terms of group grades, individual grades and student satisfaction.}
}
@article{ROSEGGER1992iii,
title = {Editorial},
journal = {Technovation},
volume = {12},
number = {1},
pages = {iii-iv},
year = {1992},
issn = {0166-4972},
doi = {https://doi.org/10.1016/0166-4972(92)90028-G},
url = {https://www.sciencedirect.com/science/article/pii/016649729290028G},
author = {Gerhard Rosegger},
abstract = {Alfred North Whitehead observed that “It is a profoundly erroneous truism, repeated by all copy-books and by eminent people when they are making speeches, that we should cultivate the habit of thinking what we are doing. The precise opposite is the case. Civilization advances by extending the number of important operations which we can perform without thinking about them.”}
}
@article{GREENWOOD2021106597,
title = {Exploring a causal model in observational cohort data: The role of parents and peers in shaping substance use trajectories},
journal = {Addictive Behaviors},
volume = {112},
pages = {106597},
year = {2021},
issn = {0306-4603},
doi = {https://doi.org/10.1016/j.addbeh.2020.106597},
url = {https://www.sciencedirect.com/science/article/pii/S0306460320307279},
author = {C.J. Greenwood and G.J. Youssef and P. Letcher and E.A. Spry and K.C. Thomson and L.J. Hagg and D.M. Hutchinson and J.A. Macdonald and J. McIntosh and A. Sanson and J.W. Toumbourou and C.A. Olsson},
keywords = {Causal modeling, substance use, Adolescence, Young adulthood, Trajectory, Parents, Peers},
abstract = {Aims
To explore the process of applying counterfactual thinking in examining causal determinants of substance use trajectories in observational cohort data. Specifically, we examine the extent to which quality of the parent-adolescent relationship and affiliations with deviant peers are causally related to trajectories of alcohol, tobacco, and cannabis use across adolescence and into young adulthood.
Methods
Data were drawn from the Australian Temperament Project, a population-based cohort study that has followed a sample of young Australians from infancy to adulthood since 1983. Parent-adolescent relationship quality and deviant peer affiliations were assessed at age 13–14 years. Latent curve models were fitted for past month alcohol, tobacco, and cannabis use (n = 1590) from age 15–16 to 27–28 years (5 waves). Confounding factors were selected in line with the counterfactual framework.
Results
Following confounder adjustment, higher quality parent-adolescent relationships were associated with lower baseline cannabis use, but not alcohol or tobacco use trajectories. In contrast, affiliations with deviant peers were associated with higher baseline binge drinking, tobacco, and cannabis use, and an earlier peak in the cannabis use trajectory.
Conclusions
Despite careful application of the counterfactual framework, interpretation of associations as causal is not without limitations. Nevertheless, findings suggested causal effects of both parent-adolescent relationships and deviant peer affiliations on the trajectory of substance use. Causal effects were more pervasive (i.e., more substance types) and protracted for deviant peer affiliations. The exploration of causal relationships in observational cohort data is encouraged, when relevant limitations are transparently acknowledged.}
}
@article{REHDER201454,
title = {Independence and dependence in human causal reasoning},
journal = {Cognitive Psychology},
volume = {72},
pages = {54-107},
year = {2014},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028514000176},
author = {Bob Rehder},
keywords = {Causal reasoning, Causal inference, Causal Markov condition, Conditional independence, Screening off},
abstract = {Causal graphical models (CGMs) are a popular formalism used to model human causal reasoning and learning. The key property of CGMs is the causal Markov condition, which stipulates patterns of independence and dependence among causally related variables. Five experiments found that while adult’s causal inferences exhibited aspects of veridical causal reasoning, they also exhibited a small but tenacious tendency to violate the Markov condition. They also failed to exhibit robust discounting in which the presence of one cause as an explanation of an effect makes the presence of another less likely. Instead, subjects often reasoned “associatively,” that is, assumed that the presence of one variable implied the presence of other, causally related variables, even those that were (according to the Markov condition) conditionally independent. This tendency was unaffected by manipulations (e.g., response deadlines) known to influence fast and intuitive reasoning processes, suggesting that an associative response to a causal reasoning question is sometimes the product of careful and deliberate thinking. That about 60% of the erroneous associative inferences were made by about a quarter of the subjects suggests the presence of substantial individual differences in this tendency. There was also evidence that inferences were influenced by subjects’ assumptions about factors that disable causal relations and their use of a conjunctive reasoning strategy. Theories that strive to provide high fidelity accounts of human causal reasoning will need to relax the independence constraints imposed by CGMs.}
}
@article{LEVINSON2012167,
title = {Tools from evolutionary biology shed new light on the diversification of languages},
journal = {Trends in Cognitive Sciences},
volume = {16},
number = {3},
pages = {167-173},
year = {2012},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2012.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661312000290},
author = {Stephen C. Levinson and Russell D. Gray},
abstract = {Computational methods have revolutionized evolutionary biology. In this paper we explore the impact these methods are now having on our understanding of the forces that both affect the diversification of human languages and shape human cognition. We show how these methods can illuminate problems ranging from the nature of constraints on linguistic variation to the role that social processes play in determining the rate of linguistic change. Throughout the paper we argue that the cognitive sciences should move away from an idealized model of human cognition, to a more biologically realistic model where variation is central.}
}
@incollection{SEN201629,
title = {3 - History of zero including its representation and role},
editor = {Syamal K. Sen and Ravi P. Agarwal},
booktitle = {Zero},
publisher = {Academic Press},
pages = {29-75},
year = {2016},
isbn = {978-0-08-100774-7},
doi = {https://doi.org/10.1016/B978-0-08-100774-7.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008100774700003X},
author = {Syamal K. Sen and Ravi P. Agarwal},
keywords = {Algorithms for arithmetic operations, alphabetical positional number system, assumption versus axioms, avoidance of subtraction, Brahmagupta’s rule to compute with zero, building block of matter, direction separator, driver of calculus, dwarf and machine epsilon, exponential growth of computing power, Godel’s incompleteness theorem, Gregorian calendar, history of zero, image of the earth, infinite versus finite precisions, infinitive universe, Maya numbers and long count, mean value theorem, Mohanjodaro and Harappa civilization, most pervasive global symbol, object of zero dimension, Quipu, representation of nothingness, Rolle’s theorem, sexagesimal (base 60) positional number system, stone/copper plate inscription, Vedas and Puranas, violation of a law of nature, Zeno’s paradoxes, zero as a number, zero as a vacant position, zero-free system, zero with its eternal spiritual significance},
abstract = {The chronological development of the history of zero over the centuries is a tough job due to both poor man to man communication and also poor publication machinery. However, the time period 7000 BC–2015 AD is broadly divided into four parts based on the landmark innovations in each part. During 7000–2000 BC, the most important contribution, that is, the modern decimal based place value system with 0 as a number due to Aryabhatta was developed and used. The Maya numbers and Long Count days that were tallied in a modified radix-20 number system are notable. Zero with representation and arithmetic operations was fully developed during 2000 BC–1000 AD. Brahmagupta’s rules for arithmetic operations were developed. The Romans and the Greeks had no zero then and their system was order-valued. Egyptian numerals were base-10 while Babylonian mathematics had a base-60 positional number system. With better understanding of zero, calculus was born. Arab and Persian mathematicians were active and became an important interface between the east and the west in promoting number systems with arithmetic. The period 1000–1900 AD saw the introduction of the Hindu–Arabic numeral system in Europe. The link between the system and European mathematics is the Italian mathematician Fibonacci. During the late eleventh century AD, Shen Gua introduced infinitesimal and exhaustion. He described piling up very small things. During 1900–2015 AD, increasingly high-speed modern digital computing made its presence felt very intensely globally by one and all. Specifically due to its finite precision, unlike the infinite precision which the regular and natural mathematics have, the advent of numerical zero, as opposed to the exact zero, changed the face of all real-world computation. Understanding natural, regular, and computational mathematics with calculus, and specifically the role of zero, became extraordinarily important in all engineering computations. The computational error (implying quality of solution) and computational complexity (cost of computation) due to the presence of numerical zero became integral parts of any algorithm to justify the acceptability of a solution. During the early twentieth century, Ramanujan, to whom each number is a living being and his personal friend carrying an important distinct message, felt intensely the eternal spiritual significance of zero and its inverse infinity. He built a theory of reality around zero and infinity.}
}
@article{LOPEZPERSEM2023273,
title = {Conceptual promises and mechanistic challenges of the creative metacognition framework: Comment on “A systematic framework of creative metacognition” by Izabela Lebuda and Mathias Benedek},
journal = {Physics of Life Reviews},
volume = {47},
pages = {273-275},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001860},
author = {Alizée Lopez-Persem and Marion Rouault and Emmanuelle Volle}
}
@article{LO2022111357,
title = {Architectural patterns for the design of federated learning systems},
journal = {Journal of Systems and Software},
volume = {191},
pages = {111357},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111357},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000899},
author = {Sin Kit Lo and Qinghua Lu and Liming Zhu and Hye-Young Paik and Xiwei Xu and Chen Wang},
keywords = {Federated learning, Pattern, Software architecture, Machine learning, Artificial intelligence},
abstract = {Federated learning has received fast-growing interests from academia and industry to tackle the challenges of data hungriness and privacy in machine learning. A federated learning system can be viewed as a large-scale distributed system with different components and stakeholders as numerous client devices participate in federated learning. Designing a federated learning system requires software system design thinking apart from the machine learning knowledge. Although much effort has been put into federated learning from the machine learning technique aspects, the software architecture design concerns in building federated learning systems have been largely ignored. Therefore, in this paper, we present a collection of architectural patterns to deal with the design challenges of federated learning systems. Architectural patterns present reusable solutions to a commonly occurring problem within a given context during software architecture design. The presented patterns are based on the results of a systematic literature review and include three client management patterns, four model management patterns, three model training patterns, four model aggregation patterns, and one configuration pattern. The patterns are associated to the particular state transitions in a federated learning model lifecycle, serving as a guidance for effective use of the patterns in the design of federated learning systems.}
}
@incollection{GOLDSCHMIDT201146,
title = {Architecture},
editor = {Mark A. Runco and Steven R. Pritzker},
booktitle = {Encyclopedia of Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {46-51},
year = {2011},
isbn = {978-0-12-375038-9},
doi = {https://doi.org/10.1016/B978-0-12-375038-9.00010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123750389000108},
author = {G. Goldschmidt},
keywords = {Architectural design, Architectural education, Culture, Digital design, Form, Function, Ideas, Leading idea, Starchitect, Style},
abstract = {Architecture is a cultural arena based on ideas, which communally produce styles and individually, at their best, generate outstanding buildings. Every building tackles form and function. In our era architecture is expected to innovate in its forms, while ensuring perfect functionality. Form and function handling are rough correlates of originality and practicality, by which we measure design creativity. Architecture is also a product of the technological state of its time. At present we experience computational advances that promise to fundamentally change buildings and the manner in which they are designed. Architectural education is groping to adjust to the changes.}
}
@article{PEREZESCOBAR202423,
title = {Minimal logical teleology in artifacts and biology connects the two domains and frames mechanisms via epistemic circularity},
journal = {Studies in History and Philosophy of Science},
volume = {104},
pages = {23-37},
year = {2024},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368124000104},
author = {José Antonio Pérez-Escobar},
keywords = {Minimal logical teleology, Analogies, Scientific explanation, Epistemic circularity, Scientific modelling, Cognitive neuroscience},
abstract = {The understanding of artifacts and biological phenomena has often influenced each other. This work argues that at the core of these epistemic bridges there are shared teleological notions and explanations manifested in analogies between artifacts and biological phenomena. To this end, I first propose a focus on the logical structure of minimal teleological explanations, which renders said epistemic bridges more evident than an ontological or metaphysical approach to teleology, and which can be used to describe scientific practices in different areas by virtue of formal generality and minimalism (section 2). Second, I show how this approach highlights some epistemic features shared by the understanding of artifacts and biological phenomena, like a specific kind of epistemic circularity, and how functional analogies between artifacts and biological phenomena translate such epistemic circularity from one domain to the other (section 3). Third, I conduct a case study on the scientific practice around the brain's “compass”, showing how the understanding of artifacts influences purpose ascription and measurement, and frames mechanisms in biology, especially in areas where purpose ascription is most difficult, like cognitive neuroscience (sections 4 and 5).}
}
@incollection{SYMMONDS20123,
title = {Chapter 1 - The Neurobiology of Preferences},
editor = {Raymond Dolan and Tali Sharot},
booktitle = {Neuroscience of Preference and Choice},
publisher = {Academic Press},
address = {San Diego},
pages = {3-31},
year = {2012},
isbn = {978-0-12-381431-9},
doi = {https://doi.org/10.1016/B978-0-12-381431-9.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123814319000012},
author = {Mkael Symmonds and Raymond J. Dolan},
keywords = {Preference, choice, neuroscience, neuroeconomics, decision-making, action, value},
abstract = {Publisher Summary
The neuroscience of choice and preference dates back to the nineteenth century, with the emergence of the idea of functional specialization as a fundamental organizational principle of the brain. The development of neuroimaging techniques—in particular, functional magnetic resonance imaging (fMRI)—has meant that questions related to choice and preference can now be addressed non-invasively in humans. There are important examples where choices do not accord with internal wants. An addict may perform an action in the present despite expressing a desire to avoid doing this very action on a prior occasion. A major conundrum when thinking about neurobiological mechanisms in decision-making is the fact that choices are often noisy or stochastic. A different network of regions in precuneus, left prefrontal, and temproparietal cortex reflected endogenous inequity aversion across subjects, illustrating that even within the context of a specific task, preferences for the same stimulus feature can be expressed in different regions and modulated in a distinct manner.}
}
@article{CHAKRABORTY2013180,
title = {Secret image sharing using grayscale payload decomposition and irreversible image steganography},
journal = {Journal of Information Security and Applications},
volume = {18},
number = {4},
pages = {180-192},
year = {2013},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.istr.2013.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1363412713000162},
author = {Soumendu Chakraborty and Anand Singh Jalal and Charul Bhatnagar},
keywords = {DSF matrix, Error matrix, Sign matrix, Bit plane},
abstract = {To provide an added security level most of the existing reversible as well as irreversible image steganography schemes emphasize on encrypting the secret image (payload) before embedding it to the cover image. The complexity of encryption for a large payload where the embedding algorithm itself is complex may adversely affect the steganographic system. Schemes that can induce same level of distortion, as any standard encryption technique with lower computational complexity, can improve the performance of stego systems. In this paper, we propose a secure secret image sharing scheme, which bears minimal computational complexity. The proposed scheme, as a replacement for encryption, diversifies the payload into different matrices which are embedded into carrier image (cover image) using bit X-OR operation. A payload is a grayscale image which is divided into frequency matrix, error matrix, and sign matrix. The frequency matrix is scaled down using a mapping algorithm to produce Down Scaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix are then embedded in different cover images using bit X-OR operation between the bit planes of the matrices and respective cover images. Analysis of the proposed scheme shows that it effectively camouflages the payload with minimum computation time.}
}
@article{LIETO20161,
title = {From human to artificial cognition and back: New perspectives on cognitively inspired AI systems},
journal = {Cognitive Systems Research},
volume = {39},
pages = {1-3},
year = {2016},
note = {From human to artificial cognition (and back): new perspectives of cognitively inspired AI systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300183},
author = {Antonio Lieto and Daniele P. Radicioni},
keywords = {Cognitive systems, Artificial intelligence, Computational models of cognition, Epistemology of the artificial},
abstract = {We overview the main historical and technological elements characterising the rise, the fall and the recent renaissance of the cognitive approaches to Artificial Intelligence and provide some insights and suggestions about the future directions and challenges that, in our opinion, this discipline needs to face in the next years.}
}
@incollection{MORGERA1986389,
title = {COMPUTATIONAL COMPLEXITY AND VLSI IMPLEMENTATION OF AN OPTIMAL FEATURE SELECTION STRATEGY††Work supported by Canada NSERC Grant AO912.},
editor = {Edzard S. GELSEMA and Laveen N. KANAL},
booktitle = {Pattern Recognition in Practice},
publisher = {Elsevier},
address = {Amsterdam},
pages = {389-400},
year = {1986},
isbn = {978-0-444-87877-9},
doi = {https://doi.org/10.1016/B978-0-444-87877-9.50036-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444878779500364},
author = {Salvatore D. Morgera}
}
@article{AIBINU2023100590,
title = {Solutions of fractional differential equations by using a blend of variational iteration method with Sumudu transform and application to price adjustment equations},
journal = {Partial Differential Equations in Applied Mathematics},
volume = {8},
pages = {100590},
year = {2023},
issn = {2666-8181},
doi = {https://doi.org/10.1016/j.padiff.2023.100590},
url = {https://www.sciencedirect.com/science/article/pii/S2666818123001031},
author = {M.O. Aibinu and S. Moyo},
keywords = {Sumudu transform, Caputo fractional derivative, Price adjustment, Model, Market equilibrium},
abstract = {The presence of delays in a mathematical model can improve its vitality and suitability in describing several phenomena. However, in the presence of delays, nonlinear fractional differential equations are more difficult to study. This paper presents the use of a blend of variational iteration method with Sumudu transform for solving delay differential equations with Caputo derivatives of fractional variable order. Moreover, the paper introduces delays into the price adjustment equations to propose new price adjustment models with more potential for vitality and suitability. The paper assigns suitable real values to the parameters for the graphical display and comparison of the obtained solutions. The paper presents the interactions that exist among the price, demand, supply and dependence of supply and demand on the price, which can be applied to estimate the equilibrium price.}
}
@article{JIANG2020556,
title = {Energy aware edge computing: A survey},
journal = {Computer Communications},
volume = {151},
pages = {556-580},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S014036641930831X},
author = {Congfeng Jiang and Tiantian Fan and Honghao Gao and Weisong Shi and Liangkai Liu and Christophe Cérin and Jian Wan},
keywords = {Edge computing, Energy efficiency, Computing offloading, Benchmarking, Computation partitioning},
abstract = {Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.}
}
@article{GAO201464,
title = {Unconscious processing modulates creative problem solving: Evidence from an electrophysiological study},
journal = {Consciousness and Cognition},
volume = {26},
pages = {64-73},
year = {2014},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2014.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053810014000464},
author = {Ying Gao and Hao Zhang},
keywords = {Unconscious processing, Divergent thinking, Creative problem solving, Creativity, Event-related potential},
abstract = {Previous behavioral studies have identified the significant role of subliminal cues in creative problem solving. However, neural mechanisms of such unconscious processing remain poorly understood. Here we utilized an event-related potential (ERP) approach and sandwich mask technique to investigate cerebral activities underlying the unconscious processing of cues in creative problem solving. College students were instructed to solve divergent problems under three different conditions (conscious cue, unconscious cue and no-cue conditions). Our data showed that creative problem solving can benefit from unconscious cues, although not as much as from conscious cues. More importantly, we found that there are crucial ERP components associated with unconscious processing of cues in solving divergent problems. Similar to the processing of conscious cues, processing unconscious cues in problem solving involves the semantic activation of unconscious cues (N280–340) in the right inferior parietal lobule (BA 40), new association formation (P350–450) in the right parahippocampal gyrus (BA 36), and mental representation transformation (P500–760) in the right superior temporal gyrus (BA 22). The present results suggest that creative problem solving can be modulated by unconscious processing of enlightening information that is weakly diffused in the semantic network beyond our conscious awareness.}
}
@article{MILLER2016102,
title = {Provision for income tax expense ASC 740: A teaching note},
journal = {Journal of Accounting Education},
volume = {35},
pages = {102-126},
year = {2016},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2015.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0748575115000858},
author = {Tad Miller and Lindsay Miller and Jeffrey Tolin},
keywords = {Provision for income tax expense, Accounting Standards Codification, Deferred tax assets, Deferred tax liabilities, Deductible temporary differences, Taxable temporary differences},
abstract = {This project requires students to think critically to synthesize concepts they learned in their financial reporting and tax classes. They will use and interpret accounting standards to prepare tax provisions, comparative financial statements and the appropriate footnote disclosures. Even a simple tax provision results in a challenging project.}
}
@article{COVINGTON2016869,
title = {Expanding the Language Network: Direct Contributions from the Hippocampus},
journal = {Trends in Cognitive Sciences},
volume = {20},
number = {12},
pages = {869-870},
year = {2016},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2016.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661316301759},
author = {Natalie V. Covington and Melissa C. Duff},
keywords = {hippocampus, language, memory, online processing, theta oscillations},
abstract = {New research suggests that the same hippocampal computations used in support of memory are also used for language processing, providing direct neurophysiological evidence of a shared neural mechanism for memory and language. This work expands classic memory and language models and represents a new opportunity for studying the memory–language interface.}
}
@article{VELOSO2023104997,
title = {Spatial synthesis for architectural design as an interactive simulation with multiple agents},
journal = {Automation in Construction},
volume = {154},
pages = {104997},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104997},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523002571},
author = {Pedro Veloso and Ramesh Krishnamurti},
keywords = {Spatial synthesis, Interactive simulation, Artificial intelligence, Agent-based modeling, Multi-agent deep reinforcement learning},
abstract = {Motivated by reflection-in-action in architectural design, this article introduces a spatial synthesis artifact that relies on multi-agent reinforcement learning to address spatial goals with fine-grained control in a simulation. It relies on parameter sharing with proximal policy optimization and a parameterized reward function to train robust agent policies in random environments with random spatial problems. The agents are evaluated in three design cases: a house design with 12 agents in three sites, a museum with 18 agents in an interstitial urban site, and a speculative design of a housing complex with 96 agents on a large empty site. The policies performed well in all the cases and produced morphologically consistent solutions. However, in cases with a larger number of agents, the system largely benefited from a spring layout algorithm for the initialization. Future research will address more complex spatial synthesis problems and mechanisms for human-computer interaction.}
}
@article{YANOVA201682,
title = {Relativistic Psychometrics in Subjective Scaling},
journal = {Procedia Computer Science},
volume = {102},
pages = {82-89},
year = {2016},
note = {12th International Conference on Application of Fuzzy Systems and Soft Computing, ICAFS 2016, 29-30 August 2016, Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.373},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325534},
author = {Natalia Yanova},
keywords = {mental representation, relativistic psychometrics, computational theory of perceptions, image understanding, significance, expert test system, psychosemiotics},
abstract = {The article announces the possibilities of semantic modeling in the development of feedback tools in social sciences. A new approach to the computational theory of perceptions (CTP) for analysis of mental object is proposed. The article demonstrates the implementation of relativistic psychometrics for the study of mental response (opinions, expectations and attitudes). The problem of image understanding and its significance is considered in combination of soft and hard computing. It is shown that the modeling of object (its coding and decoding in ‘mental map’) obeys the semiotic and mathematical logic. Computing with perceptions for the rules of mental representation proves their identity to the laws of conservation. The article demonstrates the versatility of the semiotic description of objects in Minkowski space. It also confirms by mathematical solution C. S. Peirce's metaphor, according to which the semiology of language is a truly universal algebra of relations.}
}
@article{PROKOPENKO2019134,
title = {Self-referential basis of undecidable dynamics: From the Liar paradox and the halting problem to the edge of chaos},
journal = {Physics of Life Reviews},
volume = {31},
pages = {134-156},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300077},
author = {Mikhail Prokopenko and Michael Harré and Joseph Lizier and Fabio Boschetti and Pavlos Peppas and Stuart Kauffman},
keywords = {Self-reference, Diagonalization, Undecidability, Incomputability, Program-data duality, Complexity},
abstract = {In this paper we explore several fundamental relations between formal systems, algorithms, and dynamical systems, focussing on the roles of undecidability, universality, diagonalization, and self-reference in each of these computational frameworks. Some of these interconnections are well-known, while some are clarified in this study as a result of a fine-grained comparison between recursive formal systems, Turing machines, and Cellular Automata (CAs). In particular, we elaborate on the diagonalization argument applied to distributed computation carried out by CAs, illustrating the key elements of Gödel's proof for CAs. The comparative analysis emphasizes three factors which underlie the capacity to generate undecidable dynamics within the examined computational frameworks: (i) the program-data duality; (ii) the potential to access an infinite computational medium; and (iii) the ability to implement negation. The considered adaptations of Gödel's proof distinguish between computational universality and undecidability, and show how the diagonalization argument exploits, on several levels, the self-referential basis of undecidability.}
}
@article{WANG2023109577,
title = {Study of the flow field of a new fishtail-type stirring impeller in a stirred tank},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {194},
pages = {109577},
year = {2023},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2023.109577},
url = {https://www.sciencedirect.com/science/article/pii/S0255270123003148},
author = {Zhaohui Wang and Deli Li and Quanjie Gao and Qianwen Yang and Xiao Xiong and Changzhi Jiang and Feng Zhang},
keywords = {Computational fluid dynamics, Power consumption, Particle image velocimetry, Impeller design, Blade inclination},
abstract = {Abstracts
In this study, a new fishtail impeller was introduced to improve the mixing of fluids in the stirred tank. The validity of the numerical model was first demonstrated by PIV experiments. Secondly, the CFD technique was used to analyze and predict the flow field characteristics in the stirred tank. Also, the effect of blade inclination on the mixing effect is analyzed. Finally, a comparative Analysis with the whale tail impeller is carried out to demonstrate the superiority of this research work. The results show that The results of the study showed that the power number of the fishtail impeller was reduced by 16.4 % compared to the RT impeller. The pumping efficiency of the fishtail impeller was increased by 25.52 %. The results also show that increasing the blade inclination increases the turbulent kinetic energy in the stirred tank. Comparative analysis with the whale-tail impeller reveals that the power number of the fish-tail impeller is reduced by 17.2 % and the pumping efficiency is increased by 19.97 %.}
}
@article{ZHANG2024127373,
title = {Adaptive emotion neural network based on ITCSO and grey correlation contribution},
journal = {Neurocomputing},
volume = {577},
pages = {127373},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127373},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224001449},
author = {Wei Zhang and Wanfeng Wei},
keywords = {Emotion neural network, Hormone regulation, Competitive swarm optimization, Grey correlation contribution, Convergence analysis},
abstract = {In order to further improve the performance of emotion neural network (ENN), a novel adaptive hormone regulation emotion neural network (HRENN) is proposed, which is based on the improved triple competitive swarm optimization (ITCSO) algorithm and grey correlation contribution. Firstly, the structure of HRENN is designed that is inspired by the biological mechanism of hormone regulation. The fast response characteristic of emotion processing and the feedback effect of hormone regulation can effectively improve the learning ability of HRENN. Secondly, the ITCSO algorithm is proposed for optimizing the parameters of HRENN. In order to strike a well balance between exploration and exploitation, triple competition mechanism is adopted. Two-thirds of particles participate in the optimization and different learning strategies for different particles are provided. These operations can greatly improve the optimization efficiency and the convergence accuracy. Moreover, grey correlation contribution is used to add or delete the dimension of particles. It means that the structure and parameters of HRENN can be adjusted simultaneously and the compact structure can be obtained. Finally, the stability and the convergence of ITCSO are proved using the Banach space and the principle of compression mapping. Experiment results show that the proposed ITCSO-HRENN has good self-organization ability, compact network structure, high convergence accuracy and superior computation efficiency compared with other methods.}
}
@article{LEAHY2019102422,
title = {The digital frontier: Envisioning future technologies impact on the classroom},
journal = {Futures},
volume = {113},
pages = {102422},
year = {2019},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0016328718304166},
author = {Sean M. Leahy and Charlotte Holland and Francis Ward},
keywords = {Artificial intelligence, Augmented reality, Smart materials, Educational technology, Education futures, Education},
abstract = {Global advances in technology and information are purportedly propelling transformations and disruptions across many sectors, including education. However, historical reviews of technology integration in education mainly reveal weak or ineffectual impacts on learning, and only minor reforms to date within the education system. This study adopted a futures studies methodological approach to explore how K-12 educational spaces and experiences might be shaped by emerging and emergent technologies. In this regard, a series of vignettes are presented which critically examine the potential of augmented reality technologies, artificial intelligence, and smart materials technologies to transform future learning experiences and learning environments across K-12 education contexts, while also challenging assumptions about, and considering influences on, these futures. The focus of the study was not to predict a single or desired future for education, but rather to critically consider a range of possible education futures informed by the articulation of these three vignettes. The paper concludes with discourse on an emergent pedagogic approach that has the potential to prepare teachers and learners to interact and flourish within radically re-configured learning spaces that lean on the aforementioned technologies to support transitions within and beyond the school and its connected communities.}
}
@article{DELATORRE2014653,
title = {Monte Carlo advances and concentrated solar applications},
journal = {Solar Energy},
volume = {103},
pages = {653-681},
year = {2014},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2013.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X13001448},
author = {J. Delatorre and G. Baud and J.J. Bézian and S. Blanco and C. Caliot and J.F. Cornet and C. Coustet and J. Dauchet and M. {El Hafi} and V. Eymet and R. Fournier and J. Gautrais and O. Gourmel and D. Joseph and N. Meilhac and A. Pajot and M. Paulin and P. Perez and B. Piaud and M. Roger and J. Rolland and F. Veynandt and S. Weitz},
keywords = {Monte Carlo algorithm, Concentrated solar energy, Solar energy flux density distribution, Solar concentrators design optimization, Sensitivity computation},
abstract = {The Monte Carlo method is partially reviewed with the objective of illustrating how some of the most recent methodological advances can benefit to concentrated solar research. This review puts forward the practical consequences of writing down and handling the integral formulation associated to each Monte Carlo algorithm. Starting with simple examples and up to the most complex multiple reflection, multiple scattering configurations, we try to argue that these formulations are very much accessible to the non specialist and that they allow a straightforward entry to sensitivity computations (for assistance in design optimization processes) and to convergence enhancement techniques involving subtle concepts such as control variate and zero variance. All illustration examples makePROMES - UPR CNRS 8521 - 7, rue du Four Solaire, 66120 Font Romeu Odeillo, France use of the public domain development environment EDStar (including advanced parallelized computer graphics libraries) and are meant to serve as start basis either for the upgrading of existing Monte Carlo codes, or for fast implementation of ad hoc codes when specific needs cannot be answered with standard concentrated solar codes (in particular as far as the new generation of solar receivers is concerned).}
}
@article{JIN201559,
title = {Significance and Challenges of Big Data Research},
journal = {Big Data Research},
volume = {2},
number = {2},
pages = {59-64},
year = {2015},
note = {Visions on Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000076},
author = {Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang},
keywords = {Big data, Data complexity, Computational complexity, System complexity},
abstract = {In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.}
}
@article{GAO2021107161,
title = {Optical waves/modes in a multicomponent inhomogeneous optical fiber via a three-coupled variable-coefficient nonlinear Schrödinger system},
journal = {Applied Mathematics Letters},
volume = {120},
pages = {107161},
year = {2021},
issn = {0893-9659},
doi = {https://doi.org/10.1016/j.aml.2021.107161},
url = {https://www.sciencedirect.com/science/article/pii/S089396592100080X},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Optical waves/modes, Multicomponent inhomogeneous optical fiber, Symbolic computation, Three-coupled variable-coefficient nonlinear Schrödinger system, Similarity reduction, Bäcklund transformation with analytic solutions},
abstract = {Recent progress in optical fibers is impressive, while nonlinear Schrödinger-type models are seen in fiber optics and other fields (such as ferromagnetism, plasma physics, Bose–Einstein condensation and oceanography). Hereby, our symbolic computation on a three-coupled variable-coefficient nonlinear Schrödinger system is performed, for the picosecond-pulse attenuation/amplification in a multicomponent inhomogeneous optical fiber with diverse polarizations/frequencies. For the slowly-varying envelopes of optical modes, we obtain a similarity reduction, an auto-Bäcklund transformation and some analytic solutions, which rely on the optical-fiber variable coefficients, i.e., the fiber loss/gain, nonlinearity and group velocity dispersion. Relevant variable-coefficient constraints are presented. Our results might be of some use in the construction of logic gates, optical computing, soliton switching, design of fiber directional couplers, quantum information processing, soliton amplification in the wavelength division multiplexing systems, solitonic studies in the all-optical devices and birefringence fiber systems.}
}
@article{NAKHAEI2022116422,
title = {A novel framework for technical performance evaluation of water distribution networks based on the water-energy nexus concept},
journal = {Energy Conversion and Management},
volume = {273},
pages = {116422},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116422},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422012006},
author = {Mahdi Nakhaei and Mehran Akrami and Mohammad Gheibi and Pedro {Daniel Urbina Coronado} and Mostafa Hajiaghaei-Keshteli and Jürgen Mahlknecht},
keywords = {Water distribution network, Water energy nexus, EPANET, Design of experiments, Machine learning},
abstract = {Today energy recovery using Micro-Hydropowers (MHPs) in Water Distribution Networks (WDN) is a well-known approach for recycling the wasted energy in infrastructures as a sample of circular economy. Likewise, in this study for the first time a framework for evaluation of WDN for energy harvesting have been designed with the application of statistical optimization, simulation, and artificial intelligence concepts. In this study, after modelling a WDN in Mashhad, Iran, with Environmental Protection Agency Network Evaluation Tool (EPANET) software, the potential of energy recovery using MHP technology was optimized with the application of Design of Experiment (DOE) methods, including Taguchi and Response Surface Methodology (RSM) and then the model prediction ability was improved by Artificial Neural Network (ANN) technique. Results of this investigation revealed that the combination of Taguchi and RSM methods could successfully optimize the energy recovery potential with consideration of improving the hydraulic parameters of WDN. With the application of RSM and Taguchi, high potential positions for MHP placement are detected and analyzed based on a high-performance operational decision-making methodology. According to Artificial Intelligence (AI) computations, energy harvesting and hydraulic responses can be estimated with more than a 99 % correlation coefficient. Also, it shows that the soft-operator can be executed to control the features of MHPs in WDNs. The outputs of this research demonstrated that MHP harvested energy is more than 400KW for the run time of this study with consideration of hydraulic parameters.}
}
@article{SURYANARAYANA2024100495,
title = {Artificial Intelligence Enhanced Digital Learning for the Sustainability of Education Management System},
journal = {The Journal of High Technology Management Research},
volume = {35},
number = {2},
pages = {100495},
year = {2024},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2024.100495},
url = {https://www.sciencedirect.com/science/article/pii/S104783102400004X},
author = {K.S. Suryanarayana and V.S. Prasad Kandi and G. Pavani and Akuthota Sankar Rao and Sandeep Rout and T. {Siva Rama Krishna}},
keywords = {Artificial intelligence, Digital education, Sustainability, Role of AI in education, Educational management},
abstract = {Maintenance schedules are scheduled ahead of time and automatically based on the continuous monitoring of the equipment by statistical methods, thanks to artificial intelligence-enabled digital transformation and the best fit model based on Machine Management Index in a pedagogical system. One of the most important aspects of universities is the widespread use of machine learning methods to evaluate students' progress. Machine learning approaches are designed to speed up the learning process without sacrificing accuracy. The dynamics of teaching and learning have shifted since the introduction of modern technological tools. The educational system as a whole has changed and developed over time. These days, people can get an education outside of the classroom as well, thanks to the proliferation of online courses and resources. Everyone's professional life begins with their education. By analyzing past data, artificial intelligence methods can resolve existing problems. When applied properly, artificial intelligence can be a highly efficient method for solving problems with a predictable and repeatable solution space. The learner's personality can be predicted based on a number of factors using machine learning approaches. This article examines how AI may improve digital learning in education management systems to sustain the education ecosystem. AI in education improves student results, learning experiences, and administrative processes. This study discusses AI applications in education management systems and associated problems and opportunities. We also explore ethical issues and the roadmap for using AI to improve education. Educational institutions can provide individualized curriculum for students based on their unique personalities and areas of interest. Institutions of higher learning can benefit greatly from this instrument for personality prediction by recommending a course of study that will better prepare students to enter the field of their choice and achieve professional success.}
}
@article{REMINGTON2018938,
title = {A Dynamical Systems Perspective on Flexible Motor Timing},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {10},
pages = {938-952},
year = {2018},
note = {Special Issue: Time in the Brain},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318301724},
author = {Evan D. Remington and Seth W. Egger and Devika Narain and Jing Wang and Mehrdad Jazayeri},
keywords = {dynamical systems, flexible timing, sensorimotor control, learning, movement sequences, movement planning},
abstract = {A hallmark of higher brain function is the ability to rapidly and flexibly adjust behavioral responses based on internal and external cues. Here, we examine the computational principles that allow decisions and actions to unfold flexibly in time. We adopt a dynamical systems perspective and outline how temporal flexibility in such a system can be achieved through manipulations of inputs and initial conditions. We then review evidence from experiments in nonhuman primates that support this interpretation. Finally, we explore the broader utility and limitations of the dynamical systems perspective as a general framework for addressing open questions related to the temporal control of movements, as well as in the domains of learning and sequence generation.}
}
@article{YOUNG1987309,
title = {The metaphor machine: A database method for creativity support},
journal = {Decision Support Systems},
volume = {3},
number = {4},
pages = {309-317},
year = {1987},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(87)90102-3},
url = {https://www.sciencedirect.com/science/article/pii/0167923687901023},
author = {Lawrence F Young},
keywords = {Creativity Support Systems, Methaphor Generation by Computer, Database Methods for Metaphor Generation, Idea Processing Support, Computer Support of Metaphorical Thinking, Support of Creative Thinking, Qualitative Support Systems, A Relational Calculus for Metaphor Generation, Relational Database Methods for Metaphor Generation, Interactive Support Systems for Creativity, Right-Brained Support Systems, Relational Algebra for Metaphor Generation, Database Structures for Metaphor Generation, Computer Support of Divergent Thinking},
abstract = {This paper shows how a data base method can be applied to the automatic generation of metaphors. The utility of automatic metaphor generation is based on providing interactive support to creative human thinking processes. Such interactive support systems have been called Idea Processing systems, and are seen as special qualitative types of Decision Support Systems (DSS). They include functions to support metaphorical thinking as well as other modes of creative idea development. The paper presents brief backgrounds references on creativity and the relevance of metaphors, as well as to previous work in Idea Processing. It then presents a relational data base method for automatic metaphor generation. The method is described and illustrated, as well as shown in relational algebra and relational calculus notation. In conclusion, the paper indicates how the relational data base method presented can be operationalized through using existing data base software or by integration with a specialized interface for the particular application of metaphor generation.}
}
@article{ROWBOTTOM2013161,
title = {Kuhn vs. Popper on criticism and dogmatism in science, part II: How to strike the balance},
journal = {Studies in History and Philosophy of Science Part A},
volume = {44},
number = {2},
pages = {161-168},
year = {2013},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2012.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0039368112001161},
author = {Darrell P. Rowbottom},
abstract = {This paper is a supplement to, and provides a proof of principle of, Kuhn vs. Popper on Criticism and Dogmatism in Science: A Resolution at the Group Level. It illustrates how calculations may be performed in order to determine how the balance between different functions in science—such as imaginative, critical, and dogmatic—should be struck, with respect to confirmation (or corroboration) functions and rules of scientific method.}
}