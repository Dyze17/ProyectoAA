@article{MENG2016114,
title = {Water quality permitting: From end-of-pipe to operational strategies},
journal = {Water Research},
volume = {101},
pages = {114-126},
year = {2016},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2016.05.078},
url = {https://www.sciencedirect.com/science/article/pii/S0043135416304043},
author = {Fanlin Meng and Guangtao Fu and David Butler},
keywords = {Integrated modelling, Multi-objective optimisation, Stakeholder engagement, Urban wastewater system, Water quality permitting},
abstract = {End-of-pipe permitting is a widely practised approach to control effluent discharges from wastewater treatment plants. However, the effectiveness of the traditional regulation paradigm is being challenged by increasingly complex environmental issues, ever growing public expectations on water quality and pressures to reduce operational costs and greenhouse gas emissions. To minimise overall environmental impacts from urban wastewater treatment, an operational strategy-based permitting approach is proposed and a four-step decision framework is established: 1) define performance indicators to represent stakeholders’ interests, 2) optimise operational strategies of urban wastewater systems in accordance to the indicators, 3) screen high performance solutions, and 4) derive permits of operational strategies of the wastewater treatment plant. Results from a case study show that operational cost, variability of wastewater treatment efficiency and environmental risk can be simultaneously reduced by at least 7%, 70% and 78% respectively using an optimal integrated operational strategy compared to the baseline scenario. However, trade-offs exist between the objectives thus highlighting the need of expansion of the prevailing wastewater management paradigm beyond the narrow focus on effluent water quality of wastewater treatment plants. Rather, systems thinking should be embraced by integrated control of all forms of urban wastewater discharges and coordinated regulation of environmental risk and treatment cost effectiveness. It is also demonstrated through the case study that permitting operational strategies could yield more environmentally protective solutions without entailing more cost than the conventional end-of-pipe permitting approach. The proposed four-step permitting framework builds on the latest computational techniques (e.g. integrated modelling, multi-objective optimisation, visual analytics) to efficiently optimise and interactively identify high performance solutions. It could facilitate transparent decision making on water quality management as stakeholders are involved in the entire process and their interests are explicitly evaluated using quantitative metrics and trade-offs considered in the decision making process. We conclude that the operational strategy-based permitting shows promising for regulators and water service providers alike.}
}
@article{SUN2019104141,
title = {Formal system interactive failure analysis method based on systems theoretic process analysis model},
journal = {Engineering Failure Analysis},
volume = {106},
pages = {104141},
year = {2019},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1350630718314973},
author = {Rui Sun and Deming Zhong and Weigang Li},
keywords = {Engine failures, Aircraft failures, Failure analysis},
abstract = {Interactive failures are failures caused by two or more components that often occur in complex systems when the system is modified, upgraded, or simply designed inadequately. However, the official guideline provided, namely, common cause analysis, cannot discover these problems. It cannot establish a complex interactive system model, nor can it provide a unified analysis method for all parts of the system. Another method, systems theoretic process analysis, is limited to the control system. To solve this problem, a method called system theoretic formal analysis method (STFAM) is proposed in this paper. STFAM establishes a system-component-interactive model that provides an abundance of interactive information for failure analysis and presents a unified model to support the analysis of multiple components in the system. It is divided into three steps. First, a hierarchical system structure is built and then transformed into a formalized state machine. Next, the interactive failures are determined and converted into a linear temporal logic or computation tree logic model. Finally, NuSMV is used to verify the model and record the results. To evaluate the proposed method, a practical problem that occurred in full-authority digital engine control, in which in some cases, the valve closes for unknown reasons until the system is reset is presented. An analysis of the issue demonstrates the effectiveness of our method.}
}
@article{VENKATESWARLU20201093,
title = {Modeling and fabrication of catalytic converter for emission reduction},
journal = {Materials Today: Proceedings},
volume = {33},
pages = {1093-1099},
year = {2020},
note = {International Conference on Future Generation Functional Materials and Research 2020},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.125},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320352159},
author = {K. Venkateswarlu and Revuri Ajay Kumar and Ram Krishna and M. Sreenivasan},
keywords = {Finite element analysis, Catalytic converter, CFD analysis, Thermal analysis, CO, HC, NOx},
abstract = {Automobiles throughout the world are the primary consumers of fossil fuels, which emit toxic gases when burnt; including HC, CO and NOx. Catalytic converters were developed to detoxify these gases into less harmful gases such as carbon dioxide and H2O.In this paper, the development of a catalytic converter for efficient emission reduction is presented. The results are presented after performing Computational Fluid Dynamics (CFD) on the proposed catalytic converter. In this catalytic converter Two Different materials used they are stainless steel wire mesh and ceramic stones at time and we are conducted tests with catalytic converter at different blended fluids and engine speeds such as methane, ethane and 1700,1900 RPM.3D modeling done by CATIA parametric software And analysis done in ANSYS software.}
}
@article{WANG2024104007,
title = {An efficient certificateless blockchain-enabled authentication scheme to secure producer mobility in named data networks},
journal = {Journal of Network and Computer Applications},
volume = {232},
pages = {104007},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.104007},
url = {https://www.sciencedirect.com/science/article/pii/S108480452400184X},
author = {Cong Wang and Tong Zhou and Maode Ma and Yuwen Xiong and Xiankun Zhang and Chao Liu},
keywords = {NDN, Producer, Mobile, Certificateless, Authentication, Blockchain},
abstract = {Named Data Networking (NDN) aims to establish an efficient content delivery architecture. In NDN, secure and effective identity authentication schemes ensure secure communication between producers and routers. Currently, there is no feasible solution to perform identity authentication of mobile producers in NDNs. Identity authentication schemes in other networks are either weak in security or performance, such as privacy leakage, difficulty to establish cross-domain trust, and long handover delays, and are not fully adaptable to the security requirements of NDNs. Additionally, the mobility of producers was not fully considered in the initial design of NDNs. This paper first revises the structure of packets and routers to support the identity authentication and mobility of producers. On this basis, this paper proposes a secure and efficient certificateless ECC-based producer identity authentication scheme (CL-BPA), which includes initial authentication and re-authentication, aimed at achieving rapid switch authentication and integrating blockchain to solve single-point failure issues. Using the Canetti and Krawczyk (CK) adversarial model and informal security analysis, the proposed CL-BPA scheme is demonstrated to be resistant to anonymity attacks, identity forgery attacks, and man-in-the-middle attacks. The performance analysis demonstrates that the proposed CL-BPA scheme exhibits excellent capabilities in terms of computation delay, communication cost, smart contract execution time, average response delay, and throughput.}
}
@article{FIROMSAWAKUMA2024,
title = {Advanced tuberculosis diagnosis system: Integrating case-based reasoning with nearest neighbor algorithm},
journal = {Indian Journal of Tuberculosis},
year = {2024},
issn = {0019-5707},
doi = {https://doi.org/10.1016/j.ijtb.2024.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0019570724002464},
author = {Amelework {Firomsa Wakuma}},
keywords = {Tuberculosis, Expert system, Case based reasoning, Nearest Neighbor Algorithm},
abstract = {Background
A serious infectious illness with a high morbidity and death rate worldwide, tuberculosis (TB) is more prevalent in low- and middle-income nations. Although there are a number of diagnostic techniques, the most only address tuberculosis in the lung and ignore drug-resistant strains (MDR-TB, XDR-TB) as well as tuberculosis lymphadenitis. A thorough diagnostic system that covers all types of tuberculosis is essential.
Objectives
To enhance TB diagnosis, particularly pulmonary TB, lymphadenitis, and drug-resistant TB, this study offers an expert system based on Case-Based Reasoning (CBR) and the Nearest Neighbor Algorithm.
Methods
Information was gathered from hospital records of prior tuberculosis cases, including 43 cases from Debre Tabor General Hospital. In addition to document analysis, information was acquired through both structured and unstructured interviews with medical specialists. The R4 model—Retrieve, Reuse, Revise, and Retain—is followed by the system architecture. Recall, expert acceptance, and precision were among the evaluation metrics.
Results
The system had an 86.5% expert acceptance rate, 84.7% precision, and 75.3% recall. Compared to previous medical diagnostic methods, it shown a notable improvement, especially in diagnosing mental health and hypertension.
Conclusion
By combining Case-Based Reasoning and the Nearest Neighbor Algorithm, it is possible to diagnose tuberculosis (TB) more effectively and with greater accuracy. This integration also makes it possible to diagnose cases that are resistant to drugs. In order to improve the system's performance even more, future research may investigate the integration of additional reasoning strategies.}
}
@incollection{HASIJA2023247,
title = {Chapter 11 - Bioinformatics workflow management systems},
editor = {Yasha Hasija},
booktitle = {All About Bioinformatics},
publisher = {Academic Press},
pages = {247-265},
year = {2023},
isbn = {978-0-443-15250-4},
doi = {https://doi.org/10.1016/B978-0-443-15250-4.00006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315250400006X},
author = {Yasha Hasija},
keywords = {Galaxy, GenePattern, Image analysis, KNIME, LINCS tools, NextFlow},
abstract = {In the discipline of bioinformatics, a flow of work, or a sequence of computational or analytical tasks, is managed by a bioinformatics workflow management system, which is a subtype of a workflow automation system. This type of system is used to construct and manage the flow of work. There are numerous different work process situations available at this time. Some of them have been developed with the intention that scholars in a variety of subjects, such as cosmology and geology, will be able to make use of them as frameworks for logical work processes. Workflow frameworks such as Galaxy, GenePattern, KNIME, LINCS Tools, image analysis, and NextFlow are discussed in this chapter.}
}
@article{PYKA2024107668,
title = {Unlocking the potential of higher-molecular-weight 5-HT7R ligands: Synthesis, affinity, and ADMET examination},
journal = {Bioorganic Chemistry},
volume = {151},
pages = {107668},
year = {2024},
issn = {0045-2068},
doi = {https://doi.org/10.1016/j.bioorg.2024.107668},
url = {https://www.sciencedirect.com/science/article/pii/S004520682400573X},
author = {Patryk Pyka and Sabrina Garbo and Aleksandra Murzyn and Grzegorz Satała and Artur Janusz and Michał Górka and Wojciech Pietruś and Filip Mituła and Delfina Popiel and Maciej Wieczorek and Biagio Palmisano and Alessia Raucci and Andrzej J. Bojarski and Clemens Zwergel and Ewa Szymańska and Katarzyna Kucwaj-Brysz and Cecilia Battistelli and Jadwiga Handzlik and Sabina Podlewska},
keywords = {Serotonin receptor 5-HT, G protein-coupled receptors, ADMET properties, Docking, Molecular modelling,  experiments, MTS assay, Gene expression assay},
abstract = {An increasing number of drugs introduced to the market and numerous repositories of compounds with confirmed activity have posed the need to revalidate the state-of-the-art rules that determine the ranges of properties the compounds should possess to become future drugs. In this study, we designed a series of two chemotypes of aryl-piperazine hydantoin ligands of 5-HT7R, an attractive target in search for innovative CNS drugs, with higher molecular weight (close to or over 500). Consequently, 14 new compounds were synthesised and screened for their receptor activity accompanied by extensive docking studies to evaluate the observed structure–activity/properties relationships. The ADMET characterisation in terms of the biological membrane permeability, metabolic stability, hepatotoxicity, cardiotoxicity, and protein plasma binding of the obtained compounds was carried out in vitro. The outcome of these studies constituted the basis for the comprehensive challenge of computational tools for ADMET properties prediction. All the compounds possessed high affinity to the 5-HT7R (Ki below 250 nM for all analysed structures) with good selectivity over 5-HT6R and varying affinity towards 5-HT2AR, 5-HT1AR and D2R. For the best compounds of this study, the expression profile of genes associated with neurodegeneration, anti-oxidant response and anti-inflammatory function was determined, and the survival of the cells (SH-SY5Y as an in vitro model of Alzheimer’s disease) was evaluated. One 5-HT7R agent (32) was characterised by a very promising ADMET profile, i.e. good membrane permeability, low hepatotoxicity and cardiotoxicity, and high metabolic stability with the simultaneous high rate of plasma protein binding and high selectivity over other GPCRs considered, together with satisfying gene expression profile modulations and neural cell survival. Such encouraging properties make it a good candidate for further testing and optimisation as a potential agent in the treatment of CNS-related disorders.}
}
@article{DIAZBERRIOS2022100953,
title = {High school student understanding of exponential and logarithmic functions},
journal = {The Journal of Mathematical Behavior},
volume = {66},
pages = {100953},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100953},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000219},
author = {Tomás Díaz-Berrios and Rafael Martínez-Planell},
keywords = {APOS theory, Exponentiation, Logarithm, Rational exponents, Exponential and logarithmic functions},
abstract = {We use Action-Process-Object-Schema theory (APOS) to study high school student understanding of exponentiation and their construction of exponential and logarithmic functions. We extend didactic materials similar to those of Ferrari-Escolá et al. (2016) to include exponentials on the rational numbers and to help students construct logarithms as numbers. Qualitative data from the problem-solving activities of two groups of eight students each during a series of teaching episodes suggests that some students can use these materials successfully. The data analysis enabled us to give specific suggestions on how to help other students do some of the constructions needed to understand these functions. Research shows these constructions are difficult for students. The findings of our study led to contributing a new and detailed genetic decomposition that can be tested and improved in future research cycles.}
}
@article{DANCKWARDTLILLIESTROM2025100906,
title = {Travelling through time in a process drama on plastic pollution – temporality in teaching about the complexity of wicked problems},
journal = {Learning, Culture and Social Interaction},
volume = {52},
pages = {100906},
year = {2025},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2025.100906},
url = {https://www.sciencedirect.com/science/article/pii/S221065612500025X},
author = {Kerstin Danckwardt-Lillieström and Maria Andrée and Carl-Johan Rundgren},
keywords = {Historying, Futuring, Process drama, Wicked problems, Chemistry education, Upper secondary school},
abstract = {The understanding of sustainability issues and preparedness to take action towards a sustainable future involves abilities to navigate between past, present, and future. This paper explores how the use of imaginary transitions in time – in the form of historying, and futuring in process drama – may afford student understanding of the wicked problem of plastics. The study draws on a design-based research study on process drama in upper-secondary school chemistry teaching which was conducted in collaboration with two teachers. During the process drama, the students and teachers travel in time to explore the uses of plastic; the motives and needs for using plastic as well as the consequences of plastic use in the form of plastic pollution today and in the future. The collected data consist of video- and audio recordings, which were analysed through qualitative content analysis that discerned how the students connected the temporalities, and which dimensions of the plastic problem were made visible in the temporal movements in the process drama. Our findings indicate that the temporal transitions made visible several dimensions of the plastic issue, and contributed to adding layers of complexity to the issue of plastics.}
}
@incollection{GAINOTTI2025421,
title = {Chapter 27 - Emotion: An evolutionary model of lateralization in the human brain},
editor = {Costanza Papagno and Paul Corballis},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {208},
pages = {421-432},
year = {2025},
booktitle = {Cerebral Asymmetries},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-443-15646-5.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443156465000014},
author = {Guido Gainotti},
keywords = {Emotional lateralization, Adaptive systems, Animal asymmetries, Evolutionary perspective, Covert language, Consciousness levels, Automatic functioning},
abstract = {Since several reviews have recently discussed the lateralization of emotions, this chapter will take into account the possible evolutionary meaning of this lateralization. The organization of the chapter will be based on the following steps. I will first propose that emotions must be considered as a complex adaptive system, complementary to the more phylogenetically advanced cognitive system. Second, I will remind historical aspects and consolidated results on the lateralization of emotions. Then I will discuss the phylogenetic aspects of the problem, trying to evaluate if emotional asymmetries concern only humans and some nonhuman primates or are part of a continuum between humans and many phylogenetically distant animal species. After having reviewed various aspects of emotional lateralization across different animal species and (more specifically) in nonhuman primates, I will propose a general model of hemispheric asymmetries in the human brain, based on theoretical models and empiric data. Theoretical models stem from the influence that the presence or the absence of language can have on concomitant hemispheric functions, whereas supporting neuropsychologic data have been gathered in patients with unilateral brain damage.}
}
@incollection{KLOCKING202597,
title = {Geochemical databases},
editor = {Ariel Anbar and Dominique Weis},
booktitle = {Treatise on Geochemistry (Third edition)},
publisher = {Elsevier},
edition = {Third edition},
address = {Oxford},
pages = {97-135},
year = {2025},
isbn = {978-0-323-99763-8},
doi = {https://doi.org/10.1016/B978-0-323-99762-1.00123-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323997621001236},
author = {Marthe Klöcking and Kerstin A. Lehnert and Lesley Wyborn},
keywords = {Artificial intelligence, CARE, Community standards, Data ethics, Data management, Databases, FAIR, Geochemistry, Machine learning, Machine readable data, Repository, TRUST},
abstract = {Geochemistry is a data-driven discipline. Modern laboratories produce highly diverse data, and the recent exponential increase in data volumes is challenging established practices and capabilities for organizing, analyzing, preserving, and accessing these data. At the same time, sophisticated computational techniques, including machine learning, are increasingly applied to geochemical research questions, which require easy access to large volumes of high-quality, well-organized, and standardized data. Data management has been important since the beginning of geochemistry but has recently become a necessity for the discipline to thrive in the age of digitalization and artificial intelligence. This paper summarizes the landscape of geochemical databases, distinguishing different types of data systems based on their purpose, and their evolution in a historic context. We apply the life cycle model of geochemical data; explain the relevance of current standards, practices, and policies that determine the design of modern geochemical databases and data management; the ethics of data reuse such as data ownership, data attribution, and data citation; and finally create a vision for the future of geochemical databases: data being born digital, connected to agreed community standards, and contributing to global democratization of geochemical data.}
}
@article{RAJPUT2021104270,
title = {VLSI implementation of transcendental function hyperbolic tangent for deep neural network accelerators},
journal = {Microprocessors and Microsystems},
volume = {84},
pages = {104270},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104270},
url = {https://www.sciencedirect.com/science/article/pii/S014193312100435X},
author = {Gunjan Rajput and Gopal Raut and Mahesh Chandra and Santosh Kumar Vishvakarma},
keywords = {Activation function, Artificial neural network, Hyperbolic tangent (tanh), Digital implementation, Combinational logic},
abstract = {Extensive use of neural network applications prompted researchers to customize a design to speed up their computation based on ASIC implementation. The choice of activation function (AF) in a neural network is an essential requirement. Accurate design architecture of an AF in a digital network faces various challenges as these AF require more hardware resources because of its non-linear nature. This paper proposed an efficient approximation scheme for hyperbolic tangent (tanh) function which purely based on combinational design architecture. The approximation is based on mathematical analysis by considering maximum allowable error in a neural network. The results prove that the proposed combinational design of an AF is efficient in terms of area, power and delay with negligible accuracy loss on MNIST and CIFAR-10 benchmark datasets. Post synthesis results show that the proposed design area is reduced by 66% and delay is reduced by nearly 16% compared to state-of-the-art.}
}
@article{ARGYROUDIS2022100387,
title = {Digital technologies can enhance climate resilience of critical infrastructure},
journal = {Climate Risk Management},
volume = {35},
pages = {100387},
year = {2022},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100387},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321001169},
author = {Sotirios A. Argyroudis and Stergios Aristoteles Mitoulis and Eleni Chatzi and Jack W. Baker and Ioannis Brilakis and Konstantinos Gkoumas and Michalis Vousdoukas and William Hynes and Savina Carluccio and Oceane Keou and Dan M. Frangopol and Igor Linkov},
keywords = {Emerging digital technologies, Data-driven, Critical infrastructure, Climate change, Sustainable development goals (SDGs)},
abstract = {Delivering infrastructure, resilient to multiple natural hazards and climate change, is fundamental to continued economic prosperity and social coherence. This is a strategic priority of the United Nations Sustainable Development Goals (SDGs), the World Bank, the Organisation for Economic Co-operation and Development (OECD), public policies and global initiatives. The operability and functionality of critical infrastructure are continuously challenged by multiple stressors, increasing demands and ageing, whilst their interconnectedness and dependencies pose additional challenges. Emerging and disruptive digital technologies have the potential to enhance climate resilience of critical infrastructure, by providing rapid and accurate assessment of asset condition and support decision-making and adaptation. In this pursuit, it is imperative to adopt multidisciplinary roadmaps and deploy computational, communication and other digital technologies, tools and monitoring systems. Nevertheless, the potential of these emerging technologies remains largely unexploited, as there is a lack of consensus, integrated approaches and legislation in support of their use. In this perspective paper, we discuss the main challenges and enablers of climate-resilient infrastructure and we identify how available roadmaps, tools and emerging digital technologies, e.g. Internet of Things, digital twins, point clouds, Artificial Intelligence, Building Information Modelling, can be placed at the service of a safer world. We show how digital technologies will lead to infrastructure of enhanced resilience, by delivering efficient and reliable decision-making, in a proactive and/or reactive manner, prior, during and after hazard occurrences. In this respect, we discuss how emerging technologies significantly reduce the uncertainties in all phases of infrastructure resilience evaluations. Thus, building climate-resilient infrastructure, aided by digital technologies, will underpin critical activities globally, contribute to Net Zero target and hence safeguard our societies and economies. To achieve this we set an agenda, which is aligned with the relevant SDGs and highlights the urgent need to deliver holistic and inclusive standards and legislation, supported by coordinated alliances, to fully utilise emerging digital technologies.}
}
@article{MCQUEEN2021120575,
title = {Do we really understand how drug eluted from stents modulates arterial healing?},
journal = {International Journal of Pharmaceutics},
volume = {601},
pages = {120575},
year = {2021},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2021.120575},
url = {https://www.sciencedirect.com/science/article/pii/S037851732100380X},
author = {Alistair McQueen and Javier Escuer and Ankush Aggarwal and Simon Kennedy and Christopher McCormick and Keith Oldroyd and Sean McGinty},
keywords = {Pharmacodynamics, Ligand-receptor interactions, Drug-eluting stents, Smooth Muscle Cells, Cell proliferation, Mathematical Modelling},
abstract = {The advent of drug-eluting stents (DES) has revolutionised the treatment of coronary artery disease. These devices, coated with anti-proliferative drugs, are deployed into stenosed or occluded vessels, compressing the plaque to restore natural blood flow, whilst simultaneously combating the evolution of restenotic tissue. Since the development of the first stent, extensive research has investigated how further advancements in stent technology can improve patient outcome. Mathematical and computational modelling has featured heavily, with models focussing on structural mechanics, computational fluid dynamics, drug elution kinetics and subsequent binding within the arterial wall; often considered separately. Smooth Muscle Cell (SMC) proliferation and neointimal growth are key features of the healing process following stent deployment. However, models which depict the action of drug on these processes are lacking. In this article, we start by reviewing current models of cell growth, which predominantly emanate from cancer research, and available published data on SMC proliferation, before presenting a series of mathematical models of varying complexity to detail the action of drug on SMC growth in vitro. Our results highlight that, at least for Sodium Salicylate and Paclitaxel, the current state-of-the-art nonlinear saturable binding model is incapable of capturing the proliferative response of SMCs across a range of drug doses and exposure times. Our findings potentially have important implications on the interpretation of current computational models and their future use to optimise and control drug release from DES and drug-coated balloons.}
}
@article{MADORE2022707,
title = {Readiness to remember: predicting variability in episodic memory},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {8},
pages = {707-723},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001127},
author = {Kevin P. Madore and Anthony D. Wagner},
keywords = {episodic retrieval, attention lapsing, goal processing, arousal, locus coeruleus, posterior alpha},
abstract = {Learning and remembering are fundamental to our lives, so what causes us to forget? Answers often highlight preparatory processes that precede learning, as well as mnemonic processes during the act of encoding or retrieval. Importantly, evidence now indicates that preparatory processes that precede retrieval attempts also have powerful influences on memory success or failure. Here, we review recent work from neuroimaging, electroencephalography, pupillometry, and behavioral science to propose an integrative framework of retrieval-period dynamics that explains variance in remembering in the moment and across individuals as a function of interactions among preparatory attention, goal coding, and mnemonic processes. Extending this approach, we consider how a ‘readiness to remember’ (R2R) framework explains variance in high-level functions of memory and mnemonic disruptions in aging.}
}
@article{CHOI2025,
title = {Association Between Shift Working and Brain Morphometric Changes in Workers: A Voxel-wise Comparison},
journal = {Safety and Health at Work},
year = {2025},
issn = {2093-7911},
doi = {https://doi.org/10.1016/j.shaw.2025.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2093791125000113},
author = {Joon Yul Choi and Sungmin Kim and Yongho Lee and Dohyeon Kim and Wanhyung Lee},
keywords = {Brain MRI, Neuroplasticity, Shift work, Voxel-wise comparison},
abstract = {Objective
There is abundant evidence from observational studies linking various health problems to shift work, but there is a lack of brain-based neurological evidence. Therefore, we examined morphometric changes on brain magnetic resonance imaging (MRI) between shift and non-shift workers.
Methods
A total 111 healthy workers participated in this study and underwent brain MRI, with the analysis incorporating merged workers' health surveillance data from regional hospital workers. Voxel-based morphometry analysis was used to investigate regional changes in the gray matter volume. To investigate the association of structural changes between shift workers and non-shift workers, a general linear model and threshold-free cluster enhancement were used with covariates, including total intracranial volume, age, and sex.
Results
After family-wise error correction, non-shift workers exhibited a significantly larger cerebellar region (p < 0.05) than shift workers. Conversely, the inferior parietal gyrus was found to be significantly larger in shift workers than in non-shift workers with family-wise error correction.
Conclusions
We observed increased clusters in the brains of both shift and non-shift workers, suggesting that the acquired occupational environment, including the shift work schedule, could influence brain neuroplasticity, which is an important consideration for occupational health.}
}
@article{VELICHKOVSKY201735,
title = {Consciousness and working memory: Current trends and research perspectives},
journal = {Consciousness and Cognition},
volume = {55},
pages = {35-45},
year = {2017},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1053810017301654},
author = {Boris B. Velichkovsky},
keywords = {Consciousness, Working memory, Visual masking, Attentional blink, Implicit working memory},
abstract = {Working memory has long been thought to be closely related to consciousness. However, recent empirical studies show that unconscious content may be maintained within working memory and that complex cognitive computations may be performed on-line. This promotes research on the exact relationships between consciousness and working memory. Current evidence for working memory being a conscious as well as an unconscious process is reviewed. Consciousness is shown to be considered a subset of working memory by major current theories of working memory. Evidence for unconscious elements in working memory is shown to come from visual masking and attentional blink paradigms, and from the studies of implicit working memory. It is concluded that more research is needed to explicate the relationship between consciousness and working memory. Future research directions regarding the relationship between consciousness and working memory are discussed.}
}
@article{COONEY1992237,
title = {The influence of verbal protocol methods on children's mental computation},
journal = {Learning and Individual Differences},
volume = {4},
number = {3},
pages = {237-257},
year = {1992},
issn = {1041-6080},
doi = {https://doi.org/10.1016/1041-6080(92)90004-X},
url = {https://www.sciencedirect.com/science/article/pii/104160809290004X},
author = {John B. Cooney and Stephen F. Ladd},
abstract = {The purpose of this study was to investigate the validity of children's verbal reports about the cognitive processes underlying their mental arithmetic. A within-subject comparison was made with respect to the data that could be obtained with retrospective verbal report, concurrent verbal report, and no verbal report conditions. The results of the investigation indicated that children's verbal reports of strategy use may not be veridical. The source of the nonveridicality was incompleteness rather than fabrication. It was also found that immediately retrospective and concurrent verbal reports increased students' solution accuracy relative to a no verbal report condition. Thus, the primary mental operations underlying children's mental arithmetic are reactive to giving verbal reports. It was concluded that empirical checks for reactivity and refinements to protocol procedures to reveal the progression of strategy use are needed in future research.}
}
@incollection{GILLAM199523,
title = {Chapter 2 - The Perception of Spatial Layout from Static Optical Information},
editor = {William Epstein and Sheena Rogers},
booktitle = {Perception of Space and Motion},
publisher = {Academic Press},
address = {San Diego},
pages = {23-67},
year = {1995},
series = {Handbook of Perception and Cognition},
isbn = {978-0-12-240530-3},
doi = {https://doi.org/10.1016/B978-012240530-3/50004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780122405303500043},
author = {Barbara Gillam},
abstract = {Publisher Summary
This chapter reviews the literature on absolute distance, relative distance, surface slant and curvature, and the perception of size and shape within the context of several broad issues that have influenced thinking and experimentation to varying degrees in recent years. One issue that has driven recent research is the way stimulus input is described that carries implicit assumptions about how it is encoded and represented. Euclidian and other conventional frameworks may be restricting and misleading as a basis for visual theory. Another issue raised by computational approaches is the relationship between the processing of different sources of information or cues underlying the perception of spatial layout. Machine vision has tended to treat these cues as separate modules or processing systems, a view that has also received support from psychophysics. Comparison of some seemingly separate processes, specifically perspective and stereopsis, may indicate common mechanisms.}
}
@article{KOH2020106,
title = {Automated detection of Alzheimer's disease using bi-directional empirical model decomposition},
journal = {Pattern Recognition Letters},
volume = {135},
pages = {106-113},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2020.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865520300921},
author = {Joel En Wei Koh and Vicnesh Jahmunah and The-Hanh Pham and Shu Lih Oh and Edward J Ciaccio and U Rajendra Acharya and Chai Hong Yeong and Mohd Kamil Mohd Fabell and Kartini Rahmat and Anushya Vijayananthan and Norlisah Ramli},
abstract = {The build-up of beta-amyloid and rapid spread of tau proteins in the brain cause the death of neurons, leading to Alzheimer's disease (AD). AD is a form of dementia, and the symptoms include memory loss and decision-making difficulties. Current advanced diagnostic modalities are costly or unable to detect the histopathological features of AD. Hence a computational intelligence tool (CIT) for AD diagnosis is proposed in this study. The magnetic resonance images (MRI) of the brain are pre-processed using an adaptive histogram, and decomposed into four IMFS using bidirectional empirical mode decomposition (BEMD). Local binary patterns (LBP) are then computed per IMF, and the histograms are concatenated. Adaptive synthetic sampling (ADASYN) is applied to balance the dataset and Student's t-test is utilized for selection of highly significant features, within each fold for ten-fold validation. Amongst other classifiers, SVM-Poly 1 and random forest(RF) were employed for classification, yielding the highest accuracy of 93.9% each. Our study concludes that the recommended CIT is useful for the automatic classification of AD versus normal MRI imagery in hospitals.}
}
@article{YURKOVICH2018130,
title = {Quantitative -omic data empowers bottom-up systems biology},
journal = {Current Opinion in Biotechnology},
volume = {51},
pages = {130-136},
year = {2018},
note = {Systems biology • Nanobiotechnology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0958166917302276},
author = {James T Yurkovich and Bernhard O Palsson},
abstract = {The large-scale generation of ‘-omic’ data holds the potential to increase and deepen our understanding of biological phenomena, but the ability to synthesize information and extract knowledge from these data sets still represents a significant challenge. Bottom-up systems biology overcomes this hurdle through the integration of disparate -omic data types, and absolutely quantified experimental measurements allow for direct integration into quantitative, mechanistic models. The human red blood cell has served as a starting point for the application of systems biology approaches and has been the focus of a recent burst of generated quantitative metabolomics and proteomics data. Thus, the red blood cell represents the perfect case study through which to examine our ability to glean knowledge from the integration of multiple disparate data types.}
}
@article{CASTRO2023105510,
title = {An experimental and simulation study of the impact of emotional information on analogical reasoning},
journal = {Cognition},
volume = {238},
pages = {105510},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105510},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001440},
author = {Ariana A. Castro and John E. Hummel and Howard Berenbaum},
keywords = {Reasoning, Emotion, Computational models, Attention, Analogies},
abstract = {We investigated whether and how emotional information would affect analogical reasoning. We hypothesized that task-irrelevant emotional information would impair performance whereas task-relevant emotional information would enhance it. In Study 1, 233 undergraduates completed a novel version of the People Pieces Task (Emotional Faces People Task), an analogical reasoning task in which the task characters displayed emotional or neutral facial expressions (within-participants). The emotional faces were relevant or irrelevant to the task (between-participants). We simulated the behavioral results using the Learning and Inference with Schemas and Analogies (LISA) model of relational reasoning. LISA is a neurally plausible, symbolic-connectionist computational model of analogical reasoning. In comparison to neutral trials, participants were slower but more accurate on emotion-relevant trials, and were faster but less accurate on emotion-irrelevant trials. Simulations using the LISA model demonstrated that it is possible to account for the effects of emotional information on reasoning in terms of how emotional stimuli attract attention during a reasoning task. In Study 2, 255 undergraduates completed the Emotional Faces People Task at either a high- or low-working memory load. The high working memory load condition of Study 2 replicated the findings of Study 1, showing that participants were more accurate on emotion-relevant trials than on emotion-irrelevant trials; in Study 2, this increased accuracy could not be accounted for by a speed-accuracy tradeoff. The working memory manipulation influenced the manner in which the congruence (with the correct answer) of emotion-irrelevant emotion influenced performance. Simulations using the LISA model showed that manipulating the salience of emotion, the error penalty, as well as vigilance (which determines the likelihood that LISA will notice it has attended to an irrelevant relation), could reasonably reproduce the behavioral results of both low and high working memory load conditions of Study 2.}
}
@article{JOOKEN202336,
title = {Features for the 0-1 knapsack problem based on inclusionwise maximal solutions},
journal = {European Journal of Operational Research},
volume = {311},
number = {1},
pages = {36-55},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221723003065},
author = {Jorik Jooken and Pieter Leyman and Patrick {De Causmaecker}},
keywords = {Combinatorial optimization, 0-1 knapsack problem, Packing, Problem instance hardness, Instance space analysis},
abstract = {Decades of research on the 0-1 knapsack problem led to very efficient algorithms that are able to quickly solve large problem instances to optimality. This prompted researchers to also investigate the structure of problem instances that are hard for existing solvers. In the current paper we are interested in investigating which features make 0-1 knapsack problem instances hard to solve to optimality for the state-of-the-art 0-1 knapsack solver. We propose a set of 14 features based on previous work by the authors in which so-called inclusionwise maximal solutions (IMSs) play a central role. Calculating these features is computationally expensive and requires one to solve hard combinatorial problems. Based on new structural results about IMSs, we formulate polynomial and pseudopolynomial time algorithms for calculating these features. These algorithms were executed for two large datasets on a supercomputer in approximately 540 CPU-hours. We show that the proposed features contain important information related to the empirical hardness of a problem instance that was missing in earlier features from the literature by training machine learning models that can accurately predict the empirical hardness of a wide variety of 0-1 knapsack problem instances. Moreover, we show that these features can be cheaply approximated at the cost of less accurate hardness predictions. Using the instance space analysis methodology, we show that hard 0-1 knapsack problem instances are clustered together around a relatively dense region of the instance space and several features behave differently in the easy and hard parts of the instance space.}
}
@article{BROCAS2022331,
title = {Adverse selection and contingent reasoning in preadolescents and teenagers},
journal = {Games and Economic Behavior},
volume = {133},
pages = {331-351},
year = {2022},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2022.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0899825622000616},
author = {Isabelle Brocas and Juan D. Carrillo},
keywords = {Developmental decision-making, Lab-in-the-field experiment, Contingent reasoning, Winner's curse},
abstract = {We study from a developmental viewpoint the ability to perform contingent reasoning and the cognitive abilities that facilitate optimal behavior. Individuals from 11 to 17 years old participate in a simplified version of the two-value, deterministic “acquire-a-company” adverse selection game (Charness and Levin, 2009; Martínez-Marquina et al., 2019). We find that even our youngest subjects understand well the basic principles of contingent reasoning (offer the reservation price of one of the sellers), although they do not necessarily choose the optimal price. Performance improves steadily and significantly over the developmental window but it is not facilitated by repeated exposure or feedback. High cognitive ability–measured by a high performance in a working memory task–is necessary to behave optimally in the simplest settings but it is not sufficient to solve the most complex situations.}
}
@article{CHEN199799,
title = {Towards designing sustainable urban wastewater infrastructures: A screening analysis},
journal = {Water Science and Technology},
volume = {35},
number = {9},
pages = {99-112},
year = {1997},
note = {Sustainable Sanitation},
issn = {0273-1223},
doi = {https://doi.org/10.1016/S0273-1223(97)00188-1},
url = {https://www.sciencedirect.com/science/article/pii/S0273122397001881},
author = {J. Chen and M.B. Beck},
keywords = {Sustainability, wastewater treatment technologies, screening analysis, uncertainty, urban drainage system},
abstract = {Whether sustainability can, or should, be defined in a practical operational sense, it is clear that the emergence of such a notion has prompted what seems to be a profound re-thinking of whether our society, economic system, and technology are as we would wish them to be. Sustainable development, clean technology, life-cycle analysis, pollution prevention, and so on, are expressions of a willingness to leave no stone unturned, as it were, in the search for what would be appropriate. With respect to the design and operation of a city's wastewater infrastructure, in particular, this search is characterised by a seeming explosion in the possible combinations of appropriate technologies, gross uncertainty about how novel technologies - only now emerging - might perform in the very long term, and a continuing absence of specific criteria of sustainability for determining the grounds on which any candidate technology might be preferred over another. The paper introduces a simple computational procedure for generating and screening candidate combinations of unit-process technologies for an urban wastewater infrastructure. This is based on the use of Monte Carlo simulation, with the identification of those specific technologies (and combinations thereof) that appear to have the greatest probability of being selected for use under different, possibly evolving, criteria of sustainability. Application of the procedure is illustrated with respect to just a part of this infrastructure, i.e., the wastewater treatment plant.}
}
@article{BIRD2004337,
title = {Kuhn, naturalism, and the positivist legacy},
journal = {Studies in History and Philosophy of Science Part A},
volume = {35},
number = {2},
pages = {337-356},
year = {2004},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2004.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368104000184},
author = {Alexander Bird},
keywords = {Kuhn, Naturalism, Positivism, Preston},
abstract = {I defend against criticism the following claims concerning Thomas Kuhn: (i) there is a strong naturalist streak in The structure of scientific revolutions, whereby Kuhn used the results of a posteriori enquiry in addressing philosophical questions; (ii) as Kuhn’s career as a philosopher of science developed he tended to drop the naturalistic elements and to replace them with more traditionally philosophical, a priori approaches; (iii) at the same time there is a significant residue of positivist thought in Kuhn, which Kuhn did not recognise as such; (iv) the naturalistic elements referred to in (i) are the most original and fruitful elements of Kuhn’s thinking; (v) the positivistic elements referred to in (iii) vitiated his thought and acted as factors in preventing Kuhn from developing the naturalistic elements and from following the path taken by much subsequent philosophy of science. Preston presents an alternative reading of Kuhn which emphasizes the Wittgensteinian elements in Kuhn. I argue that this alternative view is, descriptively, poorly supported by the textual evidence and the facts of the history of philosophy of science in the twentieth century. I provide some defence of the naturalistic approach and related themes.}
}
@article{OLIVEIRA202575,
title = {Quantitative and qualitative analysis in urban morphology: systematic legacy and latest developments},
journal = {Proceedings of the Institution of Civil Engineers - Urban Design and Planning},
volume = {178},
number = {2},
pages = {75-87},
year = {2025},
issn = {1755-0793},
doi = {https://doi.org/10.1680/jurdp.24.00047},
url = {https://www.sciencedirect.com/science/article/pii/S1755079325000058},
author = {Vitor Oliveira and Sergio Porta},
keywords = {built environment, quantitative versus qualitative, town & city planning, urban form, urban morphology},
abstract = {Urban morphology studies the physical forms of human settlements and how these change over time by the action of different processes and agents. The field of knowledge has developed several theories, concepts, and methods to describe and explain the phenomena at hands. As in many fields, urban morphology contains a few misconceptions. One of these is the idea that quantitative analysis is a feature of the present and the future, and qualitative analysis of the past. The paper addresses this fallacy. Our discussion of the main schools of thought in urban morphology and their influential researchers suggests that quantitative approaches are well rooted in it since at least the mid-twentieth century and that the dominance of quantitative or qualitative tools is subject to cycles, as it happens in other sciences. Demonstration of both statements leads to a focus on a line of approaches, historico-geographical, configurational, and lately morphometrics, which share a common interest in cross-cases regularities, hence practices of pattern recognition.}
}
@article{WOODWARD2006631,
title = {Does prior mathematics knowledge really lead to variation in elementary statistics performance? Evidence from a developing country},
journal = {International Journal of Educational Development},
volume = {26},
number = {6},
pages = {631-639},
year = {2006},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2006.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0738059306000071},
author = {George Woodward and Don Galagedera},
keywords = {Curriculum, Mathematics, Educational policy, Elementary statistics},
abstract = {A model incorporating prerequisite mathematics performance and other variables deemed to be associated with learning elementary statistics (ES) is developed. The relationship between ES performance and the explanatory variables is well represented by the logistics form. Aptitude, effort and motivation are the only significant explanatory variables of ES performance. Since prerequisite mathematics is not significant, statistical thinking at the tertiary level may be mostly intuitive and non-mathematical. Students with low aptitude experience increasing returns to effort over the first half of the feasible effort interval, while high-aptitude students experience diminishing returns at all levels of effort. The levels of effort required to achieve a minimum pass are interpreted.}
}
@article{ORUN2025105102,
title = {Cognitive behavioural characteristics identification for remote user authentication for cybersecurity},
journal = {Journal of Parallel and Distributed Computing},
volume = {202},
pages = {105102},
year = {2025},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2025.105102},
url = {https://www.sciencedirect.com/science/article/pii/S0743731525000693},
author = {Ahmet Orun and Emre Orun and Fatih Kurugollu},
keywords = {Cyber security, Cognitive psychology, Artificial intelligence, Bayesian Networks, Internet},
abstract = {Nowadays cyber-attacks keep threatening global networks and information infrastructures. Day-by-day, the threat is gradually getting more destructive and harder to counter, as the global networks continue to enlarge exponentially with limited security counter-measures. This occurrence urgently demands more sophisticated methods and techniques, such as multi-factor authentication and soft biometrics to respond to evolving threats. This paper is concerned with behavioural soft biometrics and proposes a multidisciplinary remote cognitive observation technique to meet today’s cybersecurity needs. The proposed method introduces a non-traditional “cognitive psychology” and “artificial intelligence” based approach. According to contemporary cognitive psychology research, human cognitive processes can be affected by many different personal factors and emotional states which are specific to an individual. Those factors mainly include personal perception, memory, decision-making, reasoning, learning, etc. In this study we focus on visual (graphical) perception with the support of graphical stimuli environments and investigate how such personal cognitive factors can be exploited within the cybersecurity area for remote user authentication. This technique enables remote access to the cognitive behavioural parameters of an intruder/hacker without any physical contact via online connection, disregarding the distance of the threat. The results show that cognitive stimuli provide crucial information for a behavioural user authentication system to classify the user as “authentic” or “intruder”. The ultimate goal of this work is to develop a supplementary cognitive cyber security tool for “next generation” secure online banking, finance or trade systems.}
}
@article{SOHAIL201947,
title = {A videographic assessment of ferrofluid during magnetic drug targeting: An application of artificial intelligence in nanomedicine},
journal = {Journal of Molecular Liquids},
volume = {285},
pages = {47-57},
year = {2019},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2019.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167732219315399},
author = {Ayesha Sohail and Maryam Fatima and Rahamt Ellahi and Khush Bakhat Akram},
keywords = {Ferrofluids, Drug targeting, Artificial intelligence, Videographic footage},
abstract = {Forecasting the thresholds via the computational analysis of magnetic drug targeting, is a useful approach since it can help to design the nanoscale experiments to get the best results and efficiency. In such investigations, an artificial intelligence when interlinked with the computational techniques provide better insight specially for rheological problems. In the proposed model mathematical framework for the magnetic drug targeting is adopted while the flow of the ferrofluid, with different concentrations is taken into account. The flow without any obstruction is compared with the flow having obstruction. The nanoscale dynamics sensitive to such obstructions are documented by videographic footage. Nanaoscale approach and the response of the nanomedicine relative to external agents are used. The pressure gradient, the magnetic susceptibility and the velocity profile of the ferrofluid provides useful thresholds to identify the geometry of the obstacle, and to forecast the resulting dynamics.}
}
@incollection{SCHAUB2022555,
title = {Chapter 22 - Conclusions},
editor = {Michael Schaub and Marc Kéry},
booktitle = {Integrated Population Models},
publisher = {Academic Press},
pages = {555-563},
year = {2022},
isbn = {978-0-323-90810-8},
doi = {https://doi.org/10.1016/B978-0-12-820564-8.24002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205648240025},
author = {Michael Schaub and Marc Kéry},
keywords = {Continuous time scale, Full annual cycle integrated population model, Future developments, Individual heterogeneity, Long-term ecological research, Multispecies integrated population model, Sampling design, Spatial integrated population model, Spatial scale},
abstract = {In this final chapter, we first look back and briefly summarize what we have learned in this book. We then look forward and sketch out possible avenues of future research into integrated population models (IPMs) and where it may or should go. We especially foresee likely future developments in three areas. The first is in developing alternative formulations of the population, or process, model in an IPM, which currently is mostly a classical matrix population model. In the future, we expect to see refinements along some or all of the spatial, temporal, and individual axes of fundamental demographic information—that is, a general shift away from discrete to more continuous scales along these dimensions of the description of population dynamics. In particular, we think a more widespread “spatialization” of IPMs is imminent. We also think that IPMs for two or more species with explicit links among them will increasingly be developed because they allow the study of interactions among species at a very basic mechanistic level. The second area of likely future progress in IPMs deals with the observation model, especially the Gaussian error model in the state-space model for population counts. In a sense, this model is a misspecification that cannot explicitly account for the false-positives and false-negatives that now are so commonly included in the capture-recapture class of models. The third area where we envision future progress in IPMs is with more fundamental statistical and computational work. We expect further progress in algorithm fitting, goodness-of-fit testing, models that account for dependence among the components of joint likelihood, and the study and development of more effective sampling designs. Finally, we are excited to see many more applications of existing and future IPMs to improve our scientific conclusions and conservation and wildlife management decisions.}
}
@article{ALCANTUD2022118276,
title = {Ranked hesitant fuzzy sets for multi-criteria multi-agent decisions},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422014142},
author = {José Carlos R. Alcantud},
keywords = {Hesitant fuzzy set, Aggregation operator, Score, Ranking, Decision making},
abstract = {This paper introduces and investigates ranked hesitant fuzzy sets, a novel extension of hesitant fuzzy sets that is less demanding than both probabilistic and proportional hesitant fuzzy sets. This new extension incorporates hierarchical knowledge about the various evaluations submitted for each alternative. These evaluations are ranked (for example by their plausibility, acceptability, or credibility), but their position does not necessarily derive from supplementary numerical information (as in probabilistic and proportional hesitant fuzzy sets). In particular, strictly ranked hesitant fuzzy sets arise when no ties exist, i.e., when for any fixed alternative, each submitted evaluation is either strictly more plausible or strictly less plausible than any other submitted evaluation. A detailed comparison with similar models from the literature is performed. Then in order to produce a natural strategy for multi-criteria multi-agent decisions with ranked hesitant fuzzy sets, canonical representations, scores and aggregation operators are designed in the framework of ranked hesitant fuzzy sets. In order to help implementation of this model, Mathematica code is provided for the computation of both scores and aggregators. The decision-making technique that is prescribed is tested with a comparative analysis with four methodologies based on probabilistic hesitant fuzzy information. A conclusion of this numerical exercise is that this methodology is reliable, applicable and robust. All these evidences show that ranked hesitant fuzzy sets are an intuitive extension of the hesitant fuzzy set model designed by V. Torra, that can be implemented in practice with the aid of computationally assisted algorithms.}
}
@article{OITAVEM2011661,
title = {A recursion-theoretic approach to NP},
journal = {Annals of Pure and Applied Logic},
volume = {162},
number = {8},
pages = {661-666},
year = {2011},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2011.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S016800721100011X},
author = {I. Oitavem},
keywords = {Computational complexity, Implicit characterization, Recursion schemes, NP},
abstract = {An implicit characterization of the class NP is given, without using any minimization scheme. This is the first purely recursion-theoretic formulation of NP.}
}
@article{MA2019367,
title = {An efficient method to compute different types of generalized inverses based on linear transformation},
journal = {Applied Mathematics and Computation},
volume = {349},
pages = {367-380},
year = {2019},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2018.12.064},
url = {https://www.sciencedirect.com/science/article/pii/S0096300318311251},
author = {Jie Ma and Feng Gao and Yongshu Li},
keywords = {Generalized inverse, Linear transformation, Rational matrix, MATHEMATICA},
abstract = {In this paper, we present functional definitions of all types of generalized inverses related to the {1}-inverse, which is a continuation of the work of Campbell and Meyer (2009). According to these functional definitions, we further derive novel representations for all types of generalized inverses related to the {1}-inverse in terms of the bases for R(A*), N(A) and N(A*). Based on these representations, we present the corresponding algorithm for computing various generalized inverses related to the {1}-inverse of a matrix and analyze the computational complexity of our algorithm for a constant matrix. Finally, we implement our algorithm and several known algorithms for symbolic computation of the Moore-–Penrose inverse in the symbolic computational package MATHEMATICA and compare their running times. Numerical experiments show that our algorithm outperforms these known algorithms when applied to compute the Moore–Penrose inverse of one-variable rational matrices, but is not the best choice for two-variable rational matrices in practice.}
}
@incollection{PIOT2006163,
title = {Gross, Maurice (1934–2001)},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {163-164},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/05135-X},
url = {https://www.sciencedirect.com/science/article/pii/B008044854205135X},
author = {M. Piot},
keywords = {comparative linguistics, computational linguistics, computer dictionaries, French language, linguistic theory, machine translation, mathematical models, syntax-based lexicon},
abstract = {Gross, Maurice (1934–2001) was a pioneer thinker in the field of modern linguistics. Long before computers could facilitate large-scale, lexically-based language study, he built an exhaustive, empirically based inventory of the ‘lexicon-grammar’ of French: the world's first lexical grammar. Since then, researchers in other countries have adopted the Gross model of description, which serves as a computational model for any language.}
}
@article{FINOTTO201385,
title = {Hybrid fuzzy-genetic system for optimising cabled-truss structures},
journal = {Advances in Engineering Software},
volume = {62-63},
pages = {85-96},
year = {2013},
note = {Special Issue dedicated to Professor Zden ek Bittnar on the occasion of his Seventieth Birthday: Part I},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2013.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0965997813000513},
author = {V.C. Finotto and W.R.L. {da Silva} and M. Valášek and P. Štemberk},
keywords = {Hybrid system, Structural optimisation, Cabled-truss, Fuzzy logic, Genetic algorithm, Nonlinear finite element analysis},
abstract = {This paper demonstrates an application of a hybrid fuzzy-genetic system in the optimisation of lightweight cabled-truss structures. These structures are described as a system of cables and triangular bar formations jointed at their ends by hinged connections to form a rigid framework. The optimised lightweight structure is determined through a stochastic discrete topology and sizing optimisation procedure that uses ground structure approach, nonlinear finite element analysis, genetic algorithm, and fuzzy logic. The latter is used to include expertise into the evolutionary search with the aim of filtering individuals with low survival possibility, thereby decreasing the total number of evaluations. This is desired because cables, which are inherently nonlinear elements, demand the use of iterative procedures for computing the structural response. Such procedures are computationally costly since the stiffness matrix is evaluated in each iteration until the structure is in equilibrium. Initially, the proposed system is applied to truss benchmarks. Next, the use of cables is investigated and the system’s performance is compared against genetic algorithms. The results indicate that the hybrid system considerably decreased the number of evaluations over genetic algorithms. Also, cabled-trusses showed a significant improvement in structural mass minimisation when compared with trusses.}
}
@article{XIAO2023103864,
title = {APRS: Automatic pruning ratio search using Siamese network with layer-level rewardsImage 1},
journal = {Digital Signal Processing},
volume = {133},
pages = {103864},
year = {2023},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103864},
url = {https://www.sciencedirect.com/science/article/pii/S105120042200481X},
author = {Huachao Xiao and Yangxin Wang and Jianyi Liu and Jiaxin Huo and Yang Hu and Yu Wang},
keywords = {Structured pruning, Deep reinforcement learning, Pruning ratio search, Siamese network},
abstract = {Structured pruning is still a mainstream model compression technique, for its merit of easy to implement and no reliance on specific hardware supporting library. In most previous works, the layer-wise channel pruning ratios were determined empirically. In this paper, we propose an Automatic Pruning Ratio Search (APRS) algorithm that can find the layer-wise optimal pruning ratio within the deep reinforcement learning framework. To solve the coarse-granularity reward problem existing in some previous works like AMC and CACP, a novel layer-level reward function is designed based on the Siamese network architecture for the fine-granularity agent-environment interaction purpose. We use a computationally efficient way to evaluate the effect of pruning action on each single layer. The incurred “backwardness disadvantage” problem has also been analyzed and addressed. The experiments are performed using the VGG-16, and MobileNet-v1 on the CIFAR10/100 and UC Merced Land-use datasets. The results verified that our method can better reveal the underlying sparse sensitivities of different layers in both high redundancy networks and compact networks, so that resulting a higher network accuracy after pruning compared to the traditional methods.}
}
@article{DAI2025100019,
title = {Why students use or not use generative AI: Student conceptions, concerns, and implications for engineering education},
journal = {Digital Engineering},
volume = {4},
pages = {100019},
year = {2025},
issn = {2950-550X},
doi = {https://doi.org/10.1016/j.dte.2024.100019},
url = {https://www.sciencedirect.com/science/article/pii/S2950550X24000190},
author = {Yun Dai},
keywords = {Artificial intelligence, generative AI, engineering education, student concern, barrier, technology integration, higher education},
abstract = {Generative artificial intelligence (GenAI) technologies are believed to transform engineering education. However, it remains underexamined how engineering students choose to use GenAI or not, along with the reasons behind their choices. To fill this research gap, this study presents a natural experiment that examines student use or non-use of GenAI tools in engineering design tasks in an undergraduate course. In this experiment, the participants (n = 403) were provided with unconstrained access to a GPT 4.0-empowered chatbot and were allowed to use it for their design projects voluntarily. Overall, 59.80 % of the students reported substantial use of GenAI in their design projects, and 40.20 % showed limited or no use. Those adopters used GenAI to aid idea generation and brainstorming, mediate discussions with instructors/TA, overcome non-technical expertise gaps, and optimize their design solutions. Conversely, non-adopters attributed their reluctance and rejection to inherent limitations in GenAI outputs, misalignment between GenAI functionalities and project needs, a lack of adaptation and prompt skills, and unclear benefits of GenAI use for personal growth. This study challenges the popular assumption of naturally active GenAI adoption among university students. It identifies three major factors—task characteristics, decision-maker characteristics, and context characteristics—that shape students' adoption of and interaction with GenAI. The findings highlight the need of establishing a consensus across various stakeholders (i.e., students, instructors, curriculum developers, policymakers, and others), while calling for adaptation and evidence-based decision-making in integrating GenAI tools into engineering education.}
}
@incollection{CELKO2008255,
title = {Chapter 13 - Turning Specifications into Code},
editor = {Joe Celko},
booktitle = {Joe Celko's Thinking in Sets},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {255-271},
year = {2008},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-0-12-374137-0},
doi = {https://doi.org/10.1016/B978-012374137-0.50014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123741370500146},
author = {Joe Celko},
abstract = {Publisher Summary
This chapter delineates the importance of unlearning the procedural thinking and moves to a pure SQL view. Programmers tend to make the same kinds of errors in their designs and their code over and over. They confuse RDBMS with the file systems and 3GL- or OO-oriented programming environments they first learned. Programmers from the C family of languages tend to put the entire program in lowercase as if they were still using a teletype on a UNIX system. Mainframe programmers tend to put the entire program in uppercase as if they were still using punch cards or a 3270 video monitor for input. Cohesion is how well a module of code does one and only one thing, that it is logically coherent. There are several types of cohesion. The original definitions have been extended from procedural code to include OO and class hierarchies. The symptom in DDL is a table with lots of NULL-able columns. It is probably two or more entities crammed into a single table. The symptom in DML is a query or other statement that tries to do too many things. When the same procedure or query checks inventory and build a personnel report, cohesion problems crop up. The table-valued function shows that the programmer still wants to see procedural coding complete with parameters. An SQL programmer would think in terms of VIEWS and CTES.}
}
@incollection{PASCAL2022231,
title = {10 - A practical guide to paleostress analysis},
editor = {Christophe Pascal},
booktitle = {Paleostress Inversion Techniques},
publisher = {Elsevier},
pages = {231-245},
year = {2022},
isbn = {978-0-12-811910-5},
doi = {https://doi.org/10.1016/B978-0-12-811910-5.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119105000087},
author = {Christophe Pascal},
keywords = {Fieldwork, Measuring, Processing, Plotting, Interpretation, Reporting},
abstract = {After having presented extensively different theoretical and methodological aspects of paleostress reconstruction methods, the purpose of the final chapter is to introduce recommendations for their practical use in tectonic problems. The discussion focuses on paleostress inversion of fault slip data, the latter being both the most elaborated and the most employed method. Detailed practical advice is given to conduct a paleostress study efficiently, starting with data acquisition in the field, proceeding with subsequent computation of paleostress tensors and ending with the reporting of the results and of their subsequent interpretations.}
}
@article{PANG201667,
title = {A hierarchical alternative updated adaptive Volterra filter with pipelined architecture},
journal = {Digital Signal Processing},
volume = {56},
pages = {67-78},
year = {2016},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2016.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1051200416000506},
author = {Yanjie Pang and Jiashu Zhang},
keywords = {Nonlinear filter, Hierarchical pipelined structure, Alternative update mechanism, Volterra filter},
abstract = {The pipelined adaptive Volterra filters (PAVFs) with a two-layer structure constitute a class of good low-complexity filters. They can efficiently reduce the computational complexity of the conventional adaptive Volterra filter. Their major drawbacks are low convergence rate and high steady-state error caused by the coupling effect between the two layers. In order to remove the coupling effect and improve the performance of PAVFs, we present a novel hierarchical pipelined adaptive Volterra filter (HPAVF)-based alternative update mechanism. The HPAVFs with hierarchical decoupled normalized least mean square (HDNLMS) algorithms are derived to adaptively update weights of its nonlinear and linear subsections. The computational complexity of HPAVF is also analyzed. Simulations of nonlinear system adaptive identification, nonlinear channel equalization, and speech prediction show that the proposed HPAVF with different independent weight vectors in nonlinear subsection has superior performance to conventional Volterra filters, diagonally truncated Volterra filters, and PAVFs in terms of initial convergence, steady-state error, and computational complexity.}
}
@article{RASHEED201686,
title = {Theoretical accounts to practical models: Grounding phenomenon for abstract words in cognitive robots},
journal = {Cognitive Systems Research},
volume = {40},
pages = {86-98},
year = {2016},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041715300310},
author = {Nadia Rasheed and Shamsudin H.M. Amin and U. Sultana and Rabia Shakoor and Naila Zareen and Abdul Rauf Bhatti},
keywords = {Grounded cognition, Symbol grounding problem, Cognitive robotics, Connectionist computation},
abstract = {This review concentrates on the issue of acquisition of abstract words in a cognitive robot with the grounding principle, from relevant theories to practical models of agents and robots. Most cognitive robotics models developed for grounding of language take inspiration from the findings of neuroscience and psychology to get the theoretical skeleton of these models. To better understand these modelling approaches, it is indispensable to work from the base (theoretical accounts) to the top (computational models). Therefore in this paper, succinct definition of abstract words is presented first, and then the symbol grounding issue and accounts of grounded cognition for abstract words are given. The next section discusses the computational modelling approaches for abstract words grounding phenomenon. Finally, important cognitive robotics models are reviewed. This paper also points out the strengths and weaknesses of relevant hypotheses and models for the representation of abstract words in the grounded cognition framework and helps the understanding of issues such as where and why modelling efforts stand to address this problem in comparison with theoretical findings.}
}
@article{WANG2022101643,
title = {Model for deep learning-based skill transfer in an assembly process},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101643},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101643},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001070},
author = {Kung-Jeng Wang and Luh {Juni Asrini} and Lucy Sanjaya and Hong-Phuc Nguyen},
keywords = {Convolutional neural network, Deep learning, Faster region-based convolutional neural network, Human machine interaction, Skill transfer},
abstract = {As the variety of products and manufacturing processes increases, the expansion of flexible training approaches is crucial to support the development of human skills. This study presents a model for skill transfer support that extracts experts’ relevant skills as actions and objects relevant to the action into a computational model for transferring skills. This model engages two modes of deep learning as the groundwork, namely, convolutional neural network (CNN) for action recognition and faster region-based convolutional neural network (R-CNN) for object detection. To evaluate the performance of the proposed model, a case study of the final assembly of a GPU card is conducted. The accuracy of CNN and faster R-CNN are 95.4% and 96.8%, respectively. The goal of this model is to guide junior operators during the assembly by providing step-by-step instructions in performing complex tasks. The present study facilitates flexible training in terms of adapting new skills from skilled operators to naïve operators by deep learning.}
}
@article{MARKOVSKY202142,
title = {Behavioral systems theory in data-driven analysis, signal processing, and control},
journal = {Annual Reviews in Control},
volume = {52},
pages = {42-64},
year = {2021},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1367578821000754},
author = {Ivan Markovsky and Florian Dörfler},
keywords = {Behavioral systems theory, Data-driven control, Missing data estimation, System identification},
abstract = {The behavioral approach to systems theory, put forward 40 years ago by Jan C. Willems, takes a representation-free perspective of a dynamical system as a set of trajectories. Till recently, it was an unorthodox niche of research but has gained renewed interest for the newly emerged data-driven paradigm, for which it is uniquely suited due to the representation-free perspective paired with recently developed computational methods. A result derived in the behavioral setting that became known as the fundamental lemma started a new class of subspace-type data-driven methods. The fundamental lemma gives conditions for a non-parametric representation of a linear time-invariant system by the image of a Hankel matrix constructed from raw time series data. This paper reviews the fundamental lemma, its generalizations, and related data-driven analysis, signal processing, and control methods. A prototypical signal processing problem, reviewed in the paper, is missing data estimation. It includes simulation, state estimation, and output tracking control as special cases. The direct data-driven control methods using the fundamental lemma and the non-parametric representation are loosely classified as implicit and explicit approaches. Representative examples are data-enabled predictive control (an implicit method) and data-driven linear quadratic regulation (an explicit method). These methods are equally amenable to certainty-equivalence as well as to robust control. Emphasis is put on the robustness of the methods under noise. The methods allow for theoretical certification, they are computationally tractable, in comparison with machine learning methods require small amount of data, and are robustly implementable in real-time on complex physical systems.}
}
@incollection{CAPLETTE2017905,
title = {Chapter 36 - The Time Course of Object, Scene, and Face Categorization},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {San Diego},
pages = {905-930},
year = {2017},
isbn = {978-0-08-101107-2},
doi = {https://doi.org/10.1016/B978-0-08-101107-2.00036-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081011072000361},
author = {Laurent Caplette and Éric McCabe and Caroline Blais and Frédéric Gosselin},
keywords = {Categorization, attention, vision, temporal processing, object recognition, scene recognition, face recognition},
abstract = {We first describe Strategy Length & Internal Practicability (SLIP), a formal model for thinking about categorization, in particular about the time course of categorization. We then discuss an early application of this model to basic-levelness. We then turn to aspects of the time course of categorization that have been neglected in the categorization literature: our limited processing capacities; the necessity of having a flexible categorization apparatus; and the paradox that this inexorably brings about. We propose a twofold resolution of this paradox, attempting, in the process, to bridge work done on categorization in vision, neuropsychology, and physiology.}
}
@article{PURI2023104439,
title = {Automatic detection of Alzheimer’s disease from EEG signals using low-complexity orthogonal wavelet filter banks},
journal = {Biomedical Signal Processing and Control},
volume = {81},
pages = {104439},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104439},
url = {https://www.sciencedirect.com/science/article/pii/S174680942200893X},
author = {Digambar V. Puri and Sanjay L. Nalbalwar and Anil B. Nandgaonkar and Jayanand P. Gawande and Abhay Wagh},
keywords = {Alzheimer’s disease, Electroencephalogram, Fractal dimension, Orthogonal filter banks, Support vector machine, Wavelets},
abstract = {Background:
Alzheimer’s disease (AD) is one of the most common neurodegenerative disorder. As the incidence of AD is rapidly increasing worldwide, detecting it at an early stage can prevent memory loss and cognitive dysfunctions in patients. Recently, Electroencephalogram (EEG) signals in AD cases show less synchronization and a slowing effect. The abrupt and transient behavior of EEG signals can be detected from specific frequency bands that are cortical rhythms of interest such as delta (0−4Hz), theta (4−8Hz), alpha (8−12Hz), beta1 (12−16Hz), beta2 (16−32Hz), and gamma (32−48Hz).
Method:
This paper proposes novel low-complexity orthogonal wavelet filter banks with vanishing moments (LCOWFBs-v) to decompose the AD and normal controlled (NC) EEG signals into subbands (SBs). A generalized design technique is suggested to reduce the computational complexity of original irrational wavelet filter banks (FBs). The two features, Higuchi’s fractal dimension (HFD) and Katz’s fractal dimension (KFD), were extracted from EEG SBs. The significance of these extracted features has been inspected using Kruskal–Wallis test.
Results:
The present study analyzed the EEG recordings of 23 subjects (AD-12 and NC-11) with the combination of LCOWFBs, HFD, and KFD. The proposed technique achieved a classification accuracy of 98.5% and 98.6% using the LCOWFBs-4 and LCOWFBs-6, respectively with a cubic-support vector machine classifier and 10-fold cross-validation technique.
Conclusion:
The proposed method with newly designed LCOWFBs is efficient compared with the well-known FBs and existing techniques for detecting AD.}
}
@article{BURIGANA2024102875,
title = {Bayesian networks and knowledge structures in cognitive assessment: Remarks on basic comparable aspects},
journal = {Journal of Mathematical Psychology},
volume = {123},
pages = {102875},
year = {2024},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2024.102875},
url = {https://www.sciencedirect.com/science/article/pii/S0022249624000440},
author = {Luigi Burigana},
keywords = {Knowledge assessment, Knowledge structure, Bayesian network, Probabilistic graphical model, Probabilistic inference},
abstract = {Two theories of current interest and of mathematical and computational substance concerning knowledge assessment in education are discussed. These are the theory of knowledge structures and the theory of Bayesian networks as specifically related to educational assessment. In four separate sections, the two theories are compared by considering the sets of variables involved in their models, the set-theoretical and relational constructs defined on those variables, the probabilistic assumptions and properties, and the problems addressed by the theories in constructing their models. For the comparison, a common-base system of symbols and terms is adopted, which overcomes the peculiarities of expression in the corresponding streams of literature. This system gives us a better recognition of the similarities and differences between the two paradigms, and a precise appreciation of their arguments and abilities.}
}
@article{BAMU20051794,
title = {Damage, deterioration and the long-term structural performance of cooling-tower shells: A survey of developments over the past 50 years},
journal = {Engineering Structures},
volume = {27},
number = {12},
pages = {1794-1800},
year = {2005},
note = {SEMC 2004 Structural Health Monitoring, Damage Detection and Long-Term Performance},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2005.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0141029605002257},
author = {P.C. Bamu and A. Zingoni},
keywords = {Cooling towers, Shell structures, Long-term performance, Damage modelling, Deterioration phenomena, Concrete cracking, Shell imperfections, Durability},
abstract = {The last 50 years have seen a gradual shift in trend in research on concrete hyperbolic cooling-tower shells, from the issues of response to short-term loading and immediate causes of collapse in the early part of this period, to the issues of deterioration phenomena, durability and long-term performance in more recent times. This paper traces these developments. After a revisit of some historical collapses of cooling-tower shells, and a brief consideration of condition surveys and repair programmes instituted in the aftermath of these events, focus shifts to the important question of damage and deterioration, and progress made over the past 30 years in the understanding of these phenomena. In particular, much research has gone into the modelling of cracking and geometric imperfections, which have a considerable effect on the load-carrying capacity of the shell, and are also manifestations of long-term deterioration. While structural monitoring of the progression of deterioration in cooling-tower shells, and the accurate prediction of this through appropriate numerical models, will always be important, the thinking now seems to be shifting towards designing for durability right from the outset.}
}
@article{MULDNER2015127,
title = {Utilizing sensor data to model students’ creativity in a digital environment},
journal = {Computers in Human Behavior},
volume = {42},
pages = {127-137},
year = {2015},
note = {Digital Creativity: New Frontier for Research and Practice},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.10.060},
url = {https://www.sciencedirect.com/science/article/pii/S074756321300410X},
author = {Kasia Muldner and Winslow Burleson},
keywords = {Creativity, Student modeling, Eye tracking, EEG, Skin conductance, Intelligent Tutoring Systems},
abstract = {While creativity is essential for developing students’ broad expertise in Science, Technology, Engineering, and Math (STEM) fields, many students struggle with various aspects of being creative. Digital technologies have the unique opportunity to support the creative process by (1) recognizing elements of students’ creativity, such as when creativity is lacking (modeling step), and (2) providing tailored scaffolding based on that information (intervention step). However, to date little work exists on either of these aspects. Here, we focus on the modeling step. Specifically, we explore the utility of various sensing devices, including an eye tracker, a skin conductance bracelet, and an EEG sensor, for modeling creativity during an educational activity, namely geometry proof generation. We found reliable differences in sensor features characterizing low vs. high creativity students. We then applied machine learning to build classifiers that achieved good accuracy in distinguishing these two student groups, providing evidence that sensor features are valuable for modeling creativity.}
}
@article{ACHARJYA2022100647,
title = {A rough set, formal concept analysis and SEM-PLS integrated approach towards sustainable wearable computing in the adoption of smartwatch},
journal = {Sustainable Computing: Informatics and Systems},
volume = {33},
pages = {100647},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100647},
url = {https://www.sciencedirect.com/science/article/pii/S221053792100130X},
author = {D.P. Acharjya and Gladys Gnana {Kiruba B}},
keywords = {Rough set, Wearable computing, Path diagram, Composite reliability, Convergence, Discriminant validity},
abstract = {The rapid growth of sustainable computing towards the energy, power and environment seize an immense attention from bigger organizations to an individual life. Besides the world is advancing towards the digital mode and smartwatch is gaining its popularity because of additional importance to improve lifestyle. Moreover, it is not restricted to only time viewer rather paves a way in user's daily life. Therefore, it is highly cardinal in identifying the factors among consumers influencing the adoption of smartwatch in sustainable wearable computing. Traditional data modelling tools limited to technology acceptance model is used to this end. After all the study deals with user's behaviour that includes uncertainties and thus studying such problems using computational intelligence techniques is pivotal. In this research work we hybridize rough set, partial least square, and formal concept analysis to study smartwatch users adoption in wearable computing. Initially, the reliability and validity of the proposed model is analysed using structural equation modelling along with partial least square. Further, decision rules are generated using the rough set. Finally, important factors affecting the user's behavioural adoption towards sustainable wearable computing is discovered using formal concept analysis.}
}
@article{ROBERTS2017225,
title = {Clinical Applications of Stochastic Dynamic Models of the Brain, Part II: A Review},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {2},
number = {3},
pages = {225-234},
year = {2017},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2016.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S2451902217300149},
author = {James A. Roberts and Karl J. Friston and Michael Breakspear},
keywords = {Computational neuroscience, Computational psychiatry, Epilepsy, Mathematical modeling, Melancholia, Stochastic},
abstract = {Brain activity derives from intrinsic dynamics (due to neurophysiology and anatomical connectivity) in concert with stochastic effects that arise from sensory fluctuations, brainstem discharges, and random microscopic states such as thermal noise. The dynamic evolution of systems composed of both dynamic and random fluctuations can be studied with stochastic dynamic models (SDMs). This article, Part II of a two-part series, reviews applications of SDMs to large-scale neural systems in health and disease. Stochastic models have already elucidated a number of pathophysiological phenomena, such as epilepsy and hypoxic ischemic encephalopathy, although their use in biological psychiatry remains rather nascent. Emerging research in this field includes phenomenological models of mood fluctuations in bipolar disorder and biophysical models of functional imaging data in psychotic and affective disorders. Together with deeper theoretical considerations, this work suggests that SDMs will play a unique and influential role in computational psychiatry, unifying empirical observations with models of perception and behavior.}
}
@article{HUEY2022101211,
title = {Assessing the impact of standards-based grading policy changes on student performance and practice work completion in secondary mathematics},
journal = {Studies in Educational Evaluation},
volume = {75},
pages = {101211},
year = {2022},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2022.101211},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X22000888},
author = {Maryann E. Huey and Patrick R. Silvey and Amy G. Vaughan and Asa L. Fisher},
keywords = {Grading, Standards-based grading, Mathematics, Secondary, Motivation, High-achieving students},
abstract = {We report upon an intervention study conducted over two academic calendar years involving high-achieving, grade 8 and 9 students (n = 122 and 123 respectively) enrolled in a year-long geometry course. The study assesses the impact of a change in grading policy, namely removing practice work from grade computations, on student performance levels and behaviors. After the change in grading policy was implemented, findings reveal that performance decreased on some, but not all, standards assessed. Completion rates of practice work also decreased overall. Potential causes are discussed as well as implications for implementing aspects of standards-based grading systems in secondary mathematics classrooms.}
}
@incollection{GEWEKE20013463,
title = {Chapter 56 - Computationally Intensive Methods for Integration in Econometrics**The authors gratefully acknowledge financial support from National Science Foundation grants SBR-9511280, SBR-9731037, SES-9814342, and SES-9819444.},
editor = {James J. Heckman and Edward Leamer},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {5},
pages = {3463-3568},
year = {2001},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(01)05009-7},
url = {https://www.sciencedirect.com/science/article/pii/S1573441201050097},
author = {John Geweke and Michael Keane},
keywords = {Bayesian inference, discrete choice, dynamic optimization, integration, Markov chain Monte Carlo, multinomial probit, normal mixtures, selection models, Primary C15, Secondary C11},
abstract = {Until recently, inference in many interesting models was precluded by the requirement of high dimensional integration. But dramatic increases in computer speed, and the recent development of new algorithms that permit accurate Monte Carlo evaluation of high dimensional integrals, have greatly expanded the range of models that can be considered. This chapter presents the methodology for several of the most important Monte Carlo methods, supplemented by a set of concrete examples that show how the methods are used. Some of the examples are new to the econometrics literature. They include inference in multinomial discrete choice models and selection models in which the standard normality assumption is relaxed in favor of a multivariate mixture of normals assumption. Several Monte Carlo experiments indicate that these methods are successful at identifying departures from normality when they are present. Throughout the chapter the focus is on inference in parametric models that permit rich variation in the distribution of disturbances. The chapter first discusses Monte Carlo methods for the evaluation of high dimensional integrals, including integral simulators like the GHK method, and Markov Chain Monte Carlo methods like Gibbs sampling and the Metropolis–Hastings algorithm. It then turns to methods for approximating solutions to discrete choice dynamic optimization problems, including the methods developed by Keane and Wolpin, and Rust, as well as methods for circumventing the integration problem entirely, such as the approach of Geweke and Keane. The rest of the chapter deals with specific examples: classical simulation estimation for multinomial probit models, both in the cross sectional and panel data contexts; univariate and multivariate latent linear models; and Bayesian inference in dynamic discrete choice models in which the future component of the value function is replaced by a flexible polynomial.}
}
@article{SZUBA2001489,
title = {A formal definition of the phenomenon of collective intelligence and its IQ measure},
journal = {Future Generation Computer Systems},
volume = {17},
number = {4},
pages = {489-500},
year = {2001},
note = {Workshop on Bio-inspired Solutions to Parallel Computing problems},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(99)00136-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X99001363},
author = {Tadeusz Szuba},
keywords = {Collective intelligence, Quasi-chaotic model of computations, Synergy, IQ, PROLOG},
abstract = {This paper presents a formalization of collective intelligence (CI). A molecular, quasi-chaotic model of computations allows us to model CI in social structures, and to define its measure (IQS). This methodology works for bacterial colonies and social insects as well as for human social structures. With the CI theory some patterns of human behavior receive formal justification, others can be explained as IQS optimization. The CI formalization assumes that it is a property of a social structure, initializing when individuals interact, and as a result, acquiring the ability to solve new or more complex problems. CI amplifies if the structure improves synergy, which further increases the spectrum and complexity of the problems, which can be solved together. The formalization covers areas where CI results in physical synergy and mental/logical cooperation.}
}
@article{ABRUSCI2025100401,
title = {AI4Design: A generative AI-based system to improve creativity in design–A field evaluation},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100401},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100401},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000414},
author = {Luca Abrusci and Karma Dabaghi and Stefano D'Urso and Filippo Sciarrone},
keywords = {Creativity, Design, Generative artificial intelligence},
abstract = {Chatbots serve as valuable instruments for enhancing students' educational experience and aiding them in their day-to-day academic tasks. Advances in Generative AI (GAI) have ushered in increasingly sophisticated and adaptive chatbots, with ChatGPT and DALL⋅E being prime examples. ChatGPT excels at generating text-based answers across diverse areas of knowledge, while DALL⋅E is adept at converting text-based concepts into visual imagery. These technologies are increasingly used by students across various levels of education. In this study, we introduce AI4Design, a web-based system designed to assist design students with their course projects by acting as an intelligent chatbot. The field of design is propitious for such work because of the increasing use of technology and the necessity of introducing its critical use during study. Comprising two integrated modules, the system is based on a two-step workflow. The first step is anchored on ChatGPT, enabling students to prompt questions and receive answers. The second step allows for the generation of one or more images based on the system's answer to the initial question. Our research assesses whether our system can offer valuable insights and inspiration to students in their design work. We conducted an exploratory study in the Design domain involving 31 students from the Lebanese American University. Over a two- to three-day period, participants used the AI4Design system to enhance their projects. A subsequent evaluation of their work indicated improvements in conceptual clarity and visual outputs that highlighted a measurable increase in creativity, supporting the efficacy of both the system and its foundational learning model, which will be confirmed in the future through a large-scale experimental study. Meanwhile, our study suggests that in the iterative design process, GAI can assist students in making better decisions by giving them just-in-time access to a broader palette of possibilities.}
}
@article{XING2024103704,
title = {Financial risk tolerance profiling from text},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103704},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103704},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000645},
author = {Frank Xing},
keywords = {Artificial intelligence in finance, Risk tolerance, Risk profiling, Text mining, Convolutional neural network},
abstract = {Traditionally, individual financial risk tolerance information is gathered via questionnaires or similar structured psychometric tools. Our abundant digital footprint, as an unstructured alternative, is less investigated. Leveraging such information can potentially support large-scale and cost-efficient financial services. Therefore, I explore the possibility of building a computational model that distills risk tolerance information from user texts in this study, and discuss the design principles discovered from empirical results and their implications. Specifically, a new quaternary classification task is defined for text mining-based risk profiling. Experiments show that pre-trained large language models set a baseline micro-F1 of circa 0.34. Using a convolutional neural network (CNN), the reported system achieves a micro-F1 of circa 0.51, which significantly outperforms the baselines, and is a circa 4% further improvement over the standard CNN configurations (micro-F1 of circa 0.47). Textual feature richness and supervised learning are found to be the key contributors to model performances, while other machine learning strategies suggested by previous research (data augmentation and multi-tasking) are less effective. The findings confirm user texts to be a useful risk profiling resource and provide several insights on this task.}
}
@article{HARTLEY1997169,
title = {Semantic networks: visualizations of knowledge},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {5},
pages = {169-175},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01057-7},
url = {https://www.sciencedirect.com/science/article/pii/S1364661397010577},
author = {Roger T. Hartley and John A. Barnden},
abstract = {The history of semantic networks is almost as long as that of their parent discipline, artificial intelligence. They have formed the basis of many fascinating, yet controversial, discussions in conferences and in the literature, ranging from metaphysics through to complexity theory in computer science. Many excellent surveys of the field have been written, and yet it is our belief that none of them has examined the important link between their use as a formal scheme for knowledge representation and their more heuristic use as an informal tool for thinking. In our consideration of semantic networks as computerized tools, we will discuss three levels of abstraction that we believe can help us understand how semantic networks are used. I}
}
@article{CHEEMA2022100123,
title = {Augmented Intelligence to Identify Patients With Advanced Heart Failure in an Integrated Health System},
journal = {JACC: Advances},
volume = {1},
number = {4},
pages = {100123},
year = {2022},
issn = {2772-963X},
doi = {https://doi.org/10.1016/j.jacadv.2022.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X22001739},
author = {Baljash Cheema and R. Kannan Mutharasan and Aditya Sharma and Maia Jacobs and Kaleigh Powers and Susan Lehrer and Firas H. Wehbe and Jason Ronald and Lindsay Pifer and Jonathan D. Rich and Kambiz Ghafourian and Anjan Tibrewala and Patrick McCarthy and Yuan Luo and Duc T. Pham and Jane E. Wilcox and Faraz S. Ahmad},
keywords = {advanced heart failure, artificial intelligence, augmented intelligence, electronic health record, integrated healthcare system, machine learning},
abstract = {Background
Timely referral for specialist evaluation in patients with advanced heart failure (HF) is a Class 1 recommendation. However, the transition from stage C HF to advanced or stage D HF often goes undetected in routine care, resulting in delayed referral and higher mortality rates.
Objectives
The authors sought to develop an augmented intelligence-enabled workflow using machine learning to identify patients with stage D HF and streamline referral.
Methods
We extracted data on HF patients with encounters from January 1, 2007, to November 30, 2020, from a HF registry within a regional, integrated health system. We created an ensemble machine learning model to predict stage C or stage D HF and integrated the results within the electronic health record.
Results
In a retrospective data set of 14,846 patients, the model had a good positive predictive value (60%) and low sensitivity (25%) for identifying stage D HF in a 100-person, physician-reviewed, holdout test set. During prospective implementation of the workflow from April 1, 2021, to February 15, 2022, 416 patients were reviewed by a clinical coordinator, with agreement between the model and the coordinator in 50.3% of stage D predictions. Twenty-four patients have been scheduled for evaluation in a HF clinic, 4 patients started an evaluation for advanced therapies, and 1 patient received a left ventricular assist device.
Conclusions
An augmented intelligence-enabled workflow was integrated into clinical operations to identify patients with advanced HF. Endeavors such as this require a multidisciplinary team with experience in design thinking, informatics, quality improvement, operations, and health information technology, as well as dedicated resources to monitor and improve performance over time.}
}
@article{HOGAN200855,
title = {Advancing the dialogue between inner and outer empiricism: A comment on O’Nualláin},
journal = {New Ideas in Psychology},
volume = {26},
number = {1},
pages = {55-68},
year = {2008},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2007.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X07000293},
author = {Michael J. Hogan},
keywords = {Consciousness, Inner empiricism, Outer empiricism, Evolution, No-mind, Mythos, Logos},
abstract = {In a recent contribution to New Ideas in Psychology, Seán O’Nualláin draws out a distinction between inner and outer empiricism, and suggests that consciousness research can benefit from analysis in both directions, that is, via the exploration of facts and relations that facilitate a third-person understanding of consciousness (by reference to an analysis of the structures, processes, and functions of the brain) and via the direct exploration of conscious experience itself, both in terms of its computational (content filled) and non-computational (content empty) aspects. In positing a substrate of subjectivity independent of the contents of consciousness (and, more specifically, a state of “nothingness”), Ò’Nualláin follows a long tradition deeply rooted in mythical, religious, and esoteric schools of belief and practice. Although there is considerable debate amongst philosophers, psychologists, and neuroscientists as to whether or not a non-computational view of consciousness is viable, O’Nualláin accepts that such a possibility does exist. Further, he suggests that a dialogue between the inner and outer empiricists will be fruitful. In this comment I, critique Ò’Nualláin's initial thoughts on the subject and draw out a series of useful distinctions that will help to advance the dialogue between inner and outer empiricism. Critical amongst these distinctions is explicit reference to (1) ontological and epistemological interdependencies in consciousness research, and (2) states of consciousness that describe the transition from “mindfulness” through “nothingness” to “no-mind”.}
}
@article{TRAUTTEUR2007106,
title = {A note on discreteness and virtuality in analog computing},
journal = {Theoretical Computer Science},
volume = {371},
number = {1},
pages = {106-114},
year = {2007},
note = {Computing and the Natural Sciences},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2006.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0304397506007699},
author = {Giuseppe Trautteur and Guglielmo Tamburrini},
keywords = {Analog computing, Virtual machine, Cognitive modelling},
abstract = {The need for physically motivated discreteness and finiteness conditions emerges in models of both analog and digital computing that are genuinely concerned with physically realizable computational processes. This is brought out by a critical examination of notional analog superTuring devices which involve physically untenable idealizations about the perfect functioning of analog apparatuses and infinite precision of physical measurements. The capability for virtual behaviour, that is, the capability of interpreting, storing, transforming, creating the code, and thereby mimicking the behaviour of (Turing) machines, is used here to introduce a new dimension in the discussion of the analog–digital watershed. In the light of recent results on the analog simulation of digital computing, we examine the role of virtuality as a discriminating factor between these two species of computing, and immerse this problem in the context of natural computing. Is virtuality instantiated in parts of the natural world other than computer technology? This broad issue is examined in connection with the computational modelling of brain and mental information processing.}
}
@article{LANG2017298,
title = {Mesoscopic Simulation Models for Logistics Planning Tasks in the Automotive Industry},
journal = {Procedia Engineering},
volume = {178},
pages = {298-307},
year = {2017},
note = {RelStat-2016: Proceedings of the 16th International Scientific Conference Reliability and Statistics in Transportation and Communication October 19-22, 2016. Transport and Telecommunication Institute, Riga, Latvia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817301182},
author = {Sebastian Lang and Tobias Reggelin and Toralf Wunder},
keywords = {automotive industry, logistics planning, production planning, mesoscopic simulation, discrete-rate simulation},
abstract = {The paper evaluates mesoscopic simulation models applied to logistics planning tasks in the automotive industry. In terms of level of detail, mesoscopic simulation models fall between object based discrete-event simulation models and flow based continuous simulation models. Mesoscopic models represent logistics flow processes on an aggregated level through piecewise constant flow rates instead of modeling individual flow objects. The results are not obtained by counting individual objects but by using mathematical formulas to calculate the results as continuous quantities in every modeling time step. This leads to a fast model creation and computation. The authors expect that mesoscopic simulation models can help to support decisions on the operational, tactical and strategic level of planning. The paper describes a mesoscopic simulation model of the goods receiving of an assembly plant and compares the simulation results and computation time with a discrete-event model.}
}
@article{DEGRANDE201960,
title = {To add or to multiply? An investigation of the role of preference in children's solutions of word problems},
journal = {Learning and Instruction},
volume = {61},
pages = {60-71},
year = {2019},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959475218304511},
author = {Tine Degrande and Lieven Verschaffel and Wim {Van Dooren}},
keywords = {Word problem solving, Additive reasoning, Multiplicative reasoning, Preference, Skill},
abstract = {Previous research has shown that upper primary school children frequently erroneously solve additive word problems multiplicatively, while younger children frequently erroneously solve multiplicative word problems additively. It has been suggested that children's preference for additive or multiplicative relations explains these errors, besides their lacking skills, but this claim has not been tested empirically yet. Therefore, we administered four test instruments (a word problem test, a preference test, and two tests measuring additive and multiplicative computation and discrimination skill) to 246 third to sixth graders. Previous research results on errors in word problems, as well as on preference were replicated and systematized. Further, they were extended by explaining this erroneous word problem solving behavior by preference, for those children who unmistakably had acquired the necessary computation and discrimination skills. This finding provides strong evidence for the unique additional role of children's preference in erroneous additive or multiplicative word problem solving behavior.}
}
@article{ROBERTSON2008436,
title = {New frontiers in space propulsion sciences},
journal = {Energy Conversion and Management},
volume = {49},
number = {3},
pages = {436-452},
year = {2008},
note = {Space Nuclear Power and Propulsion},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2007.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S019689040700369X},
author = {Glen A. Robertson and P.A. Murad and Eric Davis},
keywords = {Space propulsion, Warp drive, Worm Holes, EM propulsion},
abstract = {Mankind’s destiny points toward a quest for the stars. Realistically, it is difficult to achieve this using current space propulsion science and develop the prerequisite technologies, which for the most part requires the use of massive amounts of propellant to be expelled from the system. Therefore, creative approaches are needed to reduce or eliminate the need for a propellant. Many researchers have identified several unusual approaches that represent immature theories based upon highly advanced concepts. These theories and concepts could lead to creating the enabling technologies and forward thinking necessary to eventually result in developing new directions in space propulsion science. In this paper, some of these theoretical and technological concepts are examined – approaches based upon Einstein’s General Theory of Relativity, spacetime curvature, superconductivity, and newer ideas where questions are raised regarding conservation theorems and if some of the governing laws of physics, as we know them, could be violated or are even valid. These conceptual ideas vary from traversable wormholes, Krasnikov tubes and Alcubierre’s warpdrive to Electromagnetic (EM) field propulsion with possible hybrid systems that incorporate our current limited understanding of zero point fields and quantum mechanics.}
}
@article{KARUNA2019161,
title = {Capital markets research in accounting: Lessons learnt and future implications},
journal = {Pacific-Basin Finance Journal},
volume = {55},
pages = {161-168},
year = {2019},
issn = {0927-538X},
doi = {https://doi.org/10.1016/j.pacfin.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0927538X19301398},
author = {Christo Karuna},
abstract = {I review the capital markets literature in accounting by describing the journey taken by researchers since the inception of this stream of research in the late 1960s. Based on a discussion of topics related to the relation between earnings and stock returns, I show how thinking has evolved depending on changing paradigms, methodologies, and data availability. What is clear from a review of the literature is that the usefulness of earnings in determining firm value is both contextual and broadening over time with changes in the global environment. Thus, more research needs to be conducted on a broader notion of earnings that appeals to not just the shareholder but a wide range of firm stakeholders.}
}
@article{MACHADO2021103322,
title = {Contributions of modularity to the circular economy: A systematic review of literature},
journal = {Journal of Building Engineering},
volume = {44},
pages = {103322},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2021.103322},
url = {https://www.sciencedirect.com/science/article/pii/S2352710221011803},
author = {Natália Machado and Sandra Naomi Morioka},
keywords = {Modularity, Circular economy, Product modular design, Module optimization, Product lifecycle},
abstract = {In the creation of practices to foster the thinking of the circular economy (CE), modularity can be a facilitator. There are many studies that address modularity and the circular economy individualized, but the integration between the two is still little addressed. This study conducts a systematic review of the literature and aims to identify how modularity can contribute to the circular economy. The data analysis begins with the identification of the main characteristics of the literature that address modularity and circular economy, bringing the evolution of studies over the years, main journals, co-citation of references, co-occurrence of keywords and shows four research clusters linked to modularity and CE, being: conceptualization of modularization, modular design of products, module optimization and product lifecycle. In addition, identifies fifteen benefits of modularity that can contribute to the implementation of strategies for the circular economy and five barriers from the perspective of the circular economy that can inhibit the contribution process. So, it was possible to create an integrative conceptual framework that shows how modularity can contribute to the circular economy. From the results, future research is identified in order to contribute to the transition from a linear economy to a circular economy.}
}
@article{WANG2024130178,
title = {A measurement-device-independent quantum secure digital payment},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {655},
pages = {130178},
year = {2024},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2024.130178},
url = {https://www.sciencedirect.com/science/article/pii/S0378437124006873},
author = {Qingle Wang and Jiacheng Liu and Guodong Li and Yunguang Han and Yuqian Zhou and Long Cheng},
keywords = {Quantum digital payment, Measurement-device-independent, Quantum secure communication, Authentication},
abstract = {In contemporary society, digital payment systems are crucial, yet vulnerable to security breaches. Based on the principles of quantum physics, quantum digital payment (QDP) protocols offer a theoretically superior security paradigm compared to those reliant on computational complexity. Nevertheless, those QDP protocols in practice are frequently compromised by imperfections in measurement devices, facilitating valuable information interception by malicious entities. Addressing this vulnerability, we propose a measurement-device-independent quantum secure digital payment (MDI-QSDP) protocol, designed to enhance security in digital payment systems by eliminating side-channel attacks on measurement devices. This protocol extends the framework of a novelly developed measurement-device-independent quantum secure communication (MDI-QSC) protocol, which supports secure dialogic exchanges without prior key sharing. Utilizing the proposed MDI-QSC protocol, participants can not only engage in secure direct communication but also establish a private key for subsequent encrypted interactions. Our MDI-QSDP protocol incorporates a robust authentication mechanism, ensuring that only legitimate participants can initiate transactions, thereby bolstering security. A comprehensive security analysis of the proposed protocol demonstrates its resilience against identity theft, information leakage, and other potential security breaches. Furthermore, simulations employing practical experimental parameters validate the protocol’s applicability and effectiveness in real-world scenarios, thereby confirming its potential to significantly enhance the security of future quantum digital payments.}
}
@article{CHEAH2025100363,
title = {Integrating generative artificial intelligence in K-12 education: Examining teachers’ preparedness, practices, and barriers},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100363},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000037},
author = {Yin Hong Cheah and Jingru Lu and Juhee Kim},
keywords = {Generative artificial intelligence, In-service teachers, Preparedness, Practices, Barriers, K-12 education},
abstract = {Despite the growing body of research on developing K-12 teachers' generative AI (GenAI) knowledge and skills, its integration into daily teaching practices remains underexplored. Using a snowball sampling method, this study examined the preparedness, practices, and barriers encountered by 89 U.S. teachers in the state of Idaho. Participants were predominantly White, female teachers serving in rural schools. A mixed-methods analysis of survey responses revealed that teachers were generally underprepared for integrating GenAI, with fewer than half incorporating it into their educational practices. Unlike the widespread classroom integration patterns observed with general educational technologies, teachers in this study tended to use GenAI for out-of-classroom duties (i.e., lesson preparation, assessment, and administrative tasks) rather than for real-time teaching and learning. These preferences could be attributed to key barriers teachers faced, including doubts about GenAI's ability to manage risks (i.e., technology value beliefs), reduced human interaction in instruction (i.e., pedagogical beliefs), ethical considerations, and the absence of policies and guidance. This study highlights the need to develop support systems and targeted policies to facilitate teachers' GenAI integration, offering implications for Idaho's education system and the broader U.S. context.}
}
@article{CAI201253,
title = {On fast and accurate block-based motion estimation algorithms using particle swarm optimization},
journal = {Information Sciences},
volume = {197},
pages = {53-64},
year = {2012},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2012.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S002002551200117X},
author = {Jing Cai and W. {David Pan}},
keywords = {Particle swarm optimization, Motion estimation, Fast block-matching methods, Video sequences, Computational complexity},
abstract = {Both fast and accurate block-matching algorithms are critical to efficient compression of video frames using motion estimation and compensation. While the particle swarm optimization approach holds the promise of alleviating the local optima problem suffered typically by existing very fast block matching methods, motion estimation algorithms based on particle swarm optimization in the literature appear to be either much slower than some leading fast block-matching methods for a given accuracy of motion estimation, or less accurate for a given computational complexity. In this paper, we show that the conventional particle swarm optimization approach, which was originally designed to solve general optimization problems where fast convergence of the algorithm might not be a primary concern, could be modified appropriately so that it could provide accurate motion estimation with very low computational cost in the specific context of video motion estimation. To this end, we proposed a new block matching algorithm based on a set of strategies adapted from the standard particle swarm optimization approach. Extensive simulations showed that the proposed method could achieve significant improvements over leading fast block matching methods including the diamond search and the cross-diamond search methods, in terms of both estimation accuracy and computational cost. In particular, the proposed method based on particle swarm optimization is not only much faster, but also remarkably more accurate (about 2dB higher in terms of the Peak Signal-to-Noise-Ratio) than the competing methods on video sequences with large motion.}
}
@article{KRIVY2023100057,
title = {Digital ecosystem: The journey of a metaphor},
journal = {Digital Geography and Society},
volume = {5},
pages = {100057},
year = {2023},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2023.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2666378323000090},
author = {Maroš Krivý},
keywords = {Nature, Ecosystem, Digital capitalism, Platform capitalism, Metaphor, Imaginaries},
abstract = {The term “digital ecosystem” has become ubiquitous through a seemingly endless stream of scholarship, punditry and hyperbole around digitalization, to the point that the metaphor is becoming dead. Considering “ecosystem” as a traveling concept straddling natural, social and technical systems, this article traces the extension of “digital ecosystem,” along with the adjacent “business ecosystem” and “entrepreneurial ecosystem,” in the fields of computer science, economy, governance and environmental policy. The origins of the concept as a form of circuitry applied to nature are outlined as a background against which to trace its role as a socio-technical metaphor for digital capitalism. Since the 1990s, various formulations of “ecosystem” have offered a naturalistic interpretation to phenomena ranging from economic interactions to digital infrastructure and the urban everyday. I conclude that by representing the internet and the market as complex, self-organizing processes, the metaphor prioritizes the imperative of adapting to—and downplays the possibility of challenging—our erratic digital capitalism. The article contributes by illuminating the ideological work of naturalistic models in the digital political economy. Evidence on using digital ecosystems in environmental policy is still emerging but points to a form of legitimacy exchange that reduces environmental problems to technical issues.}
}
@article{AHMAN201351,
title = {Normalization by Evaluation and Algebraic Effects},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {298},
pages = {51-69},
year = {2013},
note = {Proceedings of the Twenty-ninth Conference on the Mathematical Foundations of Programming Semantics, MFPS XXIX},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2013.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066113000534},
author = {Danel Ahman and Sam Staton},
keywords = {Algebraic effects, Type theory, Normalization by evaluation, Presheaves, Monads},
abstract = {We examine the interplay between computational effects and higher types. We do this by presenting a normalization by evaluation algorithm for a language with function types as well as computational effects. We use algebraic theories to treat the computational effects in the normalization algorithm in a modular way. Our algorithm is presented in terms of an interpretation in a category of presheaves equipped with partial equivalence relations. The normalization algorithm and its correctness proofs are formalized in dependent type theory (Agda).}
}
@article{UENO2011385,
title = {Lichtheim 2: Synthesizing Aphasia and the Neural Basis of Language in a Neurocomputational Model of the Dual Dorsal-Ventral Language Pathways},
journal = {Neuron},
volume = {72},
number = {2},
pages = {385-396},
year = {2011},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2011.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0896627311008348},
author = {Taiji Ueno and Satoru Saito and Timothy T. Rogers and Matthew A. Lambon Ralph},
abstract = {Summary
Traditional neurological models of language were based on a single neural pathway (the dorsal pathway underpinned by the arcuate fasciculus). Contemporary neuroscience indicates that anterior temporal regions and the “ventral” language pathway also make a significant contribution, yet there is no computationally-implemented model of the dual pathway, nor any synthesis of normal and aphasic behavior. The “Lichtheim 2” model was implemented by developing a new variety of computational model which reproduces and explains normal and patient data but also incorporates neuroanatomical information into its architecture. By bridging the “mind-brain” gap in this way, the resultant “neurocomputational” model provides a unique opportunity to explore the relationship between lesion location and behavioral deficits, and to provide a platform for simulating functional neuroimaging data.}
}
@incollection{VALYAN202025,
title = {Chapter 4 - Decision-making deficits in substance use disorders: cognitive functions, assessment paradigms, and levels of evidence},
editor = {Antonio Verdejo-Garcia},
booktitle = {Cognition and Addiction},
publisher = {Academic Press},
pages = {25-61},
year = {2020},
isbn = {978-0-12-815298-0},
doi = {https://doi.org/10.1016/B978-0-12-815298-0.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152980000046},
author = {Alireza Valyan and Hamed Ekhtiari and Ryan Smith and Martin P. Paulus},
keywords = {Assessment, Behavioral tasks, Computational models, Decision-making dysfunction, fMRI, Intervention, Substance use disorder},
abstract = {Aberrant decision-making plays an important role in both the onset and maintenance of substance use disorders (SUDs). The current state of research within the field of SUDs can be usefully summarized within three broad dimensions: (1) the goal of characterizing the affected cognitive components that contribute to aberrant decision-making (i.e., value, probability, time, and learning functions), (2) the instruments/methods used to accomplish that goal (i.e., self-reports, behavioral tasks, computational modeling, and brain mapping), and (3) the levels of evidence afforded by those instruments/methods. In this chapter, we review and organize the most recent findings based on this three-dimensional framework. Our aim is to (1) provide a comprehensive synthesis of current research on decision-making in SUDs that can serve as a useful resource to guide future research, (2) highlight current limitations in the field and promising future research directions, and (3) illustrate ways in which the framework that we provide may inform the design and implementation of interventional strategies that can advance the field of addiction medicine.}
}
@article{WALLENTIN2017165,
title = {Dynamic hybrid modelling: Switching between AB and SD designs of a predator-prey model},
journal = {Ecological Modelling},
volume = {345},
pages = {165-175},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2016.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0304380016303714},
author = {Gudrun Wallentin and Christian Neuwirth},
keywords = {Hybrid model, Multi-paradigmatic modelling, Agent-based model, System-dynamics model, Predator-prey system},
abstract = {Entities and processes in complex systems are of diverse nature and operate at various spatial and temporal scales. Hybrid agent-based (AB) and system dynamics (SD) models have been suggested to capture the essence of these systems in a natural and computationally efficient way. However, the integration of the equation-based SD and individual-based AB models is not least challenged by considerable conceptual differences between these models. Examples of tightly integrated and dynamically switching hybrid models are rare. The aim of this paper is to expand on theoretical frameworks of hybrid agent-based and system dynamics models in ecology to support the model design process of dynamically switching hybrid models. We suggested six alternative model designs that switched between the two modelling paradigms. By the example of a fish-plankton lake ecosystem we demonstrated that a well-designed switching hybrid model can be a performant modelling approach that retains relevant spatial and attributive information. Important findings with respect to optimising computational versus predictive performance were (1) the most plausible results were produced by a spatially explicit design based on spatial plankton stocks and fish switching between individual agents and aggregate school-agents, (2) higher levels of aggregation did not necessarily result in higher computational performance, and (3) adaptive, emergence-based triggers for the paradigm switches minimised information loss and could connect hierarchical and spatial scales. In conclusion, we argue to reach beyond efficiency-oriented considerations and use emergent super-individuals as structural elements of dynamically switching hybrid models.}
}
@article{RUI2024100711,
title = {Simulation of e-learning in vocal network teaching experience system based on intelligent Internet of things technology},
journal = {Entertainment Computing},
volume = {50},
pages = {100711},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100711},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400079X},
author = {Yu Rui},
keywords = {Intelligent technology, Internet of Things, E-learning, Vocal music network, Teaching experience, System simulation},
abstract = {With the rapid development of smart Internet of Things technology, education is also starting to use this technology to improve the learning experience. As an art subject, vocal music teaching has many limitations in the traditional face-to-face teaching methods, and e-learning provides new possibilities for vocal music network teaching. We analyze the problems and challenges existing in the traditional face-to-face mode of vocal music teaching, and then based on the intelligent Internet of Things technology, we design a vocal music network teaching experience system. The system combines sound acquisition equipment, intelligent audio processing algorithm, virtual classroom and other technologies to realize the simulation experience of online vocal music teaching. We have developed an intelligent audio processing algorithm for analyzing and processing students’ singing sounds. This algorithm can detect problems in tone, timbre, rhythm, and more, and provide real-time feedback and advice. In this way, students can understand the shortcomings of their own singing skills, and make timely adjustments and improvements. This study shows that e-learning based on intelligent Internet of Things technology has important application value in vocal music network teaching. By simulating the classroom environment and providing real-time feedback, students can obtain a better learning experience and improve their vocal skills.}
}
@article{SUTOYO2015435,
title = {Dynamic Difficulty Adjustment in Tower Defence},
journal = {Procedia Computer Science},
volume = {59},
pages = {435-444},
year = {2015},
note = {International Conference on Computer Science and Computational Intelligence (ICCSCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.563},
url = {https://www.sciencedirect.com/science/article/pii/S187705091502092X},
author = {Rhio Sutoyo and Davies Winata and Katherine Oliviani and Dedy Martadinata Supriyadi},
keywords = {dynamic game balancing, tower defence, dynamic difficulty adjustment, computational intelligence},
abstract = {When we play tower defence game, generally we repeat the same stages several times with the same enemies. Moreover, when the players play a stage that is ridiculously hard or way too easy, they would probably quit the game because it ismoderately frustrating or boring. The purpose of this research is to createa game that can adapt to the players’ ability so the difficulty of the game becomes dynamic. In other words, the game will have different difficultiesof levels according to the players’ ability. High difficulty levels will be set if the players use good strategy and low difficulty levels will be set if the players use bad strategy. In this work, we determine the difficulties based on players’ lives, enemies’ health, and passive skills (skill points) that are chosen by the player. With three of these factors, players will have varies experience of playing tower defence because different combination will give different results to the system and difficulties of the games will be different for each gameplay. The result of this research is a dynamic difficulty tower defence game, dynamic difficulty adjustment (DDA) document, and gameplay outputs for best, average, and worst strategy cases.}
}
@article{HAID2024,
title = {Exploring AI: Transforming medical practice, education and research},
journal = {Journal of Pediatric Urology},
year = {2024},
issn = {1477-5131},
doi = {https://doi.org/10.1016/j.jpurol.2024.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1477513124006740},
author = {Bernhard Haid and Caleb Nelson and M. İrfan Dönmez and Salvatore Cascio and Massimo Garriboli and Anka Nieuwhof-Leppink and Christina Ching and Luis H. Braga and Ilina Rosklija and Luke Harper},
keywords = {Artificial intelligence, Large language models, History, Education}
}
@article{PARK2025101119,
title = {Code suggestions and explanations in programming learning: Use of ChatGPT and performance},
journal = {The International Journal of Management Education},
volume = {23},
number = {2},
pages = {101119},
year = {2025},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101119},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001903},
author = {Arum Park and Taekyung Kim},
keywords = {Future of education, Education, OpenAI, ChatGPT, Management education, Programming skills},
abstract = {This study investigates the role of generative artificial intelligence (AI) chatbots, particularly ChatGPT, in enhancing programming education for university students, specifically in big data analytics. The research addresses the growing need for innovative educational practices, especially in developed East Asian countries like South Korea, where declining university enrollment presents new challenges. Using a sample size of N = 343 students, this mixed-methods research employed controlled experiments and surveys to compare student performance in programming tasks across three groups: those using ChatGPT, those using Stack Overflow, and a control group without external assistance. Results showed that students using ChatGPT significantly outperformed those relying on Stack Overflow or no assistance, particularly in hands-on coding tasks. This research contributes to the ongoing discourse on AI in education by providing empirical evidence of generative AI's effectiveness in improving learning outcomes and engagement, while also highlighting the challenges associated with integrating AI into educational settings. The findings emphasize the potential of ChatGPT to personalize learning experiences, improve performance, and offer real-time support, underscoring the need for a balanced curriculum design that incorporates AI while maintaining academic integrity and human oversight.}
}
@incollection{BRUDER2017101,
title = {Chapter 5 - Infrastructural intelligence: Contemporary entanglements between neuroscience and AI},
editor = {Tara Mahfoud and Sam McLean and Nikolas Rose},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {233},
pages = {101-128},
year = {2017},
booktitle = {Vital Models},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2017.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079612317300547},
author = {Johannes Bruder},
keywords = {Artificial intelligence, Computational neuroscience, Brain imaging, Google DeepMind technologies, Default mode network},
abstract = {In this chapter, I reflect on contemporary entanglements between artificial intelligence and the neurosciences by tracing the development of Google's recent DeepMind algorithms back to their roots in neuroscientific studies of episodic memory and imagination. Google promotes a new form of “infrastructural intelligence,” which excels by constantly reassessing its cognitive architecture in exchange with a cloud of data that surrounds it, and exhibits putatively human capacities such as intuition. I argue that such (re)alignments of biological and artificial intelligence have been enabled by a paradigmatic infrastructuralization of the brain in contemporary neuroscience. This infrastructuralization is based in methodologies that epistemically liken the brain to complex systems of an entirely different scale (i.e., global logistics) and has given rise to diverse research efforts that target the neuronal infrastructures of higher cognitive functions such as empathy and creativity. What is at stake in this process is no less than the shape of brains to come and a revised understanding of the intelligent and creative social subject.}
}
@article{XIE20189,
title = {Detecting leadership in peer-moderated online collaborative learning through text mining and social network analysis},
journal = {The Internet and Higher Education},
volume = {38},
pages = {9-17},
year = {2018},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1096751618300332},
author = {Kui Xie and Gennaro {Di Tosto} and Lin Lu and Young Suk Cho},
keywords = {Leadership, Computer-supported collaborative learning, Text mining, Social network analysis, Learning analytics, Online learning},
abstract = {Structured tasks and peer-moderated discussions are pedagogical models that have shown unique benefits for online collaborative learning. Students appointed with leadership roles are able to positively affect the dynamics in their groups by engaging with participants, raising questions, and advancing problem solving. To help monitoring and controlling the latent social dynamics associated with leadership behavior, we propose a methodological approach that makes use of computational techniques to mine the content of online communications and analyze group structure to identify students who behave as leaders. Through text mining and social network analysis, we systematically process the discussion posts made by students from four sections of an online course in an American university. The results allow us to quantify each individual's contribution and summarize their engagement in the form of a leadership index. The proposed methodology, when compared to judgements made by experts who manually coded samples of the data, is shown to have comparable performances, but, being fully automated, has the potential to be easily replicable. The summary offered by the leadership index is intended as actionable information that can guide just-in-time interventions together with other tools based on learning analytics.}
}
@article{BENZEKRY201553,
title = {Metronomic reloaded: Theoretical models bringing chemotherapy into the era of precision medicine},
journal = {Seminars in Cancer Biology},
volume = {35},
pages = {53-61},
year = {2015},
note = {Complexity in Cancer Biology},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X15000759},
author = {Sébastien Benzekry and Eddy Pasquier and Dominique Barbolosi and Bruno Lacarelle and Fabrice Barlési and Nicolas André and Joseph Ciccolini},
keywords = {Metronomic chemotherapy, Mathematical modeling, PK/PD, Precision medicine},
abstract = {Oncology has benefited from an increasingly growing number of groundbreaking innovations over the last decade. Targeted therapies, biotherapies, and the most recent immunotherapies all contribute to increase the number of therapeutic options for cancer patients. Consequently, substantial improvements in clinical outcomes for some disease with dismal prognosis such as lung carcinoma or melanoma have been achieved. Of note, the latest innovations in targeted therapies or biotherapies do not preclude the use of standard cytotoxic agents, mostly used in combination. Importantly, and despite the rise of bioguided (a.k.a. precision) medicine, the administration of chemotherapeutic agents still relies on the maximum tolerated drug (MTD) paradigm, a concept inherited from theories conceptualized nearly half a century ago. Alternative dosing schedules such as metronomic regimens, based upon the repeated and regular administration of low doses of chemotherapeutic drugs, and adaptive therapy (i.e. modulating the dose and frequency of cytotoxics administration to control disease progression rather than eradicate it at all cost) have emerged as possible strategies to improve response rates while reducing toxicities. The recent changes in paradigm in the way we theorize cancer biology and evolution, metastatic spreading and tumor ecology, alongside the recent advances in the field of immunotherapy, have considerably strengthened the interest for these alternative approaches. This paper aims at reviewing the recent evolutions in the field of theoretical biology of cancer and computational oncology, with a focus on the consequences these changes have on the way we administer chemotherapy. Here, we advocate for the development of model-guided strategies to refine doses and schedules of chemotherapy administration in order to achieve precision medicine in oncology.}
}
@incollection{ZEYER2015235,
title = {11 - For the mutual benefit: Health information provision in the science classroom},
editor = {Catherine {Arnott Smith} and Alla Keselman},
booktitle = {Meeting Health Information Needs Outside Of Healthcare},
publisher = {Chandos Publishing},
pages = {235-261},
year = {2015},
isbn = {978-0-08-100248-3},
doi = {https://doi.org/10.1016/B978-0-08-100248-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002483000111},
author = {Albert Zeyer and Daniel M. Levin and Alla Keselman},
keywords = {Health education, Health literacy, Information, Knowledge, Science education},
abstract = {In this chapter, the authors argue that the school science classroom should help students deal with complex real-life information about health and disease. They also discuss means by which curriculum and instruction in science education can be tied to these issues. The chapter reviews opportunities and challenges presented to individuals by the expectations of participatory health care, focusing on models of health literacy that can help understand and address the challenges. The authors argue that the problem of ensuring effective information use often lies in a transmission approach to health information provision. Transmitted knowledge is often not understood nor applied, as demonstrated in studies of human papillomavirus vaccination education. An alternative to knowledge transmission is the approach that aims to foster critical literacy, which is grounded in critical thinking essential to the practice of science. The chapter reviews a number of interdisciplinary science education activities that introduce health issues in the context of biology, physics, and chemistry education, ensuring deep understanding needed for developing critical literacy. It also discusses science education approaches and theories that encourage the development of deep, culturally meaningful science knowledge. Finally, the chapter reviews professional development and the role of various professionals, including teachers and librarians, in the collaborative endeavor of effective health information provision.}
}
@article{DELEON2021281,
title = {Assessing the Efficacy of Tier 2 Mathematics Intervention for Spanish Primary School Students},
journal = {Early Childhood Research Quarterly},
volume = {56},
pages = {281-293},
year = {2021},
issn = {0885-2006},
doi = {https://doi.org/10.1016/j.ecresq.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0885200621000508},
author = {Sara C. {de León} and Juan E. Jiménez and Nuria Gutiérrez and Juan Andrés Hernández-Cabrera},
keywords = {RtI model, math, early grades, Tier 2, at-risk},
abstract = {This study explored the efficacy of a Tier 2 intervention within the context of the Response to Intervention (RtI) model implemented by Spanish first- to third-grade primary school teachers to improve at-risk students’ early math skills. Teachers were instructed in the administration of a math curriculum-based measure composed of 5 isolated measures (quantity discrimination, missing number, single-digit computation, multidigit computation, and place value) to identify at-risk students and to monitor their progress; and in the implementation of a systematic and explicit instructional program to improve basic math skills in at-risk students. Implementation fidelity was analyzed using direct observations and self-reports. The intervention was conducted with adequate fidelity and had a significant positive impact on all grades. Significant differences were found between experimental and control students at risk of math failure in the improvement rate of quantity discrimination, missing number, and place value in all grades. Experimental at-risk students showed a monthly improvement, assessed using a combination of screening and progress monitoring measures. In conclusion, Spanish first to third graders at risk of math failure benefited from a Tier 2 intervention based on basic math skills, implemented by in-service teachers.}
}
@article{BOLAND2019195,
title = {The price of discretizing time: a study in service network design},
journal = {EURO Journal on Transportation and Logistics},
volume = {8},
number = {2},
pages = {195-216},
year = {2019},
note = {Special Issue: Advances in vehicle routing and logistics optimization: exact methods},
issn = {2192-4376},
doi = {https://doi.org/10.1007/s13676-018-0119-x},
url = {https://www.sciencedirect.com/science/article/pii/S2192437620300327},
author = {Natashia Boland and Mike Hewitt and Luke Marshall and Martin Savelsbergh},
keywords = {Service network design, Time-space network, Time expanded network, Approximation},
abstract = {Researchers and practitioners have long recognized that many transportation problems can be naturally and conveniently modeled using time-expanded networks. In such models, nodes represent locations at distinct points in time and arcs represent possible actions, e.g., moving from one location to another at a particular point of time, or staying in the same location for a period of time. To use a time-expanded network, time must be discretized, i.e., the planning horizon is partitioned into discrete time intervals. The length of these intervals, therefore, must be chosen, and the parameters involving time, e.g., travel duration and due times, need to be mapped to these discrete intervals. Short intervals yield a high-quality approximation to the continuous-time problem, but typically induce a computationally intractable model; whereas long intervals can yield a computationally tractable, but low-quality model. The loss of quality is due to the approximation introduced by the mapping of parameters involving time. To guide researchers and practitioners in their use of time-expanded networks, we explore the choice of time discretization and its impact, by means of an extensive computational study on the service network design problem. The empirical results show that in some cases the loss of quality, i.e., the relative gap between the discretized and continuous-time optimal values, can be greater than 20%. We also investigate metrics that characterize and help identify instances that are likely to be sensitive to discretization and could incur a large loss of solution quality.}
}
@incollection{NOVOTNY1996149,
title = {Computational Biochemistry of Antibodies and T-Cell Receptors},
editor = {Frederic M. Richards and David E. Eisenberg and Peter S. Kim},
series = {Advances in Protein Chemistry},
publisher = {Academic Press},
volume = {49},
pages = {149-260},
year = {1996},
booktitle = {Antigen Binding Molecules: Antibodies and T-cell Receptors},
issn = {0065-3233},
doi = {https://doi.org/10.1016/S0065-3233(08)60490-8},
url = {https://www.sciencedirect.com/science/article/pii/S0065323308604908},
author = {Jiri Novotny and Jürgen Bajorath}
}
@article{LIU2024100744,
title = {Application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode},
journal = {Entertainment Computing},
volume = {51},
pages = {100744},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100744},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001125},
author = {Shanshan Liu},
keywords = {Apriori algorithm, Entertainment E-learning model, Intelligent English teaching, Auxiliary reading mode},
abstract = {With the assistance of digital media, entertainment oriented E-learning models can effectively enhance students’ learning enthusiasm. This article analyzes the application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode. At present, English reading teaching faces some problems, such as outdated teaching methods, passive learning among students, excessive emphasis on imparting grammar knowledge while neglecting the improvement of students’ reading skills and strategies, and so on. Therefore, this article conducts research on intelligent English reading comprehension tools based on semantic analysis and Apriori algorithm. This paper proposes a recreational E-learning model based on Apriori algorithm. Based on Apriori algorithm, students’ interests and preferences on different learning resources and entertainment elements are mined and incorporated into the learning model design. Then, a set of entertaining English reading assistant model is designed, which uses a variety of entertainment elements, such as gamified learning, interactive activities and reward mechanism, to increase students’ learning participation and enthusiasm. This article adopts the idea of LSA algorithm to construct a BERT semantic analysis model. We treat nodes in the network as word items and then use singular value decomposition algorithm to decompose the word document matrix. Secondly, the original association rule Apriori algorithm was optimized, and the optimized association rule Apriori algorithm effectively solved the problem of excessive computation in traditional algorithms. Finally, based on semantic analysis and Apriori algorithm, this article designs an intelligent English reading comprehension tool, mainly analyzing the practical application of the system and greatly improving the efficiency of English reading teaching.}
}
@article{BARTON201242,
title = {Looking for the future in the past: Long-term change in socioecological systems},
journal = {Ecological Modelling},
volume = {241},
pages = {42-53},
year = {2012},
note = {Modeling Across Millennia: Interdisciplinary Paths to Ancient socio-ecological Systems},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2012.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0304380012000786},
author = {C. Michael Barton and Isaac I.T. Ullah and Sean M. Bergin and Helena Mitasova and Hessam Sarjoughian},
keywords = {Socio-ecological systems, Coupled modeling, Agent-based modeling, Surface process modeling, Simulation, Prehistoric Mediterranean, Archaeology, Agricultural land-use},
abstract = {The archaeological record has been described as a key to the long-term consequences of human action that can help guide our decisions today. Yet the sparse and incomplete nature of this record often makes it impossible to inferentially reconstruct past societies in sufficient detail for them to serve as more than very general cautionary tales of coupled socio-ecological systems. However, when formal and computational modeling is used to experimentally simulate human socioecological dynamics, the empirical archaeological record can be used to validate and improve dynamic models of long term change. In this way, knowledge generated by archaeology can play a unique and valuable role in developing the tools to make more informed decisions that will shape our future. The Mediterranean Landscape Dynamics project offers an example of using the past to develop and test computational models of interactions between land-use and landscape evolution that ultimately may help guide decision-making.}
}
@article{GUTIERREZORTIZ2022100164,
title = {Biofuel production from supercritical water gasification of sustainable biomass},
journal = {Energy Conversion and Management: X},
volume = {14},
pages = {100164},
year = {2022},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2021.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2590174521000891},
author = {F.J. {Gutiérrez Ortiz}},
keywords = {Supercritical water, Gasification, Biofuel, Hydrogen, Sustainability, Process simulation},
abstract = {A review of biofuel production from supercritical water gasification (SCWG) of sustainable biomass has been performed, mainly organic waste, following a critical thinking in this field of knowledge. Thus, sub- and super- critical water properties and hydrothermal processing are briefly commented on. Then, the feedstocks usable in SCWG are fully reviewed and a brief description of the studies on the kinetics and mechanisms of reactions is carried out. Next, thermodynamic and process simulation are discussed, aimed at producing liquid and gas biofuels. After that, a brief comment on the viability of SCWG processes to produce biofuels is provided based on techno-economic and lifecycle assessments. Finally, some remarks on where we are and where we should go are given in order to advance this technology towards its maturity. This review explains some misleading concepts applied to SCWG processes, provides a brief but comprehensive overview of the technology focused on producing biofuels in a sustainable way, allows a better understanding of the SCWG of biomass for biofuel production, and proposes a series of improvements to be made and examined in the future research.}
}
@article{SUCHOW2017522,
title = {Evolution in Mind: Evolutionary Dynamics, Cognitive Processes, and Bayesian Inference},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {522-530},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300773},
author = {Jordan W. Suchow and David D. Bourgin and Thomas L. Griffiths},
keywords = {Bayesian inference, cognitive processes, creativity, evolution, learning, memory},
abstract = {Evolutionary theory describes the dynamics of population change in settings affected by reproduction, selection, mutation, and drift. In the context of human cognition, evolutionary theory is most often invoked to explain the origins of capacities such as language, metacognition, and spatial reasoning, framing them as functional adaptations to an ancestral environment. However, evolutionary theory is useful for understanding the mind in a second way: as a mathematical framework for describing evolving populations of thoughts, ideas, and memories within a single mind. In fact, deep correspondences exist between the mathematics of evolution and of learning, with perhaps the deepest being an equivalence between certain evolutionary dynamics and Bayesian inference. This equivalence permits reinterpretation of evolutionary processes as algorithms for Bayesian inference and has relevance for understanding diverse cognitive capacities, including memory and creativity.}
}
@incollection{PINTARIC20162367,
title = {Towards Outcomes-Based Education of Computer-Aided Chemical Engineering},
editor = {Zdravko Kravanja and Miloš Bogataj},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {38},
pages = {2367-2372},
year = {2016},
booktitle = {26th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63428-3.50399-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634283503994},
author = {Zorka Novak Pintarič and Zdravko Kravanja},
keywords = {Computer-Aided, Chemical Engineering, Education, Bologna process, Learning Outcomes},
abstract = {Chemical engineering education is nowadays increasingly supported by the use of various computational tools as the employers’ requirements for computing skills of graduates are growing too. However, students often acquire computational skills in an unsystematic manner due to a lack of defining and applying computer-based outcomes within the syllabuses suitable for the particular level of the Bologna three-cycle system. This paper bridges this gap by providing the review of the essential learning outcomes in the computer-aided chemical engineering education during all three cycles. The identified outcomes gradually progress from application-based competencies up to more advanced process modeling ones based on knowledge synthesis and creation. Accordingly, the educational strategies and curricula can be redesigned in order to integrate courses more efficiently both horizontally and vertically, and upgrade the use of computational tools.}
}
@article{YURKOVICH2017431,
title = {A Padawan Programmer’s Guide to Developing Software Libraries},
journal = {Cell Systems},
volume = {5},
number = {5},
pages = {431-437},
year = {2017},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2405471217303368},
author = {James T. Yurkovich and Benjamin J. Yurkovich and Andreas Dräger and Bernhard O. Palsson and Zachary A. King},
abstract = {With the rapid adoption of computational tools in the life sciences, scientists are taking on the challenge of developing their own software libraries and releasing them for public use. This trend is being accelerated by popular technologies and platforms, such as GitHub, Jupyter, R/Shiny, that make it easier to develop scientific software and by open-source licenses that make it easier to release software. But how do you build a software library that people will use? And what characteristics do the best libraries have that make them enduringly popular? Here, we provide a reference guide, based on our own experiences, for developing software libraries along with real-world examples to help provide context for scientists who are learning about these concepts for the first time. While we can only scratch the surface of these topics, we hope that this article will act as a guide for scientists who want to write great software that is built to last.}
}
@article{LEWTON202538,
title = {“Now is the time to realise useful autonomous quantum machines”},
journal = {New Scientist},
volume = {265},
number = {3534},
pages = {38-41},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00433-6},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925004336},
author = {Thomas Lewton},
abstract = {Quantum technology is still in its infancy, says Nicole Yunger Halpern. But, she tells Thomas Lewton, she intends to change that}
}
@article{JIANG2024100795,
title = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
journal = {Progress in Planning},
volume = {180},
pages = {100795},
year = {2024},
note = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
issn = {0305-9006},
doi = {https://doi.org/10.1016/j.progress.2023.100795},
url = {https://www.sciencedirect.com/science/article/pii/S0305900623000569},
author = {Feifeng Jiang and Jun Ma and Christopher John Webster and Alain J.F. Chiaradia and Yulun Zhou and Zhan Zhao and Xiaohu Zhang},
keywords = {Generative urban design, Urban form generation, Generative method, AI-generated content (AIGC), Generative AI, Human-machine collaboration},
abstract = {Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.}
}
@article{MCDANIELS200333,
title = {Decision structuring to alleviate embedding in environmental valuation},
journal = {Ecological Economics},
volume = {46},
number = {1},
pages = {33-46},
year = {2003},
issn = {0921-8009},
doi = {https://doi.org/10.1016/S0921-8009(03)00103-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921800903001034},
author = {Timothy L McDaniels and Robin Gregory and Joseph Arvai and Ratana Chuenpagdee},
keywords = {Embedding, Structured decision process, Environmental valuation, Value-focused thinking, Small group process, Fisheries valuation},
abstract = {Embedding is the widely-observed phenomenon that a good is assigned a higher value if evaluated on its own rather than as part of a more inclusive set. Embedding is considered a serious problem affecting the quality of many environmental management and health risk policy judgments. This paper presents the results of an experiment involving of a structured, small-group approach for conducting environmental policy evaluations. It focuses on eliciting problem-specific values and discussion among participants about the pros and cons of multiple project alternatives, in the context of tradeoffs between fisheries production and electricity generation from dams. Study results show a significant reduction in embedding, which is viewed as an improvement in the quality of the preference judgments compared with a standard contingent valuation (CV) approach.}
}
@incollection{SNOWDEN2015572,
title = {Semantic Memory},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {572-578},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.51059-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868510599},
author = {Julie S. Snowden},
keywords = {Amodal, Anterior temporal lobe, Brain networks, Conceptual knowledge, Distributed representations, Multimodal, Object knowledge, Schema, Semantic dementia, Semantic features, Semantic memory, Word knowledge},
abstract = {Semantic memory refers to our conceptual knowledge of the world. Understanding of semantic memory has come from several sources: cognitive studies of healthy individuals, computational modeling, patients with disordered semantic memory due to brain disease, and brain imaging and stimulation. The converging evidence indicates that semantic memory involves distributed brain networks, which, at least in part, are linked to the sensory processes involved in perception, action, and language. Whether there is also representation in amodal format remains an area of contention. Knowledge of the world, beyond word and object meanings, is a challenge for future studies of semantic memory.}
}
@article{HART2023182,
title = {Riders on a storm – The evaluation and control of creative processes A comment on: “A systematic framework of creative metacognition” by Izabela Lebuda and Mathias Benedek},
journal = {Physics of Life Reviews},
volume = {47},
pages = {182-183},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001689},
author = {Yuval Hart}
}
@article{WOLFF20124051,
title = {Constraints in the generation of photonic Wannier functions},
journal = {Physica B: Condensed Matter},
volume = {407},
number = {20},
pages = {4051-4055},
year = {2012},
note = {Proceedings of the conference - Wave Propagation: From Electrons to Photonic Crystals and Metamaterials},
issn = {0921-4526},
doi = {https://doi.org/10.1016/j.physb.2012.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0921452612002554},
author = {Christian Wolff and Kurt Busch},
keywords = {Photonic Crystals, Wannier functions},
abstract = {We report on the generation of maximally localized photonic Wannier functions under constraints. This allows us to impress certain symmetry properties of the underlying Photonic Crystal onto the Wannier functions. This added flexibility enhances the utility of the Wannier function approach to Photonic Crystal circuits by providing deeper physical insight and making computations more efficient.}
}
@article{CAI2024133949,
title = {A state-of-the-art review of solar-induced ventilation technology for built environment regulation: Classification, modeling, evaluation, potential and challenges},
journal = {Energy},
volume = {313},
pages = {133949},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.133949},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224037277},
author = {Yang Cai and Zheng-Yu Shu and Jian-Wei He and Yong-Cai Li and Yuan-Da Cheng and Kai-Liang Huang and Fu-Yun Zhao},
keywords = {Solar-induced ventilation technology, Built environment regulation, Numerical model, Thermal characteristic, Performance evaluation},
abstract = {In the face of escalating environmental challenges and dwindling fossil fuel reserves, the transition to renewable and sustainable energy sources has become a paramount global objective, which has led to a surge in research and application of renewable energy sources. Among them, solar energy utilization has been placed at the forefront of energy conservation revolution owing to its significant advantages in terms of sustainability and environmentally-friendliness. Solar-induced ventilation technology (SVT) is a typical way to integrate clean energy with buildings, considerably enhancing solar energy utilization efficiency while achieving building energy conservation and indoor thermal environment regulation. However, summaries as comprehensive as possible for SVT's application in envelopes are ambiguous in the current academia. Different analytical models, parameters and evaluation indicators need to be reviewed to describe the energy flow transfer and the impact on indoor thermal environment, which makes it indispensable to carry out an comprehensive overview for the latest investigation progress. This article endeavors to carry out an elaborate review of the theoretical analysis and constructive application of SVT from an energy utilization and building thermal environment perspective. Firstly, various types of SVT envelopes are classified simultaneously according to development and innovation in solar energy utilization. Furthermore, four different analytical models, namely, heat transfer model, thermal resistance network model, pressure balance model as well as computational fluid dynamics model, have been summarized, which would be helpful to analyze the thermal performance. Through literature review, this article discusses the impact of numerous parameters on system performance, especially the ventilation effect and thermal environment in buildings, from aspects of geometry, material properties and environmental conditions. In addition, a comprehensive collection of the important evaluation indicators based on the energy, thermal comfort and economic evaluations has been introduced to evaluate the thermal performance and indoor environment regulation capability of SVT envelopes, which provided a clear reference on developing and application SVT for high energy efficiency design towards carbon-neutral building envelopes. Finally, the challenges and potential are pointed out in terms of performance enhancement and the expansion of application scenarios. The results of the survey indicated that due to the development of novel technologies and materials, SVT holds great advantages in mitigating building energy consumption and regulating thermal environment, which shows a diversified development trend and promotes the process of global sustainable development. The review of the current SVT building envelope not only clarified the high feasibility of SVT in promoting passive building ventilation, energy saving and enhancing the level of indoor thermal environments, but also provided guidance and identifies the direction of optimization for cutting-edge research.}
}
@article{WONG2021105042,
title = {Empathic accuracy of young boys and girls in ongoing parent–child interactions: Performance and (mis)perception},
journal = {Journal of Experimental Child Psychology},
volume = {203},
pages = {105042},
year = {2021},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2020.105042},
url = {https://www.sciencedirect.com/science/article/pii/S0022096520304963},
author = {Wang Ivy Wong and Wai Bong Patrick Tsui and Tik-Sze Carrey Siu},
keywords = {Interpersonal interaction, Performance estimation, Social cognition, Gender, Empathic accuracy, Parent and child},
abstract = {Understanding others accurately is crucial in relationships and learning. Research shows that adults face challenges in empathic accuracy, that is, the ability to read the content of others’ moment-to-moment mental states during interactions. Although young children possess some empathic understanding, the extent of their empathic accuracy is uncharted. Using a new SSP, 106 Chinese children aged 60 to 80 months (M = 70 months) were assessed on their ability to infer the mental states of adults in ongoing parent–child interactions. Replicating and extending extant findings on adults and adolescents, the children’s inferences were found to be, at least computationally on a scale of .00 to 1.00, more often inaccurate than accurate regardless of the gender of the targets or participants (overall accuracy rate = .28). However, both the children and their primary caregivers overestimated the children’s performance. In addition, although the primary caregivers expected girls to outperform boys, no gender difference in empathic accuracy was found when controlling for verbal fluency. Drawing on the findings of this first-ever application of the empathic accuracy paradigm in young children, the implications of empathic accuracy performance and misperceptions about such accuracy are discussed.}
}
@article{WITTKUHN2021367,
title = {Replay in minds and machines},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {129},
pages = {367-388},
year = {2021},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0149763421003444},
author = {Lennart Wittkuhn and Samson Chien and Sam Hall-McMaster and Nicolas W. Schuck},
keywords = {Replay, Reinforcement learning, Machine learning, Representation learning, Decision-making},
abstract = {Experience-related brain activity patterns reactivate during sleep, wakeful rest, and brief pauses from active behavior. In parallel, machine learning research has found that experience replay can lead to substantial performance improvements in artificial agents. Together, these lines of research suggest that replay has a variety of computational benefits for decision-making and learning. Here, we provide an overview of putative computational functions of replay as suggested by machine learning and neuroscientific research. We show that replay can lead to faster learning, less forgetting, reorganization or augmentation of experiences, and support planning and generalization. In addition, we highlight the benefits of reactivating abstracted internal representations rather than veridical memories, and discuss how replay could provide a mechanism to build internal representations that improve learning and decision-making.}
}
@article{PECIS2025100573,
title = {In blockchain we trust: Ideologies and discourses sustaining trust in bitcoin},
journal = {Information and Organization},
volume = {35},
number = {2},
pages = {100573},
year = {2025},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2025.100573},
url = {https://www.sciencedirect.com/science/article/pii/S1471772725000193},
author = {Lara Pecis and Lucia Cervi and Lucas Introna},
keywords = {Bitcoin, Blockchain, Trust, Ideology, Discourse, Critical discourse analysis},
abstract = {In this paper, we examine the discourses and ideologies that underpin trust in Bitcoin (BTC) as an algorithm-driven socio-technical system, raising critical questions about how trust is established and sustained in complex socio-technical assemblages. Through a Critical Discourse Analysis (CDA) of three significant events in the cryptocurrency, we identify two interconnected, yet sometimes contradictory, ideologies enacted through four discourses that construct specific subject positions to produce and maintain trust in Bitcoin. The first, technical sovereignty, reflects adherence to notions of technical utopianism. The second, which we term peer-to-peer neoliberalism, frames BTC as a political experiment rooted in the individualization of responsibility and risk. Our paper contributes to the existing literature by arguing that algorithm-driven technologies like BTC neither establish trust solely through their apparent technical neutrality and security nor simply replace traditional institutional mechanisms of governance, control, and interaction. Instead, they are enacted through discourses and material arrangements that require continuous maintenance. This maintenance relies on power relations enabled by these ideologies yet remains contingent upon the ongoing reinforcement of the ideologies themselves—rendering trust inherently precarious and always at risk. This insight shifts the analytical focus from the dominant emphasis in the literature on technical features, social arrangements, and user perceptions to the underlying ideological frameworks that shape these elements, as such.}
}