@article{LI2021512,
title = {Design and implementation of neural network computing framework on Zynq SoC embedded platform},
journal = {Procedia Computer Science},
volume = {183},
pages = {512-518},
year = {2021},
note = {Proceedings of the 10th International Conference of Information and Communication Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.02.091},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921005676},
author = {Xingying Li and Zhenyu Yin and Fulong Xu and Feiqing Zhang and Guangyuan Xu},
keywords = {Neural network, embedded platform, Zynq SoC, darknet, depthwise separable convolution, MobileNetV2},
abstract = {Limited resources and low computing power of embedded platform make it difficult to apply neural network technology. To overcome this problem, a new neural network computing framework “Zynq-Darknet” was proposed. The framework is based on Darknet, which constructs depthwise separable convolution and a lightweight classiﬁcation model MobileNetV2 and was deployed to Xilinx Zynq-7000 System-on-Chip (SoC) with Linux operating system (OS). In order to verify the performance of the framework and model, experiments were conducted on imagenet-1k dataset using different network structures. The results show that the MobileNetV2 network model based on Zynq-Darknet can effectively perform image classification, and ensure a certain real-time and accuracy while reducing the computational complexity and storage overhead, assuming promising application prospects.}
}
@incollection{WANG20249,
title = {1.02 - Artificial Intelligence and Bioinformatics Applications in Precision Medicine and Future Implications},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {9-24},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00058-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000587},
author = {Ni Wang and Qiang He},
keywords = {Artificial intelligence, Bioinformatics, Clinical trials, Disease risk assessment, Drug discovery, Early disease detection, Genome sequencing, Oncology, Personalized medicine, Pharmacogenomics, Precision medicine},
abstract = {Artificial intelligence (AI) and bioinformatics have emerged as key technologies for advancing precision medicine. Many tools of AI and bioinformatics have been applied in healthcare that would be of great value to advance the goals of precision medicine. AI is a branch of computer science that deals with the automation of intelligent behavior. Bioinformatics is a field of study that combines biology, computer science, and statistics to analyze and interpret biologic data. The latter involves the development and application of computational methods and tools for storing, organizing, analyzing, and interpreting biologic information, including genomic, proteomic, and metabolomic data. AI and bioinformatics are well-positioned to help tailor medical decisions and treatments at the individual and population levels. Some of those applications and the multidisciplinary implications are presented in this chapter.}
}
@article{FRANZ1994433,
title = {A critical framework for methodological research in architecture},
journal = {Design Studies},
volume = {15},
number = {4},
pages = {433-447},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90006-X},
url = {https://www.sciencedirect.com/science/article/pii/0142694X9490006X},
author = {Jill M. Franz},
keywords = {methodological research, critical framework, architecture, design},
abstract = {This paper reviews a cross-section of methodological studies undertaken in architecture since the Second World War. Despite a variety of orientations, technically, conceptually and philosophically, most studies reflect an understanding of people and objects as discrete entities interacting in an passive and unilateral manner. This dominant dualist understanding is concluded to be the essential cause of the ‘implementation gap' between architectural research and practice. For the gap to close, the development and institution of a critical framework is needed which encourages researchers to acknowledge explicitly the ontological and epistomological issues associated with architectural practice, education and research. Underlying this recommendation is a dialectic appreciation of person-world interaction; one which accepts as a holistic theme for inquiry, the experiential and interpretative quality of human thinking, feeling and action.}
}
@article{SHU2025121176,
title = {Altered brain network dynamics during rumination in remitted depression},
journal = {NeuroImage},
volume = {310},
pages = {121176},
year = {2025},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2025.121176},
url = {https://www.sciencedirect.com/science/article/pii/S1053811925001788},
author = {Su Shu and Wenwen Ou and Mohan Ma and Hairuo He and Qianqian Zhang and Mei Huang and Wentao Chen and Aoqian Deng and Kangning Li and Zhenman Xi and Fanyu Meng and Hui Liang and Sirui Gao and Yilin Peng and Mei Liao and Li Zhang and Mi Wang and Jin Liu and Bangshan Liu and Yumeng Ju and Yan Zhang},
keywords = {Rumination, Energy landscape, Brain state, Large-scale network, Depression},
abstract = {Rumination is a known risk factor for depression relapse. Understanding its neurobiological mechanisms during depression remission can inform strategies to prevent relapse, yet the temporal dynamics of brain networks during rumination in remitted depression remain unclear. Here, we collected rumination induction fMRI data from 42 patients with remitted depression and 41 healthy controls (HCs). Using an energy landscape approach, we investigated the temporal dynamics of brain networks during rumination. The appearance frequency (AF) and transition frequency (TF) metrics were defined to quantify the dynamic properties of brain states. Patients during remission showed higher levels of rumination than HCs. Both groups exhibited four brain states during rumination, which consisted of complementary network group activation (states 1 and 2, states 3 and 4). In patients, the AFs of and reciprocal TFs between states 1 and 2 during rumination were significantly increased, while AFs of states 3 and 4 and reciprocal TFs involving states 1–3, 1–4, 2–3, and 2–4 were decreased, both when compared to HCs and relative to patients themselves during distraction. Moreover, we found that for patients, the AF of state 1 was negatively correlated with rumination levels and marginally positively associated with attention, while the AF of state 2 was negatively associated with performance on attention tasks. Our study revealed altered dynamic characteristics of brain states composed of network groups during rumination in remitted depression. Additionally, the findings suggest that heightened self-focus linked to rumination may impair the brain's ability to efficiently allocate attentional resources.}
}
@article{BALE2015150,
title = {Energy and complexity: New ways forward},
journal = {Applied Energy},
volume = {138},
pages = {150-159},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2014.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S0306261914011076},
author = {Catherine S.E. Bale and Liz Varga and Timothy J. Foxon},
keywords = {Complexity science, Energy systems, Modelling, Complex adaptive systems, Agent-based modelling, Energy policy},
abstract = {The purpose of this paper is to review the application of complexity science methods in understanding energy systems and system change. The challenge of moving to sustainable energy systems which provide secure, affordable and low-carbon energy services requires the application of methods which recognise the complexity of energy systems in relation to social, technological, economic and environmental aspects. Energy systems consist of many actors, interacting through networks, leading to emergent properties and adaptive and learning processes. Insights on these type of phenomena have been investigated in other contexts by complex systems theory. However, these insights are only recently beginning to be applied to understanding energy systems and systems transitions. The paper discusses the aspects of energy systems (in terms of technologies, ecosystems, users, institutions, business models) that lend themselves to the application of complexity science and its characteristics of emergence and coevolution. Complex-systems modelling differs from standard (e.g. economic) modelling and offers capabilities beyond those of conventional models, yet these methods are only beginning to realize anything like their full potential to address the most critical energy challenges. In particular there is significant potential for progress in understanding those challenges that reside at the interface of technology and behaviour. Some of the computational methods that are currently available are reviewed: agent-based and network modelling. The advantages and limitations of these modelling techniques are discussed. Finally, the paper considers the emerging themes of transport, energy behaviour and physical infrastructure systems in recent research from complex-systems energy modelling. Although complexity science is not well understood by practitioners in the energy domain (and is often difficult to communicate), models can be used to aid decision-making at multiple levels e.g. national and local, and to aid understanding and allow decision making. The techniques and tools of complexity science, therefore, offer a powerful means of understanding the complex decision-making processes that are needed to realise a low-carbon energy system. We conclude with recommendations for future areas of research and application.}
}
@article{HEIRDSFIELD200257,
title = {Flexibility and inflexibility in accurate mental addition and subtraction: two case studies},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {1},
pages = {57-74},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00103-7},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001037},
author = {Ann M Heirdsfield and Tom J Cooper},
keywords = {Mental computation, Addition, Subtraction, Accuracy},
abstract = {This paper reports on a study of two children’s mental computation in addition and subtraction, and compares their mental architecture. Both students were identified as being accurate, however, one student used a variety of mental strategies (was flexible) while the other student used only one strategy that reflected the written procedure for each of the addition and subtraction algorithms taught in the classroom. Interviews were used to identify both children’s knowledge and ability with respect to number sense (including numeration, number and operations, basic facts, estimation), metacognition and affects. Frameworks were developed to show how these factors interacted to explain the two types of accuracy in mental addition and subtraction. Flexible accuracy was related to the presence of strong number sense knowledge integrated with metacognitive strategies and beliefs and beliefs about self and teaching; while inflexible accuracy was a result of compensation of inadequate knowledge supported by beliefs about self and teaching.}
}
@article{DESAFERREIRA2013135,
title = {Promoting integrative medicine by computerization of traditional Chinese medicine for scientific research and clinical practice: The SuiteTCM Project},
journal = {Journal of Integrative Medicine},
volume = {11},
number = {2},
pages = {135-139},
year = {2013},
issn = {2095-4964},
doi = {https://doi.org/10.3736/jintegrmed2013013},
url = {https://www.sciencedirect.com/science/article/pii/S2095496414601096},
author = {Arthur {de Sá Ferreira}},
keywords = {traditional Chinese medicine, evidence-based practice, computer-assisted decision making},
abstract = {Background
Chinese and contemporary Western medical practices evolved on different cultures and historical contexts and, therefore, their medical knowledge represents this cultural divergence. Computerization of traditional Chinese medicine (TCM) is being used to promote the integrative medicine to manage, process and integrate the knowledge related to TCM anatomy, physiology, semiology, pathophysiology, and therapy.
Methods
We proposed the development of the SuiteTCM software, a collection of integrated computational models mainly derived from epidemiology and statistical sciences for computerization of Chinese medicine scientific research and clinical practice in all levels of prevention. The software includes components for data management (DataTCM), simulation of cases (SimTCM), analyses and validation of datasets (SciTCM), clinical examination and pattern differentiation (DiagTCM, TongueTCM, and PulseTCM), intervention selection (AcuTCM, HerbsTCM, and DietTCM), management of medical records (ProntTCM), epidemiologic investigation of sampled data (ResearchTCM), and medical education, training, and assessment (StudentTCM).
Discussion
The SuiteTCM project is expected to contribute to the ongoing development of integrative medicine and the applicability of TCM in worldwide scientific research and health care. The SuiteTCM 1.0 runs on Windows XP or later and is freely available for download as an executable application.}
}
@article{MEHREGAN2012426,
title = {An application of Soft System Methodology},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {426-433},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009317},
author = {M. Reza Mehregan and Mahnaz Hosseinzadeh and Aliyeh Kazemi},
keywords = {Soft System Methodology (SSM), University course timetabling, rich picture, conceptual model},
abstract = {The typical course timetabling problem is assigning Classes of students to appropriate faculty members, suitable classrooms and available timeslots. Hence, it involves a large number of stakeholders including students, teachers and institutional administrators. Different kinds of Hard Operational Research techniques have been employed over the years to address such problems. Due to the computational difficulties of this NP complete problem as well as the size and the complexity of the real world instances, an efficient optimal solution cannot be found easily.As an alternative strategy, this paper investigates the application of Checkland‘s Soft System Methodology (SSM) to the course timetabling problem. Besides giving an ideal course timetable, even to large and complex real problems, application of SSM, generates debate, learning, and understanding; enables key changes; facilitates negotiating the actions to be taken and makes possible the meaningful collaboration among concerned stakeholders. This paper also provides an appropriate course timetable for the management faculty at University of Tehran to show the potential of this application to real problems.}
}
@article{GERO2001283,
title = {The differences between retrospective and concurrent protocols in revealing the process-oriented aspects of the design process},
journal = {Design Studies},
volume = {22},
number = {3},
pages = {283-295},
year = {2001},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(00)00030-2},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X00000302},
author = {John S. Gero and Hsien-Hui Tang},
keywords = {design cognition, protocol studies, case study},
abstract = {This paper presents the results of studying a single designer using protocol analyses and examines the implications of the results on studies of design thinking. It contrasts two types of protocols: concurrent protocols and retrospective protocols. The results indicate that concurrent and retrospective protocols both produce very similar outcomes in terms of exploring the process-oriented aspects of designing. As a result, it is argued there is no associated interference with the ongoing design process when using concurrent protocols.}
}
@article{CHONG2024103352,
title = {Integrable approximations of dispersive shock waves of the granular chain},
journal = {Wave Motion},
volume = {130},
pages = {103352},
year = {2024},
issn = {0165-2125},
doi = {https://doi.org/10.1016/j.wavemoti.2024.103352},
url = {https://www.sciencedirect.com/science/article/pii/S0165212524000829},
author = {Christopher Chong and Ari Geisler and Panayotis G. Kevrekidis and Gino Biondini},
abstract = {In the present work we revisit the shock wave dynamics in a granular chain with precompression. By approximating the model by an α-Fermi–Pasta–Ulam–Tsingou chain, we leverage the connection of the latter in the strain variable formulation to two separate integrable models, one continuum, namely the KdV equation, and one discrete, namely the Toda lattice. We bring to bear the Whitham modulation theory analysis of such integrable systems and the analytical approximation of their dispersive shock waves in order to provide, through the lens of the reductive connection to the granular crystal, an approximation to the shock wave of the granular problem. A detailed numerical comparison of the original granular chain and its approximate integrable-system-based dispersive shocks proves very favorable in a wide parametric range. The gradual deviations between (approximate) theory and numerical computation, as amplitude parameters of the solution increase are quantified and discussed.}
}
@article{HURST2024105918,
title = {Continuous and discrete proportion elicit different cognitive strategies},
journal = {Cognition},
volume = {252},
pages = {105918},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105918},
url = {https://www.sciencedirect.com/science/article/pii/S001002772400204X},
author = {Michelle A. Hurst and Steven T. Piantadosi},
keywords = {Proportion, Strategy, Bayesian analysis, Model comparison},
abstract = {Despite proportional information being ubiquitous, there is not a standard account of proportional reasoning. Part of the difficulty is that there are several apparent contradictions: in some contexts, proportion is easy and privileged, while in others it is difficult and ignored. One possibility is that although we see similarities across tasks requiring proportional reasoning, people approach them with different strategies. We test this hypothesis by implementing strategies computationally and quantitatively comparing them with Bayesian tools, using data from continuous (e.g., pie chart) and discrete (e.g., dots) stimuli and preschoolers, 2nd and 5th graders, and adults. Overall, people's comparisons of highly regular and continuous proportion are better fit by proportion strategy models, but comparisons of discrete proportion are better fit by a numerator comparison model. These systematic differences in strategies suggest that there is not a single, simple explanation for behavior in terms of success or failure, but rather a variety of possible strategies that may be chosen in different contexts.}
}
@article{MEJIA20113964,
title = {On the complexity of sandpile critical avalanches},
journal = {Theoretical Computer Science},
volume = {412},
number = {30},
pages = {3964-3974},
year = {2011},
note = {Cellular Automata and Discrete Complex Systems},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2011.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0304397511001496},
author = {Carolina Mejia and J. {Andres Montoya}},
keywords = {Abelian sandpile model, Self-organized criticality,  complete problems, Parallel algorithms},
abstract = {In this work, we study The Abelian Sandpile Model from the point of view of computational complexity. We begin by studying the length distribution of sandpile avalanches triggered by the addition of two critical configurations: we prove that those avalanches are long on average, their length is bounded below by a constant fraction of the length of the longest critical avalanche which is, in most of the cases, superlinear. At the end of the paper we take the point of view of computational complexity, we analyze the algorithmic hardness of the problem consisting in computing the addition of two critical configurations, we prove that this problem is P complete, and we prove that most algorithmic problems related to The Abelian Sandpile Model are NC reducible to it.}
}
@article{HOFFMAN202472,
title = {AI’s impact on war’s enduring nature},
journal = {Orbis},
volume = {68},
number = {1},
pages = {72-91},
year = {2024},
issn = {0030-4387},
doi = {https://doi.org/10.1016/j.orbis.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0030438723000583},
author = {Frank Hoffman and Axel D'Amelio},
abstract = {This article reassesses the impact of Artificial Intelligence on war and revisits an article published in 2018 by one of the authors in Orbis. Despite the remarkable progress in generative AI, the authors contend that war’s essential nature will be impacted to a degree but will not be substantially altered.}
}
@article{MERIVAARA2021480,
title = {Preservation of biomaterials and cells by freeze-drying: Change of paradigm},
journal = {Journal of Controlled Release},
volume = {336},
pages = {480-498},
year = {2021},
issn = {0168-3659},
doi = {https://doi.org/10.1016/j.jconrel.2021.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S0168365921003400},
author = {Arto Merivaara and Jacopo Zini and Elle Koivunotko and Sami Valkonen and Ossi Korhonen and Francisco M. Fernandes and Marjo Yliperttula},
keywords = {Freeze-drying, Cells, Extracellular vesicles, Process analytical technology, Quality-by-design, Biophotonics},
abstract = {Freeze-drying is the most widespread method to preserve protein drugs and vaccines in a dry form facilitating their storage and transportation without the laborious and expensive cold chain. Extending this method for the preservation of natural biomaterials and cells in a dry form would provide similar benefits, but most results in the domain are still below expectations. In this review, rather than consider freeze-drying as a traditional black box we “break it” through a detailed process thinking approach. We discuss freeze-drying from process thinking aspects, introduce the chemical, physical, and mechanical environments important in this process, and present advanced biophotonic process analytical technology. In the end, we review the state of the art in the freeze-drying of the biomaterials, extracellular vesicles, and cells. We suggest that the rational design of the experiment and implementation of advanced biophotonic tools are required to successfully preserve the natural biomaterials and cells by freeze-drying. We discuss this change of paradigm with existing literature and elaborate on our perspective based on our new unpublished results.}
}
@article{PIHLAJAMAKI2020101103,
title = {Subjective cognitive complaints and sickness absence: A prospective cohort study of 7059 employees in primarily knowledge-intensive occupations},
journal = {Preventive Medicine Reports},
volume = {19},
pages = {101103},
year = {2020},
issn = {2211-3355},
doi = {https://doi.org/10.1016/j.pmedr.2020.101103},
url = {https://www.sciencedirect.com/science/article/pii/S2211335520300632},
author = {Minna Pihlajamäki and Heikki Arola and Heini Ahveninen and Jyrki Ollikainen and Mikko Korhonen and Tapio Nummi and Jukka Uitti and Simo Taimela},
keywords = {Subjective cognitive complaints, Screening questionnaire, Occupational healthcare, Self-reported data, Sickness allowance, Register data},
abstract = {Knowledge-intensive work requires capabilities like monitoring multiple sources of information, prioritizing between competing tasks, switching between tasks, and resisting distraction from the primary task(s). We assessed whether subjective cognitive complaints (SCC), presenting as self-rated problems with difficulties of concentration, memory, clear thinking and decision making predict sickness absence (SA) in knowledge-intensive occupations. We combined SCC questionnaire results with reliable registry data on SA of 7743 professional/managerial employees (47% female). We excluded employees who were not active in working life, on long-term SA, and those on a work disability benefit at baseline. The exposure variable was the presence of SCC. Age and SA before the questionnaire as a proxy measure of general health were treated as confounders and the analyses were conducted by gender. The outcome measure was the accumulated SA days during a 12-month follow-up. We used a hurdle model to analyse the SA data. SCC predicted the number of SA days during the 12-month follow-up. The ratio of the means of SA days was higher than 2.8 as compared to the reference group, irrespective of gender, with the lowest limit of 95% confidence interval 2.2. In the Hurdle model, SCC, SA days prior to the questionnaire, and age were additive predictors of the likelihood of SA and accumulated SA days, if any. Subjective cognitive complaints predict sickness absence in knowledge-intensive occupations, irrespective of gender, age, or general health. This finding has implications for supporting work ability (productivity) among employees with cognitively demanding tasks.}
}
@incollection{MAXWELL19961,
title = {Driving forces for innovation in applied catalysis},
editor = {Joe W. Hightower and W. {Nicholas Delgass} and Enrique Iglesia and Alexis T. Bell},
series = {Studies in Surface Science and Catalysis},
publisher = {Elsevier},
volume = {101},
pages = {1-9},
year = {1996},
booktitle = {11th International Congress On Catalysis - 40th Anniversary},
issn = {0167-2991},
doi = {https://doi.org/10.1016/S0167-2991(96)80210-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167299196802102},
author = {Ian E. Maxwell},
abstract = {Publisher Summary
Catalytic environmental technologies such as automobile exhaust catalysts and the selective catalytic reduction (SCR) DeNOx systems in power plants have significantly contributed to the reduction of environmentally harmful emissions into the lower atmosphere. Some studies have identified catalysis as not only being pervasive but also offering significant scope for the innovative development of new and improved technologies for environmentally acceptable processes and products in the future. The spectrum of process industries that are directly impacted by catalysis include, for example, oil refining, natural gas conversion, petrochemicals, fine chemicals, and pharmaceuticals. Environmental catalytic technologies also play an important role in emission control systems for power generation, fossil fuel driven transportation, oil refining, and chemical industries. Catalytic technologies typically embrace a wide range of disciplines, such as heterogeneous and homogeneous catalysis, materials science, process technology, reactor engineering, separation technology, surface science, computational chemistry, and analytic chemistry. Innovation in this field is, therefore, often achieved by lateral thinking across these different disciplines. This chapter attempts to develop this theme by means of examples from recent commercial successes and from this platform provides some guidelines for multi-disciplinary approaches at the academic and industrial interface to enhanced innovation in catalytic technologies in the future.}
}
@article{FESSAKIS201387,
title = {Problem solving by 5–6 years old kindergarten children in a computer programming environment: A case study},
journal = {Computers & Education},
volume = {63},
pages = {87-97},
year = {2013},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512002813},
author = {G. Fessakis and E. Gouli and E. Mavroudi},
keywords = {Programming and programming languages, Kindergarten, Improving classroom teaching, Teaching/learning strategies},
abstract = {Computer programming is considered an important competence for the development of higher-order thinking in addition to algorithmic problem solving skills. Its horizontal integration throughout all educational levels is considered worthwhile and attracts the attention of researchers. Towards this direction, an exploratory case study is presented concerning dimensions of problem solving using computer programming by 5–6 years old kindergarten children. After a short introductory experiential game the children were involved in solving a series of analogous computer programming problems, using a Logo-based environment on an Interactive White Board. The intervention was designed as a part of the structured learning activities of the kindergarten which are teacher-guided and are conducted in a whole-class social mode. The observation of the video recording of the intervention along with the analysis of teacher's interview and the researcher's notes allow for a realistic evaluation of the feasibility, the appropriateness and the learning value of integrating computer programming in such a context. The research evidence supports the view that children enjoyed the engaging learning activities and had opportunities to develop mathematical concepts, problem solving and social skills. Interesting results about children learning, difficulties, interactions, problem solving strategies and the teacher's role are reported. The study also provides proposals for the design of future research.}
}
@article{BALL2024,
title = {Trust but Verify: Lessons Learned for the Application of AI to Case-Based Clinical Decision-Making From Postmarketing Drug Safety Assessment at the US Food and Drug Administration},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/50274},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124003078},
author = {Robert Ball and Andrew H Talal and Oanh Dang and Monica Muñoz and Marianthi Markatou},
keywords = {drug safety, artificial intelligence, machine learning, natural language processing, causal inference, case-based reasoning, clinical decision support},
abstract = {Adverse drug reactions are a common cause of morbidity in health care. The US Food and Drug Administration (FDA) evaluates individual case safety reports of adverse events (AEs) after submission to the FDA Adverse Event Reporting System as part of its surveillance activities. Over the past decade, the FDA has explored the application of artificial intelligence (AI) to evaluate these reports to improve the efficiency and scientific rigor of the process. However, a gap remains between AI algorithm development and deployment. This viewpoint aims to describe the lessons learned from our experience and research needed to address both general issues in case-based reasoning using AI and specific needs for individual case safety report assessment. Beginning with the recognition that the trustworthiness of the AI algorithm is the main determinant of its acceptance by human experts, we apply the Diffusion of Innovations theory to help explain why certain algorithms for evaluating AEs at the FDA were accepted by safety reviewers and others were not. This analysis reveals that the process by which clinicians decide from case reports whether a drug is likely to cause an AE is not well defined beyond general principles. This makes the development of high performing, transparent, and explainable AI algorithms challenging, leading to a lack of trust by the safety reviewers. Even accounting for the introduction of large language models, the pharmacovigilance community needs an improved understanding of causal inference and of the cognitive framework for determining the causal relationship between a drug and an AE. We describe specific future research directions that underpin facilitating implementation and trust in AI for drug safety applications, including improved methods for measuring and controlling of algorithmic uncertainty, computational reproducibility, and clear articulation of a cognitive framework for causal inference in case-based reasoning.}
}
@article{KARUNATHILAKE2019558,
title = {Renewable energy selection for net-zero energy communities: Life cycle based decision making under uncertainty},
journal = {Renewable Energy},
volume = {130},
pages = {558-573},
year = {2019},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2018.06.086},
url = {https://www.sciencedirect.com/science/article/pii/S0960148118307389},
author = {Hirushie Karunathilake and Kasun Hewage and Walter Mérida and Rehan Sadiq},
keywords = {Multi-criteria decision making, Life cycle thinking, Fuzzy techniques, Renewable energy, Community energy system planning},
abstract = {Developing net-zero energy communities powered by renewable energy (RE) resources has become a popular concept. To make the best choices for community-level net-zero energy systems, it is necessary to identify the best energy technologies at local level. Evaluation of RE technologies has to be extended from technical and economic aspects to include environmental and social wellbeing. It is possible to identify the true costs and benefits of energy use by taking a cradle-to-grave life cycle perspective. In this study, a RE screening and multi-stage energy selection framework was developed. A fuzzy multi-criteria decision making approach was used in ranking the technologies to incorporate the conflicting requirements, stakeholder priorities, and uncertainties. Different scenarios were investigated to reflect different decision maker priorities. Under a pro-environment scenario, small hydro, onshore wind, and biomass combustion technologies perform best. Under a pro-economic decision scenario, biomass combustion, small hydro, and landfill gas have the best rankings. Triple bottom line sustainability was combined with technical feasibility through a ruled-based approach to avoid the theoretical pitfalls inherent in energy-related decision making. This assessment is geared towards providing decision makers with flexible tools, and is expected to aid in the pre-project planning stage of RE projects.}
}
@article{CLINDANIEL2024105890,
title = {Digital formation processes: A high-frequency, large-scale investigation},
journal = {Journal of Archaeological Science},
volume = {161},
pages = {105890},
year = {2024},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2023.105890},
url = {https://www.sciencedirect.com/science/article/pii/S030544032300170X},
author = {Jon Clindaniel and Matthew Magnani},
keywords = {Formation processes, Palimpsests, Big data, Digital materiality, Craigslist, Free stuff, Social stratification},
abstract = {Large sources of digital trace data (i.e. “Big Data”) have become increasingly important in the study of material culture. However, akin to the offline material culture traditionally studied by archaeologists, digital trace data is rarely a passive reflection of human behavior – it is a complex palimpsest produced through a variety of erasure and accretion formation processes. To better understand how digital trace palimpsests are formed and how digital formation processes influence and inform our ability to interpret the offline material processes they index, we introduce a computational method – high-frequency archaeological survey – which allows us to observe digital formation processes at a high temporal resolution, as well as a large spatial scale. Using this method every hour for one month, we surveyed posts from across the United States in Craigslist's “Free Stuff” category (popularly called “Curb Alert”), a user-generated source of big digital trace data, indexing material things that have been placed on users' curbs for removal by scavengers or trash collectors. For each post, we observed its time-to-erasure and any edits that were made during the study period – finding that the posts that survive represent a biased sample of those that were posted over the course of the month, conditioned by how recently and on what day the post is posted, the material characteristics of things that are posted about, as well as regional variation. Far from only being evidence of biased end-of-month data, however, we show that further analysis of identified digital formation processes can be an important object of study in its own right – in this case, shedding new light on social scientific questions linking the exchange of “free stuff” with the process of social stratification and urban inequality in the United States. Overall, our findings suggest the importance of accounting for and explicitly analyzing digital formation processes in studies that utilize digital trace data.}
}
@article{LUO2011384,
title = {Application of Improved EAHP on Stability Evaluation of Coal Seam Roof},
journal = {Procedia Earth and Planetary Science},
volume = {3},
pages = {384-393},
year = {2011},
note = {2011 Xi'an International Conference on Fine Exploration and Control of Water & Gas in Coal Mines},
issn = {1878-5220},
doi = {https://doi.org/10.1016/j.proeps.2011.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S1878522011001111},
author = {Donghai Luo and Shunxin Sun and Dunhu Zhang and Yuqing Wan and Guangchao Zhang and Junqiang Niu},
keywords = {Coal Seam Roof, Stability Evaluation, EAHP, Model},
abstract = {Stability of coal seam roof is one of the important factors to ensure safe and efficient coal production. Stability result is the complex interaction subjected to a larger number of geological factors. Only taking comprehensive consideration into evaluation can the result be in line with the actual complex geological environment. Main factors of coal seam roof stability are divided into four major factors and eight secondary factors. Major factors are sedimentary environment, structural feature, rock mechanics property and so on. Secondary factors are the combination of roof rock, lithology difference, bedding changes, and so on. Stability rank is divided into four grades: super stability, stability, basically stability and instability. EAHP model of stability evaluation of coal seam roof and the extension comparison matrix are established by means of the improved EAHP (Extension Analytical Hierarchy Process) method. Using the method based on judgment by possibility degree matrix can get the Sorting order. Evaluation results show that: the stability grade of main coal seam 5# roof of the mine is stable. It is true and credible. The method not only has the merits of “Extension to consider fuzziness of human thinking to judge”, but also eliminates a lot of spreadsheet work in traditional AHP. These studies are useful experiment and explore to study on comprehensive evaluation of coal seam roof stability.}
}
@article{WARING2015254,
title = {Managerial and non-technical factors in the development of human-created disasters: A review and research agenda},
journal = {Safety Science},
volume = {79},
pages = {254-267},
year = {2015},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2015.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925753515001575},
author = {Alan Waring},
keywords = {Major hazards, Disasters, Safety management, Safety culture, Risk decisions},
abstract = {A number of common underlying factors in the development of human-created disasters, as cited in numerous official inquiry reports, encompass in particular, safety management system defects and weaknesses in an organization’s safety culture. Human factors such as faulty risk cognition, bounded rationality, groupthink, failure of foresight and organizational learning, suspect motivations, reactive attitudes, and inappropriate risk decision-making, are commonly associated characteristics of such shortcomings. This article summarizes and discusses underlying managerial and non-technical factors in human-created major hazard accidents in the light of theories of accident causation, findings from disaster inquiries and published research, and the systemic holism-versus-reductionism debate. Ideally, all site operators would know and understand disaster aetiology and preventive requirements and be motivated to enact them. However, there is sufficient empirical evidence from inquiry reports into major hazard incidents and disasters that idealized enactment rarely occurs and in many cases safety policy and strategy as enacted is distant from espoused safety policy and strategy. Research questions relating to board level thinking and actions on major hazard risks are posited and a proposal for a more holistic and potentially more effective major hazard safety research framework is put forward.}
}
@article{RONI2022100796,
title = {Integrated water-power system resiliency quantification, challenge and opportunity},
journal = {Energy Strategy Reviews},
volume = {39},
pages = {100796},
year = {2022},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2021.100796},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X21001796},
author = {Mohammad S. Roni and Thomas Mosier and Tzvi D. Feinberg and Timothy McJunkin and Ange-Lionel Toba and Liam D. Boire and Luis Rodriguez-Garcia and Majid Majidi and Masood Parvania},
keywords = {Resiliency, Irrigation, Integrated water-power system, Optimization},
abstract = {Resiliency has been studied in the power and water systems separately. Often the resiliency study is not so comprehensive as to understand interdependent, integrated water and power systems. This research outlines the relevant factors necessary to understand and advance quantification of such integrated systems. It also presents a review of integrated water-power systems resiliency. Based on literature survey and identification of challenges, the authors present quantification and computational steps needed to understand integrated water-power systems resiliency. A conceptual framework is proposed to quantify integrated water-power system resiliency. Finally, the authors presented an opportunity for improved water and power system resilience.}
}
@article{SETO2022102891,
title = {Connected in health: Place-to-place commuting networks and COVID-19 spillovers},
journal = {Health & Place},
volume = {77},
pages = {102891},
year = {2022},
issn = {1353-8292},
doi = {https://doi.org/10.1016/j.healthplace.2022.102891},
url = {https://www.sciencedirect.com/science/article/pii/S1353829222001526},
author = {Christopher H. Seto and Corina Graif and Aria Khademi and Vasant G. Honavar and Claire E. Kelling},
keywords = {Commuting networks, COVID-19, Fixed-effects, Spatial models, Computational statistics, Mobility data},
abstract = {Biweekly county COVID-19 data were linked with Longitudinal Employer-Household Dynamics data to analyze population risk exposures enabled by pre-pandemic, country-wide commuter networks. Results from fixed-effects, spatial, and computational statistical approaches showed that commuting network exposure to COVID-19 predicted an area's COVID-19 cases and deaths, indicating spillovers. Commuting spillovers between counties were independent from geographic contiguity, pandemic-time mobility, or social media ties. Results suggest that commuting connections form enduring social linkages with effects on health that can withstand mobility disruptions. Findings contribute to a growing relational view of health and place, with implications for neighborhood effects research and place-based policies.}
}
@article{HUDLICKA201498,
title = {Affective BICA: Challenges and open questions},
journal = {Biologically Inspired Cognitive Architectures},
volume = {7},
pages = {98-125},
year = {2014},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X13000947},
author = {Eva Hudlicka},
keywords = {Emotion modeling, Emotion theories, Cognitive–affective architectures},
abstract = {In spite of the progress in emotion research over the past 20years, emotions remain an elusive phenomenon. While some underlying circuitry has been identified for some aspects of affective processing (e.g., amygdala-mediated processing of threatening stimuli, the role of orbitofrontal cortex in emotion regulation), much remains unknown about the mechanisms of emotions. Computational models of cognitive and affective processes provide a unique and powerful means of refining psychological theories, and can help elucidate the mechanisms that mediate affective phenomena. This paper outlines a number of open questions and challenges associated with developing computational models of emotion, and with their integration within biologically-inspired cognitive architectures. These include the following: the extent to which mechanisms in biological affective agents should be simulated or emulated in affective BICAs; importance of more precise, design-based terminology; identification of fundamental affective processes, and the computational tasks necessary for their implementation; improved understanding of affective dynamics and development of more accurate models of these phenomena; and understanding the alternative means of integrating emotions within agent architectures. The challenges associated with data availability and model validation are also discussed.}
}
@article{PASMAN201880,
title = {How can we improve process hazard identification? What can accident investigation methods contribute and what other recent developments? A brief historical survey and a sketch of how to advance},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {55},
pages = {80-106},
year = {2018},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0950423018300329},
author = {Hans J. Pasman and William J. Rogers and M. Sam Mannan},
keywords = {Accident-incident investigation, Hazard identification, Causation, System approach},
abstract = {Risk assessment is essential for various purposes such as facility siting, safeguarding, and licensing. Hazard identification (HAZID), which suffers greatly from incompleteness, is still the weakest link in risk assessment. Of course, this recognition is not new and many efforts have been spent to improve the situation, of which some have been rather successful. To find out what can go wrong, creative divergent thinking is required. Hazard identification should result in scenario definition. In that respect, applying the present tools as HAZOP and FMEA there is still a great emphasis on the material and equipment aspects. In contrast, underlying management and leadership failure in its many forms reflecting in organizational and human failure, due to complexity, attracts much less attention. Unlike in HAZID, in accident investigation the occurrence of an event with nasty consequences is no doubt a fact, so there must be one or more causes and the traces will lead to them. Over the years, methods for accident and incident investigation have gone through a significant evolution. From the early-on simplistic domino stone model and the human operator always at fault, via models of latent failure due to failing management involvement and via extensive root cause analysis (RCA) to a system approach. Hence, in accident investigation, management failure appearing in the many possible forms of human and organizational factors, obtained already 30 years ago with the RCA technique much attention, while it nowadays culminates in the socio-technical system approach. So, the question arises whether for improved HAZID we can learn from the accident investigation experience. In addition, safer design and advances from static risk assessment towards more accurate predictive operational dynamic risk assessment and management, will also be enabled by possibilities offered by big data and analytics. Digitization, automation and simulation, hence computerization, will be of great help in improving the identification of hazards and tracing the corresponding scenarios. The paper reviews the developmental history of both accident investigation and hazard identification methodology; incidentally it will identify commonality and differences. On the basis of the comparison and of recent advances in computerization, the paper will investigate to what extent beneficial modifications and additions can be made to obtain a higher degree of completeness in HAZID.}
}
@article{ABDULLAH2024100212,
title = {Recent development of combined heat transfer performance for engine systems: A comprehensive review},
journal = {Results in Surfaces and Interfaces},
volume = {15},
pages = {100212},
year = {2024},
issn = {2666-8459},
doi = {https://doi.org/10.1016/j.rsurfi.2024.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2666845924000321},
author = {Md. Abdullah and Mohammad {Zoynal Abedin}},
keywords = {Combined heat transfer, Performance, Engine system, Enhancement},
abstract = {Heat transfer regulation between engine components has a direct impact on engine efficiency and performance. Improving engine system efficiency, reducing emissions, and prolonging component life all depend on efficient heat management. This review looks at recent advancements in integrated heat transfer optimization to boost efficiency, reduce emissions, and improve engine system performance. This effort also investigates producing intricate heat transfer components with improved geometry using additive manufacturing techniques. With additive printing, designers may more easily construct complex structures and optimize heat transfer surfaces for better performance. The development of nanofluids and nanocoatings now allows for improving heat transfer qualities. Nanotechnology advancements have made this possible, and nanostructured materials' enhanced surface properties as well as thermal conductivity contribute to better heat dissipation in engine systems. The modern automotive and aerospace sectors have high standards, which are met through novel designs, materials, computational tools, and integrated cooling systems. Additionally, these improvements pave the way for more efficient and environmentally friendly engine operation. Because of the integration of computational technologies like numerical modeling and CFD, engineers can now see complex heat flow patterns and construct more efficient cooling solutions. Additive manufacturing has changed component manufacturing by enabling sophisticated designs that maximize heat transfer surfaces.}
}
@article{XIONG2023105811,
title = {Neural vortex method: From finite Lagrangian particles to infinite dimensional Eulerian dynamics},
journal = {Computers & Fluids},
volume = {258},
pages = {105811},
year = {2023},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2023.105811},
url = {https://www.sciencedirect.com/science/article/pii/S0045793023000361},
author = {Shiying Xiong and Xingzhe He and Yunjin Tong and Yitong Deng and Bo Zhu},
keywords = {Vortex method, Neural network, Lagrangian dynamics, Eulerian dynamics},
abstract = {In fluid analysis, there has been a long-standing problem: lacking a rigorous mathematical tool to map from a continuous flow field to finite discrete particles, hurdling the Lagrangian particles from inheriting the high resolution of a large-scale Eulerian solver. To tackle this challenge, we propose a novel learning-based framework, the neural vortex method (NVM). NVM builds a neural-network description of the Lagrangian vortex structures and their interaction dynamics to reconstruct the high-resolution Eulerian flow field in a physically-precise manner. The key components of our infrastructure consist of two networks: a vortex detection network to identify the Lagrangian vortices from a grid-based velocity field and a vortex dynamics network to learn the underlying governing interactions of these finite structures. By embedding these two networks with a vorticity-to-velocity Poisson solver and training its parameters using the fluid data obtained from grid-based numerical simulation, we can predict the accurate fluid dynamics on a precision level that was infeasible for all the previous conventional vortex methods. We demonstrate the efficacy of our method in generating highly accurate prediction results with low computational cost by predicting the evolution of the leapfrogging vortex rings system, the turbulence system, and the systems governed by Navier–Stokes (NS) equations with different external forces. We compare the prediction results made by NVM and the Lagrangian vortex method (LVM) for solving the NS equation in the periodic box and find that the relative error of the predicted velocity using NVM is more than 10 times lower than that of the LVM. Moreover, our method only requires data collected from a very short training window, more than 100 times smaller than the prediction period, which potentially facilitates data acquisition in real systems.}
}
@article{HERTZ2025105993,
title = {Beyond the matrix: Experimental approaches to studying cognitive agents in social-ecological systems},
journal = {Cognition},
volume = {254},
pages = {105993},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105993},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724002798},
author = {Uri Hertz and Raphael Köster and Marco A. Janssen and Joel Z. Leibo},
abstract = {Studying social-ecological systems, in which agents interact with each other and their environment are important both for sustainability applications and for understanding how human cognition functions in context. In such systems, the environment shapes the agents' experience and actions, and in turn collective action of agents changes social and physical aspects of the environment. Here we review current investigation approaches, which rely on a lean design, with discrete actions and outcomes and little scope for varying environmental parameters and cognitive demands. We then introduce a multiagent reinforcement learning (MARL) approach, which builds on modern artificial intelligence techniques, and provides new avenues to model complex social worlds, while preserving more of their characteristics, and allowing them to capture a variety of social phenomena. These techniques can be fed back to the laboratory where they make it easier to design experiments in complex social situations without compromising their tractability for computational modeling. We showcase the potential MARL by discussing several recent studies that have used it, detailing the way environmental settings and cognitive constraints can lead to the emergence of complex cooperation strategies. This novel approach can help researchers bring together insights from human cognition, sustainability, and AI, to tackle real world problems of social-ecological systems.}
}
@article{ALAGEEL20152003,
title = {Human Factors in the Design and Evaluation of Bioinformatics Tools},
journal = {Procedia Manufacturing},
volume = {3},
pages = {2003-2010},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.247},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915002486},
author = {Naelah Al-Ageel and Areej Al-Wabil and Ghada Badr and Noura AlOmar},
keywords = {Bioinformatics tools, Human factors, Usability metrics, Heuristics evaluation},
abstract = {Human factors contribute significantly to the information visualization design considerations and usability evaluation process, and have been shown to play an important role in the design, development and quality assurance of bioinformatics tools. Despite the technological advances in bioinformatics computational methods, humans are an indispensable part of the data mining and decision making process. The complexity of biology data visualization can make perception and analysis a complex cognitive activity for professionals in the bioinformatics domain. Information Visualization (InfoVis) can provide valuable assistance for data analysis in bioinformatics by visually depicting sequences, genomes, alignments, and macromolecular structures. InfoVis coupled with interaction modalities of bioinformatics tools also impact the efficiency and effectiveness of decision-making tasks in applied bioinformatics computing. However, the way people perceive and interact with bioinformatics tools can strongly influence their understanding of the complex data as well as the perceived usability and accessibility of these systems. In this paper, we present a synthesis of research studies and initiatives that have recently examined human factors in interaction and visualization for bioinformatics tools, particularly in perception-based design. Although bioinformatics’ visualization and interaction design research that involves human factors is considered in its infancy, a plethora of potentially promising areas have yet to be explored. The aims of this paper are to review current human factors research in interaction, usability and visualization within bioinformatics tools to provide a basis for future investigations in systems and software engineering of bioinformatics tools, and to identify promising areas for future research directions in interaction design of bioinformatics tools.}
}
@article{FIELDS2023104927,
title = {Regulative development as a model for origin of life and artificial life studies},
journal = {Biosystems},
volume = {229},
pages = {104927},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104927},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001028},
author = {Chris Fields and Michael Levin},
keywords = {Free energy principle, Kinematic replication, Learning, Multicellularity, Multiscale competency architecture, Target morphology},
abstract = {Using the formal framework of the Free Energy Principle, we show how generic thermodynamic requirements on bidirectional information exchange between a system and its environment can generate complexity. This leads to the emergence of hierarchical computational architectures in systems that operate sufficiently far from thermal equilibrium. In this setting, the environment of any system increases its ability to predict system behavior by “engineering” the system towards increased morphological complexity and hence larger-scale, more macroscopic behaviors. When seen in this light, regulative development becomes an environmentally-driven process in which “parts” are assembled to produce a system with predictable behavior. We suggest on this basis that life is thermodynamically favorable and that, when designing artificial living systems, human engineers are acting like a generic “environment”.}
}
@incollection{RAPAPORT1994225,
title = {CHAPTER 10 - Syntactic Semantics: Foundations of Computational Natural-Language Understanding},
editor = {Eric Dietrich},
booktitle = {Thinking Computers and Virtual Persons},
publisher = {Academic Press},
pages = {225-273},
year = {1994},
isbn = {978-0-12-215495-9},
doi = {https://doi.org/10.1016/B978-0-12-215495-9.50015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780122154959500156},
author = {William J. Rapaport},
abstract = {Publisher Summary
This chapter discusses the way by which it is possible to understand natural language and whether a computer could do so. It presents the argument that although a certain kind of semantic interpretation is needed for understanding natural language, it is a kind that only involves syntactic symbol manipulation of precisely the sort of which computers are capable, so that it is possible, in principle, for computers to understand natural language. The chapter highlights the recent arguments by John R. Searle and by Fred Dretske to the effect that computers cannot understand natural language. A program is like the script of a play. The computer is like the actors, sets, and a process is like an actual production of the play—the play in the process of being performed. The computer must be able to do things like: to convince someone, to imitate a human, that is, it must not merely be a cognitive agent, but also an acting one. In particular, to imitate a human, the computer needs to be able to reason about what another cognitive agent, such as a human, believes.}
}
@article{KEARNS2025256,
title = {Biomimetic Digital Twins and Multiomics: Applications to Rheumatoid Arthritis and the Potential Reclassification of Variants of Unknown Clinical Significance},
journal = {The Journal of Molecular Diagnostics},
volume = {27},
number = {4},
pages = {256-269},
year = {2025},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1525157825000376},
author = {William G. Kearns and Joe Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Chandra Germain and Laura Kearns and Georgios Stamoulis},
abstract = {The National Academies of Sciences, Engineering, and Medicine issued a report on December 15, 2023, “Foundational Research Gaps and Future Directions for Digital Twins.” This described the importance of using biomimetic digital twins and multiomics in research. These were incorporated in the current analysis of patients with rheumatoid arthritis (RA). Exome sequencing, genotype-phenotype ranking, and biomimetic digital twin analysis were used to identify five pathogenic and one likely pathogenic DNA variants in patient samples analyzed, which were absent from controls. The variants identified in these genes, P2RX7, HTRA2, PTPN22, FLG, CD46, and EIF4G1, play a role in the development of RA. Additionally, 3172 variants of unknown clinical significance (VUSs) were identified in patient samples, which were absent from controls. All VUSs appeared to be associated with RA. Hidden or dark data were identified from six genes. These genes, often found in patient samples, included HIF1A, HLA-DOA, PTGER3, HIPK3, TGFBR3, and HIF1A-AS3. VUSs identified in genes HIF1A, HLA-DOA, PTGER3, and HIPK3 were directly related to the pathogenesis of RA, whereas VUSs identified in genes TGFBR3 and HIF1A-AS3 were indirectly related. The current results suggest that biomimetic digital twins and multiomics can provide further insight into the development of RA. This may also potentially help with the process of reclassifying VUSs. The reclassification of VUSs will play a critical role in complex molecular diagnostics and drug development.}
}
@article{LI2000233,
title = {Can younger students succeed where older students fail? An examination of third graders' solutions of a division-with-remainder (DWR) problem},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {2},
pages = {233-246},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00046-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000468},
author = {Yeping Li and Edward A. Silver},
keywords = {Problem solving, Mathematics education, Sense making, Division, Elementary school students, Mathematics cognition},
abstract = {In this study, 14 third graders, who had not yet been taught the division algorithm (DA), solved a division-with-remainder (DWR) problem, and their solution processes were examined. Students also solved a set of eight numerical computation tasks involving addition, subtraction, multiplication, and division. In contrast to their poor performance on the division computation tasks and in contrast to the findings in previous studies of DWR problem solving by middle-school students and to the third graders in this study were quite successful in solving the DWR problem. Although the students lacked knowledge of formal procedures for division computation, they successfully used non-division solution strategies that were closely tied to problem context in order to solve the problem. In general, the results indicate that student performance in solving this complex problem was enhanced by their engagement in sense making as they solved the DWR problem. The highly context-embedded approaches used by these students also allowed them to avoid the difficulties in treating the “remainder,” as have been reported in research involving older students.}
}
@article{HONG2025134554,
title = {Energy-saving optimal control of secondary district cooling system based on tribal intelligent evolution optimization algorithm},
journal = {Energy},
volume = {316},
pages = {134554},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.134554},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225001963},
author = {Xiaoxi Hong and Ye Yao and Kui Wang and Jianzhong Yang and Qimei Liu},
keywords = {Secondary district cooling system, Energy-saving optimization control, Energy consumption calculation model, Tribal intelligent evolution optimization algorithm},
abstract = {With the significant increase in energy consumption for large central air conditioning systems, optimal control of district cooling systems is crucial for energy conservation and CO2 emission reduction. This study focuses on the energy-saving optimal control of secondary district cooling systems (SDCSs). Firstly, energy models for SDCSs utilizing distribution manifolds or plate heat exchangers are presented, with the goal of establishing global optimal control models for energy conservation. Next, a novel metaheuristic algorithm called Tribal Intelligent Evolution Optimization (TIEO) is proposed, which innovatively introduces human intelligent behavioral characteristics into the tribal evolution process. The TIEO and seven other optimization algorithms have been tested for optimizing the SDCSs. The test results demonstrated that the TIEO surpassed other algorithms in terms of optimization effectiveness, stability, and computational efficiency. Additionally, this study has conducted engineering validation of the TIEO algorithm on the SDCSs with distribution manifolds or plate heat exchangers. Compared to the traditional control strategies, the TIEO algorithm improved the energy efficiency ratio of the system by 13.56 % and 11.40 %, respectively. Therefore, the TIEO algorithm has the potential to serve as an optimization tool for contributing to energy conservation and promoting the sustainable development of large-scale district cooling systems.}
}
@article{AMMAR2018116,
title = {MPEG-4 AVC stream-based saliency detection. Application to robust watermarking},
journal = {Signal Processing: Image Communication},
volume = {60},
pages = {116-130},
year = {2018},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2017.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0923596517301674},
author = {Marwa Ammar and Mihai Mitrea and Marwen Hasnaoui and Patrick {Le Callet}},
keywords = {Saliency map, MPEG-4 AVC stream, Density fixation map, Saccade locations, Robust watermarking},
abstract = {By bridging uncompressed-domain saliency detection and MPEG-4 AVC compression principles, the present paper advances a methodological framework for extracting the saliency maps directly from the stream syntax elements. In this respect, inside each GOP, the intensity, color, orientation and motion elementary saliency maps are related to the energy of the luma coefficients, to the energy of chroma coefficients, to the gradient of the prediction modes and to the amplitude of the motion vectors, respectively. The three spatial saliency maps are pooled according to an average formula, while the static-temporal fusion is achieved by six different formulas. The experiments consider both ground-truth and applicative evaluations. The ground-truth benchmarking investigates the relation between the predicted MPEG-4 AVC saliency map and the actual human saliency, captured by eye-tracking devices. It is based on two corpora (representing density fixation maps and saccade locations), two objective criteria (related to the closeness between the predicted and the real saliency maps and to the difference between the behavior of the predicted saliency map in fixation and random locations), two objective measures (KLD – the Kullback Leibler Divergence and AUC – the Area Under the ROC Curve) and 5 state-of-the-art saliency models (3 acting in spatial domain and 2 acting in compressed domain). The applicative validation is carried out by integrating the MPEG-4 AVC saliency map into a robust watermarking application. As an overall conclusion, the paper demonstrates that although the MPEG-4 AVC standard does not explicitly relies on any visual saliency principle, its stream syntax elements preserve this property. Four main benefits for the MPEG-4 AVC based saliency extraction are thus brought to light: (1) it outperforms (or, at least, is as good as) state-of-the-art uncompressed domain methods, (2) it allows significant gains to be obtained in watermarking transparency (for prescribed data payload and robustness), (3) it is less sensitive to the randomness in the processed visual content, and (4) it has a linear computational complexity. For instance, the ground truth results exhibit absolute relative gains between 60% and 164% in KLD, between 17% and 21% in AUC, and relative gains in KLD sensitivity between 1.18 and 6.12 and in AUC sensitivity between 1.06 and 33.7; the applicative validation brings to light transparency gains up to 10 dB in PSNR.}
}
@article{JACOB2022470,
title = {Algorithmic Approaches to Classify Autism Spectrum Disorders: A Research Perspective},
journal = {Procedia Computer Science},
volume = {201},
pages = {470-477},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004744},
author = {Shomona Gracia Jacob and Majdi Mohammed {Bait Ali Sulaiman} and Bensujin Bennet},
keywords = {Machine learning, Supervised learning, Pattern discovery, Autism disorder, Data Mining},
abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disability that exhibits sluggish progress in vocal development, restricted interest in normal activity and repetitive disoriented behavior. This syndrome, has gained a lot of attention due to its prevalence among children across all countries and from different economic backgrounds. However, ASD detection and treatment yet remains in its infancy due to the lack of awareness among parents, limited screening of proper developmental milestones and a dearth of diagnostic tools to classify this syndrome with convincing accuracy. Recent studies report that scalable biomarkers for early detection have made little progress in research due to the erraticism of this disorder. Moreover, the study on developing tools or applications for parents, teachers, and healthcare workers to identify children who exhibit any form of autism is still a work in progress. The research work undertaken in this paper presents an analysis of supervised machine learning algorithms on mining interesting details that link the diverse nature of ASD and the possibility of computationally detecting markers for the syndrome. The preliminary findings on the performance of traditional machine learning algorithms in ASD classification is reported with the possibility of integrating deep learning architectures for ASD detection and therapy.}
}
@incollection{CAMERER2014479,
title = {Chapter 25 - The Neural Basis of Strategic Choice},
editor = {Paul W. Glimcher and Ernst Fehr},
booktitle = {Neuroeconomics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-492},
year = {2014},
isbn = {978-0-12-416008-8},
doi = {https://doi.org/10.1016/B978-0-12-416008-8.00025-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160088000255},
author = {Colin F. Camerer and Todd A. Hare},
keywords = {Game theory, Learning, Strategic choice},
abstract = {In this chapter, we present a set of concepts and tools for defining and examining strategic choice that are drawn from behavioral economics and discuss how they can be applied to and tested with neuroscience techniques. The standard language for studying strategic choice in economics is game theory. Game theory provides concrete mathematical formulas for linking strategic actions to rewarding payoffs. After outlining the four components necessary to make predictions about strategic social behavior, we present recent evidence that the computations predicted by game theory in specific strategic choice contexts are reflected in the brain. In addition, we discuss links between strategic decision making and the psychological concept of theory of mind. We conclude by suggesting that developing mathematical models of social and strategic actions may aid in the understanding of how the brain implements typical choice behavior as well as categorizing dysfunctions that lead to aberrant behavior in psychiatric disorders.}
}
@article{JANSEN2022100020,
title = {The illusion of data validity: Why numbers about people are likely wrong},
journal = {Data and Information Management},
volume = {6},
number = {4},
pages = {100020},
year = {2022},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2022.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122001188},
author = {Bernard J. Jansen and Joni Salminen and Soon-gyo Jung and Hind Almerekhi},
keywords = {People data, Measurement, Quantitative paradigm, Statistics},
abstract = {This reflection article addresses a difficulty faced by scholars and practitioners working with numbers about people, which is that those who study people want numerical data about these people. Unfortunately, time and time again, this numerical data about people is wrong. Addressing the potential causes of this wrongness, we present examples of analyzing people numbers, i.e., numbers derived from digital data by or about people, and discuss the comforting illusion of data validity. We first lay a foundation by highlighting potential inaccuracies in collecting people data, such as selection bias. Then, we discuss inaccuracies in analyzing people data, such as the flaw of averages, followed by a discussion of errors that are made when trying to make sense of people data through techniques such as posterior labeling. Finally, we discuss a root cause of people data often being wrong – the conceptual conundrum of thinking the numbers are counts when they are actually measures. Practical solutions to address this illusion of data validity are proposed. The implications for theories derived from people data are also highlighted, namely that these people theories are generally wrong as they are often derived from people numbers that are wrong.}
}
@article{HESTER2019187,
title = {Simulation of integrative physiology for medical education},
journal = {Morphologie},
volume = {103},
number = {343},
pages = {187-193},
year = {2019},
issn = {1286-0115},
doi = {https://doi.org/10.1016/j.morpho.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1286011519300554},
author = {R.L. Hester and W. Pruett and J. Clemmer and A. Ruckdeschel},
keywords = {VPH, Simulation, Physiology, Healthcare, Electronic health record},
abstract = {Summary
Medical education is founded on the understanding of physiology. While lecture materials and reading contribute to the learning of physiology, the richness and complexity of the subject suggest that more active learning methods may provide a richer introduction to the science as it applies to the practice of medicine. Simulation has been previously used in basic science to better understand the interaction of physiological systems. In the current context, simulation generally refers to interactive case studies performed with a manikin or anatomic device. More recently, simulation has grown to encompass computational simulation: virtual models of physiology and pathophysiology where students can see in a mechanistic setting how tissues and organs interact with one another to respond to changes in their environment. In this manuscript, we discuss how simulation fits into the overall history of medical education, and detail two computational simulation products designed for medical education. The first of these is an acute simulator, JustPhysiology, which reduces the scope of a large model, HumMod, down to a more focused interface. The second is Sycamore, an electronic health record-delivered, real time simulator of patients designed to teach chronic patient care to students. These products represent a new type of tool for medical and allied health students to encourage active learning and integration of basic science knowledge into clinical situations.
Résumé
L’étude de la médecine est fondée entre autres sur la compréhension de la physiologie. Bien que l’apprentissage de la physiologie puisse se faire au moyen de cours magistraux et la lecture de contenus spécialisés, la richesse et la complexité du sujet laissent supposer que des méthodes d’apprentissage plus interactives puissent susciter une initiation plus élaborée de cette science et de son application à la pratique de la médecine. La simulation a précédemment été appliquée aux sciences fondamentales afin de mieux comprendre l’interaction entre systèmes physiologiques. Dans le contexte actuel, la simulation réfère en général à des études de cas interactives réalisées à l’aide d’un mannequin ou tout autre modèle anatomique. Plus récemment, la simulation s’est étendue à la simulation informatique incluant des modèles virtuels de physiologie et de physiopathologie à partir desquels les étudiants peuvent apprécier dans un contexte mécanistique comment les tissus et organes interagissent dans leur réponse à tout changement environnemental. Dans cet article nous présentons comment la simulation s’intègre dans l’histoire de l’éducation de la médecine et détaillons deux modèles de simulation informatique adaptés à l’éducation médicale. Le premier modèle, JustPhysiology, est un simulateur de courte durée qui réduit le champ d’action d’un simulateur plus complexe, HumMod, à une interface plus spécialisée. Le second outil est Sycamore, un dossier de santé électronique généré en temps réel et conçu pour un apprentissage de la pratique de soins médicaux en continu. Ces simulateurs informatiques représentent un nouvel outil pour les étudiants en médecine et autres professions de santé afin d’encourager un apprentissage actif et l’intégration de concepts scientifiques fondamentaux aux conditions cliniques.}
}
@article{KARSTEN2020104512,
title = {Closing the gap: Merging engineering and anthropology in holistic fire safety assessments in the maritime and offshore industries},
journal = {Safety Science},
volume = {122},
pages = {104512},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518316175},
author = {Mette Marie Vad Karsten and Aqqalu Thorbjørn Ruge and Thomas Hulin},
keywords = {Anthropology, Fire safety engineering, Transdisciplinary, Risk, Maritime, Offshore},
abstract = {This article reports on the endeavor to merge the fields of anthropology and fire safety engineering in holistic fire safety assessments within the maritime and offshore industries. The article suggests a combination of the two disciplines to transition from an interdisciplinary approach towards transdisciplinarity. The approach has been developed and adjusted during three cases of risk analyses and prevention strategies on fire safety. The article presents two methodological insights illustrating the necessary attitude of interdisciplinarity as a foundation towards transdisciplinarity. It advocates for the need of willingness in organizations and project teams to consider both disciplines as equally valid, integrate them in research definition, and create a base for common understanding. Subsequently, it is proposed that transdisciplinary work requires the creation of a group of core members acting as guarantors of transdisciplinarity, thus becoming themselves transdisciplinary humans working in a joined framework of thinking and methods. The article also presents two operational findings integrating the two disciplines within the area of fire safety. The first finding concerns including ‘daily operations’ in fire safety design, as daily practices and perceptions among crew can have a high impact on fire safety. The second finding concerns ‘reclassification of space and place’. It highlights mixing and shifting between work- and leisure-related practices within the same physical space, leading to the identification of new fire scenarios. It also explores the shifts between work, leisure, and emergency places, and their link to the shifts in professional roles of crew.}
}
@article{DURSTEWITZ2008739,
title = {The Dual-State Theory of Prefrontal Cortex Dopamine Function with Relevance to Catechol-O-Methyltransferase Genotypes and Schizophrenia},
journal = {Biological Psychiatry},
volume = {64},
number = {9},
pages = {739-749},
year = {2008},
note = {Neurodevelopment and the Transition from Schizophrenia Prodrome to Schizophrenia},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2008.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S000632230800646X},
author = {Daniel Durstewitz and Jeremy K. Seamans},
keywords = {Attractor dynamics, computational model, dopamine, GABA currents, NMDA currents, prefrontal cortex, schizophrenia},
abstract = {There is now general consensus that at least some of the cognitive deficits in schizophrenia are related to dysfunctions in the prefrontal cortex (PFC) dopamine (DA) system. At the cellular and synaptic level, the effects of DA in PFC via D1- and D2-class receptors are highly complex, often apparently opposing, and hence difficult to understand with regard to their functional implications. Biophysically realistic computational models have provided valuable insights into how the effects of DA on PFC neurons and synaptic currents as measured in vitro link up to the neural network and cognitive levels. They suggest the existence of two discrete dynamical regimes, a D1-dominated state characterized by a high energy barrier among different network patterns that favors robust online maintenance of information and a D2-dominated state characterized by a low energy barrier that is beneficial for flexible and fast switching among representational states. These predictions are consistent with a variety of electrophysiological, neuroimaging, and behavioral results in humans and nonhuman species. Moreover, these biophysically based models predict that imbalanced D1:D2 receptor activation causing extremely low or extremely high energy barriers among activity states could lead to the emergence of cognitive, positive, and negative symptoms observed in schizophrenia. Thus, combined experimental and computational approaches hold the promise of allowing a detailed mechanistic understanding of how DA alters information processing in normal and pathological conditions, thereby potentially providing new routes for the development of pharmacological treatments for schizophrenia.}
}
@incollection{BAKER2023209,
title = {From capacity to ability to automation: “Western” conceptions of the figure of man and ableist subjectivities},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {209-218},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.12005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305120056},
author = {Bernadette Baker},
keywords = {Ableist subjectivities, Wynter, Colonialism, Humanism, Locke, Gall, Artificial intelligence (AI)},
abstract = {This chapter analyzes the shifting construct of ableist subjectivities that lie at the heart of a modernity-colonialism-racialization vortex and that were entangled with humanism's rise. Drawing on and building on the work of Sylvia Wynter, it examines how a “figure of Man” in three different but related iterations helped shape ontological hierarchies and violent inclusions/exclusions. It illustrates these moves via whitestream scholarship that came to dominate certain geopolitical locales from the 1600s, 1800s, and 2000s and that subsequently spread. From discourses of capacity (tutoring children), to ability (compulsory schooling), to automation (systems theory and computational superintelligence), the figure of Man not only overrepresents for “the human” but also puts in jeopardy all lives and ways of being beyond its exclusive borders. The questions that remain for education exceed standard policy debates about school reform, inclusive schooling, or evaluation, pointing instead toward a wider planetary context, existential issues, and power relations that sit within the tensions between “Man's” exclusivity, “the human's” idiosyncracies and primacy, and the potential rewriting/erasure of both via new technologies.}
}
@article{QI2021338821,
title = {Accurate diagnosis of lung tissues for 2D Raman spectrogram by deep learning based on short-time Fourier transform},
journal = {Analytica Chimica Acta},
volume = {1179},
pages = {338821},
year = {2021},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2021.338821},
url = {https://www.sciencedirect.com/science/article/pii/S0003267021006474},
author = {Yafeng Qi and Lin Yang and Bangxu Liu and Li Liu and Yuhong Liu and Qingfeng Zheng and Dameng Liu and Jianbin Luo},
keywords = {Raman spectrogram, Lung cancer, Short-time Fourier transform, Deep learning},
abstract = {Multivariate statistical analysis methods have an important role in spectrochemical analyses to rapidly identify and diagnose cancer and the subtype. However, utilizing these methods to analyze lager amount spectral data is challenging, and poses a major bottleneck toward achieving high accuracy. Here, a new convolutional neural networks (CNN) method based on short-time Fourier transform (STFT) to diagnose lung tissues via Raman spectra readily is proposed. The models yield that the accuracies of the new method are higher than the conventional methods (principal components analysis -linear discriminant analysis and support vector machine) for validation group (95.2% vs 85.5%, 94.4%) and test group (96.5% vs 90.4%, 93.9%) after cross-validation. The results illustrate that the new method which converts one-dimensional Raman data into two-dimensional Raman spectrograms improve the discriminatory ability of lung tissues and can achieve automatically accurate diagnosis of lung tissues.}
}
@article{SHAHAB2025121507,
title = {Droplet sorting computer: Design, optimization and device dynamics},
journal = {Chemical Engineering Science},
volume = {311},
pages = {121507},
year = {2025},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2025.121507},
url = {https://www.sciencedirect.com/science/article/pii/S0009250925003306},
author = {Mohammad Shahab and Raghunathan Rengaswamy},
keywords = {Droplet sorting, Microfluidics, Reinforcement learning, Multi-agent systems, Optimization, Scalability},
abstract = {Droplet sorting is a crucial component for integrated lab-on-a-chip applications, but conventional methods require active control and a sorter component. This study presents a state-of-the-art droplet sorting computer that incorporates a quality droplet sorter into a microfluidic device without the need for an additional mechanism for active detection and control. The device is computationally optimized using a novel combined design framework based on multi-agent reinforcement learning. The beauty of this sorting computer is that a device optimized for fewer drops can be used for thousands of drops, based on optimal hydrodynamic interactions between drops. The framework for developing a sorting computer is described, and the dynamics behind passive routing of droplets inside the device is explained. The proposed device can achieve droplet movement similar to that of an automated microfluidic device for droplet sorting or any given objective, making it a pioneer in passive droplet routing when implemented with appropriate process conditions.}
}
@article{MATTILA2025237,
title = {Closing the Gap: Advancing service management in the hospitality and tourism industry amidst the AI revolution},
journal = {Journal of Hospitality and Tourism Management},
volume = {62},
pages = {237-245},
year = {2025},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2025.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1447677025000178},
author = {Anna S. Mattila and Laurie Wu and Peihao Wang},
keywords = {Artificial intelligence, Service management, Gaps model, Service automation, Service experience, Service design},
abstract = {The Gaps Model of service quality has long been the keystone of the service management literature. This classic theoretical model offers valuable guidance for understanding the opportunities and challenges posed by the increasing adoption of artificial intelligence (AI) in service management. Drawing on recent research and practical insights, this research examines the impact of AI on each one of the service quality gaps identified in the Gaps Model. In addition, we discuss the dual potential of AI to either bridge or widen these gaps, cautioning for digitally responsible implementations of AI. Towards the end of the paper, we discuss future research directions to advance service management with AI integrations in the hospitality and tourism industry.}
}
@incollection{SCHILLER2018246,
title = {Some Thermodynamics and Electrostatics With a View to Electrochemistry},
editor = {Klaus Wandelt},
booktitle = {Encyclopedia of Interfacial Chemistry},
publisher = {Elsevier},
address = {Oxford},
pages = {246-257},
year = {2018},
isbn = {978-0-12-809894-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.13605-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472136054},
author = {R. Schiller},
keywords = {Activity coefficient, Chemical potential, Cole–Cole plot, Conservation laws, Dielectric relaxation time, Dipole moment, Electric dipole, Entropy of mixing, Gauss law of electrostatics, Kramers–Kronig relations, Osmotic coefficient, Poisson equation, Polarization, Relative permittivity, Solvation energy},
abstract = {This article tries to offer an overview of some basic laws of thermodynamics and electrostatics which are considered to be part of the foundations of electrochemical thinking. Equilibrium thermodynamics is introduced in terms of conservation laws paying particular attention to the notion of chemical potential. After discussing the forces, potentials, and energetics of charges in vacuum the same problems are dealt with in continuous dielectric media. Here polarization, formation and role of dipole moments, their relation to relative permittivity (dielectric constant) are discussed both in macroscopic and atomic/molecular terms. Finally the kinetics of the response of relative permittivity to the variation of polarizing fields and charges are described. Solvation processes are referred to in connection with both thermodynamic and electrostatic considerations.}
}
@article{BRIAS2016151,
title = {Computing the reliability kernel of a time-variant system: Application to a corroded beam},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {151-155},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.566},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316308242},
author = {A. Brias and J-D. Mathias and G. Deffuant},
keywords = {Discrete systems, Dynamic systems, Initial states, Reliability analysis, Reliability kernel, System failures, Transition matrix, Reliable design},
abstract = {Time-variant reliability analysis aims at assessing the probability of failure of a time-variant system within a given time horizon. We illustrate in this paper the computation of the reliability kernel which is the set of initial states for which the probability of failure remains under a threshold within the considered time horizon. This paper supposes that the time-variant system is discrete in time and space with given probabilities of transition between space states. We use a recursive relation for computing the cumulative probability of failure of the system, linking the probability of failure at time t with the probability of being at a given state x (for all possible states) at time t — 1. Applying this relation, it is possible to compute the probability of failure at any starting point in the state space and hence to derive the reliability kernel. The computation of this kernel gives informations about the system which can be further helpful in reliable design. The approach is illustrated on an example of a steel beam under corrosion.}
}
@article{BEJINES2023108405,
title = {Counting semicopulas on finite structures},
journal = {Fuzzy Sets and Systems},
volume = {462},
pages = {108405},
year = {2023},
note = {Aggregation operations (186 p.)},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2022.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0165011422004122},
author = {C. Bejines and M. Ojeda-Hernández},
keywords = {Semicopula, Fuzzy Logic, Finite plane partition, Discrete Mathematics},
abstract = {Semicopulas are the operators chosen to model conjunction in the fuzzy/many-valued logics. In fact, a special kind of semicopula, called t-norm, is widely used in many applications of logic to engineering, computer science and fuzzy systems. The main result of this paper is the computation of the exact number of semicopulas that can be defined on a finite chain in terms of its length. The final formula is achieved via relating semicopulas with finite plane partitions.}
}
@article{YANG2023103712,
title = {Study of the water entry and exit problems by coupling the APR and PST within SPH},
journal = {Applied Ocean Research},
volume = {139},
pages = {103712},
year = {2023},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2023.103712},
url = {https://www.sciencedirect.com/science/article/pii/S0141118723002535},
author = {Xi Yang and Song Feng and Jinxin Wu and Guiyong Zhang and Guangqi Liang and Zhifan Zhang},
keywords = {Water entry, Water exit, Particle shifting technique, Adaptive particle refinement, Coupled scheme},
abstract = {In this manuscript, the adaptive particle refinement (APR) and particle shifting technique (PST) are coupled and applied to investigate the entire process of water entry and exit. The PST implementation for various types of particles is quantitatively discussed in detail in the coupled APR-PST approach. A comprehensive analysis of different APR-PST coupled schemes is conducted with crucial variables compared and analyzed for the first time. The stability, accuracy and robustness of the developed numerical model are verified by three benchmark tests: 2D wedge water entry, 2D cylinder water exit and 2D cylinder water entry and exit. The obtained results show that the current model can ensure the overall computational precision in the situation of local refinement, which indicates it is a viable way to solve the problems of water entry and exit.}
}
@incollection{MANIKTALA2008247,
title = {Chapter 12 - Discussion Forums, Datasheets, and Other Real-World Issues},
editor = {Sanjaya Maniktala},
booktitle = {Troubleshooting Switching Power Converters},
publisher = {Newnes},
address = {Burlington},
pages = {247-289},
year = {2008},
isbn = {978-0-7506-8421-7},
doi = {https://doi.org/10.1016/B978-075068421-7.50014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978075068421750014X},
author = {Sanjaya Maniktala},
abstract = {Publisher Summary
This chapter explains a few concepts such as thinking is the key, one needs to cross check everything, and product liability concerns. The chapter describes that the company's online tools can be used to discover design problems and correct them as long as thinking is applied as well. But what about “errors” in the online tools themselves? The chapter deals in and highlights that if the thinking process is done assiduously, sometimes one might arrive at the opposite conclusion that one initially foresaw. Anyone can even suddenly realize that he/she can be a part of the very problem that they are trying to fix; it could be in his/her own backyard. The chapter thinks about the customers and highlights that the very idea of a company starting a forum such as this one is essentially brilliant and thoroughly laudable. It also imparts a perception of transparency to their operations from the get-go. One should not outrightly believe anything and put in front of everyone, even if it is on semiglossy paper or in high-definition video or Flash HTML format. As engineers, one is required to put pen to paper, and at least do a sanity check.}
}
@incollection{HU2025629,
title = {Chapter 28 - The lung exposome: Accelerating precision respiratory health},
editor = {Kent E. Pinkerton and Richard Harding and Elizabeth Georgian},
booktitle = {The Lung (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {629-645},
year = {2025},
isbn = {978-0-323-91824-4},
doi = {https://doi.org/10.1016/B978-0-323-91824-4.00017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918244000174},
author = {Xin Hu},
keywords = {Environmental exposure, Exposome, High-resolution mass spectrometry, Mixed chemical exposure, Multi-omics, Respiratory health, Systems biology},
abstract = {The exposome concept, first proposed as a complement to the genome for understanding non-genetic causes of disease, has rapidly undergone significant evolution to recognize the totality of environmental exposures experienced across an individual's lifespan and to embrace the complexity of biological responses to those exposures. Over almost twodecades of work, the landscape of lung exposome research has expanded to include extensive characterization of external and internal exposure levels, molecular responses, and health parameters. The exposome approach has identified new environmental factors and profiles that impact the variation of lung function trajectories and the development of chronic respiratory diseases over the life course. Aligned with the goal of precision medicine and leveraging systems biology, exposome research directly addresses the heterogeneity in disease origins and management. Recent advancements in omics technologies, particularly omics-scale chemical quantification using high-resolution mass spectrometry, have revolutionized exposure assessment and biological discovery. These analytic platforms, now widely available for large population studies, have propelled advanced statistical models and computational pipelines, which together contribute to an affordable, automatable, and standardizable framework that can be integrated into health care and respiratory epidemiology for studies with greater precision, breadth, and depth.}
}
@article{GOLDSCHMIDT2006549,
title = {Variances in the impact of visual stimuli on design problem solving performance},
journal = {Design Studies},
volume = {27},
number = {5},
pages = {549-569},
year = {2006},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2006.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X06000172},
author = {Gabriela Goldschmidt and Maria Smolkov},
keywords = {creativity, design problems, problem solving, visual stimuli},
abstract = {Research in cognitive psychology and in design thinking has shown that the generation of inner representations in imagery and external representations via sketching are instrumental in design problem solving. In this paper we focus on another facet of visual representation in design: the ‘consumption’ of external visual representations, regarded as stimuli, when those are present in the designer's work environment. An empirical study revealed that the presence of visual stimuli of different kinds can affect performance, measured in terms of practicality, originality and creativity scores attained by designs developed by subjects under different conditions. The findings suggest that the effect of stimuli is contingent on the type of the design problem that is being solved.}
}
@incollection{TURKLE20017035,
title = {Human–Computer Interface},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {7035-7038},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/04333-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767043333},
author = {S. Turkle},
abstract = {The computer/human interface refers to the modalities through which people interact with computational technologies. Looked at over the last half century, the trend has been from a style of interaction in which the computer is approached as a mechanism to a style of interaction in which the computer is approached as a behaving and aware organism. This first trend has been related to another, from people acting directly on the machine to acting indirectly on its increasingly elaborate self presentation in which many layers of programming stand between underlying processes and what is presented to the user. This movement is more than technical. Computational technologies have served as objects to think with, that is, as carrier objects for ideas. Through their changing interfaces, the computer has moved from being the carrier object of a culture of calculation to that of a culture of simulation. The culture of simulation promotes a way of understanding in which users are encouraged to take computers ‘at interface value.’}
}
@article{CAFFERATA2023106879,
title = {Financial fragility and credit risk: A simulation model},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {116},
pages = {106879},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2022.106879},
url = {https://www.sciencedirect.com/science/article/pii/S1007570422003665},
author = {Alessia Cafferata and Simone Casellina and Simone Landini and Mariacristina Uberti},
keywords = {Credit risk, Financial instability, Minsky, Agent-based model},
abstract = {Financial and economic crises are not always the same. It is important to understand why some episodes of crisis generate prolonged and systemic recessions. Developing the Financial Instability Hypothesis, Hyman Minsky introduced the idea that in periods of stability, financial actors tend to increase their risk exposure, moving from a stable hedge-dominated structure to an unstable one, characterized by speculative and ultra-speculative (Ponzi) financial positions: hence, stability turns out being destabilizing. Starting from the three different relationships introduced by Minsky (income–debt–hedge, speculative and Ponzi) for financial units, we involve a simple partial equilibrium agent-based model in which firms, the banking sector, the real and financial sides of the economy, interact. This theoretic framework is used as computational laboratory to extend the migration rates open system modelling based on the E(ntry)–S(tay)–L(eave) processes by considering the economic system, the business cycle and with attention to so-called zombie-firms.}
}
@article{LORENZODUS202015,
title = {The communicative modus operandi of online child sexual groomers: Recurring patterns in their language use},
journal = {Journal of Pragmatics},
volume = {155},
pages = {15-27},
year = {2020},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378216619306162},
author = {Nuria Lorenzo-Dus and Anina Kinzel and Matteo {Di Cristofaro}},
keywords = {Child sexual abuse, Online grooming, Linguistic patterns, Corpus assisted discourse studies},
abstract = {Online child sexual groomers manipulate their targets into partaking in sexual activity online and, in some cases, offline. To do so they use language (and other semiotic means, such as images) strategically. This study uses a Corpus-Assisted Discourse Studies methodology to identify recurring patterns in online groomers' language use, mapping them to the specific grooming goal that their use in context fulfils. The analysis of the groomers' language (c. 3.3 million words) within 622 conversations from the Perverted Justice website newly identifies 70 such recurring linguistic patterns (three-word collocations), as well as their relative strength of association to one or more grooming goals. The results can be used to inform computational models for detecting online child sexual grooming language. They can also support the development of training resources that raise awareness of typical language structures that characterise online sexual groomers’ communicative modus operandi.}
}
@article{ANAZKHAN2023,
title = {Metal additive manufacturing of alloy structures in architecture: A review on achievements and challenges},
journal = {Materials Today: Proceedings},
year = {2023},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2023.05.192},
url = {https://www.sciencedirect.com/science/article/pii/S2214785323028183},
author = {Muhammed {Anaz Khan} and Aysha Latheef},
keywords = {Architecture, Additive manufacturing, Facades, Construction industry, Structural engineering},
abstract = {There is a growing trend in modern architecture towards asymmetrical building layouts. This development can be attributed to the proliferation of cutting-edge production methods and structurally novel approaches. With the advent of computational design and digital manufacturing techniques, formerly static designs may now be modified to create one-of-a-kind, high-performance prototypes. Metal additive manufacturing (MAM) is a cutting-edge production method that paves the way for new architectural designs, construction processes, and materials. Interesting possibilities for optimising structural elements are made possible by MAM because of its ability to deposit material just where it is structurally necessary. The construction sector places a premium on directed energy deposition additive manufacturing (DED AM) and wire arc additive manufacturing (WAAM). The adaptability of these two technologies in the construction industry and their possible future applications are explored in this literature review.}
}
@article{FOLLEY2003467,
title = {Psychoses and creativity: is the missing link a biological mechanism related to phospholipids turnover?},
journal = {Prostaglandins, Leukotrienes and Essential Fatty Acids},
volume = {69},
number = {6},
pages = {467-476},
year = {2003},
note = {Recent Advances of Membran e Pathology in Schizophrenia},
issn = {0952-3278},
doi = {https://doi.org/10.1016/j.plefa.2003.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952327803001716},
author = {Bradley S Folley and Mikisha L Doop and Sohee Park},
keywords = {Creativity, Norepinephrine, Fatty acids, Schizophrenia, Psychoses},
abstract = {Recent evidence suggests that genetic and biochemical factors associated with psychoses may also provide an increased propensity to think creatively. The evolutionary theories linking brain growth and diet to the appearance of creative endeavors have been made recently, but they lack a direct link to research on the biological correlates of divergent and creative thought. Expanding upon Horrobin's theory that changes in brain size and in neural microconnectivity came about as a result of changes in dietary fat and phospholipid incorporation of highly unsaturated fatty acids, we propose a theory relating phospholipase A2 (PLA2) activity to the neuromodulatory effects of the noradrenergic system. This theory offers probable links between attention, divergent thinking, and arousal through a mechanism that emphasizes optimal individual functioning of the PLA2 and NE systems as they interact with structural and biochemical states of the brain. We hope that this theory will stimulate new research in the neural basis of creativity and its connection to psychoses.}
}
@article{BROWN2022110672,
title = {“Deep reinforcement learning for engineering design through topology optimization of elementally discretized design domains”},
journal = {Materials & Design},
volume = {218},
pages = {110672},
year = {2022},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2022.110672},
url = {https://www.sciencedirect.com/science/article/pii/S0264127522002933},
author = {Nathan K. Brown and Anthony P. Garland and Georges M. Fadel and Gang Li},
keywords = {Reinforcement learning, Topology optimization, Deep learning, Engineering design, Structural design, Data-driven},
abstract = {Advances in machine learning algorithms and increased computational efficiencies give engineers new capabilities and tools to apply to engineering design. Machine learning models can approximate complex functions and, therefore, can be useful for various tasks in the engineering design workflow. This paper investigates using reinforcement learning (RL), a subset of machine learning that teaches an agent to complete a task through accumulating experiences in an interactive environment, to automate the designing of 2D discretized topologies. RL agents use past experiences to learn sequential sets of actions to best achieve some objective. In the proposed environment, an RL agent can make sequential decisions to design a topology by removing elements to best satisfy compliance minimization objectives. After each action, the agent receives feedback by evaluating how well the current topology satisfies the design objectives. After training, the agent was tasked with designing optimal topologies under various load cases. The agent's proposed designs had similar or better compliance minimization performance to those produced by traditional gradient-based topology optimization methods. These results show that a deep RL agent can learn generalized design strategies to satisfy multi-objective design tasks and, therefore, shows promise as a tool for arbitrarily complex design problems across many domains.}
}
@article{FABOLUDE2025100246,
title = {Smart cities, smart systems: A comprehensive review of system dynamics model applications in urban studies in the big data era},
journal = {Geography and Sustainability},
volume = {6},
number = {1},
pages = {100246},
year = {2025},
issn = {2666-6839},
doi = {https://doi.org/10.1016/j.geosus.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666683924000993},
author = {Gift Fabolude and Charles Knoble and Anvy Vu and Danlin Yu},
keywords = {Urban sustainability, Smart cities, System dynamics models, Big data analytics, Urban system complexity, Data-driven urbanism},
abstract = {This paper addresses urban sustainability challenges amid global urbanization, emphasizing the need for innovative approaches aligned with the Sustainable Development Goals. While traditional tools and linear models offer insights, they fall short in presenting a holistic view of complex urban challenges. System dynamics (SD) models that are often utilized to provide holistic, systematic understanding of a research subject, like the urban system, emerge as valuable tools, but data scarcity and theoretical inadequacy pose challenges. The research reviews relevant papers on recent SD model applications in urban sustainability since 2018, categorizing them based on nine key indicators. Among the reviewed papers, data limitations and model assumptions were identified as major challenges in applying SD models to urban sustainability. This led to exploring the transformative potential of big data analytics, a rare approach in this field as identified by this study, to enhance SD models’ empirical foundation. Integrating big data could provide data-driven calibration, potentially improving predictive accuracy and reducing reliance on simplified assumptions. The paper concludes by advocating for new approaches that reduce assumptions and promote real-time applicable models, contributing to a comprehensive understanding of urban sustainability through the synergy of big data and SD models.}
}
@article{JOGO2025106357,
journal = {Environmental Modelling & Software},
volume = {186},
pages = {106357},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2025.106357},
url = {https://www.sciencedirect.com/science/article/pii/S1364815225000416},
author = {Fransiskus Serfian Jogo and Hanum Khairana Fatmah and Aufaclav Zatu {Kusuma Frisky}}
}
@incollection{HASKELL200195,
title = {Chapter 6 - Knowledge Base and Transfer: On the Usefulness of Useless Knowledge},
editor = {Robert E. Haskell},
booktitle = {Transfer of Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {95-113},
year = {2001},
series = {Educational Psychology},
issn = {18716148},
doi = {https://doi.org/10.1016/B978-012330595-4/50007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012330595450007X},
author = {Robert E. Haskell},
abstract = {Publisher Summary
Knowledge base broadly includes knowledge acquired by reading, personal experience, careful listening, and astute observing. It also includes thinking and it is the absolute requirement not only for transfer but for thinking and reasoning. This chapter presents the general knowledge-base conditions necessary for optimal transfer to occur and explain its importance. Transfer of learning requires more than quick-fix strategies and learners must have a knowledge base in a subject in order to even know enough to ask questions about it. Without a sufficient knowledge base, novices do not have a framework within which to formulate adequate questions. Possessing a large knowledge base enables a learner to think about the subject in depth. To achieve general transfer, one often requires much more than immediately useful knowledge. It requires learning that may be considered useless knowledge. There are several examples of knowledge that appeared to have absolutely no use, but years later, these "useless" knowledge was applied to other areas, which later had major applied importance.}
}
@article{KHAOUJA2025100313,
title = {Political communication and conspiracy theory sharing on twitter},
journal = {Online Social Networks and Media},
volume = {47},
pages = {100313},
year = {2025},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2025.100313},
url = {https://www.sciencedirect.com/science/article/pii/S246869642500014X},
author = {Imane Khaouja and Daniel Toribio-Flórez and Ricky Green and Cassidy Rowden and Chee Siang Ang and Karen M. Douglas},
keywords = {Conspiracy theory, Political communication, Twitter, NLP, Psycho-linguistic characteristics},
abstract = {Social media has become an influential channel for political communication, offering broad reach while enabling the proliferation of misinformation and conspiracy theories. These unchecked conspiracy narratives may allow manipulation by malign actors, posing dangers to democratic processes. Despite their intuitive appeal, little research has examined the strategic usage and timing of conspiracy theories in politicians’ social media communication compared to the spread of misinformation and fake news. This study provides an empirical analysis of how members of the U.S. Congress spread conspiracy theories on Twitter. Leveraging the Twitter Historical API, we collected a corpus of tweets from members of the US Congress between January 2012 and December 2022. We developed a classifier to identify conspiracy theory content within this political discourse. We also analyzed the linguistic characteristics, topics and distribution of conspiracy tweets. To assess classifier performance, we created ground truth data through human annotation in which experts labeled a sample of 2500 politicians’ tweets. Our findings shed light on several aspects, including the influence of prevailing political power dynamics on the propagation of conspiracy theories and higher user engagement. Moreover, we identified specific psycho-linguistic attributes within the tweets, characterized by the use of words related to power and causation, and outgroup language. Our results provide valuable insights into the motivations compelling influential figures to engage in the dissemination of conspiracy narratives in political discourse.}
}
@article{FAIRHURST1980447,
title = {Towards a rationale for neural stability: A model of neural computation and network architecture},
journal = {International Journal of Bio-Medical Computing},
volume = {11},
number = {6},
pages = {447-459},
year = {1980},
issn = {0020-7101},
doi = {https://doi.org/10.1016/0020-7101(80)90012-4},
url = {https://www.sciencedirect.com/science/article/pii/0020710180900124},
author = {M.C. Fairhurst and G.P. Goutos},
abstract = {Networks of Boolean processing cells with low connectivity are known to be inherently stable, in the sense that they exhibit only limited reverberatory activity among their possible state transitions. This paper discusses the value of such a network as a functional model of a neural system and, in the light of the observed decrease in stability with increasing cell connectivity, seeks to identify the features of network architecture and cell computation which act to protect network stability, thereby providing a framework for an understanding of neural stability.}
}
@incollection{CORTELLNICOLAU20241090,
title = {Agent-Based Modeling},
editor = {Efthymia Nikita and Thilo Rehren},
booktitle = {Encyclopedia of Archaeology (Second Edition) (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {1090-1098},
year = {2024},
isbn = {978-0-323-91856-5},
doi = {https://doi.org/10.1016/B978-0-323-90799-6.00094-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390799600094X},
author = {Alfredo Cortell-Nicolau},
keywords = {Agency, Agent-based models, Coding, Complexity, Heuristic modeling, Hypothesis testing, Netlogo, Python, R, Reproducibility, Tactical modeling},
abstract = {This work constitutes a very brief overview of Agent-Based Modeling applied to archaeology. The aim is to provide a synthetic overview of the most fundamental concepts, so that researchers interested in starting to explore this methodological tool have a first contact with it. The text covers basic concepts, as well as offers an example of a simple simulation so that interested readers gain initial insight to this topic.}
}
@article{LI2025100338,
title = {RICE AlgebraBot: Lessons learned from designing and developing responsible conversational AI using induction, concretization, and exemplification to support algebra learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100338},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100338},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001413},
author = {Chenglu Li and Wanli Xing and Yukyeong Song and Bailing Lyu},
keywords = {Conversational AI, Large language models, FAccT AI in education, Technology design and development, Math learning},
abstract = {The importance and challenge of Algebra learning is widely recognized, with students across the U.S. facing difficulties due to the subject's complexity. While extensive research has focused on enhancing Algebra learning in K-12 education, the reusability, scalability, and effectiveness of the strategies employed (e.g., manual interventions and digital tutoring platforms) remain limited. Conversational AI (ConvAI), enabled by the advancement of large language models (LLMs), emerges as a potential tool for automatic, personalized, and effective student support. However, ethical concerns surrounding diversity, safety, sentiment, and stereotype associated with ConvAI are prominent, and empirical studies examining its application in education are scarce. The purpose of this study is to develop a ConvAI system that mitigates the potential ethical concerns and empirically evaluate the effect of such a system for math learning. Specifically, we first examined computational strategies to mitigate the ethical concerns of ConvAI in educational setting with educational big data (npretraining = 2,097,139) and found that researchers could effectively enhance ConvAI responsibility through the investigated algorithmic strategies. Then, a ConvAI system was constructed using these strategies, guided by learning sciences principles. Lastly, we examined students' eye-tracking patterns, acceptance, and learning processes when using this ConvAI system to learn Algebra through a random experiment (nparticipant = 151). Participants using the developed ConvAI demonstrated generally increased visual attention levels as compared to the control group. Moreover, participants expressed a positive acceptance towards the ConvAI technology. Finally, participants' interaction patterns with the ConvAI technology influenced their Algebra learning. These results provide insights for both educational researchers and practitioners to integrate ConvAI in learning environments.}
}
@article{LOCHAB202116,
title = {An improved flux limiter using fuzzy modifiers for Hyperbolic Conservation Laws},
journal = {Mathematics and Computers in Simulation},
volume = {181},
pages = {16-37},
year = {2021},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0378475420303207},
author = {Ruchika Lochab and Vivek Kumar},
keywords = {Flux limiters, Hyperbolic equations, High resolution methods, Fuzzy logic},
abstract = {The objective of the work in this paper is to computationally tackle a range of problems in hyperbolic conservation laws, which is an interesting branch of computational fluid dynamics. For the simulation of issues in hyperbolic conservation laws, this work explores the concept of fuzzy logic-based operators. This research presents a unique mixture of fuzzy sets and logic with a new branch of conservation laws from fluid dynamics. The approach considers a computational procedure based on the reconstruction of several high-order numerical methods termed as flux-limited methods using some fuzzy logic operators. With the aid of fuzzy modifiers, these flux limiters are further modified. This approach results in improved convergence of approximations and maintains the problem’s basic properties to be solved. Additionally, to ensure improved results, modified flux-limited methods are imposed on some famous test problems. The application results are provided wherever required. The work has demonstrated that it is possible to use such technique and apply it to complex areas of computational fluid dynamics to produce a more straightforward approach to studying other topics like flux-limited methods and hence opens up an exciting gateway for future work.}
}
@article{MENG2023116227,
title = {Efficient path planning for AUVs in unmapped marine environments using a hybrid local–global strategy},
journal = {Ocean Engineering},
volume = {288},
pages = {116227},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.116227},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823026112},
author = {Wenlong Meng and Ya Gong and Fan Xu and Pingping Tao and Pengbo Bo and Shiqing Xin},
keywords = {Path planning, Autonomous undersea vehicle, Unmapped obstacle environment, Rapidly exploring random tree, Dynamic step},
abstract = {The ability of autonomous undersea vehicles (AUVs) to plan paths in unknown marine environments is the precondition for executing complicated missions. However, existing path planning algorithms based on underwater sensing equipment often struggle to achieve efficient exploration and generate high-quality trajectories. In this paper, we introduce a novel approach to efficiently handle the challenge of AUV navigation under limited information. Our solution combines global and local planning techniques to generate optimized paths that guarantee collision-free and efficient operations. In global path planning, we incrementally use the rolling windows to make decisions on high-level path branching while utilizing waypoints from selected branches to refine the calculation of local paths for enhanced accuracy. We employ an efficient small-scale path search strategy at the local path computation level by leveraging sensor-detected environments. In this stage, we propose an advanced rapidly exploring random tree (RRT) algorithm called circle-RRT. By combining adaptive circle sampling with dynamic step sizes, this algorithm can significantly reduce the generation of redundant sampling points and improve the efficiency of local path planning. We evaluated the efficiency of our algorithm in unknown environments through simulations and compared it with previous leading methods.}
}
@article{HUANG2025135787,
title = {Integrated economic and environmental optimization for industrial consumers: A dual-objective approach with multi-carrier energy systems and fuzzy decision-making},
journal = {Energy},
volume = {324},
pages = {135787},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.135787},
url = {https://www.sciencedirect.com/science/article/pii/S036054422501429X},
author = {Anzhong Huang and Qiuxiang Bi and Luote Dai},
keywords = {Multi-carrier energy hub, Bi-objective optimization, Peak load management, Fuzzy-decision making, Stability of industrial consumers},
abstract = {Industrial energy systems require innovative optimization strategies to simultaneously minimize costs and reduce environmental impact. This study presents a dual-objective optimization model that integrates economic and environmental considerations within a multi-carrier energy hub framework. The proposed approach incorporates a peak load management strategy to optimize energy consumption patterns, a fuzzy decision-making method to handle operational uncertainties, and an enhanced non-dominated sorting genetic algorithm II to improve multi-objective optimization efficiency. The model employs Pareto-optimal solutions, offering decision-makers a structured method to balance cost reduction and emissions minimization. The effectiveness of the proposed framework is validated through case studies, demonstrating that peak load management significantly flattens load profiles, optimizes distributed energy resources, and reduces reliance on grid electricity during peak hours. The enhanced non-dominated sorting genetic algorithm II ensures better convergence toward optimal trade-offs, improving computational performance. Results indicate that the integration of peak load management leads to a 0.93 % reduction in operational costs and a 0.79 % decrease in carbon emissions compared to conventional energy management approaches. These findings underscore the potential of the developed model in enhancing industrial energy efficiency and sustainability, providing a robust and adaptable solution for modern industrial consumers.}
}
@article{LI2022296,
title = {The role of information structures in game-theoretic multi-agent learning},
journal = {Annual Reviews in Control},
volume = {53},
pages = {296-314},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578822000086},
author = {Tao Li and Yuhan Zhao and Quanyan Zhu},
keywords = {Multi-agent learning, Information structures, Reinforcement learning, Belief generation, Game theory, Value of Information},
abstract = {Multi-agent learning (MAL) studies how agents learn to behave optimally and adaptively from their experience when interacting with other agents in dynamic environments. The outcome of a MAL process is jointly determined by all agents’ decision-making. Hence, each agent needs to think strategically about others’ sequential moves, when planning future actions. The strategic interactions among agents makes MAL go beyond the direct extension of single-agent learning to multiple agents. With the strategic thinking, each agent aims to build a subjective model of others decision-making using its observations. Such modeling is directly influenced by agents’ perception during the learning process, which is called the information structure of the agent’s learning. As it determines the input to MAL processes, information structures play a significant role in the learning mechanisms of the agents. This review creates a taxonomy of MAL and establishes a unified and systematic way to understand MAL from the perspective of information structures. We define three fundamental components of MAL: the information structure (i.e., what the agent can observe), the belief generation (i.e., how the agent forms a belief about others based on the observations), as well as the policy generation (i.e., how the agent generates its policy based on its belief). In addition, this taxonomy enables the classification of a wide range of state-of-the-art algorithms into four categories based on the belief-generation mechanisms of the opponents, including stationary, conjectured, calibrated, and sophisticated opponents. We introduce Value of Information (VoI) as a metric to quantify the impact of different information structures on MAL. Finally, we discuss the strengths and limitations of algorithms from different categories and point to promising avenues of future research.}
}
@article{MEJIA201899,
title = {The Power of Writing, a Pebble Hierarchy and a Narrative for the Teaching of Automata Theory},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {339},
pages = {99-110},
year = {2018},
note = {The XLII Latin American Computing Conference},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066118300513},
author = {Carolina Mejía and J. {Andres Montoya} and Christian Nolasco},
keywords = {Pebble automata, finite state automata},
abstract = {In this work we study pebble automata. Those automata constitute an infinite hierarchy of discrete models of computation. The hierarchy begins at the level of finite state automata (0-pebble automata) and approaches the model of one-tape Turing machines. Thus, it can be argued that it is a complete hierarchy that covers, in a continuous way, all the models of automata that are important in the theory of computation. We investigate the use of this hierarchy as a narrative for the teaching of automata theory. We also investigate some fundamental questions concerning the power of pebble automata.}
}
@article{VUONG2025,
title = {Critical remarks on current practices of data article publishing: Issues, challenges, and recommendations},
journal = {Data Science and Informetrics},
year = {2025},
issn = {2694-6106},
doi = {https://doi.org/10.1016/j.dsim.2025.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S2694610625000098},
author = {Quan-Hoang Vuong and Viet-Phuong La and Minh-Hoang Nguyen},
keywords = {Data paper, FAIR Principles, Editing and reviewing processes, Quality control, Open science, Mindsponge Theory},
abstract = {ABSTRACT
The contribution of the data paper publishing paradigm to the knowledge generation and validation processes is becoming substantial and pivotal. In this paper, through the information-processing perspective of Mindsponge Theory, we discuss how the data article publishing system serves as a filtering mechanism for quality control of the increasingly chaotic datasphere. The overemphasis on machine-actionality and technical standards presents some shortcomings and limitations of the data article publishing system, such as the lack of consideration of humanistic values, radical race for big data, and inadequate use of expertise in data evaluation. Without addressing the shortcomings and limitations, the reusability of data will be hindered, and scientific investment to facilitate data sharing will be wasted. Thus, we suggest that the current data paper publishing paradigm needs to be updated with a new philosophy of data.}
}
@article{WANG201715,
title = {An overview on the roles of fuzzy set techniques in big data processing: Trends, challenges and opportunities},
journal = {Knowledge-Based Systems},
volume = {118},
pages = {15-30},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116304452},
author = {Hai Wang and Zeshui Xu and Witold Pedrycz},
keywords = {Big data, Data-intensive science, Fuzzy sets, Fuzzy logic, Granular computing},
abstract = {In the era of big data, we are facing with an immense volume and high velocity of data with complex structures. Data can be produced by online and offline transactions, social networks, sensors and through our daily life activities. A proper processing of big data can result in informative, intelligent and relevant decision making completed in various areas, such as medical and healthcare, business, management and government. To handle big data more efficiently, new research paradigm has been engaged but the ways of thinking about big data call for further long-term innovative pursuits. Fuzzy sets have been employed for big data processing due to their abilities to represent and quantify aspects of uncertainty. Several innovative approaches within the framework of Granular Computing have been proposed. To summarize the current contributions and present an outlook of further developments, this overview addresses three aspects: (1) We review the recent studies from two distinct views. The first point of view focuses on what types of fuzzy set techniques have been adopted. It identifies clear trends as to the usage of fuzzy sets in big data processing. Another viewpoint focuses on the explanation of the benefits of fuzzy sets in big data problems. We analyze when and why fuzzy sets work in these problems. (2) We present a critical review of the existing problems and discuss the current challenges of big data, which could be potentially and partially solved in the framework of fuzzy sets. (3) Based on some principles, we infer the possible trends of using fuzzy sets in big data processing. We stress that some more sophisticated augmentations of fuzzy sets and their integrations with other tools could offer a novel promising processing environment.}
}
@article{YUE2023940,
title = {A guidebook of spatial transcriptomic technologies, data resources and analysis approaches},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {940-955},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023000156},
author = {Liangchen Yue and Feng Liu and Jiongsong Hu and Pin Yang and Yuxiang Wang and Junguo Dong and Wenjie Shu and Xingxu Huang and Shengqi Wang},
keywords = {Spatial transcriptomic technologies},
abstract = {Advances in transcriptomic technologies have deepened our understanding of the cellular gene expression programs of multicellular organisms and provided a theoretical basis for disease diagnosis and therapy. However, both bulk and single-cell RNA sequencing approaches lose the spatial context of cells within the tissue microenvironment, and the development of spatial transcriptomics has made overall bias-free access to both transcriptional information and spatial information possible. Here, we elaborate development of spatial transcriptomic technologies to help researchers select the best-suited technology for their goals and integrate the vast amounts of data to facilitate data accessibility and availability. Then, we marshal various computational approaches to analyze spatial transcriptomic data for various purposes and describe the spatial multimodal omics and its potential for application in tumor tissue. Finally, we provide a detailed discussion and outlook of the spatial transcriptomic technologies, data resources and analysis approaches to guide current and future research on spatial transcriptomics.}
}
@article{CLARKE2009460,
title = {The mediating effects of coping strategies in the relationship between automatic negative thoughts and depression in a clinical sample of diabetes patients},
journal = {Personality and Individual Differences},
volume = {46},
number = {4},
pages = {460-464},
year = {2009},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908004285},
author = {Dave Clarke and Tanya Goosen},
keywords = {Automatic thoughts, Cognitive behaviour therapy, Coping, Depression, Diabetes},
abstract = {High levels of depression have been found among diabetes patients, but few studies have examined the influence of coping strategies on the relationship between diabetics’ negative thoughts and their depression. The purpose of the study was to investigate the effects of coping strategies as mediators in the path from automatic negative thoughts to depression. A questionnaire containing the Automatic Thoughts Questionnaire, the Ways of Coping Checklist, a depression inventory and demographic questions was completed by 57 male and 57 female New Zealand diabetic patients, aged 28–88 years (median=60.5, mean=59.3, SD=14.6). Automatic negative thoughts, emotion-focused coping and depression, but not problem-focused coping, were significantly correlated, after controlling for relevant demographic and diabetes variables. Hierarchical linear regression analysis of data showed that emotion-focused coping functioned as a partial mediator between negative thoughts and depression. Cognitive therapy was suggested to control both automatic negative thoughts and emotion-focused coping behaviours of self-blame, wishful thinking and avoidance.}
}
@article{CRESPO201916,
title = {General solution procedures to compute the stored energy density of conservative solids directly from experimental data},
journal = {International Journal of Engineering Science},
volume = {141},
pages = {16-34},
year = {2019},
issn = {0020-7225},
doi = {https://doi.org/10.1016/j.ijengsci.2019.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0020722517327635},
author = {José Crespo and Francisco J. Montáns},
keywords = {Hyperelasticity, Soft materials, Classical invariants, Data-driven constitutive modelling},
abstract = {Energy-conservative, hyperelastic solids assume the existence of a stored energy density which relates stresses and strains for any deformation state. The usual approach to model such materials is to impose an analytical expression of the stored energy function as a function of some invariants and material parameters. These material parameters are best-fitted to available experimental data. This approach is good for analytical derivations but less optimal for data-driven computational approaches and for accurate and efficient finite element analyses. We show in this paper that the stored energy solution of a solid may be accurately obtained in a general case from suitable numerical procedures, regardless of the invariants being use, without using material parameters nor fitting any assumed analytical form. We explain two general, simple, computational procedures to solve the problem. The numerically computed stored energies may be used in general-purpose finite element programs, yielding more general procedures that have an efficiency equivalent to that of the classical approach, which uses pre-defined analytical “models” and fitting parameters.}
}
@article{SOLEIMANIJAVID2024113958,
title = {Challenges and opportunities of occupant-centric building controls in real-world implementation: A critical review},
journal = {Energy and Buildings},
volume = {308},
pages = {113958},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.113958},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824000744},
author = {Atiye Soleimanijavid and Iason Konstantzos and Xiaoqi Liu},
keywords = {Occupant-centric control, Comfort, Sensing, Controls, Learning, Energy efficiency, Smart buildings},
abstract = {Over the past few decades, attention in buildings’ design and operation has gradually shifted from promoting only energy efficiency objectives to also addressing human comfort and well-being. Researchers have developed a wide range of control algorithms ranging from rule-based controls to complex learning approaches that can fully capture occupants’ personalized preferences in smart buildings. This direction of occupant-centric building controls can bridge the gap between occupants’ satisfaction and sustainability objectives. However, most of these promising technologies have not yet found their way into real-world applications. This study will perform a critical review on occupant-centric thermal and lighting control studies aiming to (i) analyze the strengths and weaknesses of different approaches; (ii) identify the requirements for these techniques to be implemented in real-world systems; and (iii) propose new research directions that will promote the usability of such controls and will be a catalyst towards their wide adoption. Computational complexity, integration with Building Automation Systems (BAS), data availability and data quality, scalability, and the lack of more research featuring actual building implementation emerge as critical barriers. Addressing these challenges is imperative for the successful deployment of occupant-centric controls in real-world applications.}
}
@article{AUGSBURGER2009270,
title = {Methodologies to assess blood flow in cerebral aneurysms: Current state of research and perspectives},
journal = {Journal of Neuroradiology},
volume = {36},
number = {5},
pages = {270-277},
year = {2009},
issn = {0150-9861},
doi = {https://doi.org/10.1016/j.neurad.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0150986109000479},
author = {L. Augsburger and P. Reymond and E. Fonck and Z. Kulcsar and M. Farhat and M. Ohta and N. Stergiopulos and D. {A. Rüfenacht}},
keywords = {Cerebral aneurysms, Blood flow assessment, Particle image velocimetry, Computational fluid dynamics},
abstract = {Summary
With intracranial aneurysms disease bringing a weakened arterial wall segment to initiate, grow and potentially rupture an aneurysm, current understanding of vessel wall biology perceives the disease to follow the path of a dynamic evolution and increasingly recognizes blood flow as being one of the main stakeholders driving the process. Although currently mostly morphological information is used to decide on whether or not to treat a yet unruptured aneurysm, among other factors, knowledge of blood flow parameters may provide an advanced understanding of the mechanisms leading to further aneurismal growth and potential rupture. Flow patterns, velocities, pressure and their derived quantifications, such as shear and vorticity, are today accessible by direct measurements or can be calculated through computation. This paper reviews and puts into perspective current experimental methodologies and numerical approaches available for such purposes. In our view, the combination of current medical imaging standards, numerical simulation methods and endovascular treatment methods allow for thinking that flow conditions govern more than any other factor fate and treatment in cerebral aneurysms. Approaching aneurysms from this perspective improves understanding, and while requiring a personalized aneurysm management by flow assessment and flow correction, if indicated.}
}
@article{ZHANG2025100978,
title = {The paradox of self-efficacy and technological dependence: Unraveling generative AI's impact on university students' task completion},
journal = {The Internet and Higher Education},
volume = {65},
pages = {100978},
year = {2025},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2024.100978},
url = {https://www.sciencedirect.com/science/article/pii/S109675162400040X},
author = {Ling Zhang and Junzhou Xu},
keywords = {Generative AI, Technological dependence, Self-efficacy, Learning efficiency, Educational application, Paradoxical relationship},
abstract = {In the era of proliferating artificial intelligence (AI) technology, generative AI is reshaping educational landscapes, prompting a critical examination of its influence on students' learning processes and their self-efficacy amid concerns over growing technological dependence. This study investigates the nuanced relationship between generative AI use and university students' self-efficacy and technological dependence, illuminating the underlying paradoxes and implications for inclusive education practices. Through a survey of 348 university students, with 200 valid responses analyzed, we uncover the direct and indirect impacts of generative AI usage frequency on AI dependence. Our findings reveal a paradoxical effect: enhanced AI usage significantly amplifies students' confidence and efficiency in learning, yet simultaneously intensifies their dependence on AI. This dual impact both supports and complicates the incorporation of AI technologies into educational settings, underscoring the need for a balanced approach to leveraging AI in teaching and learning. Our study underscores the critical importance of a nuanced understanding of AI's role in education. It highlights the necessity of crafting an educational landscape where technology augments learning processes without compromising independent learning capabilities. By navigating the complex interplay between technological advancement and educational inclusivity, our findings guide the development of AI-assisted learning environments that are not only effective but also equitable and accessible.}
}
@article{BRIERLEY2021107870,
title = {The dark art of interpretation in geomorphology},
journal = {Geomorphology},
volume = {390},
pages = {107870},
year = {2021},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2021.107870},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X21002786},
author = {Gary Brierley and Kirstie Fryirs and Helen Reid and Richard Williams},
keywords = {Landform, Landscape, Explanation, Prediction, Big Data, Fieldwork, Modelling},
abstract = {The process of interpretation, and the ways in which knowledge builds upon interpretations, has profound implications in scientific and managerial terms. Despite the significance of these issues, geomorphologists typically give scant regard to such deliberations. Geomorphology is not a linear, cause-and-effect science. Inherent complexities and uncertainties prompt perceptions of the process of interpretation in geomorphology as a frustrating form of witchcraft or wizardry — a dark art. Alternatively, acknowledging such challenges recognises the fun to be had in puzzle-solving encounters that apply abductive reasoning to make sense of physical landscapes, seeking to generate knowledge with a reliable evidence base. Carefully crafted approaches to interpretation relate generalised understandings derived from analysis of remotely sensed data with field observations/measurements and local knowledge to support appropriately contextualised place-based applications. In this paper we develop a cognitive approach (Describe-Explain-Predict) to interpret landscapes. Explanation builds upon meaningful description, thereby supporting reliable predictions, in a multiple lines of evidence approach. Interpretation transforms data into knowledge to provide evidence that supports a particular argument. Examples from fluvial geomorphology demonstrate the data-interpretation-knowledge sequence used to analyse river character, behaviour and evolution. Although Big Data and machine learning applications present enormous potential to transform geomorphology into a data-rich, increasingly predictive science, we outline inherent dangers in allowing prescriptive and synthetic tools to do the thinking, as interpreting local differences is an important element of geomorphic enquiry.}
}
@article{KOU2023109788,
title = {Infrared small target segmentation networks: A survey},
journal = {Pattern Recognition},
volume = {143},
pages = {109788},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004867},
author = {Renke Kou and Chunping Wang and Zhenming Peng and Zhihe Zhao and Yaohong Chen and Jinhui Han and Fuyu Huang and Ying Yu and Qiang Fu},
keywords = {Infrared small target, Characteristic analysis, Segmentation network, Deep learning, Collaborative technology, Data-driven, False alarm, Missed detection},
abstract = {Fast and robust small target detection is one of the key technologies in the infrared (IR) search and tracking systems. With the development of deep learning, there are many data-driven IR small target segmentation algorithms, but they have not been extensively surveyed; we believe our proposed survey is the first to systematically survey them. Focusing on IR small target segmentation tasks, we summarized 7 characteristics of IR small targets, 3 feature extraction methods, 8 design strategies, 30 segmentation networks, 8 loss functions, and 13 evaluation indexes. Then, the accuracy, robustness, and computational complexities of 18 segmentation networks on 5 public datasets were compared and analyzed. Finally, we have discussed the existing problems and future trends in the field of IR small target detection. The proposed survey is a valuable reference for both beginners adapting to current trends in IR small target detection and researchers already experienced in this field.}
}
@article{CHEN1998541,
title = {Toward a better understanding of idea processors},
journal = {Information and Software Technology},
volume = {40},
number = {10},
pages = {541-553},
year = {1998},
issn = {0950-5849},
doi = {https://doi.org/10.1016/S0950-5849(98)00080-9},
url = {https://www.sciencedirect.com/science/article/pii/S0950584998000809},
author = {Z. Chen},
keywords = {Artificial intelligence, Brainstorming, Computational creativity, Idea processors, Creativity support systems},
abstract = {Idea processors, as a kind of software widely used in the business world, have not received much attention from academia. In this paper we provide an overview of the current status of idea processors. We start from the foundations of idea processors, pointing out their roots in brainstorming techniques. By examining several experimental systems and commercial products, we further discuss how idea processors work, their nature, and their typical architecture. We also summarize some research work related to idea processors, as well as relationships between idea processors and studies of computational creativity in artificial intelligence. Other related issues, such as group decision support systems and evaluation methods, are also briefly examined.}
}
@article{SUZEN2020726,
title = {Automatic short answer grading and feedback using text mining methods},
journal = {Procedia Computer Science},
volume = {169},
pages = {726-743},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.171},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302945},
author = {Neslihan Süzen and Alexander N. Gorban and Jeremy Levesley and Evgeny M. Mirkes},
keywords = {Natural Language Processing, Information Extraction, Automatic Grading, Machine Learning, Text Mining, Similarity, Clustering, k-means},
abstract = {Automatic grading is not a new approach but the need to adapt the latest technology to automatic grading has become very important. As the technology has rapidly became more powerful on scoring exams and essays, especially from the 1990s onwards, partially or wholly automated grading systems using computational methods have evolved and have become a major area of research. In particular, the demand of scoring of natural language responses has created a need for tools that can be applied to automatically grade these responses. In this paper, we focus on the concept of automatic grading of short answer questions such as are typical in the UK GCSE system, and providing useful feedback on their answers to students. We present experimental results on a dataset provided from the introductory computer science class in the University of North Texas. We first apply standard data mining techniques to the corpus of student answers for the purpose of measuring similarity between the student answers and the model answer. This is based on the number of common words. We then evaluate the relation between these similarities and marks awarded by scorers. We consider an approach that groups student answers into clusters. Each cluster would be awarded the same mark, and the same feedback given to each answer in a cluster. In this manner, we demonstrate that clusters indicate the groups of students who are awarded the same or the similar scores. Words in each cluster are compared to show that clusters are constructed based on how many and which words of the model answer have been used. The main novelty in this paper is that we design a model to predict marks based on the similarities between the student answers and the model answer. We argue that computational methods be used to enhance the reliability of human scoring, and not replace it. Humans are required to calibrate the system, and to deal with situations that are challenging. Computational methods can provide insight into which student answers will be found challenging and thus be a place human judgement is required.}
}
@article{WANG2023170277,
title = {MLKCA-Unet: Multiscale large-kernel convolution and attention in Unet for spine MRI segmentation},
journal = {Optik},
volume = {272},
pages = {170277},
year = {2023},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2022.170277},
url = {https://www.sciencedirect.com/science/article/pii/S0030402622015352},
author = {Biao Wang and Juan Qin and Lianrong Lv and Mengdan Cheng and Lei Li and Dan Xia and Shike Wang},
keywords = {Deep learning, Spine segmentation, Receptive fields, Multiscale large-kernel convolution, Attention},
abstract = {Medical image segmentation plays a key role in the diagnosis of spinal diseases. Unet has become a universal structure for image segmentation because of its unique skip connection structure in recent years. However, since Unet uses small-kernel convolution, the relationship between remote features is difficult to obtain due to the small receptive fields, and the key information cannot be highlighted, resulting in insufficient edge information. To overcome these problems, this paper proposes multiscale large-kernel convolution Unet (MLKCA-Unet), which develops MLKC block for effective feature extraction. Large-kernel convolution with different convolution kernels is used according to the feature map. For large feature maps, smaller large- kernel convolution is used, and for small feature maps, larger large-kernel convolution is used. All large-kernel convolution can be reduced the dimension by 1 × 1 convolution kernel. This method has a significant reduction in computation. By paralleling each large kernel convolution branch with the 3 × 3 convolution branch, it helps to capture detailed information. At the same time, an attention mechanism is added to the network to emphasize rich feature areas and enhance useful information. Finally, various indicators are employed to evaluate the network’s accuracy, similarity and speed, including IOU, DSC, TPR, PPV, and ET. The published spinesagt2wdataset3 spinal MRI image dataset is adopted in the experiment. The IOU, DSC, TPR, PPV, and ET on the test set are 0.8302, 0.9017, 0.9000, 0.9051 and 70 s/epoch respectively. The experimental result shows that MLKCA-Unet demonstrates superior segmentation performance and robustness, which can be well extended to other medical image segmentation.}
}
@article{MILLER200021,
title = {Representational Tools and Conceptual Change: The Young Scientist's Tool Kit},
journal = {Journal of Applied Developmental Psychology},
volume = {21},
number = {1},
pages = {21-25},
year = {2000},
issn = {0193-3973},
doi = {https://doi.org/10.1016/S0193-3973(99)00047-7},
url = {https://www.sciencedirect.com/science/article/pii/S0193397399000477},
author = {Kevin F Miller},
abstract = {We interpret the world and its regularities through representations and procedures that are a complex mélange of formal experience, rules of thumb, and naive concepts that precede formal education. These representational tools give us the language in which we can think about science. Three propositions are argued: (a) that such tools are fundamental to scientific reasoning and science education; (b) that cognitive science has a great deal to say about how cognitive tools affect thinking and conceptual change, particularly how the representations intrinsic to ordinary language relate to the symbol systems of formal science and mathematics; and finally, (c) that cognitive science may play a role in developing representational tools that make scientific information more accessible.}
}
@article{YENI2025111157,
title = {Revealing risk mitigation strategies for supply chain resilience in aquaculture industry through a methodology equipped with lean tools and stochastic programming},
journal = {Computers & Industrial Engineering},
volume = {205},
pages = {111157},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111157},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225003031},
author = {Fatma Betül Yeni and Beren Gürsoy Yılmaz and Gökhan Özçelik and Ömer Faruk Yılmaz and Ozan Kalaycıoğlu},
keywords = {Lean tools, Two-stage stochastic programming model, Aquaculture industry, Ripple effect, Disruption},
abstract = {With a notable focus on seafood, especially salmon, which is crucial for global food security, substantial investments have fueled the growth of Turkish salmon farming in the Black Sea region. Despite being profitable, the industry faces challenges, including supply chain vulnerabilities to internal and external disruptions, leading to a ripple effect. One of the most significant challenges in the sector is fish escapes, which directly impact lead time and customer satisfaction levels. By specifically focusing on undercurrent and storm factors, this study proposes a comprehensive methodology, adopting a continuous improvement cycle to manage this challenge. This methodology involves a novel scenario-based two-stage stochastic programming model with the objective of minimizing the expected overall cost. The model is formulated for the transportation problem, addressing fish escapes within the aquaculture industry to establish a resilient supply chain structure. It also explores the comprehensiveness of lean implementation associated with various reliability levels, representing the decision-maker’s risk propensity. Moreover, a design of experiment (DoE) setting is established to scrutinize the impact of controlled factors and their interactions on the outcomes. After conducting computational analysis, considering the impact of factors, lean maturity levels, and the comprehensiveness of lean implementation on the results, valuable managerial insights are provided within the context of risk mitigation strategies. The findings indicate that in the event of severe disruptions, significant improvements in risk mitigation can be achieved through the continuous improvement-based methodology, primarily driven by lean tools.}
}
@article{ABRAMSKY200637,
title = {What are the Fundamental Structures of Concurrency?: We still don't know!},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {162},
pages = {37-41},
year = {2006},
note = {Proceedings of the Workshop "Essays on Algebraic Process Calculi" (APC 25)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.12.075},
url = {https://www.sciencedirect.com/science/article/pii/S1571066106004105},
author = {Samson Abramsky},
keywords = {Concurrency, process algebra, Petri nets, geometry, quantum information and computation},
abstract = {Process algebra has been successful in many ways; but we don't yet see the lineaments of a fundamental theory. Some fleeting glimpses are sought from Petri Nets, physics and geometry.}
}
@article{JEREMIAH2025103529,
title = {The human-AI dyad: Navigating the new frontier of entrepreneurial discourse},
journal = {Futures},
volume = {166},
pages = {103529},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103529},
url = {https://www.sciencedirect.com/science/article/pii/S001632872400212X},
author = {Faith Jeremiah},
keywords = {AI, Entrepreneurship, Human, New technologies, Self-identity},
abstract = {The rapid progression of Artificial Intelligence (AI) is further solidifying the importance of redefining entrepreneurship and reshaping how ventures and innovations are conceived, developed, and managed. This paper investigates how AI is transforming entrepreneurship by reshaping traditional business paradigms and entrepreneurs’ roles. It examines AI's influence on entrepreneurial decision-making, innovation, and identity. Through a comprehensive literature review and thematic analysis of previously published data, the research identifies emergent themes in AI's integration into entrepreneurial discourse. The findings indicate that AI enhances operational efficiency and decision-making but challenges traditional notions of entrepreneurial identity and creativity. Furthermore, entrepreneurs increasingly depend on AI for data-driven insights, strategic foresight, and personalised customer interactions, reshaping business strategies and competitive landscapes. This article emphasises the importance of AI literacy and adaptive strategies for entrepreneurs to leverage the human-AI dyad effectively while maintaining values and ethics. It also highlights the significance of concentrating research efforts on entrepreneurs as they are the very cohort to shape new norms and pioneer new business models and innovations. Future research should explore AI's long-term impacts on entrepreneurial ecosystems, including psychological, ethical, and socio-cultural dimensions, with comparative studies across industries and regions providing further insights.}
}
@article{KNUUTTILA201476,
title = {Varieties of noise: Analogical reasoning in synthetic biology},
journal = {Studies in History and Philosophy of Science Part A},
volume = {48},
pages = {76-88},
year = {2014},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2014.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368114000612},
author = {Tarja Knuuttila and Andrea Loettgers},
keywords = {Synthetic biology, Interdisciplinarity, Analogical reasoning, Engineering sciences, Complex systems, Noise},
abstract = {The picture of synthetic biology as a kind of engineering science has largely created the public understanding of this novel field, covering both its promises and risks. In this paper, we will argue that the actual situation is more nuanced and complex. Synthetic biology is a highly interdisciplinary field of research located at the interface of physics, chemistry, biology, and computational science. All of these fields provide concepts, metaphors, mathematical tools, and models, which are typically utilized by synthetic biologists by drawing analogies between the different fields of inquiry. We will study analogical reasoning in synthetic biology through the emergence of the functional meaning of noise, which marks an important shift in how engineering concepts are employed in this field. The notion of noise serves also to highlight the differences between the two branches of synthetic biology: the basic science-oriented branch and the engineering-oriented branch, which differ from each other in the way they draw analogies to various other fields of study. Moreover, we show that fixing the mapping between a source domain and the target domain seems not to be the goal of analogical reasoning in actual scientific practice.}
}
@article{FURTADO2024100086,
title = {A task-oriented framework for generative AI in design},
journal = {Journal of Creativity},
volume = {34},
number = {2},
pages = {100086},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100086},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000128},
author = {Lara Sucupira Furtado and Jorge Barbosa Soares and Vasco Furtado},
keywords = {Generative artificial intelligence, Product, Creative computing, Transformational Creativity},
abstract = {The intersection of Artificial Intelligence and Design disciplines such as Architecture, Urban Planning, Engineering and Product Design has been a longstanding pursuit, with Generative AI (GAI) ushering in a new era of possibilities. The research presented here explores how GAI can enhance creativity and assist Design practitioners with tasks to create products such as, but not limited to, renderings, concepts, construction techniques, materials, data analytics or maps. We apply a framework of combinational, exploratory and transformational creativity to organize how recent advancements in GAI can support each creative category. We propose a conceptual framework of GAI towards transformational creativity, and identify real-world examples to demonstrate GAI's impact, such as transforming sketches into detailed renders, facilitating real-time 3D model generation, predicting trends through analytics and creating images or reports via text prompts. Our work envisions a future where GAI becomes a real-time collaborator to complete certain automated tasks while liberating Designers to focus on transformational innovation.}
}
@article{CARAMIA2022100040,
title = {Sustainable two stage supply chain management: A quadratic optimization approach with a quadratic constraint},
journal = {EURO Journal on Computational Optimization},
volume = {10},
pages = {100040},
year = {2022},
issn = {2192-4406},
doi = {https://doi.org/10.1016/j.ejco.2022.100040},
url = {https://www.sciencedirect.com/science/article/pii/S2192440622000168},
author = {Massimiliano Caramia and Giuseppe Stecca},
keywords = {Supply chain optimization, Green management, Successive linear approximations},
abstract = {Designing a supply chain to comply with environmental policy requires awareness of how work and/or production methods impact the environment and what needs to be done to reduce those environmental impacts and make the company more sustainable. This is a dynamic process that occurs at both the strategic and operational levels. However, being environmentally friendly does not necessarily mean improving the efficiency of the system at the same time. Therefore, when allocating a production budget in a supply chain that implements the green paradigm, it is necessary to figure out how to properly recover costs in order to improve both sustainability and routine operations, offsetting the negative environmental impact of logistics and production without compromising the efficiency of the processes to be executed. In this paper, we study the latter problem in detail, focusing on the CO2 emissions generated by the transportation from suppliers to production sites, and by the production activities carried out in each plant. We do this using a novel mathematical model that has a quadratic objective function and all linear constraints except one, which is also quadratic, and models the constraint on the budget that can be used for green investments caused by the increasing internal complexity created by large production flows in the production nodes of the supply network. To solve this model, we propose a multistart algorithm based on successive linear approximations. Computational results show the effectiveness of our proposal.}
}
@article{GAO2022112486,
title = {Regarding the shallow water in an ocean via a Whitham-Broer-Kaup-like system: hetero-Bäcklund transformations, bilinear forms and M solitons},
journal = {Chaos, Solitons & Fractals},
volume = {162},
pages = {112486},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112486},
url = {https://www.sciencedirect.com/science/article/pii/S0960077922006944},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Ocean, Shallow water, Whitham-Broer-Kaup-like system, Hetero-Bäcklund transformations, Bilinear forms,  solitons, Symbolic computation},
abstract = {Considering the water waves, people have investigated many systems. In this paper, what we study is a Whitham-Broer-Kaup-like system for the dispersive long waves in the shallow water in an ocean. With respect to the water-wave horizontal velocity and deviation height from the equilibrium of the water, we construct (A) two branches of the hetero-Bäcklund transformations, from that system to a known constant-coefficient nonlinear dispersive-wave system, (B) two branches of the bilinear forms and (C) two branches of the M-soliton solutions, with M as a positive integer. Results rely upon the oceanic shallow-water coefficients in that system.}
}
@article{MANNUCCI2023103265,
title = {Exploring potential futures: Evaluating the influence of deep uncertainties in urban planning through scenario planning: A case study in Rome, Italy},
journal = {Futures},
volume = {154},
pages = {103265},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103265},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723001696},
author = {Simona Mannucci and Jan H. Kwakkel and Michele Morganti and Marco Ferrero},
keywords = {Scenario planning, Deep uncertainties, Urban planning, Decision-making, Climate adaptation},
abstract = {Cities play a critical role in developing adaptable strategies to address the challenges posed by climate change. However, the inherent complexity of urban environments and their uncertain future conditions necessitate exploring innovative approaches and tools to assist the current planning practices. This paper presents a workflow rooted in model-based scenario planning for long-term adaptation planning given uncertain futures. To demonstrate the workflow’s effectiveness, a pertinent case study was conducted in a flood-prone area of Rome. The study employed a land-use change model to examine potential urban growth patterns, considering the uncertain implementation of new poles of attraction. This interdisciplinary study constitutes an initial stride toward implementing uncertainty within urban planning frameworks. Future prospects encompass the integration of multiple models for cross-scale analysis, embracing further critical environmental and social aspects. This research contributes to advancing urban resilience strategies. It enhances the understanding of adapting to an uncertain future in the face of climate change, as urban areas must embrace comprehensive planning to ensure flexible adaptation when faced with climate-driven uncertainties in long-term planning. In conclusion, the study underscores that embracing uncertainty is a challenge and a pivotal opportunity to shape resilient and adaptable urban futures.}
}
@article{SHAHRYARI2021101395,
title = {Energy and task completion time trade-off for task offloading in fog-enabled IoT networks},
journal = {Pervasive and Mobile Computing},
volume = {74},
pages = {101395},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101395},
url = {https://www.sciencedirect.com/science/article/pii/S1574119221000535},
author = {Om-Kolsoom Shahryari and Hossein Pedram and Vahid Khajehvand and Mehdi Dehghan TakhtFooladi},
keywords = {Internet of Things, Fog computing, Task offloading, Genetic algorithm, Particle swarm optimization, Resource allocation},
abstract = {In order to improve the quality of experience in executing computation-intensive tasks of real-time IoT applications in a fog-enabled IoT network, resource-constrained IoT devices can offload the tasks to resource-rich nearby fog nodes. It causes a reduction in energy consumption compared with local processing, although it extends task completion time due to communication latency. In this paper, we propose a task offloading scheme that optimizes task offloading decision, fog node selection, and computation resource allocation, investigating the trade-off between task completion time and energy consumption. Weighting coefficients of time and energy consumption are determined based on specific demands of the user and residual energy of devices’ battery. Accordingly, we formulate the task offloading problem as a mixed-integer nonlinear program (MINLP), which is NP-hard. A sub-optimal algorithm based on the hybrid of genetic algorithm and particle swarm optimization is designed to solve the formulated problem. Extensive simulations prove the convergence of the proposed algorithm and its superior performance in comparison with baseline schemes.}
}
@article{PAVLOVA2024103631,
title = {A dual process model of spontaneous conscious thought},
journal = {Consciousness and Cognition},
volume = {118},
pages = {103631},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103631},
url = {https://www.sciencedirect.com/science/article/pii/S105381002300168X},
author = {Maria K. Pavlova},
keywords = {Automatic processing, Cognitive control, Executive failure, Involuntary attention, Mental effort, Mind wandering, Meta-awareness, Modality and interference in working memory, Process–occurrence framework, Spontaneous thought},
abstract = {In the present article, I review theory and evidence on the psychological mechanisms of mind wandering, paying special attention to its relation with executive control. I then suggest applying a dual-process framework (i.e., automatic vs. controlled processing) to mind wandering and goal-directed thought. I present theoretical arguments and empirical evidence in favor of the view that mind wandering is based on automatic processing, also considering its relation to the concept of working memory. After that, I outline three scenarios for an interplay between mind wandering and goal-directed thought during task performance (parallel automatic processing, off-task thought substituting on-task thought, and non-disruptive mind wandering during controlled processing) and address the ways in which the mind-wandering and focused-attention spells can terminate. Throughout the article, I formulate empirical predictions. In conclusion, I discuss how automatic and controlled processing may be balanced in human conscious cognition.}
}
@article{MORIN1992371,
title = {From the concept of system to the paradigm of complexity},
journal = {Journal of Social and Evolutionary Systems},
volume = {15},
number = {4},
pages = {371-385},
year = {1992},
issn = {1061-7361},
doi = {https://doi.org/10.1016/1061-7361(92)90024-8},
url = {https://www.sciencedirect.com/science/article/pii/1061736192900248},
author = {Edgar Morin},
abstract = {This paper is an overview of the author's ongoing reflections on the need for a new paradigm of complexity capable of informing all theories, whatever their field of application or the phenomena in question. Beginning with a critique of General System Theory and the principle of holism with which it is associated, the author suggests that contemporary advances in our knowledge of organization call for a radical reformation in our organization of knowledge. This reformation involves the mobilization of recursive thinking, which is to say a manner of thinking capable of establishing a dynamic and generative feedback loop between terms or concepts (such as whole and part, order and disorder, observer and observed, system and ecosystem, etc.) that remain both complementary and antagonistic. The paradigm of complexity thus stands as a bold challenge to the fragmentary and reductionistic spirit that continues to dominate the scientific enterprise.}
}
@article{PRADEEP2025101194,
title = {Sustainable solutions in the era of water scarcity: Multi-basin solar still innovations},
journal = {Desalination and Water Treatment},
volume = {322},
pages = {101194},
year = {2025},
issn = {1944-3986},
doi = {https://doi.org/10.1016/j.dwt.2025.101194},
url = {https://www.sciencedirect.com/science/article/pii/S1944398625002103},
author = {B Pradeep and S Joe Patrick Gnanaraj and R Samuel Sanjay Raja},
keywords = {Solar desalination, Stepped basin configuration, Thermal efficiency, Compartmental basin, Corrugated basin},
abstract = {This study addresses the urgent challenge of clean water scarcity in the era of sustainability by investigating advanced multi-basin solar stills with stepped configurations designed to enhance water yield and thermal efficiency. Traditional single basin designs often fall short in productivity and thermal performance, which this research overcomes through geometric innovations such as fins, corrugated surfaces, and compartmental structures. Six basin configurations were tested, revealing that the compartmental basin with fins achieved the highest productivity of 27.8 liters over 12 hours, a 124 % improvement compared to the plain single basin and 26 % higher yield than the corrugated basin with fins. Adding fins to corrugated and compartmental basins further increased productivity by 40 % and 41 %, respectively. These advanced designs align with the sustainability goals of this era, demonstrating significant cost-effectiveness and environmental benefits, including a 70 % energy utilization efficiency and carbon emissions reduction of up to 320 kg CO₂/year. The findings emphasize the transformative potential of innovative basin configurations in revolutionizing solar desalination systems to meet the pressing water needs of a growing global population while adhering to sustainable development objectives.}
}
@article{ANTONIETTI20082172,
title = {Undergraduates’ metacognitive knowledge about the psychological effects of different kinds of computer-supported instructional tools},
journal = {Computers in Human Behavior},
volume = {24},
number = {5},
pages = {2172-2198},
year = {2008},
note = {Including the Special Issue: Internet Empowerment},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2007.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0747563207001598},
author = {Alessandro Antonietti and Barbara Colombo and Yuri Lozotsev},
keywords = {Metacognition, Belief, Learning, Education, Computer},
abstract = {Literature about metacognition suggests that learners develop personal beliefs about the educational technologies that they are asked to employ and that such beliefs can influence learning outcomes. In this perspective, opinions about the psychological effects of computer-supported instructional tools were analysed by means of a questionnaire which included items about the motivational and emotional aspects of learning, the behaviour to have during the learning process, the mental abilities and the style of thinking required, and the cognitive benefits. Items were presented five times: each time they made reference to a different kind of tool (online courses, hypertexts, Web forums, multimedia presentations, and virtual simulations). The questionnaire was filled out by 99 undergraduates attending engineering courses. Results showed that students ranked the psychological effects of the computer-supported tools in a relative different order according to the kind of tool and attributed distinctive effects to each tool. Gender and expertise played a minor role in modulating undergraduates’ beliefs. Implications for instruction were discussed.}
}
@article{AUBIN2014204,
title = {“Principles of Mechanics that are Susceptible of Application to Society”: An unpublished notebook of Adolphe Quetelet at the root of his social physics},
journal = {Historia Mathematica},
volume = {41},
number = {2},
pages = {204-223},
year = {2014},
issn = {0315-0860},
doi = {https://doi.org/10.1016/j.hm.2014.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0315086014000020},
author = {David Aubin},
keywords = {Mechanics, Sociology, Adolphe Quetelet, Astronomy, Social physics, Average man, Applications of mathematics, Analogical thinking},
abstract = {Founder of the Brussels Observatory, Adolphe Quetelet (1796–1874) is especially well known for his theory of the average man. Like the average position of a star obtained through a large quantity of observed data, the average man was, according to Quetelet, subject to fixed causal laws. Published in 1835, his book On Man: Essay of Social Physics is one of the founding works of sociology and mathematical statistics. The sources of the analogy between astronomy and social physics have been debated by historians. To shed light on this question and the conditions of application of mathematics in the 19th century, we publish for the first time a manuscript that is kept in Quetelet's papers at the Royal Academy of Belgium, and give an English translation of it.}
}
@article{GOSWAMI2024111921,
title = {Real-time evaluation of object detection models across open world scenarios},
journal = {Applied Soft Computing},
volume = {163},
pages = {111921},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111921},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006951},
author = {Puneet Goswami and Lakshita Aggarwal and Arun Kumar and Rahul Kanwar and Urvi Vasisht},
keywords = {Computer vision, Faster R-CNN, DETR, YOLO, Resnet 50, Resnet 101, Object detection, Model comparison, Evaluation metrices},
abstract = {Object detection models have been experiencing significant improvements over the years due to advancements in deep learning techniques, increased availability of large-scale annotated datasets, and computational resources. Different object detection models have varying levels of accuracy, speed, and robustness. With the increasing complexity and diversity of object detection models, it becomes a problem for researchers and practitioners to choose the most suitable model for their specific needs. This research paper outlines the escalating demand for robust comparison of object detection models in response to rapidly advancing technology. This evaluation helps in identifying the strengths and weaknesses of these models and selecting the most suitable one for a specific task. This highlights a significant challenge stemming from the lack of recent comparative studies on object detection models across various image qualities, object sizes, and training data sizes. The above challenges are tackled by a meticulous evaluation of three state-of-the-art object detection models: YOLO-v8, Faster R-CNN with ResNet 50 and 101 backbones, and End-to-End Object Detection Transformers (DETR) utilizing ResNet 50 and 101 backbones by employing a rigorous assessment framework encompassing mean Average Precision (mAP), accuracy, and inference speed. This study focuses on thoroughly examining how well the models perform across three different datasets: TACO, PlastOPol, and TACO 4.5. These datasets consist of open-world images captured in real-time from various locations. They include 1500, 2500, and 6500 images respectively, depicting real-world environments with varying lighting conditions and complex backgrounds. The results identify YOLOv8 as the superior model for high and medium-quality images, while Faster R-CNN performs better for low-quality images. However, DETR's accuracy falls short compared to other models. The paper fills a crucial gap in understanding model performance across varying image qualities and object sizes and helps in taking informed decisions in object detection systems.}
}