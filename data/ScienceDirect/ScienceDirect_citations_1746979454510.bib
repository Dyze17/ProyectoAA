@article{GOMES2019411,
title = {State-of-the-art of transmission expansion planning: A survey from restructuring to renewable and distributed electricity markets},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {111},
pages = {411-424},
year = {2019},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2019.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S014206151831888X},
author = {Phillipe Vilaça Gomes and João Tomé Saraiva},
keywords = {Heuristics, Optimization, Mathematical programming, Metaheuristic, Transmission expansion planning},
abstract = {Transmission Expansion Planning (TEP) problem aims at identifying when and where new equipment as transmission lines, cables and transformers should be inserted on the grid. The transmission upgrade capacity is motivated by several factors as meeting the increasing electricity demand, increasing the reliability of the system and providing non-discriminatory access to cheap generation for consumers. However, TEP problems have been changing over the years as the electrical system evolves. In this way, this paper provides a detailed historical analysis of the evolution of the TEP over the years and the prospects for this challenging task. Furthermore, this study presents an outline review of more than 140 recent articles about TEP problems, literature insights and identified gaps as a critical thinking in how new tools and approaches on TEP can contribute for the new era of renewable and distributed electricity markets.}
}
@article{SHAN201032,
title = {Study on large time-delay constant temperature control system based on TEC},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {17},
pages = {32-35},
year = {2010},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(09)60586-0},
url = {https://www.sciencedirect.com/science/article/pii/S1005888509605860},
author = {Jiang-dong SHAN and Ge WU and Xiao-jian TIAN},
keywords = {thermoelectric cooler, large time-delay control system, proportion integration differentiation (PID) control, constant temperature control},
abstract = {This paper designes a diminutive constant temperature control system based on thermoelectric cooler (TEC). Considering that the system is a large time-delay control system, the paper proposes a new method to determine the transfer function of the controlled system which gets the transfer function by doing nonlinear fitting of the step response of the controlled system. The characteristics of system model which is established by the method are basically same as the actual constant temperature control system. This method provides a new way of thinking to the design of large time-delay control system.}
}
@incollection{KATZ202453,
title = {Chapter Three - The role of big data and analytics in utilities innovation},
editor = {Reza Arghandeh and Yuxun Zhou},
booktitle = {Big Data Application in Power Systems (Second Edition)},
publisher = {Elsevier Science},
edition = {Second Edition},
pages = {53-68},
year = {2024},
isbn = {978-0-443-21524-7},
doi = {https://doi.org/10.1016/B978-0-443-21524-7.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443215247000037},
author = {Jeffrey S. Katz},
keywords = {Big data, Analytics, Data science, Smart grid, Power system simulation, Numerical weather analysis, Smarter energy research},
abstract = {The computational technology known as big data and its subsequent processing, analytics, are driving innovation in electric power system integration of renewable energy, outage prediction, processing of increasing volumes of smart grid data, as well as the velocity of such data. In the age of cybersecurity, the veracity of this data is also a factor. The almost concurrent rise of cognitive computing gives new importance to unstructured data such as drone images and text in maintenance reports. The intelligent connection of real-time numerical data with written and visual data gives rise to even more innovation. The benefits of high-precision weather modeling on power demand, grid damage, and solar- and wind-based generation are also considered.}
}
@article{2024100676,
title = {Erratum regarding missing declaration of competing interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100676},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100676},
url = {https://www.sciencedirect.com/science/article/pii/S221286892400045X}
}
@article{EBELING2023120768,
title = {A multi-dimensional framework to analyze group behavior based on political polarization},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120768},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120768},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423012708},
author = {Régis Ebeling and Jéferson Nobre and Karin Becker},
keywords = {Analysis framework, Political polarization, Group behavior, Topic modeling, Social network analysis, COVID-19},
abstract = {The recent wave of elections won by right-wing worldwide brings up increased discussions biased by political polarization, including in social media. Social media data enables the investigation of the contexts where political polarization occurs, enabling to derive insights into how it affects human behavior. Related work has shown how computing techniques can be leveraged to understand political polarization in restricted scenarios, but the complexity of this behavior can be better understood when considered from different viewpoints. This article describes a multi-dimensional analysis framework to study the behavior of groups on Twitter in politically polarized scenarios. It can be applied to various themes where groups display stances that can be politically biased, and it aggregates a wide range of computational techniques in an innovative way to provide rich insights. The framework includes guidelines and techniques to: (a) collect data on Twitter to represent the groups; (b) automatically infer the political leaning of users; (c) derive topological properties of the groups’ social network and analyze political influence; (d) identify topics representing concerns at coarse and fine-grained granularity levels using a hybrid topic modeling approach; (e) identify psychological aspects based on linguistic cues, and (f) analyze the sources of information disseminated by the groups. Applying the framework in two case studies related to COVID-19 revealed patterns of behavior common to ideologies. We confirmed that the stances were politically motivated and that both groups, left/right, were subject to the echo chamber effect. Comparatively, the social structure of the right-oriented groups is more connected, and they rely on politicians and social media for information spreading. Left-oriented groups are less connected and more prone to facts. The psychological aspects reveal that both groups are emotionally distressed in arguing about being right, given their beliefs.}
}
@article{2024100677,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100677},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100677},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000461}
}
@article{KEE19891479,
title = {A computational model of the structure and extinction of strained, opposed flow, premixed methane-air flames},
journal = {Symposium (International) on Combustion},
volume = {22},
number = {1},
pages = {1479-1494},
year = {1989},
issn = {0082-0784},
doi = {https://doi.org/10.1016/S0082-0784(89)80158-4},
url = {https://www.sciencedirect.com/science/article/pii/S0082078489801584},
author = {Robert J. Kee and James A. Miller and Gregory H. Evans and Graham Dixon-Lewis},
abstract = {The application of laminar flamelet concepts to turbulent flame propagation requires a detailed understanding of strained laminar flames. Here we use numerical methods, including are-length continuation, to simulate the complex chemical kinetic behavior in premixed methane-air flames that are stabilized between two opposed-flow burners. We predict both the detailed structure and the extinction limits for these flames over a range of fuel-air mixtures. In addition to discussing the flame structure, a sensitivity analysis provides further insight on the chemical behavior near extinction. Finally, we discuss the comparison of the predictions with Law's experimental extinction data. An especially important aspect of this comparison is the recognition that fluid mechanical aspects of the traditional strained-flame analysis are deficient in representing experiments such as Law's. We develop and solve a new system of equations that is able to describe the experiments much more accurately.}
}
@article{THEODOROU20075697,
title = {Hierarchical modelling of polymeric materials},
journal = {Chemical Engineering Science},
volume = {62},
number = {21},
pages = {5697-5714},
year = {2007},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2007.04.048},
url = {https://www.sciencedirect.com/science/article/pii/S000925090700382X},
author = {Doros N. Theodorou},
keywords = {Polymers, Mathematical modelling, Simulation, Rheology, Nanostructure, Diffusion},
abstract = {Within the last 20 years, computer simulations of materials have evolved from an academic curiosity to a predictive tool for addressing structure–property–processing–performance relations that are critical to the design of new products and processes. Chemical engineers, with their problem-oriented thinking and their systems approach, have played a significant role in this development. The computational prediction of physical properties is particularly challenging for polymeric materials, because of the extremely broad spectra of length and time scales governing structure and molecular motion in these materials. This challenge can only be met through the development of hierarchical analysis and simulation strategies encompassing many interconnected levels, each level addressing phenomena over a specific window of time and length scales. In this paper we will briefly discuss the fundamental underpinnings and example applications of new methods and algorithms for the hierarchical modelling of polymers. Questions to be addressed include: How can one equilibrate atomistic models of long-chain polymer melts at all length scales and thereby predict thermodynamic and conformational properties reliably? How can one quantify the structure of entanglement networks present in these melts through topological analysis and relate it to rheological properties? Are there ways to predict the microphase-separated morphology and stress–strain behaviour of multicomponent block copolymer-based materials, such as pressure sensitive adhesives? Is it possible to anticipate changes in the barrier properties of glassy amorphous polymers used in packaging applications as a consequence of modifications in the chemical constitution of chains?}
}
@article{SHI2024100685,
title = {Drug development in the AI era: AlphaFold 3 is coming!},
journal = {The Innovation},
volume = {5},
number = {5},
pages = {100685},
year = {2024},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100685},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824001231},
author = {Yi Shi}
}
@article{NASCIMENTO2023105421,
title = {Core–shell clustering approach for detection and analysis of coastal upwelling},
journal = {Computers & Geosciences},
volume = {179},
pages = {105421},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001255},
author = {Susana Nascimento and Alexandre Martins and Paulo Relvas and Joaquim F. Luís and Boris Mirkin},
keywords = {Spatio-temporal clustering, Time series segmentation, SST images, Coastal upwelling, Core–shell cluster},
abstract = {A comprehensive approach is presented to analyze season’s coastal upwelling represented by weekly sea surface temperature (SST) image grids. The proposed model, core–shell clustering, assumes that the season’s upwelling can be divided into shorter periods of stability, time ranges, consisting of constant core and variable shell parts. A one-by-one core–shell clustering algorithm is provided. The algorithm parameters are automatically derived from the least-squares clustering criterion. The approach applies to SST gridded data for sixteen successive years (2004–2019) of coastal upwelling in the western Iberian coast, the northernmost branch of the Canary Current Upwelling System. Our results show that at each season, there are 3 to 5 time intervals, the ranges, at which the upwelling presents stable core patterns of relatively cold water surrounded by somewhat larger shell areas of warmer waters. Based on other experimental computations performed by our team, we conclude that this pattern is not just a purely local phenomenon but has a more global meaning. Inter-annual time series analysis are consistent among themselves and with existing expert domain knowledge.}
}
@article{BATTLEDAY20151865,
title = {Modafinil for cognitive neuroenhancement in healthy non-sleep-deprived subjects: A systematic review},
journal = {European Neuropsychopharmacology},
volume = {25},
number = {11},
pages = {1865-1881},
year = {2015},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2015.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X15002497},
author = {R.M. Battleday and A.-K. Brem},
keywords = {Neuroenhancement, Modafinil, Cognitive, Psychometric, Enhancement, Nootropic},
abstract = {Modafinil is an FDA-approved eugeroic that directly increases cortical catecholamine levels, indirectly upregulates cerebral serotonin, glutamate, orexin, and histamine levels, and indirectly decreases cerebral gamma-amino-butrytic acid levels. In addition to its approved use treating excessive somnolence, modafinil is thought to be used widely off-prescription for cognitive enhancement. However, despite this popularity, there has been little consensus on the extent and nature of the cognitive effects of modafinil in healthy, non-sleep-deprived humans. This problem is compounded by methodological discrepancies within the literature, and reliance on psychometric tests designed to detect cognitive effects in ill rather than healthy populations. In order to provide an up-to-date systematic evaluation that addresses these concerns, we searched MEDLINE with the terms “modafinil” and “cognitive”, and reviewed all resultant primary studies in English from January 1990 until December 2014 investigating the cognitive actions of modafinil in healthy non-sleep-deprived humans. We found that whilst most studies employing basic testing paradigms show that modafinil intake enhances executive function, only half show improvements in attention and learning and memory, and a few even report impairments in divergent creative thinking. In contrast, when more complex assessments are used, modafinil appears to consistently engender enhancement of attention, executive functions, and learning. Importantly, we did not observe any preponderances for side effects or mood changes. Finally, in light of the methodological discrepancies encountered within this literature, we conclude with a series of recommendations on how to optimally detect valid, robust, and consistent effects in healthy populations that should aid future assessment of neuroenhancement.}
}
@article{BORIS2013113,
title = {Flux-Corrected Transport looks at forty},
journal = {Computers & Fluids},
volume = {84},
pages = {113-126},
year = {2013},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2013.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0045793013001874},
author = {Jay Paul Boris},
keywords = {Flux-Corrected Transport (FCT), Monotonicity, Positivity, Computational Fluid Dynamics (CFDs), Large Eddy Simulation (LES), Monotone Integrated Large Eddy Simulation (MILES), Implicit Large Eddy Simulation (ILES)},
abstract = {This year, 2013, marks the 40th anniversary of the journal article “Flux-Corrected Transport I. SHASTA, A Fluid Transport Algorithm That Works” by Jay Boris and David Book [1]. Flux-Corrected Transport (FCT) removed a serious roadblock to advances in Computational Fluid Dynamics (CFD) by enabling the accurate treatment of strong, time-dependent shock problems in blast, reactive-flow, and combustion physics, and in aerodynamics and astrophysics. Steep gradients in conserved fluid variables could now be convected across a computational grid without the appearance of spurious oscillations and physically impossible negative values. The nonlinear “flux-correction” algorithm introduced in FCT imposes the physical properties of conservation, locality, causality, and monotonicity on the numerical solutions for convection without adding a great deal of numerical diffusion. This article shows that implementing these physical properties in solving the continuity equation through high-resolution FCT also results in a serviceable Large-Eddy Simulation treatment of turbulent flows without need for additional “subgrid turbulence models.” We have named this simplified approach Monotone Integrated Large Eddy Simulation (MILES).}
}
@article{DILUZIO2023104418,
title = {A randomized deep neural network for emotion recognition with landmarks detection},
journal = {Biomedical Signal Processing and Control},
volume = {81},
pages = {104418},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104418},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422008722},
author = {Francesco {Di Luzio} and Antonello Rosato and Massimo Panella},
keywords = {Emotion recognition, Randomized neural networks, Facial landmarks, Deep learning, Video sequences},
abstract = {In this paper, we present an innovative deep neural architecture employing parameter randomization in a complex classification model for emotion recognition. Actually, randomized deep neural networks represent an interesting alternative to exploring the efficiency-to-accuracy balance in real-life applications. Moreover, we also introduce the use of input frames composed of 468 facial landmarks coordinates and an innovative sampling procedure avoiding padding. The proposed randomized classifier is trained for emotion recognition on video sequences and the related accuracy is compared with a non-randomized version of the same model and with well-known benchmark architectures, demonstrating the robustness of the proposed approach in terms of classification accuracy and training time.}
}
@article{CAI20051145,
title = {BioSim—a biomedical character-based problem solving environment},
journal = {Future Generation Computer Systems},
volume = {21},
number = {7},
pages = {1145-1156},
year = {2005},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2004.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X04000469},
author = {Yang Cai and Ingo Snel and Betty Cheng and B. {Suman Bharathi} and Clementine Klein and Judith Klein-Seetharaman},
keywords = {Scientific visualization, Biological discovery, Game design, Problem solving, Artificial life, Education},
abstract = {Understanding and solving biomedical problems requires insight into the complex interactions between the components of biomedical systems by domain and non-domain experts. This is challenging because of the enormous amount of data and knowledge in this domain. Therefore, non-traditional educational tools have been developed such as a biological storytelling system, animations of biomedical processes and concepts, and interactive virtual laboratories. The next-generation problem solving tools need to be more interactive to include users with any background, while remaining sufficiently flexible to target open research problems at any level of abstraction, from the conformational changes of a protein to the interaction of the various biochemical pathways in our body. Here, we present an interactive and visual problem solving environment for the biomedical domain. We designed a biological world model, in which users can explore biological interactions by role-playing “characters” such as cells and molecules or as an observer in a “shielded vessel”, both with the option of networked collaboration between simultaneous users. The system architecture of these “characters” contains four main components: (1) bio-behavior is modeled using cellular automata; (2) bio-morphing uses vision-based shape tracking techniques to learn from recordings of real biological dynamics; (3) bio-sensing is based on molecular principles of recognition to identify objects, environmental conditions and progression in a process; (4) bio-dynamics implements mathematical models of cell growth and fluid-dynamic properties of biological solutions. The principles are implemented in a simple world model of the human vascular system and a biomedical problem that involves an infection by Neisseria meningitides where the biological characters are white and red blood cells and Neisseria cells. Our case studies show that the problem solving environment can inspire user's strategic, creative and innovative thinking.}
}
@article{DIRKSEN2022102994,
title = {From agent to action: The use of ethnographic social simulation for crime research},
journal = {Futures},
volume = {142},
pages = {102994},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102994},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000945},
author = {Vanessa Dirksen and Martin Neumann and Ulf Lotzmann},
keywords = {Agent-based modelling, Complementarity, Computational social science, Simulation, Ethnography, Policing},
abstract = {This paper proposes a methodology for grounding agent-based social simulation in ethnographic data, using the example of crime research. The application of computational tools in crime research typically entails a removal of the “intelligible frame” of criminal behaviour and, hence, of meaningful evidence. Ethnography is a microscopic research tradition geared towards the preservation of contextualized meaning deemed essential for the exploration of the variety of prospective alternative scenarios and, hence, of plausible futures. On the basis of exemplary empirical material from a qualitative study on the transit trade of cocaine in the Netherlands, this paper looks into the complementarity and potential integration of the research traditions of ethnography and agent-based modelling. That is to say, it explores the compatibility of the formal languages of both these domains and the mutual benefit of “stitching together” these at first sight very different methods. The ethnographic approach to social simulation specifies the what-if relations of traditional/conventional ABM modelling into condition-action sequences. As we contend, it is exactly this more microscopic level of condition-action sequences that is needed to facilitate ”thick description” and, in turn, enable the grounding of ABM in meaningful evidence.}
}
@article{FALL2010140,
title = {Artificial states? On the enduring geographical myth of natural borders},
journal = {Political Geography},
volume = {29},
number = {3},
pages = {140-147},
year = {2010},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2010.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0962629810000533},
author = {Juliet J. Fall},
keywords = {Artificial states, Boundaries, Ethnic homogeneity, Failed states, Nationalism, Natural boundaries, Territorial trap},
abstract = {Alberto Alesina, William Easterly and Janina Matuszeski's paper Artificial States, published as a National Bureau of Economic Research Working Paper in June 2006, suggests a theory linking the nature of country borders to the economic success of countries (Alesina, Easterly, & Matuszeski, 2006). This paper critically examines this suggestion that natural boundaries and ethnic homogeneity are desirable for economic reasons. It takes issue with the understanding of artificial and natural boundaries that they develop, arguing that this ignores two centuries of critical and quantitative geographical scholarship that has mapped, documented and critiqued the obsession of a link between topography and the appropriate shape of states and boundaries. It explores how their argument is linked to a defence of ethnically homogeneous states. The focus is on their teleological and paradoxically ahistorical vision that naturalizes politics by appealing to spatial myths of homogeneity and geometric destiny, grounded in a reactionary understanding of space as container. In so doing, I am mindful of the strong links between such proposals and calls for post-conflict partition, and the corresponding discourses of ethnic and cultural homogenization on which they rely. Instead of thinking of boundaries as geometric objects, squiggly or not, I consider boundaries through the simultaneous processes of reification, naturalization, and fetishization.}
}
@article{IVANITSKY2009101,
title = {Brain science: On the way to solving the problem of consciousness},
journal = {International Journal of Psychophysiology},
volume = {73},
number = {2},
pages = {101-108},
year = {2009},
note = {Neural Processes in Clinical Psychophysiology},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2009.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167876009001044},
author = {Alexey M. Ivanitsky and George A. Ivanitsky and Olga V. Sysoeva},
keywords = {Consciousness and brain problem, Event-related potentials, EEG rhythms, Semantic brain systems, Artificial intelligence},
abstract = {Four issues are discussed: the possible mechanism of subjective events, conscious versus unconscious brain functions, the rhythmic coding of mental operations and the possible brain basis of understanding.i.Several approaches have been developed to explain how subjective experience emerges from brain activity. One of them is the return of the nervous impulses to the sites of their primary projections, providing a synthesis of sensory information with memory and motivation [Ivanitsky, A.M., 1976. Brain Mechanisms of the Signal Evaluation. Medicina, Moscow 264 pp. (in Russian)]. Support for the existence of such a mechanism stems from studies upon the brain activity that subserves perception (visual and somato-sensory) and thought (verbal and imaginative). The cortical centres for information synthesis have been found. For perception, these are located in projection areas; for thinking — in frontal and temporal-parietal associative cortex. Closely related ideas were also developed by G. Edelman [Edelman, G.M., 1978. Group selection and phasic reentrant signaling: A theory of higher brain function. In: Eds. Edelman, G.M., Mountcastle, V.B. The Mindful Brain. Cortical Organization and the Group-selective Theory of Higher Brain Function. Cambridge, MA, MIT Press, pp 51–100.] in his re-entry theory of consciousness. Both theories emphasize the key role of memory and motivation in the origin of conscious function.ii.Conscious experience elucidates not all, but only salient brain functions. As a rule, voluntary control is switched on when additional cognitive resources are needed. Even a rather complicated mental operation, such as the discrimination between concrete and abstract words, could be executed very rapidly and implicitly; explicit analysis being engaged only in more difficult tasks. Furthermore, these two different kinds of mental operations, i.e., automatic and conscious, are predominantly associated with two different kinds of memory: a recognition memory for implicit analysis, and an episodic memory for explicit functions.iii.Rearrangements of EEG rhythms underlie mental functions. Certain rhythmical patterns are related with definite types of mental activity. The dependence of one upon the other is rather pronounced and expressive, so it becomes possible to recognize the type of mental operation being performed in mind with few seconds of the ongoing EEG, provided that the analysis of rhythms is accomplished using an artificial neural network.iv.It is commonly recognized that the computer, in contrast to the living brain, can calculate, yet cannot understand [Penrose, R., 1996. Shadows of the Mind: A Search for the Missing Science of Consciousness New York, Oxford, Oxford University Press 480 pp.]. Comprehension implies the comparison of new and old information that requires the ability to search for associations, grouping similar objects together, and distinguishing different objects from one another. However, these functions may also be implemented on a computer. Still, it is believed that computers perform these complicated operations without genuine understanding. Evidently, comprehension additionally has to be based upon some biologically significant ground. It is hypothesized that the subjective feeling of understanding appears when current information is attributed to a definite need, which is scaled in sign (+/−) coordinates. This coordinate system ceases the brain calculations, when “comprehension” is reached, i.e., the acceptable level of need satisfaction is attained.}
}
@incollection{REYESGARCIA2025151,
title = {Chapter 7 - Decoding imagined speech for EEG-based BCI},
editor = {Ayman S. El-Baz and Jasjit S. Suri},
booktitle = {Brain-Computer Interfaces},
publisher = {Academic Press},
pages = {151-175},
year = {2025},
series = {Advances in Neural Engineering},
isbn = {978-0-323-95439-6},
doi = {https://doi.org/10.1016/B978-0-323-95439-6.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323954396000041},
author = {Carlos A. Reyes-García and Alejandro A. Torres-García and Tonatiuh Hernández-del-Toro and Jesús S. García-Salinas and Luis Villaseñor-Pineda},
keywords = {Brain–computer interfaces (BCI), Classification, Electroencephalograms (EEG), Imagined speech, Silent speech interfaces (SSI)},
abstract = {Brain–computer interfaces (BCIs) are systems that transform the brain's electrical activity into commands to control a device. To create a BCI, it is necessary to establish the relationship between a certain stimulus, internal or external, and the brain activity it provokes. A common approach in BCIs is motor imagery, which involves imagining limb movement. Unfortunately, this approach allows few commands. As an alternative, this chapter presents another approach, an internal language-related stimulus known as imagined speech, which is the action of imagining the diction of a word without emitting any sound or articulating any movement. This neuroparadigm is more intuitive, less subjective, and ambiguous, which are very relevant advantages; however, the cost to properly process the brain signal is not trivial. This chapter describes the main components of an EEG-based imagined speech BCI, along with key works, emerging trends, and challenges in this research area. Regarding the challenges, we present four of them in the pursuit of decoding imagined speech. The first challenge involves accurately recognizing isolated words. The second one is the automatic selection of a subset of EEG channels aiming to reduce computational cost and provide evidence of promising locations for studying imagined speech. The third challenge introduces an innovative approach to addressing scenarios where a new word needs to be added to the vocabulary after the computational model has been trained. Lastly, the fourth challenge concerns the online recognition of words from continuous EEG signals. Despite advances in the area, there is still much work to be done. Important initial steps have been taken in terms of the application of novel techniques for preprocessing, artifact removal, feature extraction, and classification which are the stages to be taken to process the collected signal. Additionally, the community has shared datasets and organized evaluation forums to accelerate the search for solutions.}
}
@article{HOME200255,
title = {Fluids and forces in eighteenth-century electricity},
journal = {Endeavour},
volume = {26},
number = {2},
pages = {55-59},
year = {2002},
issn = {0160-9327},
doi = {https://doi.org/10.1016/S0160-9327(02)01411-4},
url = {https://www.sciencedirect.com/science/article/pii/S0160932702014114},
author = {Roderick W. Home},
abstract = {Our understanding of the history of electricity in the eighteenth century has changed significantly since the early 1960s, when Thomas Kuhn presented it as a leading example to support his general view of the history of science. In particular, while the ideas of Benjamin Franklin are still seen as important, they are no longer seen as constituting a revolution in the theory of electricity. They appear instead as merely one stage in a long drawn-out process of evolution in electrical thinking.}
}
@article{BURCH2023101039,
title = {Investigating two teachers’ development of combinatorial meaning for algebraic structure},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101039},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101039},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000093},
author = {Lori J. Burch},
keywords = {Mathematical meanings, Combinatorial reasoning, Algebraic reasoning, Polynomial operations, Secondary teachers},
abstract = {This paper reports on the results of a four-day teaching experiment that supported two algebra teachers to develop a combinatorial meaning for algebraic structure. The purpose of the teaching episodes was to support the teachers (a) to establish a combinatorial understanding for algebraic structure (Tillema & Burch, 2022) by generalizing the cubic identity, a+b3=a3+3a2b+3ab2+b3, as a symbolization of quantitative and combinatorial relationships out of a contextualized problem (Tillema & Gatza, 2016) and (b) to develop a combinatorial meaning as a mobilization of their understanding through a series of algebraic tasks (cf. Thompson et al., 2014). The findings from this study contribute to research literature on teachers’ mathematical meanings within secondary algebra by investigating how teachers’ combinatorial meanings developed and how differences in their combinatorial meanings impacted their algebraic reasoning. The findings demonstrate a combinatorial pathway for supporting the development of expanding and factoring as reversible polynomial operations (cf. Sangwin & Jones, 2017).}
}
@article{JAYAWARDENA2025118097,
title = {Marine specialized metabolites: Unveiling Nature's chemical treasures from the deep blue},
journal = {TrAC Trends in Analytical Chemistry},
volume = {183},
pages = {118097},
year = {2025},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2024.118097},
url = {https://www.sciencedirect.com/science/article/pii/S0165993624005806},
author = {Thilina U. Jayawardena and Natacha Merindol and Nuwan Sameera Liyanage and Fatima Awwad and Isabel Desgagné-Penix},
keywords = {Natural products, Conservation, Bioprospecting, metabolomics, Biotechnology, Biosynthesis, Isolation and spectroscopic characterization},
abstract = {Marine specialized metabolites (MSM) represent a fascinating realm of chemical diversity with multifaceted functions across the spectrum of life on Earth. These metabolites serve as weapons, metal transporters, regulatory agents, and more. The conservation of genes responsible for their production over extensive evolutionary timescales underscores their selective advantage. Recent decades have witnessed an upsurge in MSM studies, driven by advancements in analytical techniques and the ever-growing accessibility of the aquatic environment. Marine macro and microorganisms offer a rich tapestry of specialized metabolites, some exhibiting potent activities in diverse domains, including medicine. The study of MSM presents several challenges, reflecting the need to separate complex mixtures into individual bioactive metabolites and utilize state-of-the-art extraction methods. Comprehensive structural analysis relies on advanced spectroscopic approaches, including nuclear magnetic resonance and mass spectrometry. These tools are instrumental in unravelling the chemical diversity of MSM and understanding their potential applications. While bioprospecting offers enormous potential, it raises critical challenges concerning sustainability, conservation, and equitable benefit-sharing. International protocols like the Nagoya Protocol seeks to regulate access to and share benefits from genetic resources, with considerable implications for marine bioprospecting. The convergence of advanced metabolomics, metagenomics, and synthetic biology offers promising avenues for accelerating the discovery and sustainable production of MSM, shaping the future of this field. This comprehensive review provides a deep dive into the challenges, methodologies, and emerging trends in studying marine-derived natural products, underscoring the immense potential of MSM for advancing chemical sciences and their transformative applications in diverse areas such as food, medicine, biotechnology, and environmental conservation. By bridging multiple disciplines, the continued exploration and sustainable utilization of these metabolites hold the promise of unlocking new innovations for society's benefit.}
}
@article{KEAN2025109125,
title = {Intuitive physical reasoning is not mediated by linguistic nor exclusively domain-general abstract representations},
journal = {Neuropsychologia},
volume = {213},
pages = {109125},
year = {2025},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2025.109125},
url = {https://www.sciencedirect.com/science/article/pii/S0028393225000600},
author = {Hope H. Kean and Alexander Fung and R.T. Pramod and Jessica Chomik-Morales and Nancy Kanwisher and Evelina Fedorenko},
abstract = {The ability to reason about the physical world is a critical tool in the human cognitive toolbox, but the nature of the representations that mediate physical reasoning remains debated. Here, we use fMRI to illuminate this question by investigating the relationship between the physical-reasoning system and two well-characterized systems: a) the domain-general Multiple Demand (MD) system, which supports abstract reasoning, including mathematical and logical reasoning, and b) the language system, which supports linguistic computations and has been hypothesized to mediate some forms of thought. We replicate prior findings of a network of frontal and parietal areas that are robustly engaged by physical reasoning and identify an additional physical-reasoning area in the left frontal cortex, which also houses components of the MD and language systems. Critically, direct comparisons with tasks that target the MD and the language systems reveal that the physical-reasoning system overlaps with the MD system, but is dissociable from it in fine-grained activation patterns, which replicates prior work. Moreover, the physical-reasoning system does not overlap with the language system. These results suggest that physical reasoning does not rely on linguistic representations, nor exclusively on the domain-general abstract reasoning that the MD system supports.}
}
@article{NELSON2019100758,
title = {Designing and transforming yield-stress fluids},
journal = {Current Opinion in Solid State and Materials Science},
volume = {23},
number = {5},
pages = {100758},
year = {2019},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359028619300762},
author = {Arif Z. Nelson and Kenneth S. Schweizer and Brittany M. Rauzan and Ralph G. Nuzzo and Jan Vermant and Randy H. Ewoldt},
keywords = {Soft matter, Yield-stress fluid, Design, Engineering, Extension, Thixotropy, Elasticity, Colloids, Emulsions, Polymers, 3D printing, Chemistry, Physics, Rheology, Complex fluids},
abstract = {We review progress in designing and transforming multi-functional yield-stress fluids and give a perspective on the current state of knowledge that supports each step in the design process. We focus mainly on the rheological properties that make yield-stress fluids so useful and the trade-offs which need to be considered when working with these materials. Thinking in terms of “design with” and “design of” yield-stress fluids motivates how we can organize our scientific understanding of this field. “Design with” involves identification of rheological property requirements independent of the chemical formulation, e.g. for 3D direct-write printing which needs to accommodate a wide range of chemistry and material structures. “Design of” includes microstructural considerations: conceptual models relating formulation to properties, quantitative models of formulation-structure-property relations, and chemical transformation strategies for converting effective yield-stress fluids to be more useful solid engineering materials. Future research directions are suggested at the intersection of chemistry, soft-matter physics, and material science in the context of our desire to design useful rheologically-complex functional materials.}
}
@article{BUI2017115,
title = {Envisioning the future of ‘big data’ biomedicine},
journal = {Journal of Biomedical Informatics},
volume = {69},
pages = {115-117},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417300709},
author = {Alex A.T. Bui and John Darrell {Van Horn}},
keywords = {Biomedicine, Data science, Software, Computing, Training},
abstract = {Through the increasing availability of more efficient data collection procedures, biomedical scientists are now confronting ever larger sets of data, often finding themselves struggling to process and interpret what they have gathered. This, while still more data continues to accumulate. This torrent of biomedical information necessitates creative thinking about how the data are being generated, how they might be best managed, analyzed, and eventually how they can be transformed into further scientific understanding for improving patient care. Recognizing this as a major challenge, the National Institutes of Health (NIH) has spearheaded the “Big Data to Knowledge” (BD2K) program – the agency’s most ambitious biomedical informatics effort ever undertaken to date. In this commentary, we describe how the NIH has taken on “big data” science head-on, how a consortium of leading research centers are developing the means for handling large-scale data, and how such activities are being marshalled for the training of a new generation of biomedical data scientists. All in all, the NIH BD2K program seeks to position data science at the heart of 21st Century biomedical research.}
}
@article{LASSITER201927,
title = {Language and simplexity: A powers view},
journal = {Language Sciences},
volume = {71},
pages = {27-37},
year = {2019},
note = {Simplexity, agency and language},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118300366},
author = {Charles Lassiter},
keywords = {Distributed language, Simplexity, Causal powers, Speech acts},
abstract = {The notion of simplexity is that complex problems are often solved by novel combinations of simple mechanisms. These solutions aren't simple; they're simplex. Language use, as a complex behavior, is ripe for simplex analysis. In this paper, I argue that the notion of powers—an organism's capacity to instigate or undergo change—is doubly useful. First, powers, as opposed to mental representations, are a suitable object for simplex analysis. So conceptualizing languaging in terms of powers gets us one step closer to a simplex analysis of language. But thinking of languaging in terms of powers has an additional payoff. Berthoz asserts that the concept of simplexity is related to the concept of meaning. How they're related is unclear. Conceptualizing languaging in terms of powers injects meaningfulness into lived world of the organism. Consequently, the concept of powers can act as a bridge between the concepts of meaningfulness and simplexity.}
}
@article{LORIMER2009152,
title = {Empathic accuracy in coach–athlete dyads who participate in team and individual sports},
journal = {Psychology of Sport and Exercise},
volume = {10},
number = {1},
pages = {152-158},
year = {2009},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2008.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1469029208000526},
author = {Ross Lorimer and Sophia Jowett},
keywords = {Empathy, Understanding, Interaction, Coach–athlete dyads},
abstract = {Objective
The purpose of the present study was to investigate the empathic accuracy of coach–athlete dyads participating in team and individual sports.
Method
An adaptation of Ickes's [2001. Measuring empathic accuracy. In J. A. Hall & F. J. Bernieri (Eds.), Interpersonal sensitivity (pp. 219–242). Mahwah, NJ: Lawrence Erlbaum Associates] unstructured dyadic interaction paradigm was used to assess the empathic accuracy of 40 coach–athlete dyads. Accordingly, each dyad was filmed during a training session. The dyad members viewed selected video footage that displayed discrete interactions that had naturally occurred during that session. Dyad members reported what they remembered thinking/feeling while making inferences about what their partner's thought/felt at each point. Empathic accuracy was estimated by comparing self-reports and inferences.
Results
The results indicted that accuracy for coaches in individual sports was higher than coaches in team sports. Shared cognitive focus also differed between team and individual sports, and fully mediated the effect of sport-type on coach empathic accuracy. Moreover, coaches whose training sessions were longer demonstrated increased empathic accuracy. No differences were found for athletes.
Conclusions
The results suggest that the dynamics of the interaction between a coach and an athlete play a key role in how accurately they perceive each other.}
}
@incollection{HEFNER20163,
title = {Chapter 1 - A Brief History of Biological Distance Analysis},
editor = {Marin A. Pilloud and Joseph T. Hefner},
booktitle = {Biological Distance Analysis},
publisher = {Academic Press},
address = {San Diego},
pages = {3-22},
year = {2016},
isbn = {978-0-12-801966-5},
doi = {https://doi.org/10.1016/B978-0-12-801966-5.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019665000019},
author = {J.T. Hefner and M.A. Pilloud and J.E. Buikstra and C.C.M. Vogelsberg},
keywords = {aDNA, Analytical scales, Biodistance, Cranial nonmetric traits, Craniometrics, Kinship, Odontometrics, Typology},
abstract = {Biological distance, or biodistance, analysis employs data derived from skeletal remains to reflect population relatedness (similarity/dissimilarity) through the application of multivariate statistical methods. The approaches used in biodistance studies have changed markedly over recent centuries, exploring phenotypic expressions assumed to be informative. Biodistance analysis began as the study of anomalous variants in the human skull, but the field has transformed over the centuries now seeking to incorporate skeletal morphology in the interpretation of genetic affinity, providing insight into the genetics governing trait expression, and providing understanding into the role of developmental biology on the expression of morphological variants. As methodological approaches improve, so too has the application of these analyses. We present here a brief historical overview of biodistance analysis research, focusing on meta-themes in the field, shifts in thinking among researchers in biological anthropology, and several of the outside influences that impact biodistance analysis.}
}
@article{COLTHER2024100625,
title = {Artificial intelligence: Driving force in the evolution of human knowledge},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100625},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100625},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001641},
author = {Cristian Colther and Jean Pierre Doussoulin},
keywords = {Artificial intelligence, Evolution knowledge, Noosphere, Ethical considerations, Future scenarios},
abstract = {This article proposes that artificial intelligence (AI) is positioned as a key driver of a new evolutionary stage of human knowledge, complementing human intelligence and facilitating the creation and development of sophisticated collective intelligence, defined as the noosphere, understood as the sphere of collective human thought. The study reveals several key insights into the transformative potential of AI, including its capacity to accelerate, mediate, and diffuse human knowledge. It concludes that AI not only catalyzes the existence of the noosphere but also redefines the structures and mechanisms through which human knowledge is expanded and democratized. Additionally, the document presents potential risks and significant ethical, social, and legal challenges of an AI-mediated noosphere, offering recommendations and a research agenda around the topic, and limitations and proposals for improvement to be considered in the future.}
}
@incollection{SNYDER200089,
title = {Chapter 5 - Hope as a Common Factor across Psychotherapy Approaches: A Lesson from the Dodo's Verdict},
editor = {C.R. Snyder},
booktitle = {Handbook of Hope},
publisher = {Academic Press},
address = {San Diego},
pages = {89-108},
year = {2000},
isbn = {978-0-12-654050-5},
doi = {https://doi.org/10.1016/B978-012654050-5/50007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780126540505500075},
author = {C.R. Snyder and Julia D. Taylor},
abstract = {Publisher Summary
Despite providing different explanations and targeting disparate symptoms, various psychological approaches for producing change appear to be equally effective. The chapter compares this phenomenon to the Dodo's verdict in “Alice in Wonderland,” in the end race. It explores the question—specifically, what mechanism (or mechanisms) underlie the equal, high efficacy produced by differing types of psychological interventions. Agency reflects people's thoughts about their capacity to use the pathways they have selected to reach their goals. Agency is crucial for the psychotherapy process because it provides mental energy so that a client can undertake various therapy-related activities. This type of goal-directed motivation is reflected in self-affirming mental statements. Operating across differing samples and methodologies, agentic thinking both initiates and helps sustain clients' improvements in psychotherapy. Furthermore, enlisting the literature on placebo effects to illustrate the impact of agentic thinking. The chapter demonstrates how agency alone propels clinical improvement. Agentic thought is the motivational force or engine in hope theory. All the mental energy imaginable, however, cannot guarantee successful goal attainment in psychotherapy. Perceptions that one can produce the routes to those goals is a second necessary component. The chapter also explores pathways thinking in the context of varying psychotherapies.}
}
@article{ROBERTS2020116758,
title = {Creative, internally-directed cognition is associated with reduced BOLD variability},
journal = {NeuroImage},
volume = {219},
pages = {116758},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116758},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920302457},
author = {Reece P. Roberts and Cheryl L. Grady and Donna Rose Addis},
keywords = {Episodic simulation, Imagination, Creativity, BOLD variability},
abstract = {In a range of externally-directed tasks, intra-individual variability of fMRI BOLD signal has been shown to be a stronger predictor of cognitive performance than mean BOLD signal. BOLD variability’s strong association with cognitive performance is hypothesised to be due to it capturing the dynamic range of neural systems. Although increased BOLD variability is also speculated to play a role in internally-directed thought, particularly when creative and flexible cognition is required, there is a relative lack of research exploring whether BOLD variability is related to internally-directed cognition. Thus, we investigated the relationship between BOLD variability and a key component of creativity – divergent thinking – in various tasks that required participants to think flexibly. We also determined whether any associations between BOLD variability and creativity overlapped with, or differed, from associations between mean BOLD signal and creativity. First, we performed task Partial Least Squares (PLS) analyses that compared BOLD signal (either mean or variability) during two future imagination conditions that differed in the amount of cognitive flexibility required: a Congruent condition in which autobiographical details (people, places, objects) comprising an imagined event belonged to the same social sphere (e.g., university) and an Incongruent condition in which details belonged to different social spheres and required greater cognitive flexibility to integrate. Results indicated that the Incongruent condition was associated with a widespread reduction in both BOLD variability and mean signal (relative to the Congruent condition), but in largely non-overlapping regions. Next, we used behavioral PLS to determine whether individual differences in performance on future simulation tasks as well as the Alternate Uses Task relates to BOLD variability and mean BOLD signal. Better performance on these tasks was predominantly associated with increases in mean BOLD signal and decreases in BOLD variability, in a range of disparate brain regions. Together, the results suggest that, unlike tasks requiring externally-directed cognition, superior performance on tasks requiring creative internal mentation is associated with less (not more) variability.}
}
@article{WALHA20252959,
title = {Deep Learning and Machine Learning Architectures for Dementia Detection from Speech in Women},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {142},
number = {3},
pages = {2959-3001},
year = {2025},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2025.060545},
url = {https://www.sciencedirect.com/science/article/pii/S1526149225000396},
author = {Ahlem Walha and Amel Ksibi and Mohammed Zakariah and Manel Ayadi and Tagrid Alshalali and Oumaima Saidani and Leila Jamel and Nouf Abdullah Almujally},
keywords = {Dementia detection in women, Alzheimer’s disease, deep learning, machine learning, support vector machine, voting classifier},
abstract = {Dementia is a neurological disorder that affects the brain and its functioning, and women experience its effects more than men do. Preventive care often requires non-invasive and rapid tests, yet conventional diagnostic techniques are time-consuming and invasive. One of the most effective ways to diagnose dementia is by analyzing a patient’s speech, which is cheap and does not require surgery. This research aims to determine the effectiveness of deep learning (DL) and machine learning (ML) structures in diagnosing dementia based on women’s speech patterns. The study analyzes data drawn from the Pitt Corpus, which contains 298 dementia files and 238 control files from the Dementia Bank database. Deep learning models and SVM classifiers were used to analyze the available audio samples in the dataset. Our methodology used two methods: a DL-ML model and a single DL model for the classification of diabetics and a single DL model. The deep learning model achieved an astronomic level of accuracy of 99.99% with an F1 score of 0.9998, Precision of 0.9997, and recall of 0.9998. The proposed DL-ML fusion model was equally impressive, with an accuracy of 99.99%, F1 score of 0.9995, Precision of 0.9998, and recall of 0.9997. Also, the study reveals how to apply deep learning and machine learning models for dementia detection from speech with high accuracy and low computational complexity. This research work, therefore, concludes by showing the possibility of using speech-based dementia detection as a possibly helpful early diagnosis mode. For even further enhanced model performance and better generalization, future studies may explore real-time applications and the inclusion of other components of speech.}
}
@article{BENDETOWICZ2017216,
title = {Brain morphometry predicts individual creative potential and the ability to combine remote ideas},
journal = {Cortex},
volume = {86},
pages = {216-229},
year = {2017},
note = {Is a "single" brain model sufficient?},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2016.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0010945216303161},
author = {David Bendetowicz and Marika Urbanski and Clarisse Aichelburg and Richard Levy and Emmanuelle Volle},
keywords = {Creativity, Semantic associations, Rostral prefrontal, Frontal pole, Morphometry},
abstract = {For complex mental functions such as creative thinking, inter-individual variability is useful to better understand the underlying cognitive components and brain anatomy. Associative theories propose that creative individuals have flexible semantic associations, which allows remote elements to be formed into new combinations. However, the structural brain variability associated with the ability to combine remote associates has not been explored. To address this question, we performed a voxel-based morphometry (VBM) study and explored the anatomical connectivity of significant regions. We developed a Remote Combination Association Task adapted from Mednick's test, in which subjects had to find a solution word related to three cue words presented to them. In our adaptation of the task, we used free association norms to quantify the associative distance between the cue words and solution words, and we varied this distance. The tendency to solve the task with insight and the ability to evaluate the appropriateness of a proposed solution were also analysed. Fifty-four healthy volunteers performed this task and underwent a structural MRI. Structure–function relationships were analysed using regression models between grey matter (GM) volume and task performance. Significant clusters were mapped onto an atlas of white matter (WM) tracts. The ability to solve the task, which depended on the associative distance of the solution word, was associated with structural variation in the left rostrolateral prefrontal and posterior parietal regions; the left rostral prefrontal region was connected to distant regions through long-range pathways. By using a creative combination task in which the semantic distance between words varied, we revealed a brain network centred on the left frontal pole that appears to support the ability to combine information in new ways by bridging the semantic distance between pieces of information.}
}
@incollection{WALSH2017,
title = {Sensory Systems},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2017},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.06867-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809324506867X},
author = {V. Walsh},
keywords = {Auditory system, Multisensory integration, Nerves, Somatosensation, Visual system},
abstract = {Sensory systems have an old school ring to them, a very old school ring to them. In the 15th century Benedetti was able to write, “By means of nerves, the pathways of the senses are distributed like the roots and fibers of a tree” (Alessandro Benedetti, 1497). This is still a good place to start because it gives one a feel for the 3D structure of our sensory apparatus, but the challenge of understanding the senses has, of course, gone well beyond structure (which is not to imply that all structural descriptions are complete or that we have joined all the dots of structure–function relationships), and any serious scholar needs to have a working knowledge of the development, physiology, psychophysics (physiology without the blood), genetics, pathology, and computational models of the senses.}
}
@article{ARAGONES20141,
title = {Rhetoric and analogies},
journal = {Research in Economics},
volume = {68},
number = {1},
pages = {1-10},
year = {2014},
issn = {1090-9443},
doi = {https://doi.org/10.1016/j.rie.2013.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090944313000410},
author = {Enriqueta Aragones and Itzhak Gilboa and Andrew Postlewaite and David Schmeidler},
keywords = {Rhetoric, Analogies, Complexity},
abstract = {The art of rhetoric may be defined as changing other people's minds (opinions, beliefs) without providing them new information. One technique heavily used by rhetoric employs analogies. Using analogies, one may draw the listener's attention to similarities between cases and to re-organize existing information in a way that highlights certain regularities. In this paper we offer two models of analogies, discuss their theoretical equivalence, and show that finding good analogies is a computationally hard problem.}
}
@incollection{SIEGEL20163,
title = {Chapter 1 - Introduction: Defining the Role of Statistics in Business},
editor = {Andrew F. Siegel},
booktitle = {Practical Business Statistics (Seventh Edition)},
publisher = {Academic Press},
edition = {Seventh Edition},
pages = {3-17},
year = {2016},
isbn = {978-0-12-804250-2},
doi = {https://doi.org/10.1016/B978-0-12-804250-2.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042502000018},
author = {Andrew F. Siegel},
abstract = {We begin this chapter with an overview of the competitive advantage provided by a knowledge of statistical methods, followed by some basic facts about statistics and probability and their role in business. Statistical activities can be grouped into five main activities (designing, exploring, modeling, estimating, and hypothesis testing), and one way to clarify statistical thinking is to be able to match the business task at hand with the correct collection of statistical methods. This chapter sets the stage for the rest of the book, which follows up with many important detailed procedures for accomplishing business goals that involve these activities. Next follows an overview of data mining of Big Data (which involves these main activities) and its importance in business. Then we distinguish the field of probability (where, based on assumptions, we reach conclusions about what is likely to happen—a useful exercise in business where nobody knows for sure what will happen) from the field of statistics (where we know from the data what happened, from which we infer conclusions about the system that produced these data) while recognizing that probability and statistics will work well together in future chapters. The chapter concludes with some words of advice on how to integrate statistical thinking with other business viewpoints and activities.}
}
@article{HEDE2015522,
title = {TRIZ and the Paradigms of Social Sustainability in Product Development Endeavors},
journal = {Procedia Engineering},
volume = {131},
pages = {522-538},
year = {2015},
note = {TRIZ and Knowledge-Based Innovation in Science and Industry},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.12.447},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815043398},
author = {Shantesh Hede and Paula Verandas Ferreira and Manuel Nunes Lopes and Luis Alexandre Rocha},
keywords = {Sustainability, Decision Modeling, TRIZ, Product Development ;},
abstract = {The Business practices of an industrialized civilization are responsible for intensifying the dynamics of the interdependent environmental, social and economic domains of our ecosystem. The worldwide objective to accomplish Sustainability is invariably addressed by Policy makers and Institutions by means of moderately disparate co-relations between Environmental and Social considerations. The dimension of Social Sustainability has a direct co-relation towards the extended continuation of a globalized Enterprise. The stated co-relation is an interconnected and interdependent network comprising of growth in Innovation and Sustainability at the Environmental and Economic frontiers. From the standpoint of Innovation, the 20th century has been dominated by both TRIZ with OTSM and Kurzweil's Law of Accelerating Returns to steer the future of revolutionary innovations. Moreover, TRIZ and its evolved counterpart OTSM have been extensively utilized for macro-scale problem solving scenarios, while Kurzweil's Law has reached up to quantum scale whereby matter as we know exhibits an entire range of unique properties with a potential to dramatically transform our human civilization. Accordingly, the perceived limitations and vague applicability of TRIZ in sub-macro scale innovations has been discussed. The contemporary tools for project evaluation (e.g.: cost benefit analysis) and product development (e.g.: linear stage-gate process) quintessential for commercializing innovations are identified to be limited, both in scope and accuracy for delivering a long term ‘sustainable’ competitive advantage to an Enterprise. Consequently, the proposed conceptual Multifaceted Framework addresses the issue of social sustainability in Product Development. The underpinnings of Systems Thinking, TRIZ and OTSM, Complex Adaptive Systems, Socio-Economics & Human Behavior forms the fundamental basis of the proposed Multifaceted Framework. The novel perspective offered by the proposed Framework enables product development teams to overcome the inherent myopia and other limitations associated with the contemporary Environmental Life Cycle Analysis and Sustainability related Decision Models. An Expert opinion based evaluation technique in conjugation with a Multilayered Decision Modeling Method have been incorporated as a salient features in the proposed framework. The evaluation technique is utilized for assigning numerical values to the pertinent sustainability related criteria of the Multilayered Decision Model. The proposed Framework plays a crucial role in product development and decision modeling across the Idea Screening Phase (Stage 2) up to the Feasibility Analysis Phase (Stage 4). In addition, a modified version Taguchi Loss Function is included to exemplify a tangible relation between Product Quality parameters and Sustainability. The objective of the proposed framework is to provide an efficient, yet comprehensive evaluation as well as an effective product development strategy with a distinct and a holistic outlook on Social Sustainability.}
}
@article{GONG2025113594,
title = {A Survey of Video Action Recognition Based on Deep Learning},
journal = {Knowledge-Based Systems},
volume = {320},
pages = {113594},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113594},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125006409},
author = {Ping Gong and Xudong Luo},
keywords = {Video action recognition, Deep learning, Multi-modal learning, AI-powered human behaviour analysis, Action recognition benchmark dataset},
abstract = {Video Action Recognition (VAR) involves identifying and classifying human actions from video data. Deep Learning (DL) has revolutionised VAR, significantly enhancing its accuracy and efficiency. However, large-scale practical applications of VAR using DL remain limited, underscoring the need for further research and innovation. Thus, this survey provides a comprehensive overview of recent advancements in DL-based VAR. Specifically, we summarise the key DL architectures for VAR, including two-stream networks, 3D-CNNs, RNNs, LSTMs, and Attention Mechanisms, and analyse their strengths, limitations, and benchmark performances. The survey also explores the diverse applications of DL-based VAR, such as surveillance, human–computer interaction, sports analytics, healthcare, and education, while presenting a detailed summary of commonly used datasets and evaluation metrics. Moreover, critical challenges, such as computational demands and the need for robust temporal modelling, are identified, along with potential future directions. This paper is a valuable resource for researchers and practitioners striving to advance VAR using DL techniques by systematically presenting concepts, methodologies, and trends.}
}
@article{HUDA2024122380,
title = {Experts and intelligent systems for smart homes’ Transformation to Sustainable Smart Cities: A comprehensive review},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122380},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122380},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423028828},
author = {Noor Ul Huda and Ijaz Ahmed and Muhammad Adnan and Mansoor Ali and Faisal Naeem},
keywords = {Artificial intelligence, Automation, Block chain, Energy management, Expert intelligent systems, Management systems, Smart cities},
abstract = {In this constantly evolving landscape of urbanization, the relationship between technology and automation, in regards to sustainability, holds immense significance. The intricate strands of human intelligence are seamlessly interwoven with the fabric of technological progress, giving rise to exquisite patterns of synergy and collaborative innovation. Automation is just another step in this process which started with the industrial revolution and now has paved way towards urbanization. Smart homes or home automation is a subset of Internet of Things (IoT) based automation that has added into the comfort, ease, and quality of our living standards and is now being integrated to form the concept of Smart Cities. In the past decade, various techniques and processes of smart home automation have been proposed and implemented. To extend and translate the existing methods into new one, the understanding of the former is imperative to the research procedure. This review stands as a comprehensive exploration, diving into the pivotal role of intelligent systems and expert knowledge in driving the transformation of smart homes into sustainable smart cities. By meticulously analyzing and aggregating an array of contemporary techniques used in smart homes, this paper offers profound contributions to the intersection of urban evolution and technological innovation. The review’s holistic approach not only facilitates a deep understanding of smart homes’ contributions but also charts a course for innovative strategies in city planning, infrastructure, and technological integration. In bridging the gap between technology and sustainable urban development, this exploration underscores the transformative power of leveraging smart home techniques to lay the foundation for harmonious and forward-thinking smart cities. The technologies cover a wide range of methodologies and intelligent systems used for communication, security and management in an urban infrastructure. The paper focuses on analysis of the technology to provide an outlook into achieving the goal of sustainable smart cities and deal with challenges like scalability and big data computation. Our comprehensive analysis yields a holistic set of technology comparisons and illuminates the promising future prospects within this domain. The information is highly insightful in creating a bigger picture for adopting state of the art technologies like Federated Learning (FL), Digital Twin and Embedded Edge computing in better planning and infrastructure management in smart cities. These findings offer reliable and potent methods to chart not only the course of research but also to enhance these technologies for the betterment of mankind’s convenience and advancement.}
}
@incollection{GARDNER202451,
title = {Chapter 3 - Designing and prototyping smarter urban spaces},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {51-74},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000094},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Design education, Design pedagogy, Design process, Human-computer interaction, Interaction, Interaction design, IoT, Physical computing, Prototyping, Smart city, Urban technology},
abstract = {This chapter outlines how the concept of the smart city is explored and challenged in an undergraduate design course that adopts a cross-scale framework to design and prototype urban technology projects. It sets out an integrated and interdisciplinary approach to urban technology design that combines context-oriented spatial design methods with physical computing and interaction principles. It construes the design and prototyping of urban technology projects as sociotechnical thought experiments that can materialize ethical concerns and explore alternate ways that urban life can be lived with technology. The chapter concludes by outlining the themes that organize selected existing and speculative urban technology projects in the following chapters of the book.}
}
@incollection{FLOTHER202583,
title = {Chapter 6 - Early quantum computing applications on the path towards precision medicine},
editor = {Laura Kelly and William P. Stanford},
booktitle = {Implementation of Personalized Precision Medicine},
publisher = {Academic Press},
pages = {83-96},
year = {2025},
isbn = {978-0-323-98808-7},
doi = {https://doi.org/10.1016/B978-0-323-98808-7.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988087000011},
author = {Frederik F. Flöther},
keywords = {Precision medicine, Quantum computers, Artificial intelligence/machine learning, EHRs},
abstract = {The last few years have seen rapid progress in transitioning quantum computing from lab to industry. In healthcare and life sciences, more than 40 proof-of-concept experiments and studies have been conducted; an increasing number of these are even run on real quantum hardware. Major investments have been made with hundreds of millions of dollars already allocated towards quantum applications and hardware in medicine. In addition to pharmaceutical and life sciences uses, clinical and medical applications are now increasingly coming into the picture. This chapter focuses on three key use case areas associated with (precision) medicine, including genomics and clinical research, diagnostics, and treatments and interventions. Examples of organizations and the use cases they have been researching are given; ideas how the development of practical quantum computing applications can be further accelerated are described.}
}
@article{RENNA2023104420,
title = {A randomized controlled trial comparing two doses of emotion regulation therapy: Preliminary evidence that gains in attentional and metacognitive regulation reduce worry, rumination, and distress},
journal = {Behaviour Research and Therapy},
volume = {170},
pages = {104420},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104420},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001687},
author = {Megan E. Renna and Phillip E. Spaeth and Jean M. Quintero and Mia S. O'Toole and Christina F. Sandman and David M. Fresco and Douglas S. Mennin},
keywords = {Emotion regulation, Randomized controlled trial, Distress, Anxiety, Depression},
abstract = {Background
Emotion regulation therapy (ERT) promotes resilience in distress disorders by strengthening attentional and metacognitive capacities. Regulation skills are presented with the goal of ameliorating the perseverative negative thinking (PNT) that characterizes these disorders. This study tested ERT in a randomized controlled trial comparing the effectiveness of 16-session (ERT16) versus 8-session (ERT8) doses.
Method
Patients (N = 72) endorsing elevated worry and/or rumination and meeting diagnostic criteria for a distress disorder were randomized to ERT8 or ERT16. PNT, anxiety/depressive symptoms, functioning/quality of life, and treatment mechanisms (attention shifting, attention focusing, decentering, reappraisal) were measured at pre, mid, and post treatment. Clinical symptom severity was also assigned via diagnostic interview at each timepoint.
Results
ERT produced significant improvements across outcomes. ERT16 showed an advantage over ERT8 for distress disorder severity, worry, rumination, and attention shifting from pre-post treatment. Changes in ERT treatment mechanisms mediated changes in clinical improvement.
Conclusion
These findings provide evidence of the effectiveness of two doses of ERT in reducing PNT and distress through improvements in regulation skills.
Clinicaltrials.gov identifier
NCT04060940.}
}
@article{BASOV2024101869,
title = {Professional patios, emotional studios: Locating social ties in European art residences},
journal = {Poetics},
volume = {102},
pages = {101869},
year = {2024},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2024.101869},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X24000081},
author = {Nikita Basov and Dafne Muntanyola-Saura and Sergi Méndez and Oleksandra Nenko},
keywords = {Material space, Social network, Socio-material network analysis, Mixed method, Statistical modeling of ethnographic data, Artistic residence},
abstract = {To foster creativity through sociality, residences put artists together. At the same time, in their quest for originality, artists often opt for individualism. Little is known on how physical collocation in residences affects artistic sociality. Addressing this gap, we draw on a combination of interviews, observations, and surveys, analysed with an innovative mixture of abductive coding, computational space analysis, and statistical network modeling. This allows us to unveil how room sharing and object usage relate to friendships and collaborations between residents. Along with explicit individualism of artists, we spot plenty of social ties between them. And these ties are positively related to joint material embeddedness. Simultaneously, the two main types of residential zones – working studios and leisure areas – appear to encourage the types of social ties inverse to our expectations. Our findings inform the practice of artistic residence organising and the proposed approach enables explanatory analysis of the relation between material space and sociality in various settings.}
}
@article{KELLER2011174,
title = {Towards a science of informed matter},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {42},
number = {2},
pages = {174-179},
year = {2011},
note = {When Physics Meets Biology},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2010.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S1369848610001172},
author = {Evelyn Fox Keller},
keywords = {Information, Self-assembly, Evolution, Selection, Embodiment, Supramolecular chemistry},
abstract = {Over the last couple of decades, a call has begun to resound in a number of distinct fields of inquiry for a reattachment of form to matter, for an understanding of ‘information’ as inherently embodied, or, as Jean-Marie Lehn calls it, for a “science of informed matter.” We hear this call most clearly in chemistry, in cognitive science, in molecular computation, and in robotics—all fields looking to biological processes to ground a new epistemology. The departure from the values of a more traditional epistemological culture can be seen most clearly in changing representations of biological development. Where for many years now, biological discourse has accepted a sharp distinction (borrowed directly from classical computer science) between information and matter, software and hardware, data and program, encoding and enactment, a new discourse has now begun to emerge in which these distinctions have little meaning. Perhaps ironically, much of this shift depends on drawing inspiration from just those biological processes which the discourse of disembodied information was intended to describe.}
}
@article{TUPPURAINEN2024108835,
title = {Conceptual design of furfural extraction, oxidative upgrading and product recovery: COSMO-RS-based process-level solvent screening},
journal = {Computers & Chemical Engineering},
volume = {191},
pages = {108835},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108835},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424002539},
author = {Ville Tuppurainen and Lorenz Fleitmann and Jani Kangas and Kai Leonhard and Juha Tanskanen},
keywords = {Furfural oxidation, Hydrogen peroxide, Conceptual process design, COSMO-RS predictive thermodynamics, Solvent screening},
abstract = {Liquid phase oxidation of furfural using hydrogen peroxide offers a promising route for bio-based C4 furanones and diacids; however, only dilute water-based process designs have been previously suggested that have limited techno-economic potential. In this study, a conceptual process design is presented, where aqueous furfural is extracted using an organic solvent, coupled with peroxide oxidation and product recovery in the presence of the solvent. To address the problem of solvent selection, the COSMO-RS-based solvent screening framework is applied, where quantum mechanics-based thermodynamics are utilized in pinch-based process models. About 2500 solvent candidates were identified as feasible. Focusing on a set of 400 solvent candidates revealed energy consumption values (Qreb,tot/ṁprod recov) between approximately 2 MWh/tonne and 33 MWh/tonne, signifying the potential of the solvent-based process in outperforming the reference aqueous process (49.4 MWh/tonne). The study provides potential solvent candidates and future directions to consider in more costly computational and experimental efforts.}
}
@article{NAKHLE2024100411,
title = {Shrinking the giants: Paving the way for TinyAI},
journal = {Device},
volume = {2},
number = {8},
pages = {100411},
year = {2024},
issn = {2666-9986},
doi = {https://doi.org/10.1016/j.device.2024.100411},
url = {https://www.sciencedirect.com/science/article/pii/S2666998624002473},
author = {Farid Nakhle},
keywords = {accelerated models, compressed models, miniaturized intelligence, tiny artificial intelligence, tiny machine learning},
abstract = {Summary
In the current era of technological advancement, the quest for more efficient and accessible artificial intelligence (AI) is driving the investigation of the predictive potential of small architecture-based, compressed, and accelerated AI models (TinyAI) and the benefits of running those on small-scale digital edge computing devices. This perspective delves into the expanding world of TinyAI, envisioning a future in which powerful machine intelligence can be encapsulated within pocket-sized devices, and discusses the technological challenges and opportunities associated with it. In addition, some of the myriad applications and benefits that can arise from their deployment will be discussed.}
}
@article{YUTING2023e15851,
title = {Current status of digital humanities research in Taiwan},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e15851},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15851},
url = {https://www.sciencedirect.com/science/article/pii/S240584402303058X},
author = {Pan Yuting and Jiang Yinfeng and Zhang Jingli},
keywords = {Digital humanities, Text mining, Social network analysis, GIS},
abstract = {Purpose
Review the current research status of the theory, techniques, and practice of digital humanities in Taiwan.
Methods
Select the 8 issues of the Journal of Digital Archives and Digital Humanities from its inception in 2018–2021, and the papers of the 5-year International Conference of Digital Archives and Digital Humanities from 2017 to 2021 as the research data, and conduct text analysis of the collected 252 articles.
Results
From the statistical analysis results, the number of practical articles is the largest, followed by tools and techniques, and the least number of theoretical articles. Text tools and literature research are the most concentrated aspects of digital humanities research in Taiwan.
Limitations
It still needs to be further compared with the current research status of digital humanities in Mainland China.
Conclusions
Digital humanities in Taiwan focuses on the development of tools and techniques, and practical applications of literature and history, and focuses on Taiwan's native culture to form its own digital humanities research characteristics.}
}
@article{REN2024122745,
title = {Pooling-based Visual Transformer with low complexity attention hashing for image retrieval},
journal = {Expert Systems with Applications},
volume = {241},
pages = {122745},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122745},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423032475},
author = {Huan Ren and Jiangtao Guo and Shuli Cheng and Yongming Li},
keywords = {Pooling-based Visual Transformers, Attention, Image retrieval, Deep hash},
abstract = {Retrieving similar images is becoming an urgent need for us with the continuous growth of large-scale data. However, whether the dominant image retrieval methods are Convolutional Neural Networks (CNNs) or the recently emerging Visual Transformer (ViT), their complex computation, insufficient feature extraction, and mismatched weights greatly influence the efficiency and retrieval accuracy. In this paper, we propose a Pooling-based Visual Transformer with low complexity attention hashing (PTLCH) for image retrieval. First, a backbone network for Pooling-based Vision Transformer (PiT) feature learning is designed to combine the pooling in CNN and the ViT to achieve the purpose of spatial dimensionality reduction while learning rich semantic information. Second, a low complexity attention (LCA) module is incorporated into PiT, which works by combining the positional deviation with the key matrix and the value matrix and then matrix multiplying with the query matrix. LCA explores rich contextual information to enable network learning of more granular feature information. Finally, a new loss framework is proposed where we focus on the effect of difficult and erroneous samples on accuracy. By using different improved cross-entropy losses, better weights are assigned to the learning samples of our network, which effectively improves learning hash coding. We have conducted extensive experiments on three public datasets, CIFAR-10, ImageNet100, and MS-COCO, which have the highest mean average precision of 93.76%, 92.62%, and 90.60%, respectively.}
}
@article{MOHANAN2024100997,
title = {Integrating Ayurveda and modern mainstream medicine},
journal = {Journal of Ayurveda and Integrative Medicine},
volume = {15},
number = {5},
pages = {100997},
year = {2024},
issn = {0975-9476},
doi = {https://doi.org/10.1016/j.jaim.2024.100997},
url = {https://www.sciencedirect.com/science/article/pii/S0975947624001128},
author = {K.P. Mohanan},
abstract = {This article is an attempt to understand the challenge of integrating the education provided by BAMS programs and MBBS programs, in order to initiate the process of integrating research and practice in Ayurveda and Modern Mainstream Medicine. The specific issues discussed in the article are framed within the broader context of the challenge of integrating any two bodies of knowledge, theories, or knowledge systems in education and research.}
}
@incollection{SWARNALINGAM202535,
title = {Chapter 3 - Electroencephalographic evaluation of epileptogenicity: traditional versus novel biomarkers to guide surgery},
editor = {Aria Fallah and George M. Ibrahim and Alexander G. Weil},
booktitle = {Pediatric Epilepsy Surgery Techniques},
publisher = {Academic Press},
pages = {35-55},
year = {2025},
isbn = {978-0-323-95981-0},
doi = {https://doi.org/10.1016/B978-0-323-95981-0.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323959810000060},
author = {Eroshini Swarnalingam and Julia Jacobs},
keywords = {Medically refractory epilepsy, drug resistant epilepsy, epilepsy surgery, epileptogenic zone, invasive EEG, Stereo EEG, high-frequency oscillations, infra-slow activity, epileptogenicity index},
abstract = {Objectives
Precise localization of the epileptogenic zone (EZ) is a crucial step prior to any planned surgical intervention for medication-refractory epilepsy. However, in reality, a single gold standard biomarker of the EZ does not exist, emphasizing the need for novel biomarkers. The objectives of this chapter are to discuss some of these novel biomarkers utilized to guide epilepsy surgery and discuss their utility and evidence.
Methods
We discuss available evidence to support the use of some of the novel biomarkers of epileptogenicity and compare these with the use of more traditional biomarkers.
Results
There are two main types of developments in the biomarker for epileptic activity. Studies that expand the conventional frequency spectrum of cortical activity such as high-frequency oscillations or those that use computational analysis to assess epileptogenic networks such as the epileptogenicity index. Clinical evidence in most of the new biomarkers is limited to retrospective data analysis and most novel biomarkers require clinical trials prior to incorporating them into day-to-day pre surgical evaluation.
Conclusion
A better understating of the epileptogenic networks as a whole, rather than conceptualizing the EZ as a single focus, will lead to improved surgical outcomes. Currently no single biomarker can be considered a gold standard in outlining the epileptogenic network. Therefore, a combination of complementary investigative methods currently is the best approach to decide on brain areas that need to be removed for seizure free outcomes.}
}
@article{ZHANG20247,
title = {Large language model in electrocatalysis},
journal = {Chinese Journal of Catalysis},
volume = {59},
pages = {7-14},
year = {2024},
issn = {1872-2067},
doi = {https://doi.org/10.1016/S1872-2067(23)64612-1},
url = {https://www.sciencedirect.com/science/article/pii/S1872206723646121},
author = {Chengyi Zhang and Xingyu Wang and Ziyun Wang},
keywords = {Large language model, Electrocatalysis, Artificial intelligence, Multimodal large language model},
abstract = {ABSTRACT
Large language models have recently brought a massive storm on modern society in all fields. While many view them as mere search engines for specific answers or text refinement tools like a chatbot, their broader applications remain largely unexplored. These large language models, consisting of billions of interconnected neurons, derived from all knowledge of the human, possess the remarkable ability to engage in smooth and precise conversations with individuals across the globe. Human-like intelligence enables them to address modern challenges and display immense potential in various scientific domains. In this perspective, we delve into the potential applications of modern large language model and its future iterations within the field of catalysis, aiming to shed light on how these AI-driven models can contribute to a deeper understanding of catalysis science and the intelligent design of catalysts.}
}
@incollection{MORA2019215,
title = {Chapter 7 - The social shaping of smart cities},
editor = {Luca Mora and Mark Deakin},
booktitle = {Untangling Smart Cities},
publisher = {Elsevier},
pages = {215-234},
year = {2019},
isbn = {978-0-12-815477-9},
doi = {https://doi.org/10.1016/B978-0-12-815477-9.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128154779000074},
author = {Luca Mora and Mark Deakin},
keywords = {Interdisciplinarity, Expectations, Social shaping, Open innovation, Technological advancement, Urban innovations, Technological change, Hype, Smart city, Smart city research, Lessons, Recommendations, Open community, Co-design, Collaboration, Collaborative environment, Quadruple helix, Triple helix, Organization dynamics, Urban innovation, Urban sustainability, Sustainable urban development, Hyped behaviors, Urban utopia, Expectations, Smart city development, Smart urbanism},
abstract = {This last chapter concludes the investigation by summing up the key lessons and recommendations that this book can offer to the community of stakeholders involved in smart city research, policy, and practice. The series of complementary analyses that the previous chapters report on demonstrate that, when untangled from the technocentric urban utopia pictured by the corporate sector, smart cities have the potential to develop into innovation systems that set the stage for a technology-enabled approach to urban sustainability. But realizing this opportunity requires to move beyond traditional boundaries, separate the hype from reality, and strengthen the focus on the social shaping of smart cities. The investigation demonstrates that, in order for such a social shaping to develop, the design of smart cities needs to be understood as a collective action in which two complementary forces are combined. On the one hand, the faith in the technological advancement exposed in the utopian thinking. On the other, the knowledge, skills, and interests of a quadruple-helix collaborative environment where the need for technological innovation in response to urban sustainability goals is not shaped by the corporate sector and its technocentric and market-oriented logic, but an open community whose actions serve the public interest and are based on a holistic interpretation of smart city development.}
}
@article{OZTOP2022106240,
title = {Analysis of melting of phase change material block inserted to an open cavity},
journal = {International Communications in Heat and Mass Transfer},
volume = {137},
pages = {106240},
year = {2022},
issn = {0735-1933},
doi = {https://doi.org/10.1016/j.icheatmasstransfer.2022.106240},
url = {https://www.sciencedirect.com/science/article/pii/S0735193322003621},
author = {Hakan F. Öztop and Hakan Coşanay and Fatih Selimefendigil and Nidal Abu-Hamdeh},
keywords = {Partially open cavity, PCM, Melting, Computational, Finned heater},
abstract = {A numerical work has been conducted to explore the effects of opening parameters on melting of phase change material (PCM) during natural convection in a partially open enclosure. A finned heater is located on bottom wall while the remaining parts are insulated. Paraffin wax is used as PCM and two-dimensional time dependent analysis is performed by using the finite volume method for the parameters of location of opening and temperature difference. The governing parameters for the study are chosen for the range of Ra = 1.45 × 108 ≤ Ra ≤ Ra = 1.97 × 108, 0.25 ≤ w/H ≤ 0.75 and 0.25 ≤ c/H ≤ 0.75. It is found that both opening ratio and opening length are effective parameter on melting time and these can be used as control parameters for improving the energy efficiency. Also, heat transfer can be controlled by using PCM inserted block and opening parameters. Among different cases of opening ratios and locations of opening, the most favorable configuration is obtained at Ra = 1.97 × 108, w/H = 0.25, c/H = 0.25 while average heat transfer enhancement by about 60% is achieved. At the lowest and highest value of Rayleigh numbers, the most favorable location of the opening is obtained at c/H = 0.25 in order to have the highest reduction amount of phase completion time.}
}
@article{YANG2024234071,
title = {Application and development of the Lattice Boltzmann modeling in pore-scale electrodes of solid oxide fuel cells},
journal = {Journal of Power Sources},
volume = {599},
pages = {234071},
year = {2024},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2024.234071},
url = {https://www.sciencedirect.com/science/article/pii/S0378775324000223},
author = {Xiaoxing Yang and Guogang Yang and Shian Li and Qiuwan Shen and He Miao and Jinliang Yuan},
keywords = {Lattice Boltzmann method modeling, Pore-scale simulation, Reactive transport, Solid oxide fuel cells simulation},
abstract = {The lattice Boltzmann method (LBM) plays an important role in the study of the internal flow behavior at the pore-scale inside the electrodes of solid oxide fuel cells (SOFCs). Porosity, tortuosity, and particle size have a remarkable effect on gas transport and electrocatalytic processes, determining the performance of cells when SOFCs are applied in electric power generation, energy storage systems, and industrial production in recent years. However, these pore-scale transport progresses are not well characterized in the numerical studies of conventional computational fluid dynamics (CFD), thus modeling with LBM at the pore-scale is an effective tool for simulating gas transport and electrochemical reactions in electrodes. It overcomes the drawbacks of experimental techniques that do not characterize these processes accurately enough and detail the distribution of important variables. In this review, the methodology and process of electrode pore-scale modeling are presented, along with the application and current studies of LBM for diffusion, electrochemical reactions, and ion migration in SOFC porous electrodes. Important results are discussed. Finally, future perspectives on pore-scale studies of porous electrodes are given. This in-depth review intends to provide ideas for the development and further application of LBM in porous SOFC electrodes.}
}
@article{BETTINGER2023105459,
title = {Conceptual foundations of physiological regulation incorporating the free energy principle and self-organized criticality},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {155},
pages = {105459},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105459},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423004281},
author = {Jesse S. Bettinger and Karl J. Friston},
keywords = {Physiological regulation, Homeostasis, Allostasis, Variational systems, Free energy principle, Criticality, Griffiths region, Complex adaptive systems, Dynamic stability, Metastability, Control theory, Neuro-immunology, Computational psychiatry, Resilience},
abstract = {Bettinger, J. S., K. J. Friston. Conceptual Foundations of Physiological Regulation incorporating the Free Energy Principle & Self-Organized Criticality. NEUROSCI BIOBEHAV REV 23(x) 144-XXX, 2022. Since the late nineteen-nineties, the concept of homeostasis has been contextualized within a broader class of "allostatic" dynamics characterized by a wider-berth of causal factors including social, psychological and environmental entailments; the fundamental nature of integrated brain-body dynamics; plus the role of anticipatory, top-down constraints supplied by intrinsic regulatory models. Many of these evidentiary factors are integral in original descriptions of homeostasis; subsequently integrated; and/or cite more-general operating principles of self-organization. As a result, the concept of allostasis may be generalized to a larger category of variational systems in biology, engineering and physics in terms of advances in complex systems, statistical mechanics and dynamics involving heterogenous (hierarchical/heterarchical, modular) systems like brain-networks and the internal milieu. This paper offers a three-part treatment. 1) interpret "allostasis" to emphasize a variational and relational foundation of physiological stability; 2) adapt the role of allostasis as "stability through change" to include a "return to stability" and 3) reframe the model of homeostasis with a conceptual model of criticality that licenses the upgrade to variational dynamics.}
}
@article{CRAGG201463,
title = {Skills underlying mathematics: The role of executive function in the development of mathematics proficiency},
journal = {Trends in Neuroscience and Education},
volume = {3},
number = {2},
pages = {63-68},
year = {2014},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2013.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211949313000422},
author = {Lucy Cragg and Camilla Gilmore},
keywords = {Mathematics, Executive function, Working memory, Development},
abstract = {The successful learning and performance of mathematics relies on a range of individual, social and educational factors. Recent research suggests that executive function skills, which include monitoring and manipulating information in mind (working memory), suppressing distracting information and unwanted responses (inhibition) and flexible thinking (shifting), play a critical role in the development of mathematics proficiency. This paper reviews the literature to assess concurrent relationships between mathematics and executive function skills, the role of executive function skills in the performance of mathematical calculations, and how executive function skills support the acquisition of new mathematics knowledge. In doing so, we highlight key theoretical issues within the field and identify future avenues for research.}
}
@article{OXMAN2000337,
title = {Design media for the cognitive designer1This paper is based on the keynote speech on `The Challenge of Design Computation' given by the author at ECAADE '97 in Vienna.1},
journal = {Automation in Construction},
volume = {9},
number = {4},
pages = {337-346},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00017-5},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000175},
author = {Rivka Oxman},
keywords = {Generic design knowledge, Design collaboration, Re-representation, Typology, },
abstract = {Work on media for design which are responsive to the cognitive processes of the human designer are introduced as a paradigm for research and development. Design media are intended to support the cognitive nature of design and, particularly, the exploitation of design knowledge in computational environments. Basic theoretical assumptions are presented which underlie the development of design media. A central assumption is that designers share common forms of design knowledge which can be formalized, represented, and employed in computational environments. Generic knowledge is proposed as one such seminal form of design knowledge. We then develop a cognitive model which relates to the internal mental representations, strategies and mechanisms of generic design. The paper emphasizes the theoretical foundations of design media. This theoretical discussion is then exemplified through case studies presenting current research for the support of visual cognition in design. We introduce an approach to design schema as a visual form of generic design knowledge. Secondly we present a conceptual framework for the support of schema emergence in visual reasoning in design media. Finally, some implications of schema emergence in design collaboration are presented and discussed.}
}
@article{POTTS2025104968,
title = {Explaining institutional technology},
journal = {European Economic Review},
volume = {173},
pages = {104968},
year = {2025},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2025.104968},
url = {https://www.sciencedirect.com/science/article/pii/S0014292125000182},
author = {Jason Potts and Kurt Dopfer and Bill Tulloh},
keywords = {Technology, Economic evolution, Knowledge, Information theory, User innovation},
abstract = {This paper offers a review and several refinements and extensions of Explaining Technology, by Koppl et al. (2023), which develops a combinatorial theory of the evolution of technology. First, we suggest that the mechanism of tinkering can be formulated in the theory of user innovation. Second, we propose that a useful refinement is to focus on institutional technologies. This offers a better explanation of emergent levels of evolutionary selection, or major transitions, and also adapts their framework to better explain the nature of a digital economy. Third, we propose a more ambitious line of generalisation from explaining technology to explaining knowledge. We suggest this is possible from the rubric of the new ‘physics of information’ in constructor theory, assembly theory and Bayesian mechanics.}
}
@article{MALOMO2024106108,
title = {Discontinuum models for the structural and seismic assessment of unreinforced masonry structures: a critical appraisal},
journal = {Structures},
volume = {62},
pages = {106108},
year = {2024},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2024.106108},
url = {https://www.sciencedirect.com/science/article/pii/S2352012424002601},
author = {D. Malomo and B. Pulatsu},
keywords = {Unreinforced masonry, Structural analysis, Seismic analysis, Discontinuum analysis, Discrete Element Methods, DEM, AEM, NSCD, Computational modelling, Numerical modelling},
abstract = {In the last few decades, discontinuum (or discrete, discontinuous) numerical modelling strategies – i.e. those capable of representing the motion of multiple, intersecting discontinuities explicitly – have become increasingly popular for the structural and seismic assessment of unreinforced masonry (URM) structures. The automatic recognition of new contact points and prediction of large deformations up to complete separation are unique features of discontinuum-based models, making them particularly suitable for unit-by-unit simulations. The adaptation of discrete computational models, primarily used for analyzing rock mechanics and geomechanics problems, to the conservation, structural and earthquake engineering evaluation of URM assemblies is still ongoing, and recent advances in computer-aided technologies are accelerating significantly their adoption. Researchers have now developed fracture energy-based contact models tailored to unreinforced masonry mechanics, explored discontinuum analysis from the mortar joint- to the 3D building-level, combined discrete modelling strategies with analytical or continuum approaches, integrated the latest structural health monitoring and image-based developments into discontinuum-based analysis framework. Concurrently, new and still unsolved issues have also arisen, including the selection of appropriate damping schemes, degree of idealization and discretization strategies, identification of appropriate lab or onsite tests to infer meaningful equivalent mechanical input parameters. This paper offers to the research and industry communities an updated critical appraisal and practical guidelines on the use of discontinuum-based structural and seismic assessment strategies for URM structures, providing opportunities to uncover future key research paths. First, masonry mechanics and discontinuum-based idealization options are discussed by considering micro-, meso- and macro-scale modelling strategies. Pragmatic suggestions are provided to select appropriate input parameters essential to model masonry composite and its constituents at different scales. Then, discontinuum approaches are classified based on their formulation, focusing on the Distinct Element Method (DEM), Applied Element Method (AEM) and Non-Smooth Contact Dynamics (NSCD), and an overview of primary differences, capabilities, pros and cons are thoroughly discussed. Finally, previous discontinuum-based analyses of URM small-scale specimens, isolated planar or curved components, assemblies or complex structures are critically reviewed and compared in terms of adopted strategies and relevant outcomes. This paper presents to new and experienced analysts an in-depth summary of what modern discontinuum-based tools can provide to the structural and earthquake engineering fields, practical guidelines on implementing robust and meaningful modelling strategies at various scales, and potential future research directions.}
}
@article{MOHAMED2023108104,
title = {A discrete-based multi-scale modeling approach for the propagation of seismic waves in soils},
journal = {Soil Dynamics and Earthquake Engineering},
volume = {173},
pages = {108104},
year = {2023},
issn = {0267-7261},
doi = {https://doi.org/10.1016/j.soildyn.2023.108104},
url = {https://www.sciencedirect.com/science/article/pii/S0267726123003494},
author = {Tarek Mohamed and Jérôme Duriez and Guillaume Veylon and Laurent Peyras and Patrick Soulat},
keywords = {Multi-scale, DEM, Toyoura sand, Seismic waves propagation, Bounding surface plasticity, Inertial effect},
abstract = {A three-dimensional multi-scale discrete–continuum model (Finite Volume Method × Discrete Element Method, FVM × DEM) is developed for a discrete-based description of the mechanical behavior of granular soils in boundary value problems (BVPs). In such a scheme, the constitutive response of the material is derived through direct DEM computations on a representative volume element attached to each mesh element. The developed multi-scale approach includes the inertial effect in the stress homogenization formulation and serves to study the mechanism of propagation of seismic waves, in comparison with a more classical BVP simulation that adopts an advanced bounding surface plasticity model “P2PSand”. We start with a detailed and fair calibration and validation of these two models against laboratory tests for Toyoura sand under monotonic and cyclic loading. Then, the performance of the two approaches is compared for the case of a seismic wave loading passing through a saturated soil column with different relative densities, revealing several differences between the results of the two models.}
}
@article{BOTTI2017481,
title = {Integrating ergonomics and lean manufacturing principles in a hybrid assembly line},
journal = {Computers & Industrial Engineering},
volume = {111},
pages = {481-491},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217302152},
author = {Lucia Botti and Cristina Mora and Alberto Regattieri},
keywords = {Lean manufacturing, Occupational safety, Ergonomics, Automation, Human factors, Hybrid assembly line},
abstract = {Lean manufacturing is a production method that was established in the wake of the Japanese Toyota Production System and rapidly established in the worldwide manufacturing industry. Lean characteristics combine just-in-time practices, work-in-progress and waste reduction, improvement strategies, defect-free production, and standardization. The primary goal of lean thinking is to improve profits and create value by minimizing waste. This study introduces a novel mathematical model to design lean processes in hybrid assembly lines. The aim was to provide an effective, efficient assembly line design tool that meets the lean principles and ergonomic requirements of safe assembly work. Given the production requirements, product characteristics and assembly tasks, the model defines the assembly process for hybrid assembly lines with both manual workers and automated assembly machines. Each assembly line solution ensures an acceptable risk level of repetitive movements, as required by current law. This model helps managers and practitioners to design hybrid assembly lines with both manual workers and automated assembly machines. The model was tested in a case study of an assembly line for hard shell tool cases. Results show that worker ergonomics is a key parameter of the assembly process design, as other lean manufacturing parameters, e.g. takt time, cycle time and work in progress.}
}
@article{WAN2025122214,
title = {Hesitant multiplicative best and worst method for multi-criteria group decision making},
journal = {Information Sciences},
volume = {715},
pages = {122214},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.122214},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525003469},
author = {Shu-Ping Wan and Xi-Nuo Chen and Jiu-Ying Dong and Yu Gao},
keywords = {Hesitant multiplicative preference relation, Multi-criteria decision-making, Best and worst method, Analytic hierarchy process},
abstract = {Best-worst method (BWM) has been extended in various uncertain scenarios owing to fewer comparisons and better reliability. This article utilizes hesitant multiplicative (HM) sets (HMSs) to express reference comparisons (RCs) and develops a novel HM BWM (HMBWM). We first define the multiplicative consistency for HM preference relation (HMPR). A fast and effective approach is designed to derive the priority weights (PWs) from an HMPR. To extend BW into HMPR, the score value of each criterion is computed to identify the best and worst criteria. Then, the PWs are acquired through constructing a 0–1 mixed goal programming model based on the HM RCs (HMRCs). The consistency ratio is given to judge the multiplicative consistency of HMRCs. An approach is proposed to enhance the consistency when the HMRCs are unacceptably consistent. Thereby, a novel HMBWM is proposed. On basis of HMBWM, this article further develops a novel method for group decision making (GDM) with HMPRs. The decision makers’ weights are determined by consistency ratio and the group PWs of alternatives are obtained by minimum relative entropy model. Four examples show that HMBWM possesses higher consistency and the proposed GDM method has stronger distinguishing ability, less computation workload and fewer modifications of elements.}
}
@article{LI2023297,
title = {BalanceHRNet: An effective network for bottom-up human pose estimation},
journal = {Neural Networks},
volume = {161},
pages = {297-305},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000485},
author = {Yaoping Li and Shuangcheng Jia and Qian Li},
keywords = {Multi-branch structure, Fusion, Balance structure, Branch attention},
abstract = {In the study of human pose estimation, which is widely used in safety and sports scenes, the performance of deep learning methods is greatly reduced in high overlap rate and crowded scenes. Therefore, we propose a bottom-up model, called BalanceHRNet, which is based on balanced high-resolution module and a new branch attention module. BalanceHRNet draws on the multi-branch structure and fusion method of a popular model HigherHRNet. And our model overcomes the shortcoming of HigherHRNet that cannot obtain a large enough receptive field. Specifically, through the connecting structure in balanced high-resolution module, we can connect almost all convolutional layers and obtain a sufficiently large receptive field. At the same time, the multi-resolution representation can be maintained due to the use of balanced high-resolution module, which enable our model to recognize objects with richer scales and obtain more complex semantics information. And for branch fusion method, we design branch attention to obtain the importance of different branches at different stages. Finally, our model improves the accuracy while ensuring a smaller amount of computation than HigherHRNet. The CrowdPose dataset is used as test dataset, and HigherHRNet, AlphaPose, OpenPose and so on are taken as comparison models. The AP measured by BalanceHRNet is 63.0%, increased by 3.1% compared to best model — HigherHRNet. We also demonstrate the effectiveness of our network through the COCO(2017) keypoint detection dataset. Compared with HigherHRNet-w32, the AP of our model is improved by 1.6%.}
}
@article{WANG2024119836,
title = {Novel score function and standard coefficient-based single-valued neutrosophic MCDM for live streaming sales},
journal = {Information Sciences},
volume = {654},
pages = {119836},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119836},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523014214},
author = {Fei Wang},
keywords = {Single-valued neutrosophic set, Score function, Standard coefficient, Multi-criteria decision making, Live streaming sales},
abstract = {Single-valued neutrosophic sets (SVNS) provide a comprehensive approach to express uncertainty in decision scenarios, surpassing the utility of fuzzy sets (FS) and intuitionistic fuzzy sets (IFS). Yet, current SVNS score function concepts stem from FS and IFS construction methods, showing inconsistencies. Thus, we introduce a novel SVNS score function based on inherent uncertainty essence. Additionally, we devise a standard coefficient to gauge SVNS standardization akin to fuzzy sets. Addressing SVNS researchability and limitations in fundamental concepts, especially the score function, we propose an SVNS-based multi-criteria decision-making (MCDM) model. This leverages the new score function and standard coefficient. We demonstrate its effectiveness on two decisions: “software engineer recruitment” with known weights and “investment selection” with unknown weights. Ultimately, we successfully applied the model to the field of live streaming sales to solve the actual MCDM problem. By comparing with existing methods, we affirm the model's validity and practicality. Compared to prior approaches, the new method exhibits: (1) Enhanced stability and credibility in result values and rankings, promoting robust optimal solutions. (2) Reduced computational steps and workload, enhancing usability and practicality.}
}
@article{CERONGARCIA20221,
title = {Jigsaw cooperative learning of multistage counter-current liquid-liquid extraction using Mathcad®},
journal = {Education for Chemical Engineers},
volume = {38},
pages = {1-13},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1749772821000488},
author = {Mª Carmen Cerón-García and Lorenzo López-Rosales and Juan José Gallardo-Rodríguez and Elvira Navarro-López and Asterio Sánchez-Mirón and Francisco García-Camacho},
keywords = {Liquid-liquid extraction, Mass transfer, Jigsaw cooperative learning, Computational tools, Mathcad®},
abstract = {This work shows the improvement in comprehending counter-current liquid-liquid extraction by applying jigsaw-type cooperative learning and the engineering math software Mathcad® in a chemical engineering course, part of the Chemistry degree. This study was performed on two different groups at the University of Almería (Spain) over three academic years. The students were divided into two groups: one half of the class followed a non-cooperative learning methodology (the control) while the other half were spread among the jigsaw cooperative groups following the methodology known as “Jigsaw Experts Groups”. A main template made with Mathcad® of a multistage counter-current liquid-liquid extraction was supplied to both the jigsaw and non-jigsaw groups. The assessment of this educational experience in the course revealed that the jigsaw group outperformed the control group. The use of Mathcad® proved to be very intuitive and effective in explaining these relatively complex problems and utilising it is highly recommended; we suggest it is used rather than classical and less intuitive graphical methods.}
}
@article{HANOCH20021,
title = {“Neither an angel nor an ant”: Emotion as an aid to bounded rationality},
journal = {Journal of Economic Psychology},
volume = {23},
number = {1},
pages = {1-25},
year = {2002},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(01)00065-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167487001000654},
author = {Yaniv Hanoch},
keywords = {Bounded rationality, Emotion},
abstract = {The role of emotion as a source of bounded rationality has been largely ignored. Following Herbert Simon, economists as well as psychologists have mainly focused on cognitive constraints while neglecting to integrate the growing body of research on emotion which indicates that reason and emotion are interconnected. Accordingly, the present paper aims to bridge the existing gap. By establishing a link between the two domains of research, emotion and bounded rationality, it will be suggested that emotions work together with rational thinking in two distinct ways, and thereby function as an additional source of bounded rationality. The aim, therefore, is not to offer an alternative to bounded rationality; rather, the purpose is to elaborate and supplement themes emerging out of bounded rationality.}
}
@article{ANDRAS2021110734,
title = {Where do successful populations originate from?},
journal = {Journal of Theoretical Biology},
volume = {524},
pages = {110734},
year = {2021},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2021.110734},
url = {https://www.sciencedirect.com/science/article/pii/S0022519321001569},
author = {Peter Andras and Adam Stanton},
keywords = {Computational modelling, Socio-technical evolution, Socio-biological simulation, Human geography, Geography of speciation},
abstract = {In order to understand the dynamics of emergence and spreading of socio-technical innovations and population moves it is important to determine the place of origin of these populations. Here we focus on the role of geographical factors, such as land fertility and mountains in the context of human population evolution and distribution dynamics. We use a constrained diffusion-based computational model, computer simulations and the analysis of geographical and land-quality data. Our analysis shows that successful human populations, i.e. those which become dominant in their socio – geographical environment, originate from lands of many valleys with relatively low land fertility, which are close to areas of high land fertility. Many of the homelands predicted by our analysis match the assumed homelands of known successful populations (e.g. Bantus, Turkic, Maya). We also predict other likely homelands as well, where further archaeological, linguistic or genetic exploration may confirm the place of origin for populations with no currently identified urheimat. Our work is significant because it advances the understanding of human population dynamics by guiding the identification of the origin locations of successful populations.}
}
@article{TRELEAVEN198859,
title = {Parallel architecture overview},
journal = {Parallel Computing},
volume = {8},
number = {1},
pages = {59-70},
year = {1988},
note = {Proceedings of the International Conference on Vector and Parallel Processors in Computational Science III},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(88)90109-3},
url = {https://www.sciencedirect.com/science/article/pii/0167819188901093},
author = {Philip C. Treleaven},
keywords = {Computer architecture, parallel computing},
abstract = {An increasing number of parallel computer products are appearing in the market place. Their design motivations and market areas cover a broad spectrum: (i) Transaction Processing Systems, such as Parallel UNIX systems (e.g. SEQUENT Balance), for data processing applications; (ii) Numeric Supercomputers, such as Hypercube systems (e.g. INTEL iPSC), for scientific and engineering applications; (iii) VLSI Architectures, such as parallel microcomputers (e.g. INMOS Transputer), for exploiting very large scales of integration; (iv) High-Level Language Computers, such as Logic machines (e.g. FUJITSU Kabu-Wake), for symbolic computation; and (v) Neurocomputers, such as Connectionist computers (e.g. THINKING MACHINES Connection Machine), for general-purpose pattern matching applications. This survey paper gives an overview of these novel parallel computers and discusses the likely commercial impact of parallel computers.}
}
@article{PITKOW2017943,
title = {Inference in the Brain: Statistics Flowing in Redundant Population Codes},
journal = {Neuron},
volume = {94},
number = {5},
pages = {943-953},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2017.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S089662731730466X},
author = {Xaq Pitkow and Dora E. Angelaki},
keywords = {brain, inference, theory, population code, message-passing, redundant, coding, nuisance, nonlinear},
abstract = {It is widely believed that the brain performs approximate probabilistic inference to estimate causal variables in the world from ambiguous sensory data. To understand these computations, we need to analyze how information is represented and transformed by the actions of nonlinear recurrent neural networks. We propose that these probabilistic computations function by a message-passing algorithm operating at the level of redundant neural populations. To explain this framework, we review its underlying concepts, including graphical models, sufficient statistics, and message-passing, and then describe how these concepts could be implemented by recurrently connected probabilistic population codes. The relevant information flow in these networks will be most interpretable at the population level, particularly for redundant neural codes. We therefore outline a general approach to identify the essential features of a neural message-passing algorithm. Finally, we argue that to reveal the most important aspects of these neural computations, we must study large-scale activity patterns during moderately complex, naturalistic behaviors.}
}
@article{ASAI1991323,
title = {Discipline Pascal with descriptive environment; precise writing to learn programming and to avoid errors},
journal = {Computers & Education},
volume = {16},
number = {4},
pages = {323-335},
year = {1991},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(91)90006-D},
url = {https://www.sciencedirect.com/science/article/pii/036013159190006D},
author = {Hitohisa Asai},
abstract = {Dijkstra's article “On the cruelty of really teaching computing science” [l] has encouraged me to review my thoughts and experience. In the teaching environments of an introductory programming course (CS1, CS2 and others), I have discovered a mental gap in the minds of some students, which has often led to difficulties in class. A Pascal statement contains highly condensed information. In order to deal with this type of information, the students must apply a certain thinking level. In a Pascal statement, much information is concealed behind the written words. For example, consider a variable in a program. The data type and locality of the variable are not explicitly expressed in a statement. In other words, this condensed information is hiding, e.g. invisible. Hence a degree of the student's ability to focus on it may fade away in his mind. If programming code demands writing precise information in a statement then the student's difficulty may be alleviated because he can see it. This would be an attempt to narrow the gap by writing Discipline Pascal code closer to the precise thinking level. I believe that this proposal would also bring benefits to experienced programmers.}
}
@article{TEY2021153,
title = {The Impact of Concession Patterns on Negotiations: When and Why Decreasing Concessions Lead to a Distributive Disadvantage},
journal = {Organizational Behavior and Human Decision Processes},
volume = {165},
pages = {153-166},
year = {2021},
issn = {0749-5978},
doi = {https://doi.org/10.1016/j.obhdp.2021.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0749597821000613},
author = {Kian Siong Tey and Michael Schaerer and Nikhil Madan and Roderick I. Swaab},
keywords = {Negotiations, Concessions, Reservation price, Offers, Signaling, Distributive},
abstract = {We propose that making a series of decreasing concessions (e.g., $1,500–1,210–1,180–1,170) signals that negotiators are reaching their limit and that this results in a negotiation disadvantage for offer recipients. Although we find that most negotiators do not use this strategy naturally, seven studies (N = 2,311) demonstrate that decreasing concessions causes recipients to make less ambitious counteroffers (Studies 1–5) and reach worse deals (Study 2) in distributive negotiations. We find that this disadvantage occurs because decreasing concessions shape recipients’ expectations of the subsequent offers that will be made, which results in inflated perceptions of the counterparts’ reservation price relative to the other concession strategies (Study 3). In addition, we find that this disadvantage is particularly large when concessions decrease at a moderate rate (Study 4a) and when decreasing concessions takes place over more (vs. fewer) rounds (Study 4b). Finally, we find that recipients can protect themselves against the deleterious effects of decreasing concession by thinking of a target before they enter the negotiation (Study 5).}
}
@article{SINCLAIR2004169,
title = {Improving computer-assisted instruction in teaching higher-order skills},
journal = {Computers & Education},
volume = {42},
number = {2},
pages = {169-180},
year = {2004},
issn = {0360-1315},
doi = {https://doi.org/10.1016/S0360-1315(03)00070-8},
url = {https://www.sciencedirect.com/science/article/pii/S0360131503000708},
author = {Kelsey J Sinclair and Carl E Renshaw and Holly A Taylor},
keywords = {Evaluation methodologies, Multimedia/hypermedia systems, Secondary education, Applications in subject areas},
abstract = {Computer-assisted instruction (CAI) has been shown to enhance rote memory skills and improve higher order critical thinking skills. The challenge now is to identify what aspects of CAI improve which specific higher-order skills. This study focuses on the effectiveness of using CAI to teach logarithmic graphing and dimensional analysis. Two groups of ninth graders participated in a one-class period laboratory. Experiment 1 compared a fully automated computer laboratory to an equivalent paper-and-pencil exercise. Experiment 2 compared the same automated computer laboratory in Experiment 1 with a revised, less automated computer version. Both the paper-and-pencil exercise and the less automated computer exercise required students to perform basic mathematical calculations. The results from a post-test revealed that very few students were able to master the complex task of dimensional analysis, but students who took the paper-based and revised, less automated version scored higher overall. These results imply that students required to perform basic calculations had a better understanding of the lab as a whole. These results suggest that until students master basic skills, they do not have the cognitive resources to concentrate on higher-order concepts. This is supported by cognitive load theory.}
}
@article{LI2013262,
title = {Improved Particle Filter for Target Tracing Application based on ChinaGrid},
journal = {AASRI Procedia},
volume = {5},
pages = {262-267},
year = {2013},
note = {2013 AASRI Conference on Parallel and Distributed Computing and Systems},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2013.10.087},
url = {https://www.sciencedirect.com/science/article/pii/S2212671613000887},
author = {Yuqiang Li and Xixu He and Haitao Jia},
keywords = {Target tracing, Particle filter, ChinaGrid},
abstract = {Most practical target tracking are usually maneuvering, while most target tracking algorithm are linear filter. More estimation error is introduced from linear filter. Nowadays more and more researchers pay their attention in Maneuvering Target Tracking algorithm. Particle filter has been developed for estimation of nonlinear system states. This paper presents an improved particle filter, which can apply the maneuvering target tracking problem. In practice, the particle filter would take abundant computation for estimate the maneuvering target tracking. The ChinaGrid system use the agile and distributed federations to reduce the computing time, which achieve to fast resolution for particle filter computation of target tracing application. Lastly the simulation proves it.}
}
@article{NAIK2017509,
title = {Metastability in Senescence},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {509-521},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300797},
author = {Shruti Naik and Arpan Banerjee and Raju S. Bapi and Gustavo Deco and Dipanjan Roy},
keywords = {healthy aging, whole-brain computational modeling, metastability},
abstract = {The brain during healthy aging exhibits gradual deterioration of structure but maintains a high level of cognitive ability. These structural changes are often accompanied by reorganization of functional brain networks. Existing neurocognitive theories of aging have argued that such changes are either beneficial or detrimental. Despite numerous empirical investigations, the field lacks a coherent account of the dynamic processes that occur over our lifespan. Taking advantage of the recent developments in whole-brain computational modeling approaches, we hypothesize that the continuous process of aging can be explained by the concepts of metastability − a theoretical framework that gives a systematic account of the variability of the brain. This hypothesis can bridge the gap between existing theories and the empirical findings on age-related changes.}
}
@article{NEUDERT2024100092,
journal = {Journal of Responsible Technology},
volume = {20},
pages = {100092},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100092},
url = {https://www.sciencedirect.com/science/article/pii/S2666659624000180},
author = {Philipp Neudert and Mareike Smolka and Britta Acksel and Yana Boeva}
}
@article{HARTMAN2025100398,
title = {Recommendations for the Development of Artificial Intelligence Applications for the Retail Level},
journal = {Journal of Food Protection},
volume = {88},
number = {1},
pages = {100398},
year = {2025},
issn = {0362-028X},
doi = {https://doi.org/10.1016/j.jfp.2024.100398},
url = {https://www.sciencedirect.com/science/article/pii/S0362028X24001820},
author = {Jim Hartman},
keywords = {AI applications based on cognitive models, Development of HACCP plans, Food safety root cause analyses, Foodborne illness outbreak investigations},
abstract = {Some of the early applications of artificial intelligence (AI) for food safety appear to be intended for use at the level of manufacturing and distribution. Artificial intelligence applications to facilitate foodborne illness outbreak investigations, development of HACCP plans, and food safety root cause analyses at the retail level are needed. For example, the interview form in the International Association for Food Protection booklet, Procedures to Investigate Foodborne Illness, could be filled out by humans, but much of the rest of the forms could be completed by artificial intelligence applications. Humans would still have to do the environmental assessments. Most AI applications to date have consisted of pattern identification. Pattern recognition applications may not be capable of assisting in all the proposed retail applications, but it would not be helpful to propose these retail applications without offering a possible path forward. Progress in the proposed directions may require the development of more robust artificial intelligence based on cognitive models. Because this paradigm shift is less familiar to food safety professionals, a comparison between pattern recognition algorithms and cognitive models is offered. An explanation of cognitive models is included to raise awareness of this approach.}
}
@article{DRANKO2021738,
title = {Structural Analysis of Large-Scale Socio-Technical Systems Based on the Concept of Influence},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {738-743},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.540},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321019789},
author = {O.I. Dranko and Yu.G. Rykov and A.A. Karandeev},
keywords = {Complex systems, Cognitive maps, Influence, Structural analysis, Russian Federation economics structure},
abstract = {The paper is devoted to studying the stability of complex systems, which include diversified and heterogeneous elements. In order to analyze the stability of such systems, it is proposed to develop the process of dynamic determination of the main factors. For this purpose, this paper uses a variant of cognitive modeling based on the concept of “influence”. The proposed approach implies, in a sense, a broader view of the concept of the “fuzzy cognitive map” introduced by B. Kosko. “Influence” does not mean “causality”, allows a broader interpretation range, and is calculated in a special way that makes it possible to prove rigorous theorems. A complex system is represented as a digraph, and the selection of the main factors is proposed to be based on the concept of “influence”, which is introduced as follows. All nodes of the graph are assigned an abstract property “significance”, which allows you to compare heterogeneous factors, and a particular computational procedure is introduced that allows this “significance” to be calculated. The “influence” of factors on each other is defined as a mathematically formalized response in accordance with the introduced computational procedure of the “significance” of the studied factor to the variation in the “significance” of input factors. For example, the analysis of the sustainability of a large-scale model of the Russian economy in terms of the strength of the “influence” is provided.}
}
@article{BRINK201339,
title = {Computing with networks of spiking neurons on a biophysically motivated floating-gate based neuromorphic integrated circuit},
journal = {Neural Networks},
volume = {45},
pages = {39-49},
year = {2013},
note = {Neuromorphic Engineering: From Neural Systems to Brain-Like Engineered Systems},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2013.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S089360801300052X},
author = {S. Brink and S. Nease and P. Hasler},
keywords = {Neuromorphic VLSI, Floating-gate transistor, Single transistor learning synapse, Spiking winner-take-all, Synfire chain},
abstract = {Results are presented from several spiking network experiments performed on a novel neuromorphic integrated circuit. The networks are discussed in terms of their computational significance, which includes applications such as arbitrary spatiotemporal pattern generation and recognition, winner-take-all competition, stable generation of rhythmic outputs, and volatile memory. Analogies to the behavior of real biological neural systems are also noted. The alternatives for implementing the same computations are discussed and compared from a computational efficiency standpoint, with the conclusion that implementing neural networks on neuromorphic hardware is significantly more power efficient than numerical integration of model equations on traditional digital hardware.}
}
@article{KHODADADI2022104354,
title = {Design exploration by using a genetic algorithm and the Theory of Inventive Problem Solving (TRIZ)},
journal = {Automation in Construction},
volume = {141},
pages = {104354},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104354},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002278},
author = {Anahita Khodadadi and Peter {von Buelow}},
keywords = {Conceptual design, Design exploration, TRIZ, Genetic algorithm, Multi-objective design},
abstract = {This paper presents a computational design exploration method called GA+TRIZ, which aids designers in defining the design problem clearly, making a parametric model where pertinent variables are included, obtaining a series of suitable solutions, and resolving existing conflicts among design objectives. The goal is to include the designer's qualitative and performance-based quantitative design goals in the design process, while promoting innovative ideas for resolving contradictory design objectives. The method employed is a Genetic Algorithm (GA), earlier implemented in an automated design exploration process called ParaGen, in combination with the Theory of Inventive Problem Solving (TRIZ), a novel methodology to assist architects and structural engineers in the conceptual phase of design. The GA+TRIZ method promotes automated design exploration, investigation of unexpected solutions, and continuous interaction with the computational generating system. Finally, this paper presents two examples that illustrate how the GA+TRIZ method assists designers in problem structuring, design exploration, and decision-making.}
}
@article{FANOUS2023105658,
title = {Challenges and prospects of climate change impact assessment on mangrove environments through mathematical models},
journal = {Environmental Modelling & Software},
volume = {162},
pages = {105658},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105658},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000440},
author = {Majdi Fanous and Jonathan M. Eden and Renji Remesan and Alireza Daneshkhah},
keywords = {Mangrove environments, Climate change, Hydro-morphodynamic modelling, Adaptation policies, Machine learning, Data-driven modelling},
abstract = {The impacts of climate change, especially sea-level rise, are an increasing threat to the world’s coastal regions. Following recommendations made by the United Nations about the preservation of mangrove environments, particularly given their potential for effective natural defence against wave-driven hazards, a series of experiments have been conducted to quantify the ability of mangroves to counter climate change impacts. To date, these experiments have been limited by computational cost and inability to model multiple scenarios. With improved data quality and availability, machine learning has enormous potential to supplement, or even replace, existing numerical methods. This article presents both an outline of the importance of protecting mangrove environments and a review of methods currently used to quantify the capacity of mangroves to adapt to climate change impacts. In view of the limitations of existing numerical methods, the article also discusses the potential of machine learning as an efficient and effective alternative.}
}
@article{LOPEZ2015289,
title = {DAMQT 2.0: A new version of the DAMQT package for the analysis of electron density in molecules},
journal = {Computer Physics Communications},
volume = {192},
pages = {289-294},
year = {2015},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2015.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0010465515000855},
author = {Rafael López and Jaime Fernández Rico and Guillermo Ramírez and Ignacio Ema and David Zorrilla},
keywords = {Electron density, Electrostatic potential, Electric field, Hellmann–Feynman forces, Density deformations},
abstract = {DAMQT 2.0 is a new version of the DAMQT package for the analysis of electron density in molecules and the fast computation of the density, density deformations, electrostatic potential and field, and Hellmann–Feynman forces. Algorithms for the partition of the electron density and the computation of related properties like density deformations, electrostatic potential and field and Hellmann–Feynman forces have been improved and their codes, fully rewritten. MPI versions of the most computational demanding modules are now included in the package for parallel computation. The Graphical User Interface has been also enhanced, with new features including a 2D plotter and significant improvements in the 3D viewer.
Program summary
Program title: DAMQT 2.0 Catalogue identifier: AEDL_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GPLv3 No. of lines in distributed program, including test data, etc.: 317,270 No. of bytes in distributed program, including test data, etc.: 40,193,220 Distribution format: tar.gz Programming language: Fortran90 and C++. Computer: Any. Operating system: Linux, Windows (7, 8). RAM: 200 Mbytes Classification: 16.1. Catalogue identifier of previous version: AEDL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180(2009)1654 External routines: Qt (4.8 or higher), OpenGL (3.x or higher), freeGLUT 2.8.x Nature of problem: Analysis of the molecular electron density and density deformations, including fast evaluation of electrostatic potential, electric field and Hellmann–Feynman forces on nuclei. Solution method: The method of Deformed Atoms in Molecules, reported elsewhere [1], is used for partitioning the molecular electron density into atomic fragments, which are further expanded in spherical harmonics times radial factors. The partition is used for defining molecular density deformations and for the fast calculation of several properties associated to density. Restrictions: Density must come from an LCAO calculation (any level) with spherical (not Cartesian) Slater or Gaussian functions. Unusual features: The program contains an OPEN statement to binary files (stream) in several files. This statement has not a standard syntax in Fortran 90. Two possibilities are considered in conditional compilation: Intel’s ifort and Fortran2003 standard. The latter is applied to compilers other than ifort (gfortran uses this one, for instance). Additional comments: Quick-start guide and User’s manual in PDF format included in the package. User’s manual is also accessible from the Graphical User Interface. The distribution file for this program is over 40 Mbytes and therefore is not delivered directly when downloaded or Email is requested. Instead an html file giving details of how the program can be obtained is sent. Running time: Largely dependent on the system size and the module run (from fractions of a second to hours). References:[1]J. Fernández Rico, R. López, I. Ema and G. Ramírez, J. Mol. Struct. Theochem 727 (2005) 115.}
}
@article{COWLEY2021101364,
title = {Reading: skilled linguistic action},
journal = {Language Sciences},
volume = {84},
pages = {101364},
year = {2021},
note = {A Dialogue between Distributed Language and Reading Disciplines},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2021.101364},
url = {https://www.sciencedirect.com/science/article/pii/S0388000121000103},
author = {Stephen J. Cowley},
keywords = {Reading, Distributed language, Embodied cognitive science, Languaging, Literacy, Radical embodied cognitive science},
abstract = {The paper links critique of ‘inner process’ to a perspective that treats language as activity that is accomplished by living beings. The view traces reading to human ways of coordinating with ‘the seen’. Having contrasted this distributed view with organism-first alternatives, I use a case study of reports to sketch how readers engage with written materials to both select details and project an imagined ‘source’ (e.g. a meaning, author or intention). Far from using inner process (‘decoding’) readers coordinate with a field of patternings. Where skilled, they use recollecting to link looking, silent thinking, expectations and strategic moves. Using judgements, they transform what they observe by setting off experience. I thus build on Wittgenstein's critique of inner process while also endorsing Trybulec’s (2019) radicalization of his view. To avoid treating the sense of ‘written words’ as subjective, the material aspect of patternings is taken to index outward criteria (roughly, standards of judgement). In seeking to replace theories that presuppose ‘text’, I stress how patternings invite directed sensorimotor activity by an intelligent person. Indeed, since persons learn to see wordings (or take a language stance) arrangements of patternings act as marks, ‘symbols’ and aggregations that set off recollection, judgements and iterated action. Skilled readers can use re-reading, the already read etc. to modulate ways of attending. Readers link the said, hints, recollections and ways of actualizing movements to grant reading experience a specific sense. By considering how outer criteria are evoked, reading is traced back to skilled linguistic action.}
}
@article{LI2024933,
title = {A Novel Quantifiable Weight Model for the Digital Twin Technology in Enterprise},
journal = {Procedia Computer Science},
volume = {247},
pages = {933-942},
year = {2024},
note = {The 11th International Conference on Applications and Techniques in Cyber Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.113},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029132},
author = {Yanchao Li},
keywords = {Quantifiable weight model, digital twin technology, main core},
abstract = {The performance evaluation involves a variety of composite factors, and most factors are subjectively determined by the people, so the fuzziness of this evaluation is inevitable. This paper firstly focuses on this issue based on the dynamic influence of digital twin technology input for different kinds of digital twin technology activities. Then this paper carries out a detailed analysis and research on the main core and criterion of digital twin technology. Thirdly this paper puts forward the following suggestions so as to give full play to the role of digital twin technology input of three different kinds in activities. Lastly this paper constructs a novel quantifiable weight model of performance evaluation by applying fuzzy mathematics theory. The calculation results indicate that this model can keep an eye on the evolution rule of digital twin technology to timely change the system are the optimal plans and measures based on weight analysis, and their weights are 0.336, 0.334 and 0.330 respectively, and the rationality of quantifiable weight behavior has a significant impact on the technology development.}
}
@incollection{CAULLER1992199,
title = {Chapter 8 - Functions of Very Distal Dendrites: Experimental and Computational Studies of Layer I Synapses on Neocortical Pyramidal Cells},
editor = {THOMAS MCKENNA and JOEL DAVIS and STEVEN F. ZORNETZER},
booktitle = {Single Neuron Computation},
publisher = {Academic Press},
address = {San Diego},
pages = {199-229},
year = {1992},
series = {Neural Networks: Foundations to Applications},
isbn = {978-0-12-484815-3},
doi = {https://doi.org/10.1016/B978-0-12-484815-3.50014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124848153500141},
author = {LARRY J. CAULLER and BARRY W. CONNORS},
abstract = {Publisher Summary
This chapter reviews an approach that combines quantitative morphology, physiology, and computational analysis to understandthe functions of a complex synaptic–neuronal interaction in the cortex. A variation of the in vitro slice of rat somatosensory neocortex examines the effectiveness of layer 1 inputs to pyramidal cells whose bodies lie 0.5–1 mm deeper, in layers 3 or 5. The horizontal fibers in layer 1 (HL1) were isolated by disconnecting all deeper horizontal fibers with a cut perpendicular to the surface, extending from just below layer 1 downward through subcortical white matter and Layer 1 was stimulated on one side of the cut and the response mediated by HL1 fibers passing to the other side was recorded extracellularly and intracellularly. Backward cortico–cortical projections, which end largely on distal apical dendrites in layer 1, are important for higher cortical functions. By isolating horizontal afferents to layer 1 in an in vitro neocortical slice, layer 1 synapses can strongly excite pyramidal cells as deep as layer 5b.}
}
@article{KAMEUGNE2025505,
title = {Quadratic horizontally elastic not-first/not-last filtering algorithm for cumulative constraint},
journal = {European Journal of Operational Research},
volume = {320},
number = {3},
pages = {505-515},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724006945},
author = {Roger Kameugne and Sévérine Fetgo Betmbe and Thierry Noulamo},
keywords = {Not-first/not-last algorithm, Profile data structure, TimeTable data structure, Cumulative scheduling, Constraint programming, Horizontally elastic scheduling, RCPSP},
abstract = {The not-first/not-last rule is a pendant of the edge finding rule, generally embedded in the cumulative constraint during constraint-based scheduling. It is combined with other filtering rules for more pruning of the tree search. In this paper, the Profile data structure in which tasks are scheduled in a horizontally elastic way is used to strengthen the classic not-first/not-last rule. Potential not-first task intervals are selected using criteria (specified later in the paper), and the Profile data structure is applied to selected task intervals. We prove that this new rule subsumes the classic not-first rule. A quadratic filtering algorithm is proposed for the new rule, thus improving the complexity of the horizontally elastic not-first/not-last algorithm from O(n3) to O(n2). The fixed part of external tasks that overlap with the selected task intervals is considered during the computation of the earliest completion time of task intervals. This improvement increases the filtering power of the algorithm while remaining quadratic. Experimental results, on a well-known suite of benchmark instances of Resource-Constrained Project Scheduling Problems (RCPSPs), show that the propounded algorithms are competitive with the state-of-the-art not-first algorithms in terms of tree search and running time reduction.}
}
@article{BAKHTAVAR2020122886,
title = {Assessment of renewable energy-based strategies for net-zero energy communities: A planning model using multi-objective goal programming},
journal = {Journal of Cleaner Production},
volume = {272},
pages = {122886},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122886},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620329310},
author = {Ezzeddin Bakhtavar and Tharindu Prabatha and Hirushie Karunathilake and Rehan Sadiq and Kasun Hewage},
keywords = {Community energy planning, Renewable energy, Life cycle assessment, Multi-objective optimi, z, ation, Grey numbers, Goal programming},
abstract = {Planning decentralised community-level hybrid energy systems has emerged as a solution to the various environmental and economic issues associated with conventional centralised energy supply systems. However, the optimal planning of community energy systems is a challenging issue due to the complexities, uncertainties, conflicting objectives, and high computational times in analysis. This study introduces a new multi-objective model based on weighted goal programming and grey pairwise comparison to assess renewable energy-based strategies in the case of net-zero energy communities. The problem was formulated to determine the optimal energy mix based on minimization of life cycle impacts and costs and maximization of renewable contributions and operational energy savings. To this end, binary integer and continues variables were applied on the code developed in a CPLEX environment. A pairwise comparison based on grey numbers was used to find the impacts of the goals in the objective function of the model under uncertainty. In addition to the grey-based weighting scenario, different weighting scenarios were employed to consider the importance of all goals on the system. These weighting scenarios were used to investigate the effects of changing decision priorities on the outcomes and on stakeholder interests at different levels. The developed goal-programming model was applied to the data of a case example and solved based on the weighting scenarios. Results indicated that the model is capable of finding the best possible strategies with the lowest total undesirable deviations from the desired levels of the goals compared to the literature of the decision-making techniques. The integration of maximum renewable energy (RE) supply in the energy mix with the locally available energy resources can deliver considerable benefits in terms of energy supply cost reduction as well as in mitigating life cycle environmental impacts. When environmental goals are prioritized, integrating low emissions RE as much as possible and excluding waste-to-energy technologies makes best sense, while under a pro-economic perspective, solar integration is comparatively discouraged. The findings of the study are expected to assist community developers and other decision makers involved in regional energy planning. The developed method will also be of use for those who are interested in the use of goal programming to solve complex planning issues involving numerous uncertain parameters.}
}
@article{ZHAO201568,
title = {Approximate methods for optimal replacement, maintenance, and inspection policies},
journal = {Reliability Engineering & System Safety},
volume = {144},
pages = {68-73},
year = {2015},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015001957},
author = {Xufeng Zhao and Khalifa N. Al-Khalifa and Toshio Nakagawa},
keywords = {Hazard function, Mean time to failure, Age replacement, Imperfect maintenance, Approximate inspection},
abstract = {It might be difficult sometimes to derive theoretical and numerical solutions for analytical maintenance modelings due to the computational complexity. This paper takes up several approximate models in maintenance theory, by using the cumulative hazard function H(t) and the newly proposed asymptotic MTTF (Mean Time to Failure) skilfully. We firstly denote by tx the time when the expected number of failures is x. Using H(tx)=x, we estimate failure times, model age and periodic replacements, and sequential imperfect maintenance. Motivated by the asymptotic method of computation of MTTF, we secondly model the expected cost rate for a parallel system when replacement is made at system failure, and give approximate computations for the sequential inspection policy. Optimizations of each model are obtained approximately in an easier way. When failure times have a Weibull distribution, it is shown from numerical examples that the obtained approximate optimal solutions have good approximations of the exact ones.}
}
@article{STROM2024111887,
title = {Diborane anharmonic vibrational frequencies and Intensities: Experiment and theory},
journal = {Journal of Molecular Spectroscopy},
volume = {400},
pages = {111887},
year = {2024},
issn = {0022-2852},
doi = {https://doi.org/10.1016/j.jms.2024.111887},
url = {https://www.sciencedirect.com/science/article/pii/S0022285224000146},
author = {Aaron I. Strom and Ibrahim Muddasser and Guntram Rauhut and David T. Anderson},
keywords = {Matrix Isolation Spectroscopy, Anharmonic Vibrational Dynamics, Infrared Spectroscopy, Computational Spectroscopy, Diborane, Fermi Resonance, Darling-Dennison Resonance},
abstract = {The vibrational dynamics of diborane have been extensively studied both theoretically and experimentally ever since the bridge structure of diborane was established in the 1950s. Numerous infrared and several Raman spectroscopic studies have followed in the ensuing years at ever increasing levels of spectral resolution. In parallel, ab initio computations of the underlying potential energy surface have progressed as well as the methods to calculate the anharmonic vibration dynamics beyond the double harmonic approximation. Nevertheless, even 70 years after the bridge structure of diborane was established, there are still significant discrepancies between experiment and theory for the fundamental vibrational frequencies of diborane. In this work we use parahydrogen (pH2) matrix isolation infrared spectroscopy to characterize six fundamental vibrations of B2H6 and B2D6 and compare them with results from configuration-selective vibrational configuration interaction theory. The calculated frequencies and intensities are in very good agreement with the pH2 matrix isolation spectra, even several combination bands are well reproduced. We believe that the reason discrepancies have existed for so long is related to the large amount of anharmonicity that is associated with the bridge BH stretching modes. However, the calculated frequencies and intensities reported here for the vibrational modes of all three boron isotopologues of B2H6 and B2D6 are within ± 2.00 cm−1 and ± 1.44 cm−1, respectively, of the experimental frequencies and therefore a refined vibrational assignment of diborane has been achieved.}
}
@article{BLEMKER2023111745,
title = {In vivo imaging of skeletal muscle form and function: 50 years of insight},
journal = {Journal of Biomechanics},
volume = {158},
pages = {111745},
year = {2023},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2023.111745},
url = {https://www.sciencedirect.com/science/article/pii/S0021929023003159},
author = {Silvia S. Blemker},
keywords = {Skeletal muscle, Imaging, In vivo},
abstract = {Skeletal muscle form and function has fascinated scientists for centuries. Our understanding of muscle function has long been driven by advancements in imaging techniques. For example, the sliding filament theory of muscle, which is now widely leveraged in biomechanics research, stemmed from observations made possible by scanning electron microscopy. Over the last 50 years, advancing in medical imaging, combined with ingenuity and creativity of biomechanists, have provide a wealth of new and important insights into in vivo human muscle function. Incorporation of in vivo imaging has also advanced computational modeling and allowed our research to have an impact in many clinical populations. While this review does not provide a comprehensive or meta-analysis of the all the in vivo muscle imaging work over the last five decades, it provides a narrative about the past, present, and future of in vivo muscle imaging.}
}
@article{MCGEE201440,
title = {The pragmatics of paragraphing English argumentative text},
journal = {Journal of Pragmatics},
volume = {68},
pages = {40-72},
year = {2014},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2014.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378216614000770},
author = {Iain McGee},
keywords = {Paragraphing, Lexical cohesion, Argumentative text, Textual colligation, Foregrounding, Discourse signaling, Rhetorical devices, Computational Linguistics},
abstract = {Computational linguistic work into the paragraph and paragraphing has highlighted the significant role that intra-paragraph lexical cohesion plays in ‘marking off’ one paragraph unit from another. The goal of the research reported on in this paper is to consider, in some detail, the relationship that exists between the lexical repetition patterns in an argumentative text (as identified by a computational procedure), the genre moves within it, the actual paragraphing of the texts, and the textual colligation features of the paragraphs. The Link Set Median procedure (Berber-Sardinha, 1997, Berber-Sardinha, 2001, Berber-Sardinha, 2002) is used to document exact, inflectional and derivational lexical repetition usage across 10 short English argumentative texts, and to predict where segmentations originally occurred in the texts. The resulting data are then analyzed in the light of diverse research interests into the paragraph, and classified accordingly. A comparison of these results is made with data where there is either a marginal or no difference in the link set medians of adjacent sentences across paragraph junctures within the same texts. It is suggested that this novel approach of analyzing computational data from multiple paragraph-specific research interests results in a clearer picture of paragraphing practice emerging.}
}
@article{UWIZEYE2016647,
title = {A comprehensive framework to assess the sustainability of nutrient use in global livestock supply chains},
journal = {Journal of Cleaner Production},
volume = {129},
pages = {647-658},
year = {2016},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.108},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616301792},
author = {Aimable Uwizeye and Pierre J. Gerber and Rogier P.O. Schulte and Imke J.M. {de Boer}},
keywords = {Nitrogen, Phosphorus, Nutrient use efficiency, Life-cycle thinking, Livestock supply chain, Soil nutrient stock change},
abstract = {The assessment of the performance of nutrient use along livestock supply chains can help to identify targeted nutrient management interventions, with a goal to benchmark and to monitor the improvement of production practices. It is necessary, therefore, to develop indicators that are capable to describe all nutrient dynamics and management along the chain. This paper proposed a comprehensive framework, based on life-cycle thinking, to assess the sustainability of nitrogen and phosphorus use. The proposed framework represents nutrient flows in typical livestock supply chain from the “cradle-to-primary-processing-gate”, including crop/pasture production, animal production, and primary processing stage as well as the transportation of feed materials, live-animals or animal products. In addition, three indicators, including the life-cycle nutrient use efficiency (life-cycle-NUE), life-cycle net nutrient balance (life-cycle-NNB) and nutrient hotspot index (NHI) were proposed and tested in a case study of mixed dairy supply chains in Europe. Proposed indicators were found to be suitable to describe different aspects of nitrogen and phosphorus dynamics and, therefore, were all needed. Moreover, the disaggregation of life-cycle-NUE and life-cycle-NNB has been investigated and the uncertainties related to the choice of the method used to estimate changes in nutrient soil stock have been discussed. Given these uncertainties, the choice of method to compute the proposed indicators is determined by data availability and by the goal and scope of the exercise.}
}
@article{BUESODEBARRIO2025101019,
title = {Executable contracts for Elixir},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {142},
pages = {101019},
year = {2025},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2024.101019},
url = {https://www.sciencedirect.com/science/article/pii/S2352220824000737},
author = {Luis Eduardo {Bueso de Barrio} and Lars-Åke Fredlund and Ángel Herranz and Julio Mariño and Clara {Benac Earle}},
abstract = {This article presents the design of a library for attaching and checking executable contracts to code written in the Elixir programming language. In addition to classical contract constructs such as preconditions and postconditions, the library allows specifying exceptional behaviour (i.e., which exceptions are thrown and under which conditions), detecting non-termination issues in recursive functions by specifying a strictly decreasing order in function arguments, and associating timers with function calls to detect slow computations. The library also focuses on language-specific features, enabling the association of contracts with the reception of messages sent by processes and the attachment of constraints to variable names (useful due to variable shadowing in Elixir). Moreover, stateful contracts (i.e., with a model state) permit specifying the behaviour of stateful APIs whose operations can be linearized. Using the stateful contracts, a monitor can be employed to check that the observed state can be explained in terms of possible linearizations.}
}
@article{JOHANNESJOSEFIJEN202173,
title = {An adaptive temporal-causal network model to analyse extinction of communication over time},
journal = {Cognitive Systems Research},
volume = {68},
pages = {73-83},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000231},
author = {Lucas {Johannes José Fijen} and Julio {Joaquín López González} and Jan Treur},
keywords = {Extinction of communication, Network modeling, Adaptive network, Social simulation},
abstract = {The persistence of information communicated between humans is difficult to measure as it is affected by many features. This paper presents an approach to computationally model the cognitive processes of information sharing to describe persistence or extinction of communication in Twitter over time. The adaptive mental network model explains, for example, how an individual can experience information overflow on a topic, and how this affects the sharing of information. Parameter tuning by Simulated Annealing is used to identify characteristics of the network model that fit to empirical data from Twitter. The data collected is related to the independentism in Catalunya, Spain, which is considered a global issue with repercussion in Europe.}
}
@article{SCHOEN2025102018,
title = {Improving the teaching and learning of statistics},
journal = {Learning and Instruction},
volume = {95},
pages = {102018},
year = {2025},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2024.102018},
url = {https://www.sciencedirect.com/science/article/pii/S0959475224001452},
author = {Robert C. Schoen and Christopher Rhoads and Alexandra Perez and Tim Jacobbe and Lanrong Li},
keywords = {Curriculum, Teacher professional development, Statistics education, Many-facet rasch, Hierarchical linear modeling},
abstract = {Structured Abstract
Background
Statistical literacy is more important now than ever. Mathematics teachers are often expected to teach statistics, but statistics and mathematics differ in important ways. The mathematics teaching workforce needs more opportunities to learn statistics and how to teach it accurately and effectively.
Aims
This study was designed to estimate the effects of an intervention. The intervention consisted of a combination of an inquiry-oriented curriculum replacement unit and teacher learning opportunities in statistics and probability. Primary outcomes of interest were instructional practice and student understanding of statistics and probability.
Sample
The study sample included seventh-grade teachers and their students (age 13) in a single, urban school district in the southeastern United States. There were 74 classrooms represented in the analytic sample for the instructional outcome and 2,283 students in the analytic sample for the student outcome.
Methods
Schools were randomly assigned to the treatment or control conditions with equal probability of assignment to condition. Treatment-condition teachers participated in four days of professional learning workshops focused on teaching a 20-day curriculum unit. The Instructional Quality Assessment was used to measure instructional practice. The Levels of Conceptual Understanding in Statistics assessment instrument was used to measure student learning outcomes. Data analysis used hierarchical linear modeling.
Results
Positive, statistically significant effects on both instructional practice (ES = .99) and student understanding of statistics (ES = .25) were found.
Conclusions
The study results indicate that the inquiry-oriented lessons in the curriculum—with the support of teacher-learning opportunities—can improve instruction and increase student learning in statistics.}
}
@article{NI2011100,
title = {Influence of curriculum reform: An analysis of student mathematics achievement in Mainland China},
journal = {International Journal of Educational Research},
volume = {50},
number = {2},
pages = {100-116},
year = {2011},
note = {Curricular effect on the teaching and learning of mathematics: Findings from two longitudinal studies in China and the United States},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2011.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0883035511000413},
author = {Yujing Ni and Qiong Li and Xiaoqing Li and Zhong-Hua Zhang},
keywords = {Curriculum reform, Primary mathematics, Curriculum evaluation, Student mathematics achievement, Cognitive, Affective},
abstract = {This study investigated curriculum influences on student mathematics achievement by following two groups of students from fifth to sixth grade that were taught either the reformed curriculum or the conventional curriculum. Analyses with three-level modeling were conducted to examine learning outcomes of the students who were assessed three times over a period of 18 months. Achievement was measured with regard to computation, routine problem solving, and complex problem solving. Affective aspects included self-reported interest in learning mathematics, classroom participation, views of the nature of mathematics, and views of learning mathematics. The results showed overall improved performance among all the students over the time on computation, routine problem solving, and complex problem solving but not on the affective measures. There were differentiated patterns of performance between the groups. On the initial assessment, the reform group performed better than the non-reform group on calculation, complex problem solving, and indicated higher interest in learning mathematics. The two groups did not differ on the other achievement and affective measures at the first time of assessment. There was no significant difference in growth rate between the groups on the cognitive and affective measures except that the non-reform group progressed at a faster pace on calculation. Therefore, the non-reform group outperformed the reform group on computation at the third (last) assessment. These results are discussed with respect to the possible influence of the curriculum on student learning.}
}
@article{SCOTNEY2020102981,
title = {The form of a ‘half-baked’ creative idea: Empirical explorations into the structure of ill-defined mental representations},
journal = {Acta Psychologica},
volume = {203},
pages = {102981},
year = {2020},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2019.102981},
url = {https://www.sciencedirect.com/science/article/pii/S0001691819303129},
author = {Victoria S. Scotney and Jasmine Schwartz and Nicole Carbert and Adam Saab and Liane Gabora},
keywords = {Analogy, Art, Creative process, Honing, Mental representation, Structure mapping},
abstract = {Creative thought is conventionally believed to involve searching memory and generating multiple independent candidate ideas followed by selection and refinement of the most promising. Honing theory, which grew out of the quantum approach to describing how concepts interact, posits that what appears to be discrete, separate ideas are actually different projections of the same underlying mental representation, which can be described as a superposition state, and which may take different outward forms when reflected upon from different perspectives. As creative thought proceeds, this representation loses potentiality to be viewed from different perspectives and manifest as different outcomes. Honing theory yields different predictions from conventional theories about the mental representation of an idea midway through the creative process. These predictions were pitted against one another in two studies: one closed-ended and one open-ended. In the first study, participants were interrupted midway through solving an analogy problem and wrote down what they were thinking in terms of a solution. In the second, participants were instructed to create a painting that expressed their true essence and describe how they conceived of the painting. For both studies, naïve judges categorized these responses as supportive of either the conventional view or the honing theory view. The results of both studies were significantly more consistent with the predictions of honing theory. Some implications for creative cognition, and cognition in general, are discussed.}
}
@article{CHEN2019381,
title = {How Big Data and High-performance Computing Drive Brain Science},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {381-392},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2019.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301561},
author = {Shanyu Chen and Zhipeng He and Xinyin Han and Xiaoyu He and Ruilin Li and Haidong Zhu and Dan Zhao and Chuangchuang Dai and Yu Zhang and Zhonghua Lu and Xuebin Chi and Beifang Niu},
keywords = {Brain science, Big data, High-performance computing, Brain connectomes, Deep learning},
abstract = {Brain science accelerates the study of intelligence and behavior, contributes fundamental insights into human cognition, and offers prospective treatments for brain disease. Faced with the challenges posed by imaging technologies and deep learning computational models, big data and high-performance computing (HPC) play essential roles in studying brain function, brain diseases, and large-scale brain models or connectomes. We review the driving forces behind big data and HPC methods applied to brain science, including deep learning, powerful data analysis capabilities, and computational performance solutions, each of which can be used to improve diagnostic accuracy and research output. This work reinforces predictions that big data and HPC will continue to improve brain science by making ultrahigh-performance analysis possible, by improving data standardization and sharing, and by providing new neuromorphic insights.}
}
@article{CUMMING2012923,
title = {Better compounds faster: the development and exploitation of a desktop predictive chemistry toolkit},
journal = {Drug Discovery Today},
volume = {17},
number = {17},
pages = {923-927},
year = {2012},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2012.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1359644612000840},
author = {John G. Cumming and Jon Winter and Andrew Poirrette},
abstract = {Today's drug designer has access to vast quantities of data and an impressive array of sophisticated computational methods. At the same time, there is increasing pressure on the pharmaceutical industry to improve its productivity and reduce candidate drug attrition. We set out to develop a highly integrated suite of design and data analysis tools underpinned by the best predictive chemistry methods and models, with the aim of enabling multi-disciplinary compound design teams to make better informed design decisions. In this article we address the challenges of developing a powerful, flexible and user-friendly toolkit, and of maximising its exploitation by the design community. We describe the impact the toolkit has had on drug discovery projects and give our perspective on the future direction of this activity.}
}
@article{TRINH20254926,
title = {Sonochemistry and sonocatalysis: current progress, existing limitations, and future opportunities in green and sustainable chemistry††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d5gc01098e},
journal = {Green Chemistry},
volume = {27},
number = {18},
pages = {4926-4958},
year = {2025},
issn = {1463-9262},
doi = {https://doi.org/10.1039/d5gc01098e},
url = {https://www.sciencedirect.com/science/article/pii/S1463926225003000},
author = {Quang Thang Trinh and Nicholas Golio and Yuran Cheng and Haotian Cha and Kin Un Tai and Lingxi Ouyang and Jun Zhao and Tuan Sang Tran and Tuan-Khoa Nguyen and Jun Zhang and Hongjie An and Zuojun Wei and Francois Jerome and Prince Nana Amaniampong and Nam-Trung Nguyen},
abstract = {Sonocatalysis is a specialised field within sonochemistry that leverages the interaction between ultrasound and solid catalysts to enhance the rate and selectivity of chemical reactions. As a non-traditional catalytic activation method, sonocatalysis can profoundly modify reaction mechanisms and unlock novel activation pathways that are not typically accessible through standard catalysis. This unique approach offers new opportunities for driving reactions under milder conditions while potentially improving selectivity and efficiency. This review highlights the recent progress of sonocatalytic applications in green chemistry and their contribution to the United Nations' Sustainable Development Goals (SDGs), including environmental remediation, sonotherapy, and biomass conversion. In these applications, we explore the underlying sonocatalytic mechanisms and the interaction between solid catalysts and ultrasound, which drive the enhanced reactivity. A key feature of this manuscript is its comprehensive analysis of the primary technical challenges in sonocatalysis, specifically its low energy efficiency and the complexity of reaction control. To address these hurdles, we examine various effective strategies, such as the incorporation of nanostructured catalytic cavitation agents and the design of advanced microfluidic sonoreactors. These innovations improve energy transfer, control bubble dynamics, and enhance catalytic activity under ultrasound. Furthermore, we implement molecular modelling to gain fundamental insights into the mechanisms fundamental to the effectiveness of sonocatalysts. This approach provides a deeper understanding of how nanostructured catalysts interact with ultrasonic fields, guiding the design of next-generation catalytic materials. The integration of nanostructured catalytic cavitation agents, microfluidic reactor technologies, and computational molecular modelling forms a trilateral synergistic platform that unlocks new potential in sonocatalysis. This multidisciplinary framework paves the way for significant advancements in green and sustainable chemistry, offering innovative solutions to global challenges in energy, health, and environmental sustainability.}
}
@article{AJIMATI2025112300,
title = {Adoption of low-code and no-code development: A systematic literature review and future research agenda},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112300},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112300},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003443},
author = {Matthew Oladeji Ajimati and Noel Carroll and Mary Maher},
keywords = {Citizen development, Low-code, No-code, Digital transformation, Systematic literature review},
abstract = {Context
Low-code/no-code (LCNC) is an emerging technology trend that extends software development beyond professionalsoftware engineers, making it accessible to individuals throughout organizations and society.
Objective
We aim to provide a systematic review of the current research on the adoption of LCNC technologies within citizen development (CD) practices for digital transformation (DT), and to propose a research agenda for this field.
Method
This review is primarily conducted using a multi-phase systematic literature review of publications from the past five years, i.e., between 2017 and 2023.
Results
We identified 40 primary studies that describes the application of LCNC development and CD practices, the theoretical lenses/frameworks used, and the associated benefits and challenges.
Conclusion
In this study, we present three key contributions. First, we provide a comprehensive review of the benefits, challenges, theoretical perspectives, and methods used to explore LCNC and CD adoption. Second, we introduce a framework designed to guide managers in effectively adopting LCNC and CD practices. Finally, our systematic review uncovers gaps in existing research and identifies opportunities for further exploration, which paves the way for a future research agenda.}
}
@article{CHRISLEY2008119,
title = {Philosophical foundations of artificial consciousness},
journal = {Artificial Intelligence in Medicine},
volume = {44},
number = {2},
pages = {119-137},
year = {2008},
note = {Artificial Consciousness},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2008.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0933365708001000},
author = {Ron Chrisley},
keywords = {Artificial consciousness, Machine consciousness, Prosthetic artificial intelligence, Synthetic phenomenology, Interactive empiricism, Heterophenomenology},
abstract = {Summary
Objective
Consciousness is often thought to be that aspect of mind that is least amenable to being understood or replicated by artificial intelligence (AI). The first-personal, subjective, what-it-is-like-to-be-something nature of consciousness is thought to be untouchable by the computations, algorithms, processing and functions of AI method. Since AI is the most promising avenue toward artificial consciousness (AC), the conclusion many draw is that AC is even more doomed than AI supposedly is. The objective of this paper is to evaluate the soundness of this inference.
Methods
The results are achieved by means of conceptual analysis and argumentation.
Results and conclusions
It is shown that pessimism concerning the theoretical possibility of artificial consciousness is unfounded, based as it is on misunderstandings of AI, and a lack of awareness of the possible roles AI might play in accounting for or reproducing consciousness. This is done by making some foundational distinctions relevant to AC, and using them to show that some common reasons given for AC scepticism do not touch some of the (usually neglected) possibilities for AC, such as prosthetic, discriminative, practically necessary, and lagom (necessary-but-not-sufficient) AC. Along the way three strands of the author's work in AC – interactive empiricism, synthetic phenomenology, and ontologically conservative heterophenomenology – are used to illustrate and motivate the distinctions and the defences of AC they make possible.}
}