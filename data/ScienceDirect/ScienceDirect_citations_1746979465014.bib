@article{YAN2024120835,
title = {CPS-3WS: A critical pattern supported three-way sampling method for classifying class-overlapped imbalanced data},
journal = {Information Sciences},
volume = {676},
pages = {120835},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120835},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524007497},
author = {Yuanting Yan and Zhong Zheng and Yiwen Zhang and Yanping Zhang and Yiyu Yao},
keywords = {Three-way sampling, Class-imbalance problem, Critical pattern, Class overlap},
abstract = {Class-imbalance problem widely exists in real applications ranging from medial diagnosis to economic fraud detection, etc. As one of the mainstream techniques in dealing with imbalanced data, SMOTE (Synthetic Minority Over-sampling TEchnique) and its extensions mainly rebalance the datasets via generation of observations in specific regions with various adapted strategies. Many of them do not consider the cost of role assignment of samples, and the intractable data complexity (overlap, small disjuncts, etc.) poses additional challenges to them. This paper proposes a critical pattern supported three-way sampling method (CPS-3WS) for classifying class-overlapped imbalanced data, introducing the philosophy of thinking in threes to effective classification in imbalanced learning. Specifically, CPS-3WS uses a three-way sample partition strategy with the Bayes posterior probability by dividing majority and minority classes into three disjoint subsets: risky, critical and safe patterns. CPS-3WS conducts a three-way hybrid sampling through (i) evaluating the risky majority pattern to be eliminated and (ii) selecting critical minority pattern to synthesize new samples under local information constraint. Extensive experiments on 42 UCI benchmark datasets demonstrate the superiority of the proposed CPS-3WS compared with 11 data-level methods. The source code of CPS-3WS is available at https://github.com/ytyancp/CPS-3WS.}
}
@incollection{BLACKBURN19941,
title = {1 - Structures, Languages and Translations: the Structural Approach to Feature Logic},
editor = {C.J. Rupp and M.A. Rosner and R.L. Johnson},
booktitle = {Constraints, Language and Computation},
publisher = {Academic Press},
address = {San Diego},
pages = {1-27},
year = {1994},
series = {Cognitive Science},
isbn = {978-0-08-050296-0},
doi = {https://doi.org/10.1016/B978-0-08-050296-0.50008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080502960500085},
author = {Patrick Blackburn},
abstract = {Publisher Summary
This chapter reviews methodological issues involved in the structural approach used in feature logic. A direct consequence of the systematic approach to a variety of feature logics is that it clarifies the relationships between them. This is most explicit in the presentation of translations between various existing and putative feature logics which draws heavily on the correspondence theory that relates modal and classical languages. The chapter describes a general approach to the subject called the structural approach, and explains that thinking in structural terms is a useful way of thinking about unification formalisms and their interrelationships. At present, in contemporary unification-based linguistic frameworks, linguistic data is modelled by certain kinds of decorated labelled (directed) graphs. Perhaps the most prevalent way of thinking about unification-based grammar formalisms is that they are languages for expressing constraints on feature structures. Two basic ideas drive modal logic: one syntactic, and the other semantic. Modal languages are interpreted on Kripke models, which are set theoretic entities providing the following information. Propositional Dynamic Logic (PDL) is an extension of modal logic; PDL and some of its extensions are natural constraint languages for dealing with feature structures. Subsequently, Attribute Value Matrices (AVMs) are one of the most widely used methods of describing feature structures. A general setting for feature logic is the space of relational structures of model theory, together with the various languages for describing these structures, and the satisfiability preserving translations that exist among these languages. The basic ideas are very simple: feature structures are certain sorts of relational structures, and while there is a vast range of languages for talking about these structures, these languages are interrelated by satisfiability preserving translations.}
}
@article{VAIS2013718,
title = {Laplacians on flat line bundles over 3-manifolds},
journal = {Computers & Graphics},
volume = {37},
number = {6},
pages = {718-729},
year = {2013},
note = {Shape Modeling International (SMI) Conference 2013},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2013.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0097849313000873},
author = {Alexander Vais and Daniel Brandes and Hannes Thielhelm and Franz-Erich Wolter},
keywords = {Spectral geometry, Vector bundles, Computational topology, Laplace operator, Knots, Seifert surfaces, FEM},
abstract = {The well-known Laplace–Beltrami operator, established as a basic tool in shape processing, builds on a long history of mathematical investigations that have induced several numerical models for computational purposes. However, the Laplace–Beltrami operator is only one special case of many possible generalizations that have been researched theoretically. Thereby it is natural to supplement some of those extensions with concrete computational frameworks. In this work we study a particularly interesting class of extended Laplacians acting on sections of flat line bundles over compact Riemannian manifolds. Numerical computations for these operators have recently been accomplished on two-dimensional surfaces. Using the notions of line bundles and differential forms, we follow up on that work giving a more general theoretical and computational account of the underlying ideas and their relationships. Building on this we describe how the modified Laplacians and the corresponding computations can be extended to three-dimensional Riemannian manifolds, yielding a method that is able to deal robustly with volumetric objects of intricate shape and topology. We investigate and visualize the two-dimensional zero sets of the first eigenfunctions of the modified Laplacians, yielding an approach for constructing characteristic well-behaving, particularly robust homology generators invariant under isometric deformation. The latter include nicely embedded Seifert surfaces and their non-orientable counterparts for knot complements.}
}
@article{BOLER201875,
title = {The affective politics of the “post-truth” era: Feeling rules and networked subjectivity},
journal = {Emotion, Space and Society},
volume = {27},
pages = {75-85},
year = {2018},
issn = {1755-4586},
doi = {https://doi.org/10.1016/j.emospa.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1755458617301585},
author = {Megan Boler and Elizabeth Davis},
keywords = {Affect, Emotion, Post-truth, Feeling rules, Truthiness, Digital media, Algorithmic governance, Computational propaganda},
abstract = {This essay maps interdisciplinary lines of inquiry to assess current research on affect and emotion in relation to digital and social media, in the context of the fractured news media landscape and increasingly visible emotionality in political life. The essay sketches the context of polarized emotionality and the crisis of truth characterizing current U.S. politics, centrally engaging Arlie Hochschild's concept of “feeling rules”. We explore the limitations of “affect theory” for researching mediatized politics, contending that the stark differentiation of “affect” from “emotion” reifies the rational, autonomous, liberal conception of the subject, and is of limited value for political communications research. Instead, we emphasize the relational nature of affect and emotion, and the value of feminist politics of emotion research. Our analysis evaluates the limitations of contemporary scholarship on affect, social media, and politics in the context of the grave challenges posed by algorithmic governance and computational propaganda. We conclude by suggesting the concept of “networked subjectivity” for understanding mediatized politics, and the importance of the “affective feedback loop” within the context of the social media “culture of likes.”}
}
@article{STAMMITTI2013e58,
title = {Spreadsheets for assisting Transport Phenomena Laboratory experiences},
journal = {Education for Chemical Engineers},
volume = {8},
number = {2},
pages = {e58-e71},
year = {2013},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2013.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1749772813000067},
author = {Aurelio Stammitti},
keywords = {Educational spreadsheets, Transport Phenomena Laboratory, Laboratory experience quality, Data processing task, Hands-on learning, Student analytical thinking},
abstract = {Academic laboratories have been traditionally used for complementing and reinforcing in a practical way the theoretical instruction received in classroom lectures. However, data processing and model evaluation tasks are time consuming and do not add much value to the student's learning experience as they reduce available time for result analysis, critical thinking and report writing skills development. Therefore, this project addressed this issue by selecting three experiences of the Transport Phenomena Laboratory, namely: metallic bar temperature profiles, transient heat conduction and fixed and fluidised bed behaviour, and developed a spreadsheet for each one of them. These spreadsheets, without demanding programming skills, easily process experimental data sets, evaluate complex analytical and numerical models and correlations, not formerly considered and, convey results in tables and plots. Chemical engineering students that tested the spreadsheets were surveyed and expressed the added value of the sheets, being user-friendly, helped them to fulfil lab objectives by reducing their workload and, allowed them to complete deeper analyses that instructors could not request before, as they were able to quickly evaluate, compare and validate different model assumptions and correlations. Students also provided valuable suggestions for improving the spreadsheet experience. Through these sheets, students’ lab learning experience was updated.}
}
@article{KAMATH2020100944,
title = {Making grammars for material and tectonic complexity: An example of a thin-tile vault},
journal = {Design Studies},
volume = {69},
pages = {100944},
year = {2020},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2020.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X20300326},
author = {Ayodh Vasant Kamath},
keywords = {affordance, architectural design, creativity, reflective practice, making grammars},
abstract = {Shape grammars are a framework to view design as non-deterministic, creative, visual computation, and making as the deterministic execution of a design in the material world. Making grammars conceive of making as creative, multi-sensory, material computation. However, examples in the literature on making grammar are insufficiently complex to demonstrate the creativity of non-visual senses in making. This paper develops a making grammar for thin-tile vault construction as a sensory ethnography to ‘show making’ to designers as being a creative practice involving visual and non-visual senses. To do so, the role of drawing in shape grammar and making grammar is differentiated, and environmental psychology is used to develop a framework for the use of drawing to depict multi-sensory processes in making grammar.}
}
@article{NG2023116585,
title = {Development of a system model to predict flows and performance of regional waste management planning: A case study of England},
journal = {Journal of Environmental Management},
volume = {325},
pages = {116585},
year = {2023},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.116585},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722021582},
author = {Kok Siew Ng and Aidong Yang},
keywords = {Circular economy, Recycling, Stock-and-flow, Sustainable waste management, Resource recovery, Systems thinking},
abstract = {Significant loss of valuable resources and increasing burdens on landfills are often associated with a lack of proper planning in waste management and resource recovery strategy. A sustainable waste management model is thus urgently needed to improve resource efficiency and divert more waste from landfills. This paper proposes a comprehensive system model using stock-and-flow diagram to examine the current waste management performance and project the future waste generation, treatment and disposal scenarios, using England as a case study. The model comprises three integrated modules to represent household waste generation and collection; waste treatment and disposal; and energy recovery. A detailed mass and energy balance has been established and waste management performance has been evaluated using six upstream and downstream indicators. The base case scenario that assumes constant waste composition shows that waste to landfills can be reduced to less than 10% of the total amount, by 2035. However, it entails greater diversion of waste to energy-from-waste facilities, which is not sustainable and would incur higher capital investment and gate fees. Alternative case scenarios that promote recycling instead of energy recovery result in lower capital investment and gate fees. Complete elimination of the food and organic fraction from the residual waste stream will help meet the 65% recycling target by 2035. In light of the need for achieving a more circular economy in England, enhancing material recovery through reuse and recycling, reducing reliance on energy-from-waste and deploying more advanced waste valorisation technologies should be considered in future policy and planning for waste management.}
}
@article{LILI20171611,
title = {An Inverse Optimization Model for Human Resource Allocation Problem Considering Competency Disadvantage Structure},
journal = {Procedia Computer Science},
volume = {112},
pages = {1611-1622},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.248},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316575},
author = {Zhang Lili},
keywords = {inverse optimization, linear programming, human resource allocation, competency, evaluation according to disadvantage structure},
abstract = {Most of serious and major accidents that happened during the production procedure of process industry are caused by improper equipment operations, which is further owing to inappropriate human resources allocation and ignorance of individual competencies differences. In order to take both of competency disadvantage and adjustment requirement into consideration, we use an inverse optimization method to solve a human resource allocation problem, and furthermore, adjust equipment operating parameters to make the per-defined settings optimized, such as the total number of jobs, security-related parameters and so on.In the solving process, firstly a standard competence hierarchy system is conducted; secondly we propose an assessment method according to disadvantage structure; thirdly we use inverse optimization method to solve the problem and optimize the predefined allocation plan. Lastly, we give an example to prove its feasibility and effectiveness. In this paper a novel formulation of human resource allocation problem is proposed, in which some of main individual characteristics are considered and described mathematically, including psychology, behaviour and characteristics diged from them such as weakness. The other contribution of this paper is using inverse optimization to adjust parameters based on the given ideal allocation plan. Both of these propositions have a positive significance on promoting development and security construction for process industries.This research incorporates the academic thinking of inverse optimization, it not only puts psychology and behavior into optimization model, but also data mines weakness characteristics under the psychology and behavior data, and find a new way to introducing the weakness characteristics into decision making model. It provides a new thought for the following decision making problem, that is the ideal decision plan is known, and optimization parameters are changeable. It promotes the combining of psychology, behavior and operations research, it is good for process industries to develop in a safety and efficiency way.}
}
@incollection{NG202451,
title = {Chapter 4 - System modeling and mapping},
editor = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
booktitle = {A New Systems Thinking Approach to Sustainable Resource Management},
publisher = {Elsevier},
pages = {51-140},
year = {2024},
isbn = {978-0-323-99869-7},
doi = {https://doi.org/10.1016/B978-0-323-99869-7.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998697000036},
author = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
keywords = {Causal loop diagram, GIS, MFA, Resource availability, Sensitivity and uncertainty analyses, System dynamics},
abstract = {This chapter presents a series of representative techniques for system modeling and mapping, including resource availability analysis, material flow analysis, system dynamics, and sensitivity and uncertainty analyses. These are among the well-established computational modeling methods adopted widely in engineering, environmental, and social science disciplines. They are particularly useful in the context of resource and waste management, providing clearer visualization of the system and enabling reliable prediction of system behavior. Resource availability assessment provides a bird's eye view of the interdependencies among resources and the overall feasibility for a system to operate with the available resources. Material flow analysis facilitates a robust mapping of resource consumption, production, and losses, offering insights for identifying opportunities to improve resource efficiency and minimize waste. System dynamics enables us to understand the complex behavior of a system through exploring the interaction of different factors, serving as a forecasting tool for future scenarios. Sensitivity analysis determines the system's responsiveness to different input values, identifying the most influential inputs in the system's response. Uncertainty analysis quantifies variations and uncertainties in potential system responses due to variations in inputs.}
}
@incollection{WHANGBO2005765,
title = {Chapter 26 - Concepts of perturbation, orbital interaction, orbital mixing and orbital occupation},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {765-784},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50069-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044451719750069X},
author = {Myung-Hwan Whangbo},
abstract = {Publisher Summary
This chapter describes the concepts of perturbation, orbital interaction, orbital mixing, and orbital occupation work at all levels of electronic structure. These qualitative concepts provide a conceptual framework in which to rationalize experimental/theoretical observations and to generate qualitative predictions. An important role of an electronic structure theory is to provide quantitative predictions. In this role theoretical predictions require developments of efficient programs for theoretical computations. Another important role of an electronic structure theory is to provide a conceptual framework in which to think and organize. In this role, the theoretical predictions need not be quantitative but should provide a bias toward correct thinking about further experimental and theoretical studies. When combined with the ideas of symmetry and overlap, the concepts of perturbation, orbital interaction, orbital mixing, and orbital occupation have been indispensable not only in understanding structure – property relationships in various chemical compounds but also in interpreting results of electronic structure calculations. These qualitative concepts work at all levels of electronic structure descriptions from one-electron theory neglecting self-consistent-field adjustments of orbitals to theories including electron correlation and to those including relativistic effects.}
}
@article{MCLOUGHLIN2022173,
title = {Midfrontal Theta Activity in Psychiatric Illness: An Index of Cognitive Vulnerabilities Across Disorders},
journal = {Biological Psychiatry},
volume = {91},
number = {2},
pages = {173-182},
year = {2022},
note = {Biomarkers of Psychosis},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2021.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0006322321015651},
author = {Gráinne McLoughlin and Máté Gyurkovics and Jason Palmer and Scott Makeig},
keywords = {Biomarker, Cognitive control, EEG, ERP, Oscillations, Theta},
abstract = {There is an urgent need to identify the mechanisms that contribute to atypical thinking and behavior associated with psychiatric illness. Behavioral and brain measures of cognitive control are associated with a variety of psychiatric disorders and conditions as well as daily life functioning. Recognition of the importance of cognitive control in human behavior has led to intensive research into behavioral and neurobiological correlates. Oscillations in the theta band (4–8 Hz) over medial frontal recording sites are becoming increasingly established as a direct neural index of certain aspects of cognitive control. In this review, we point toward evidence that theta acts to coordinate multiple neural processes in disparate brain regions during task processing to optimize behavior. Theta-related signals in human electroencephalography include the N2, the error-related negativity, and measures of theta power in the (time-)frequency domain. We investigate how these theta signals are affected in a wide range of psychiatric conditions with known deficiencies in cognitive control: anxiety, obsessive-compulsive disorder, attention-deficit/hyperactivity disorder, and substance abuse. Theta-related control signals and their temporal consistency were found to differ in most patient groups compared with healthy control subjects, suggesting fundamental deficits in reactive and proactive control. Notably, however, clinical studies directly investigating the role of theta in the coordination of goal-directed processes across different brain regions are uncommon and are encouraged in future research. A finer-grained analysis of flexible, subsecond-scale functional networks in psychiatric disorders could contribute to a dimensional understanding of psychopathology.}
}
@article{ZEITHAMMER2024,
title = {Strange Case of Dr. Bidder and Mr. Entrant: Consumer Preference Inconsistencies in Costly Price Offers},
journal = {International Journal of Research in Marketing},
year = {2024},
issn = {0167-8116},
doi = {https://doi.org/10.1016/j.ijresmar.2024.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S016781162400079X},
author = {Robert Zeithammer and Lucas Stich and Martin Spann and Gerald Häubl},
keywords = {Pricing, Auctions, Entry costs, Behavioral economics, Experiments},
abstract = {Consumers make price offers to sellers in a variety of domains, such as when buying cars or houses or when bidding in auctions for airline upgrades, art, or collectibles. Submitting an offer typically entails administrative, waiting, and opportunity costs. Making such costly price offers involves two intertwined decisions—in addition to determining how much to offer, consumers must also decide whether to make an offer in the first place. We examine the impact of offer-submission costs on consumer behavior using a series of incentive-compatible experiments. Our findings reveal a preference inconsistency, whereby the preferences implied by one of the decisions do not align with the preferences implied by the other. In particular, potential buyers enter more often than their offer amounts would predict based on standard economic models. This preference inconsistency is robust to two interventions designed to help consumers make offer-amount and entry decisions—(1)the provision of interactive-feedback decision aids and (2)the sequencing of the two sub-decisions in the normative order. Neither of these interventions resolves the inconsistency. Instead, the patterns of results suggest that consumers approach the offer-amount and entry decisions as if they were unrelated. We discuss the implications of our findings for the design of offer-submission interfaces, as well as for econometric attempts to infer consumer preferences from offer and bidding data.}
}
@incollection{FARMER2008228,
title = {Chapter 11 - Fragment-Based Drug Discovery},
editor = {Camille Georges Wermuth},
booktitle = {The Practice of Medicinal Chemistry (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {New York},
pages = {228-243},
year = {2008},
isbn = {978-0-12-374194-3},
doi = {https://doi.org/10.1016/B978-0-12-374194-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123741943000111},
author = {Bennett T. Farmer and Allen B. Reitz},
abstract = {Publisher Summary
Although target proteins are flexible and can adopt one or more of a manifold of induced conformations, binding sites on proteins have evolved to recognize a limited number of endogenous modulators and substrates and to exclude others. This chapter reviews fragment-based drug discovery (FBDD) on the historical and operational level and explains how it can be applied on a case-by-case basis. FBBD determines which molecular substructures or fragments interact with targets of interest and how they bind, and then uses that information to obtain drugs for therapy. It represents a paradigm shift in thinking of how to approach the lead generation process in drug discovery, and is an attempt to get more information rapidly while doing the same amount of work overall. The study draws comparison between the FBDD and HTS/HTL approaches. In the FBDD approach, the medicinal chemist plays the role of a combined synthetic and structural chemist. The emphasis on informatics is greatly reduced because there is less data overall and most of it, such as from NMR or X-ray crystal structures, is visually analyzed, typically being complemented only by functional assay data on just the target itself. Several different computational methods have been developed to prescreen fragment libraries as a way to select members for further study and consideration.}
}
@article{BAMOROVAT202321,
title = {Poor adherence is a major barrier to the proper treatment of cutaneous leishmaniasis: A case-control field assessment in Iran},
journal = {International Journal for Parasitology: Drugs and Drug Resistance},
volume = {21},
pages = {21-27},
year = {2023},
issn = {2211-3207},
doi = {https://doi.org/10.1016/j.ijpddr.2022.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2211320722000331},
author = {Mehdi Bamorovat and Iraj Sharifi and Setareh {Agha Kuchak Afshari} and Ali Karamoozian and Amirhossein Tahmouresi and Amireh Heshmatkhah and Ehsan Salarkia and Ahmad Khosravi and Maryam {Hakimi Parizi} and Maryam Barghi},
keywords = {Poor adherence, Cutaneous leishmaniasis, Major barrier, Treatment, Iran},
abstract = {Leishmaniasis is an overlooked, poverty-stricken, and complex disease with growing social and public health problems. In general, leishmaniasis is a curable disease; however, there is an expansion of unresponsive cases to treatment in cutaneous leishmaniasis (CL). One of the effective and ignored determinants in the treatment outcome of CL is poor treatment adherence (PTA). PTA is an overlooked and widespread phenomenon to proper Leishmania treatment. This study aimed to explore the effect of poor adherence in unresponsiveness to treatment in patients with anthroponotic CL (ACL) by comparing conventional statistical modalities and machine learning analyses in Iran. Overall, 190 cases consisting of 50 unresponsive patients (case group), and 140 responsive patients (control group) with ACL were randomly selected. The data collecting form that included 25 queries (Q) was recorded for each case and analyzed by R software and genetic algorithm (GA) approaches. Complex treatment regimens (Q11), cultural and lay views about the disease and therapy (Q8), life stress, hopelessness and negative feelings (Q22), adverse effects of treatment (Q13), and long duration of the lesion (Q12) were the most prevalent significant variables that inhibited effective treatment adherence by the two methods, in decreasing order of significance. In the inherent algorithm approach, similar to the statistical approach, the most significant feature was complex treatment regimens (Q11). Providing essential knowledge about ACL and treatment of patients with chronic diseases and patients with misconceptions about chemical drugs are important issues directly related to the disease's unresponsiveness. Furthermore, early detection of patients to prevent the long duration of the disease and the process of treatment, efforts to minimize side effects of treatment, induction of positive thinking, and giving hope to patients with stress and anxiety by medical staff, and family can help patients adhere to the treatment.}
}
@article{MARSELLA200970,
title = {EMA: A process model of appraisal dynamics},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {70-90},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000314},
author = {Stacy C. Marsella and Jonathan Gratch},
keywords = {Emotion, Cognitive models, Appraisal theory, Coping},
abstract = {A computational model of emotion must explain both the rapid dynamics of some emotional reactions as well as the slower responses that follow deliberation. This is often addressed by positing multiple levels of appraisal processes such as fast pattern directed vs. slower deliberative appraisals. In our view, this confuses appraisal with inference. Rather, we argue for a single and automatic appraisal process that operates over a person’s interpretation of their relationship to the environment. Dynamics arise from perceptual and inferential processes operating on this interpretation (including deliberative and reactive processes). This article discusses current developments in a computational model of emotion processes and illustrates how a single-level model of appraisal obviates a multi-level approach within the context of modeling a naturalistic emotional situation.}
}
@article{PAZZANI1991401,
title = {A computational theory of learning causal relationships},
journal = {Cognitive Science},
volume = {15},
number = {3},
pages = {401-424},
year = {1991},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(91)80003-N},
url = {https://www.sciencedirect.com/science/article/pii/036402139180003N},
author = {Michael Pazzani},
abstract = {I present a cognitive model of the human ability to acquire causal relationships. I report on experimental evidence demonstrating that human learners acquire accurate causal relationships more rapidly when training examples are consistent with a general theory of causality. This article describes a learning process that uses a general theory of causality as background knowledge. The learning process, which I call theory-driven learning (TDL), hypothesizes causal relationships consistent both with observed data and the general theory of causality. TDL accounts for data on both the rate at which human learners acquire causal relationships, and the types of causal relationships they acquire. Experiments with TDL demonstrate the advantage of TDL for acquiring causal relationships over similarity-based approaches to learning: Fewer examples are required to learn an accurate relationship.}
}
@article{CHERNYSHOV2004535,
title = {A System Identification Approach to Assessing Airline Pilot Skills},
journal = {IFAC Proceedings Volumes},
volume = {37},
number = {6},
pages = {535-540},
year = {2004},
note = {16th IFAC Symposium on Automatic Control in Aerospace 2004, Saint-Petersburg, Russia, 14-18 June 2004},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)32230-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017322309},
author = {Kirill Chernyshov},
keywords = {Aircraft control, Skill, Human factors, Performance monitoring, Identification, Stochastic systems, Coupling coefficients, Cross correlation functions, Estimation algorithms, Sampled data},
abstract = {The paper presents a new approach to eliciting information on current professional airline pilot skills and pilotage experience as a decision making person (DMP). Such an approach is regarded to the heuristic regularities of the DMP thinking process. In turn, the regularities are revealed on basis of recording the motions of the pilot eyes over the information field of the flight deck and processing the experimental data obtained. For the data mining, a probability theoretical approach is involved. Such an approach is based on applying the notion of consistency of measures of stochastic dependence of random variables; and a method of deriving almost sure converging estimate of such a measure by sample data is proposed.}
}
@incollection{KEENAN2015394,
title = {Psychology of Inferences},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {394-399},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868570111},
author = {Janice M. Keenan},
keywords = {Backward inferences, Coherence, Cortical networks, Explicit inferences, Forward inferences, Implicit inferences, Individual differences, Inferences, Inferencing, computational processes of, Inferencing, methodological issues, Inferencing, neural basis of, Knowledge, Language, Right hemisphere language},
abstract = {The goal of comprehension is to understand what the speaker intended the text to mean. Inferences are driven by a desire to make the interpretation both more coherent and more elaborate than the text itself. The goal of research on inferencing is to specify how the computational processes involved in making inferences vary with the comprehender's knowledge, the conditions that promote inferencing, the various types of inferences and methodological problems involved in assessing them, and, most recently, the neural bases of inferencing.}
}
@article{NUMRICH201169,
title = {Self-similarity of parallel machines},
journal = {Parallel Computing},
volume = {37},
number = {2},
pages = {69-84},
year = {2011},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819110001444},
author = {Robert W. Numrich and Michael A. Heroux},
keywords = {Parallel algorithms, Benchmark analysis, Computational intensity, Computational force, Dimensional analysis, Equivalence class, Self-similarity, Scaling, Mixing coefficient},
abstract = {Self-similarity is a property of physical systems that describes how to scale parameters such that dissimilar systems appear to be similar. Computer systems are self-similar if certain ratios of computational forces, also known as computational intensities, are equal. Two machines with different computational power, different network bandwidth and different inter-processor latency behave the same way if they have the same ratios of forces. For the parallel conjugate gradient algorithm studied in this paper, two machines are self-similar if and only if the ratio of one force describing latency effects to another force describing bandwidth effects is the same for both machines. For the two machines studied in this paper, this ratio, which we call the mixing coefficient, is invariant as problem size and processor count change. The two machines have the same mixing coefficient and belong to the same equivalence class.}
}
@article{RAVI2023151,
title = {Evolving trends in student assessment in chemical engineering education},
journal = {Education for Chemical Engineers},
volume = {45},
pages = {151-160},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000453},
author = {Manoj Ravi},
keywords = {Assessment, Authentic assessment, Digitalisation},
abstract = {Alongside innovation in teaching practice, student assessment in chemical engineering has seen significant changes in the recent past. This article undertakes a systematic review of the recent advances that have been reported in assessment practice in chemical engineering education. The main trends that emerge are: a shift towards authentic assessment methods, an increase in emphasis on peer-assessment and other approaches for group-based assignments, and a greater use of digital tools for the delivery of authentic assessments and improvement of marking and feedback practice. The analysis also examines the diversity of assessment methods used across the different chemical engineering subjects and how these map against assessment frameworks reported in the wider pedagogical literature. The emerging strand of research on synoptic and interdisciplinary assessment is used to develop an assessment framework for producing chemical engineering graduates who are also socially responsible and competent global citizens.}
}
@article{SHANAHAN2005157,
title = {Applying global workspace theory to the frame problem},
journal = {Cognition},
volume = {98},
number = {2},
pages = {157-176},
year = {2005},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2004.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010027704002288},
author = {Murray Shanahan and Bernard Baars},
keywords = {Frame problem, Relevance problem, Global workspace theory, Consciousness, Analogical reasoning},
abstract = {The subject of this article is the frame problem, as conceived by certain cognitive scientists and philosophers of mind, notably Fodor for whom it stands as a fundamental obstacle to progress in cognitive science. The challenge is to explain the capacity of so-called informationally unencapsulated cognitive processes to deal effectively with information from potentially any cognitive domain without the burden of having to explicitly sift the relevant from the irrelevant. The paper advocates a global workspace architecture, with its ability to manage massively parallel resources in the context of a serial thread of computation, as an answer to this challenge. Analogical reasoning is given particular attention, since it exemplifies informational unencapsulation in its most extreme form. Because global workspace theory also purports to account for the distinction between conscious and unconscious information processing, the paper advances the tentative conclusion that consciousness may go hand-in-hand with a solution to the frame problem in the biological brain.}
}
@incollection{HADAP2023319,
title = {Chapter 16 - Theories methods and the parameters of quantitative structure–activity relationships and artificial neural network},
editor = {Dakeshwar Kumar Verma and Chandrabhan Verma and Jeenat Aslam},
booktitle = {Computational Modelling and Simulations for Designing of Corrosion Inhibitors},
publisher = {Elsevier},
pages = {319-335},
year = {2023},
isbn = {978-0-323-95161-6},
doi = {https://doi.org/10.1016/B978-0-323-95161-6.00019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951616000199},
author = {Arti Hadap and Ashutosh Pandey and Bhawana Jain and Reena Rawat},
keywords = {Corrosion, QSAR (quantitative structure and activity relationship), ANN (artificial neural network), inhibitor adsorption, metal surface},
abstract = {Quantitative Structure–Activity Relationships (QSAR) is a computational model used to describe and anticipate the interaction and surface interactions of substances. In addition, Artificial Neural Network (ANN) has been used for the development of linear and sigmoidal functionals, aiming to predict low-carbon steel, copper, and aluminum corrosion rates corresponding to environmental parameters. Thus this chapter explains the theory behind the QSAR/ANN and demonstrates its effectiveness related to corrosion. QSAR aims to draw an attention to the link between the effectiveness of prevention (any function) and the structural features (adjectives). It involves finding one or more items that, by mathematical equation, link these definitions to their blocking function. ANN (artificial neural network) shows excellent performance in the prediction (output) for various complex characteristic data (input). The obtained results give deep insight into the corrosion systems by analyzing the point of surface corrosion attack, the more stable site of the inhibitor adsorption, and the binding power of the adsorbed coating.}
}
@incollection{PROFILLIDIS2019383,
title = {Chapter 9 - Fuzzy Methods},
editor = {V.A. Profillidis and G.N. Botzoris},
booktitle = {Modeling of Transport Demand},
publisher = {Elsevier},
pages = {383-417},
year = {2019},
isbn = {978-0-12-811513-8},
doi = {https://doi.org/10.1016/B978-0-12-811513-8.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128115138000091},
author = {V.A. Profillidis and G.N. Botzoris},
keywords = {4-step model, Accidents, Airports, Crisp, Fuzzification, Fuzzy logic, Fuzzy model, Fuzzy regression, Gaussian, Linear programming, Membership degree, Membership function, Objective function, Railways, Traffic, Transport economics, Trapezoidal, Triangular},
abstract = {This chapter deals with applications of fuzzy methods, which give the ability to study quantitatively problems characterized by ambiguity, imprecision, uncertainty, linguistic variables, and missing or few or no data. The fuzzy method introduces another way of thinking: a statement, instead of being true or false, may be partially true or false. Thus, instead of taking into account the typically used fixed numerical values (such as, e.g., 2.34), the fuzzy method employs a set of plausible values (e.g., around the value 2.34) within a specific domain. Although this approach may look similar to the error of statistical methods, the fuzzy method can tackle situations (such as missing or vague data), for which classic methods are inefficient. The principles of fuzzy numbers, fuzzy sets, and fuzzy logic are presented. The case of symmetric triangular fuzzy numbers is analyzed in detail. Next, linear regression analysis with the use of fuzzy numbers is explained. A detailed application of fuzzy linear regression for a transport demand problem is surveyed analytically. The chapter includes many applications of fuzzy linear regression for the forecast of a variety of transport demand problems: air transport, rail transport, road transport, transport at urban level, and transport economics. Applications of the fuzzy method to other transport problems are explained: route choice, road safety, accident analysis, logistics and routing of freight vehicles, and the optimization of capacity of airports.}
}
@article{LAMB2023101031,
title = {Flexibility across and flexibility within: The domain of integer addition and subtraction},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101031},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101031},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000019},
author = {Lisa Lamb and Jessica Bishop and Ian Whitacre and Randolph Philipp},
keywords = {Number concepts and operations, Cognition, Flexibility, Adaptive expertise, Strategy variability, Integers},
abstract = {To better understand the role that flexibility plays in students’ success on integer addition and subtraction problems, we examined students’ flexibility when solving open number sentences. We define flexibility as the degree to which a learner uses more than one strategy to solve a single task when prompted, as well as the degree to which a learner changes strategies when solving a range of tasks to accommodate task differences. We introduce the categorizations of flexibility within and flexibility across to distinguish these two ways of operationalizing flexibility. We examined flexibility and performance within and among three groups of students — 2nd and 4th graders who had negative numbers in their numerical domains, 7th graders, and college-track 11th graders. Profiles of five students are shared to provide insight in relation to the quantitative findings.}
}
@article{CHENG20115100,
title = {Equilibrium Conditions In Service Supply Chain},
journal = {Procedia Engineering},
volume = {15},
pages = {5100-5104},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.946},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811024477},
author = {Fei Cheng and Shanlin Yang and Xijun Ma},
keywords = {service supply chain, service volume, equilibrium},
abstract = {Service supply chain features human players as service vendor, service integrator, customer and service resource. It tends to be digitally connected, such as consulting, e-business and integrated enterprises. Our study uses a formal model and simulations to develop the effect of a service supply chain on equilibrium computation. Two insights arise on how a network can obtain equilibrium computation: forming the network structure of service supply chain; exploring entities behavior and equilibrium conditions. These results highlight the importance for service supply chain of adapting its network structure to equilibrium and application.}
}
@article{HUNTER198763,
title = {What is fundamental in an information age? A focus on curriculum},
journal = {Education and Computing},
volume = {3},
number = {1},
pages = {63-73},
year = {1987},
note = {Special Issue on Educational Computer Policy Alternatives in the United States},
issn = {0167-9287},
doi = {https://doi.org/10.1016/S0167-9287(87)80513-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167928787805137},
author = {Beverly Hunter},
keywords = {Curriculum change, Knowledge-creative Learning, Problem Solving Tools, Information Handling, Algorithmic Thinking, Critical Thinking Skills, Higher-order Thinking Skills, Information Age, Computer Literacy, Problem Solving, Decision Making, Inquiry, Reasoning, Valuing},
abstract = {Systematic reassessment of both overt and covert curriculum content and methods is needed in response to broader social change involved in the information revolution. The educational system in the United States is moving (unevenly) through three overlapping stages of curriculum change: (1) focus on technology, (2) integration of technology into curriculum, and (3) focus on fundamental change in curriculum. Indicators of the current state of change in elementary and secondary schools include state education agency mandates, teacher-oriented publications, past and present surveys of computer use in schools, teacher attitudes, private industry initiatives, and recommendations of national study groups and commissions.}
}
@article{FURNHAM2025102637,
title = {Personality and the education process: Individual difference preferences for teacher, technology, testing, time and topic},
journal = {Learning and Individual Differences},
volume = {119},
pages = {102637},
year = {2025},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2025.102637},
url = {https://www.sciencedirect.com/science/article/pii/S1041608025000135},
author = {Adrian Furnham},
keywords = {Preferences, Time of teaching, Teacher, Technology, Testing method},
abstract = {The present paper looks at the relationship between well-established personality traits and five different features of the educational process. Specifically, I explore the relationship between pupil Extraversion, Neuroticism, Openness, Agreeableness and Conscientiousness and personal preferences for Teacher (who the instructor is), Technology (the mode of instruction used), Testing (how the learning is evaluated), Time (the pace, length and time-of-day of the instruction period), and Topic (what is taught/discipline). There is a scattered literature on these topics which is briefly reviewed with a particular interest in how they relate to personality trait correlates. Evidence suggests the importance of understanding the role personality trait preferences in various educational choices and outcomes.}
}
@article{MARGINEANU20161,
title = {Neuropharmacology beyond reductionism – A likely prospect},
journal = {Biosystems},
volume = {141},
pages = {1-9},
year = {2016},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2015.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0303264715002026},
author = {Doru Georg Margineanu},
keywords = {Neuropharmacology, Systems pharmacology, Reductionism, Multi-target drug, Phenotypic screening, Emergent properties, Serendipity},
abstract = {Neuropharmacology had several major past successes, but the last few decades did not witness any leap forward in the drug treatment of brain disorders. Moreover, current drugs used in neurology and psychiatry alleviate the symptoms, while hardly curing any cause of disease, basically because the etiology of most neuro-psychic syndromes is but poorly known. This review argues that this largely derives from the unbalanced prevalence in neuroscience of the analytic reductionist approach, focused on the cellular and molecular level, while the understanding of integrated brain activities remains flimsier. The decline of drug discovery output in the last decades, quite obvious in neuropharmacology, coincided with the advent of the single target-focused search of potent ligands selective for a well-defined protein, deemed critical in a given pathology. However, all the widespread neuro-psychic troubles are multi-mechanistic and polygenic, their complex etiology making unsuited the single-target drug discovery. An evolving approach, based on systems biology considers that a disease expresses a disturbance of the network of interactions underlying organismic functions, rather than alteration of single molecular components. Accordingly, systems pharmacology seeks to restore a disturbed network via multi-targeted drugs. This review notices that neuropharmacology in fact relies on drugs which are multi-target, this feature having occurred just because those drugs were selected by phenotypic screening in vivo, or emerged from serendipitous clinical observations. The novel systems pharmacology aims, however, to devise ab initio multi-target drugs that will appropriately act on multiple molecular entities. Though this is a task much more complex than the single-target strategy, major informatics resources and computational tools for the systemic approach of drug discovery are already set forth and their rapid progress forecasts promising outcomes for neuropharmacology.}
}
@article{MOTA2023985,
title = {Speech as a Graph: Developmental Perspectives on the Organization of Spoken Language},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {985-993},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223000988},
author = {Natália Bezerra Mota and Janaina Weissheimer and Ingrid Finger and Marina Ribeiro and Bárbara Malcorra and Lilian Hübner},
keywords = {Clinical high risk, Cognitive development, Computational psychiatry, Dementia, Diagnosis, Psychosis},
abstract = {Language has been used as a privileged window to investigate mental processes. More recently, descriptions of psychopathological symptoms have been analyzed with the help of natural language processing tools. An example is the study of speech organization using graph theoretical approaches that began approximately 10 years ago. After its application in different areas, there is a need to better characterize what aspects can be associated with typical and atypical behavior throughout the lifespan, given the variables related to aging as well as biological and social contexts. The precise quantification of mental processes assessed through language may allow us to disentangle biological/social markers by looking at naturalistic protocols in different contexts. In this review, we discuss 10 years of studies in which word recurrence graphs were adopted to characterize the chain of thoughts expressed by individuals while producing discourse. Initially developed to understand formal thought disorder in the context of psychotic syndromes, this line of research has been expanded to understand the atypical development in different stages of psychosis and differential diagnosis (such as dementia) as well as the typical development of thought organization in school-age children/teenagers in naturalistic and school-based protocols. We comment on the effects of environmental factors, such as education and reading habits (in monolingual and bilingual contexts), in clinical and nonclinical populations at different developmental stages (from childhood to older adulthood, considering aging effects on cognition). Looking toward the future, there is an opportunity to use word recurrence graphs to address complex questions that consider biological/social factors within a developmental perspective in typical and atypical contexts.}
}
@article{IZMALKOV2011121,
title = {Perfect implementation},
journal = {Games and Economic Behavior},
volume = {71},
number = {1},
pages = {121-140},
year = {2011},
note = {Special Issue In Honor of John Nash},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2010.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0899825610000758},
author = {Sergei Izmalkov and Matt Lepinski and Silvio Micali},
keywords = {Mechanism design, Trust, Privacy},
abstract = {Privacy and trust affect our strategic thinking, yet have not been precisely modeled in mechanism design. In settings of incomplete information, traditional implementations of a normal-form mechanism—by disregarding the players' privacy, or assuming trust in a mediator—may fail to reach the mechanism's objectives. We thus investigate implementations of a new type. We put forward the notion of a perfect implementation of a normal-form mechanism M: in essence, a concrete extensive-form mechanism exactly preserving all strategic properties of M, without relying on trusted mediators or violating the players' privacy. We prove that any normal-form mechanism can be perfectly implemented by a verifiable mediator using envelopes and an envelope-randomizing device. Differently from a trusted mediator, a verifiable one only performs prescribed public actions, so that everyone can verify that he is acting properly, and that he never learns any information that should remain private.}
}
@article{LIANG20242457,
title = {Adaptive Video Dual Domain Watermarking Scheme Based on PHT Moment and Optimized Spread Transform Dither Modulation},
journal = {Computers, Materials and Continua},
volume = {81},
number = {2},
pages = {2457-2492},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.056438},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008051},
author = {Yucheng Liang and Ke Niu and Yingnan Zhang and Yifei Meng and Fangmeng Hu},
keywords = {Dual-domain, H.264, group of pictures, polar harmonic transform, spread transform dither modulation},
abstract = {To address the challenges of video copyright protection and ensure the perfect recovery of original video, we propose a dual-domain watermarking scheme for digital video, inspired by Robust Reversible Watermarking (RRW) technology used in digital images. Our approach introduces a parameter optimization strategy that incrementally adjusts scheme parameters through attack simulation fitting, allowing for adaptive tuning of experimental parameters. In this scheme, the low-frequency Polar Harmonic Transform (PHT) moment is utilized as the embedding domain for robust watermarking, enhancing stability against simulation attacks while implementing the parameter optimization strategy. Through extensive attack simulations across various digital videos, we identify the optimal low-frequency PHT moment using adaptive normalization. Subsequently, the embedding parameters for robust watermarking are adaptively adjusted to maximize robustness. To address computational efficiency and practical requirements, the unnormalized high-frequency PHT moment is selected as the embedding domain for reversible watermarking. We optimize the traditional single-stage extended transform dithering modulation (STDM) to facilitate multi-stage embedding in the dual-domain watermarking process. In practice, the video embedded with a robust watermark serves as the candidate video. This candidate video undergoes simulation according to the parameter optimization strategy to balance robustness and embedding capacity, with adaptive determination of embedding strength. The reversible watermarking is formed by combining errors and other information, utilizing recursive coding technology to ensure reversibility without attacks. Comprehensive analyses of multiple performance indicators demonstrate that our scheme exhibits strong robustness against Common Signal Processing (CSP) and Geometric Deformation (GD) attacks, outperforming other advanced video watermarking algorithms under similar conditions of invisibility, reversibility, and embedding capacity. This underscores the effectiveness and feasibility of our attack simulation fitting strategy.}
}
@article{SCHUH20181,
title = {Exact satisfiability of linear CNF formulas},
journal = {Discrete Applied Mathematics},
volume = {251},
pages = {1-4},
year = {2018},
issn = {0166-218X},
doi = {https://doi.org/10.1016/j.dam.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0166218X18302762},
author = {Bernd R. Schuh},
keywords = {Complexity, XSAT, Exact linear formula, l-regularity, k-uniformity, NP-completeness},
abstract = {Open questions with respect to the computational complexity of linear CNF (LCNF) formulas are addressed. Focus lies on exact linear CNF formulas (XLCNF), in which any two clauses have exactly one variable in common. It is shown that l-regularity, i.e. each variable occurs exactly l times in the formula, imposes severe restrictions on the structure of XLCNF formulas. In particular it is proven that l-regularity in XLCNF implies k-uniformity, i.e. all clauses have the same number k of literals. Allowed k- values obey k (k−1)=0 (mod l), and the number of clauses m is given by m =kl-(k−1). Then the computational complexity of monotone l-regular XLCNF formulas with respect to exact satisfiability (XSAT) is determined. XSAT turns out to be either trivial, if m is not a multiple of l, or it can be decided in sub-exponential time, namely O(nn).}
}
@article{MASSO2025100112,
title = {Research ethics committees as knowledge gatekeepers: The impact of emerging technologies on social science research},
journal = {Journal of Responsible Technology},
volume = {21},
pages = {100112},
year = {2025},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2025.100112},
url = {https://www.sciencedirect.com/science/article/pii/S2666659625000083},
author = {Anu Masso and Jevgenia Gerassimenko and Tayfun Kasapoglu and Mai Beilmann},
keywords = {Research ethics, Ethics committees, Social sciences, Research methods, Data, Algorithms, Artificial intelligence},
abstract = {This article investigates the evolution of research ethics within the social sciences, emphasising the shift from procedural norms borrowed from medical and natural sciences to social scientific discipline-specific and method-based principles. This transformation acknowledges the unique challenges and opportunities in social science research, particularly in the context of emerging data technologies such as digital data, algorithms, and artificial intelligence. Our empirical analysis, based on a survey conducted among international social scientists (N = 214), highlights the precariousness researchers face regarding these technological shifts. Traditional methods remain prevalent, despite the recognition of new digital methodologies that necessitate new ethical principles. We discuss the role of ethics committees as influential gatekeepers, examining power dynamics and access to knowledge within the research landscape. The findings underscore the need for tailored ethical guidelines that accommodate diverse methodological approaches, advocate for interdisciplinary dialogue, and address inequalities in knowledge production. This article contributes to the broader understanding of evolving research ethics in an increasingly data-driven world.}
}
@article{RUAN2022103133,
title = {Closed-form Minkowski sums of convex bodies with smooth positively curved boundaries},
journal = {Computer-Aided Design},
volume = {143},
pages = {103133},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2021.103133},
url = {https://www.sciencedirect.com/science/article/pii/S0010448521001445},
author = {Sipu Ruan and Gregory S. Chirikjian},
keywords = {Minkowski sums, Computer-aided design, Computational geometry},
abstract = {This article derives closed-form parametric formulas for the Minkowski sums of convex bodies in d-dimensional Euclidean space with boundaries that are smooth and have all positive sectional curvatures at every point. Under these conditions, there is a unique relationship between the position of each boundary point and the surface normal. The main results are presented as two theorems. The first theorem directly parameterizes Minkowski sum boundaries using the unit normal vector at each surface point. Although simple to express mathematically, such a parameterization is not always practical to obtain computationally. Therefore, the second theorem derives a more useful parametric closed-form expression using the gradient that is not normalized. In the special case of two ellipsoids, the proposed expressions are identical to those derived previously using geometric interpretations. In order to examine the results, numerical validations and comparisons of the Minkowski sums between two superquadric bodies are conducted. Applications to generate configuration space obstacles in motion planning problems and to improve optimization-based collision detection algorithms are introduced and demonstrated.}
}
@incollection{PIGGOTT2022176,
title = {8.10 - Optimization of Marine Renewable Energy Systems},
editor = {Trevor M. Letcher},
booktitle = {Comprehensive Renewable Energy (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {176-220},
year = {2022},
isbn = {978-0-12-819734-9},
doi = {https://doi.org/10.1016/B978-0-12-819727-1.00179-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197271001795},
author = {Matthew D. Piggott and Stephan C. Kramer and Simon W. Funke and David M. Culley and Athanasios Angeloudis},
keywords = {Tidal stream, Tidal range, Optimization, Modelling},
abstract = {Optimizing marine renewable energy systems to maximize performance is key to their success. However, a range of physical, environmental, engineering, economic as well as computational challenges means that this is not straightforward. This article considers this topic, focusing on those systems whose performance is coupled to the hydrodynamics providing the resource; tidal power represents a clear example of this. In such cases system design must be optimal in relation to the resource׳s magnitude as well as its spatial and temporal variation, which are all dependent on the system׳s configuration and operation and so cannot be assumed to be known at the design stage. Designing based on the ambient resource could lead to under-performance. Coupling between the design and the resource has implications for the complexity of the optimization problem and potential hydrodynamical and environmental impacts. This coupling distinguishes many marine energy systems from other renewables which do not impact in any significant manner on the resource. The optimal design of marine energy systems thus represents a challenging and somewhat unique problem. However, feedback also opens up a number of possibilities where the resource can be ‘controlled’, to maximize the cumulative power obtained from multiple devices or plants, or to achieve some other complementary goal. Design optimization is thus critical, with many issues to consider. Due to the complexity of the problem a computational based solution is a necessity in all but the simplest scenarios. However, the coupled feedback requires that an iterative solution approach be used, which combined while the vast range of spatial and temporal scales means that methodological compromises need to be made. These compromises need to be understood, with the correct computational tool used at the appropriate point in the design process. This article reviews these challenges as well as the progress that has been made in addressing them.}
}
@article{WANDELL2017298,
title = {Diagnosing the Neural Circuitry of Reading},
journal = {Neuron},
volume = {96},
number = {2},
pages = {298-311},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0896627317306980},
author = {Brian A. Wandell and Rosemary K. Le},
keywords = {reading, diffusion imaging, development, fMRI, computational modeling},
abstract = {We summarize the current state of knowledge of the brain’s reading circuits, and then we describe opportunities to use quantitative and reproducible methods for diagnosing these circuits. Neural circuit diagnostics—by which we mean identifying the locations and responses in an individual that differ significantly from measurements in good readers—can help parents and educators select the best remediation strategy. A sustained effort to develop and share diagnostic methods can support the societal goal of improving literacy.}
}
@article{DELIGKAS2022103784,
title = {Two's company, three's a crowd: Consensus-halving for a constant number of agents},
journal = {Artificial Intelligence},
volume = {313},
pages = {103784},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103784},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001242},
author = {Argyrios Deligkas and Aris Filos-Ratsikas and Alexandros Hollender},
keywords = {Consensus-halving, Fair division, Computational complexity, Query complexity, Robertson-Webb},
abstract = {We consider the ε-Consensus-Halving problem, in which a set of heterogeneous agents aim at dividing a continuous resource into two (not necessarily contiguous) portions that all of them simultaneously consider to be of approximately the same value (up to ε). This problem was recently shown to be PPA-complete, for n agents and n cuts, even for very simple valuation functions. In a quest to understand the root of the complexity of the problem, we consider the setting where there is only a constant number of agents, and we consider both the computational complexity and the query complexity of the problem. For agents with monotone valuation functions, we show a dichotomy: for two agents the problem is polynomial-time solvable, whereas for three or more agents it becomes PPA-complete. Similarly, we show that for two monotone agents the problem can be solved with polynomially-many queries, whereas for three or more agents, we provide exponential query complexity lower bounds. These results are enabled via an interesting connection to a monotone Borsuk-Ulam problem, which may be of independent interest. For agents with general valuations, we show that the problem is PPA-complete and admits exponential query complexity lower bounds, even for two agents.}
}
@article{AGUIRRE2011305,
title = {Geovisual evaluation of public participation in decision making: The grapevine},
journal = {Journal of Visual Languages & Computing},
volume = {22},
number = {4},
pages = {305-321},
year = {2011},
note = {Part Special Issue on Challenging Problems in Geovisual Analytics},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2010.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X10000790},
author = {Robert Aguirre and Timothy Nyerges},
keywords = {Grapevine, Geovisual analytics, Public participation, Decision making, Spatio-temporal events, Human–computer–human interaction},
abstract = {This article reports on a three-dimensional (time–space) geovisual analytic called a “grapevine.” People often use metaphors to describe the temporal and spatial structure of online discussions, e.g., “threads” growing as a result of message exchanges. We created a visualization to evaluate the temporal and spatial structure of online message exchanges based on the shape of a grapevine naturally cultivated in a vineyard. Our grapevine visualization extends up through time with features like buds, nodes, tendrils, and leaves produced as a result of message posting, replying, and voting. Using a rotatable and fully interactive three-dimensional GIS (Geographic Information System) environment, a geovisual analyst can evaluate the quality of deliberation in the grapevine visualization by looking for productive patterns in fine-grained human–computer–human interaction (HCHI) data and then sub-sampling the productive parts for content analysis. We present an example of how we used the technique in a study of participatory interactions during an online field experiment about improving transportation in the central Puget Sound region of Washington called the Let's Improve Transportation (LIT) Challenge. We conclude with insights about how our grapevine could be applied as a general purpose technique for evaluation of any participatory learning, thinking, or decision making situation.}
}
@article{KAR2023102661,
title = {Guest Editorial: Big data-driven theory building: Philosophies, guiding principles, and common traps},
journal = {International Journal of Information Management},
volume = {71},
pages = {102661},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102661},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223000427},
author = {Arpan Kumar Kar and Spyros Angelopoulos and H. Raghav Rao},
keywords = {Big data, Information systems, Artificial intelligence, Machine learning, Theory building, Computational social science},
abstract = {While data availability and access used to be a major challenge for information systems research, the growth and ease of access to large datasets and data analysis tools has increased interest to use such resources for publishing. Such publications, however, seem to offer weak theoretical contributions. While big data-driven studies increasingly gain popularity, they rarely introspect why a phenomenon is better explained by a theory and limit the analysis to data descriptive by mining and visualizing large volumes of big data. We address this pressing need and provide directions to move towards theory building with Big Data. We differentiate based on inductive and deductive approaches and provide guidelines how may undertake steps for theory building. In doing so, we further provide directions surrounding common pitfalls that should be avoided in this journey of Big-Data driven theory building.}
}
@article{CHARLES20243693,
title = {Weaving innovative fabrics of knowledge between institutionalized sciences and Indigenous ways of knowing},
journal = {Matter},
volume = {7},
number = {11},
pages = {3693-3698},
year = {2024},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2024.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S259023852400540X},
author = {Michael Charles},
abstract = {In the rapid chase to address humanity’s grand challenges, we must embrace multiple knowledge systems, including Indigenous ways of knowing, to fuel innovation, translate science into practice, and invite institutional sciences to evolve in an increasingly globalized world.}
}
@incollection{OLSON200116640,
title = {Writing Systems, Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {16640-16643},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01563-1},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015631},
author = {D.R. Olson},
abstract = {The writing systems of the world differ importantly in how they relate to spoken language. Tokening and pictographic scripts relate to meanings or intentions directly. So-called full writing systems represent properties of the spoken language but in completely different ways. Morphophonemic (logographic) scripts represent the words or morphemes of the language, syllabic scripts represent the syllables of the language whether or not they also represent word boundaries. Alphabetic scripts represent, with varying degrees of success, the phonemes of the language, but also by means of spaces, the words of the language. Not only do these differences have an effect on learning to read, they also have an important effect on the ways in which one thinks about language and consequently about the world and the mind. Writing systems provide models for thinking about speech.}
}
@article{HEUNG2025100361,
title = {How ChatGPT impacts student engagement from a systematic review and meta-analysis study},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100361},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100361},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000013},
author = {Yuk Mui Elly Heung and Thomas K.F. Chiu},
keywords = {Generative artificial intelligence, Student engagement, ChatGPT, Systematic review, Meta-analysis},
abstract = {Generative artificial intelligence, such as ChatGPT, has been increasingly integrated into education to change student learning experience. Current empirical studies have mixed results on how ChatGPT impacts student behavioral, cognitive, and emotional engagement. This systematic literature review and meta-analysis explores whether and how ChatGPT impacts student behavioral, cognitive, and emotional engagement. We used the PRISMA method to select, analyze, and report the results. We screened 766 articles from four databases and identified 17 empirical studies with 1735 students for analysis. We compared the effect on student engagement between ChatGPT-based and non-ChatGPT learning. We found a medium effect size on overall student engagement in ChatGPT-based learning in the random effects model. Our analyses further suggest that ChatGPT-based learning is more effective in fostering student behavioral (medium effective size), cognitive (large effective size), and emotional engagement (medium effective size) than non-ChatGPT learning. Our findings revealed ChatGPT is an effective tool for engaging students in learning. We also suggested three roles ChatGPT plays in fostering student engagement: personalized tutoring, programming and technical assistance, and content generation and collaboration. Our systematic literature review revealed potential risks and results in student disengagement, such as over-reliance.}
}
@article{ZHUANG2024e29830,
title = {Artificial multi-verse optimisation for predicting the effect of ideological and political theory course},
journal = {Heliyon},
volume = {10},
number = {9},
pages = {e29830},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29830},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024058614},
author = {Xingzhong Zhuang and Zhaodi Yi and Yuqing Wang and Yi Chen and Sudan Yu},
keywords = {Teaching sufficiency, Artificial multi-verse optimizer, Classification, Art ideological and political theory course},
abstract = {Enhancing teaching sufficiency is crucial because low teaching efficiency has always been a widespread issue in ideological and political theory course. Evaluating data on the course is obtained from a freshmen class of 2022 using questionnaires. The data is organised and condensed for mining and analysis. Subsequently, an intelligent artificial multi-verse optimizer (AMVO) method s developed to predict the effect of ideological and political theory course. The proposed AMVO approach was tested against various cutting-edge algorithms to demonstrate its effectiveness and stability on the benchmark functions. The experimental results indicated that AMVO ranked first among the 23 test functions. Furthermore, the binary AMVO enhanced k-nearest neighbour classifier had excellent performance in the art ideological and political theory course in terms of error rate, accuracy, specificity and sensitivity. This model can predict the overall evaluation attitude of freshmen towards the course based on the dataset. In addition, we can further analyse the potential correlations between factors that enhance the intellectual and political content of the course. This model can further refine the evaluation of ideological and political courses by teachers and students in our school, thereby achieving the fundamental goal of moral cultivation.}
}
@article{TEMIZER2011114,
title = {Thermomechanical contact homogenization with random rough surfaces and microscopic contact resistance},
journal = {Tribology International},
volume = {44},
number = {2},
pages = {114-124},
year = {2011},
issn = {0301-679X},
doi = {https://doi.org/10.1016/j.triboint.2010.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0301679X10002318},
author = {İ. Temizer},
keywords = {Contact mechanics, Homogenization, Thermal contact resistance, Randomness},
abstract = {We extend an earlier computational thermomechanical contact homogenization framework [Temizer İ, Wriggers P. International Journal for Numerical Methods in Engineering 2010; 83:27–58] to random rough surfaces generated through the random-field model based on the concepts of ensemble averaging and sample enlargement towards the effective limit. Additionally, the homogenization theory is revisited in order to incorporate thermal dissipation at the microscopic contact interface within a thermodynamically consistent approach that preserves dissipation across the scales. Large-scale three-dimensional computations were performed to demonstrate the effectiveness and feasibility of the computational framework for an accurate characterization of the macroscopic thermomechanical response of rough surfaces in contact.}
}
@article{ANDERSON1998159,
title = {Mental retardation general intelligence and modularity},
journal = {Learning and Individual Differences},
volume = {10},
number = {3},
pages = {159-178},
year = {1998},
issn = {1041-6080},
doi = {https://doi.org/10.1016/S1041-6080(99)80128-9},
url = {https://www.sciencedirect.com/science/article/pii/S1041608099801289},
author = {Mike Anderson},
abstract = {This article presents a case for distinguishing between mental retardation as a general deficit of thinking and mental retardation that might result from the global effects of a specific deficit in a cognitive module. Using Anderson's (1992a) theory of the minimal cognitive architecture of intelligence and developmental, I show how this distinction can explain the pattern of intellectual strengths and weaknesses in Savant syndrome, Williams syndrome, Down syndrome, and autism. In addition, I discuss the developmental versus difference view and the distinction between organic and cultural familial mental retardation in the light of this theory. I conclude that not only is there no inherent incompatibility between the constructs of general intelligence and modularity of mind but that both are essential to understanding the different patterns of abilities and developmental profiles found in individuals with low IQ.}
}
@article{OSPINAAGUDELO2021121224,
title = {Application domain extension of incremental capacity-based battery SoH indicators},
journal = {Energy},
volume = {234},
pages = {121224},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121224},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221014729},
author = {Brian {Ospina Agudelo} and Walter Zamboni and Eric Monmasson},
keywords = {Battery, State of health, Battery ageing, Capacity degradation, Incremental capacity, Randomised usage pattern},
abstract = {The Incremental Capacity (IC) analysis is used to characterise the capacity and the battery state of health, aged by cycling patterns with randomly selected pulsed current levels and duration. The batteries are periodically characterised at 1C current, which is a high value with respect to the typical IC tests in pseudo-equilibrium condition. The high-current IC curves generation from raw voltage/current data includes two filtering stages, one for the input voltage and one for the incremental capacity curve smoothing, which are optimised for the application on the basis of the data characteristics. The correlations between the IC main peak features and the battery full capacity for 28 Lithium–Cobalt oxide batteries with 18650 packaging were evaluated, finding that the main peak area is a general feature to evaluate the state of health under high current tests and random usage pattern, and, therefore, it can be used as a battery health indicator in practical applications. The effects of the computational parameters on the relationship between the peak area and the battery capacity are also investigated. The results are confirmed by a further analysis performed over an additional set of cells with different technology, aged with a fixed cycling pattern. Additionally, the performance of the peak area as a health indicator was compared with an ohmic resistance-based estimation approach.}
}
@article{DIESTER20242265,
title = {Internal world models in humans, animals, and AI},
journal = {Neuron},
volume = {112},
number = {14},
pages = {2265-2268},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324004549},
author = {Ilka Diester and Marlene Bartos and Joschka Bödecker and Adam Kortylewski and Christian Leibold and Johannes Letzkus and Matthew M. Nour and Monika Schönauer and Andrew Straw and Abhinav Valada and Andreas Vlachos and Thomas Brox},
abstract = {Summary
How do brains—biological or artificial—respond and adapt to an ever-changing environment? In a recent meeting, experts from various fields of neuroscience and artificial intelligence met to discuss internal world models in brains and machines, arguing for an interdisciplinary approach to gain deeper insights into the underlying mechanisms.}
}
@article{STENNING1988143,
title = {Knowledge-rich solutions to the binding problem: a simulation of some human computational mechanisms},
journal = {Knowledge-Based Systems},
volume = {1},
number = {3},
pages = {143-152},
year = {1988},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(88)90072-X},
url = {https://www.sciencedirect.com/science/article/pii/095070518890072X},
author = {Keith Stenning and Joe Levy},
keywords = {binding, memory, PDP system, knowledge-rich, human memory, representations},
abstract = {The binding problem, how properties are represented as belonging to individuals, is identified as a severe problem for human memory, for which the memory adopts knowledge-rich solutions. It is argued that it is the nature of these solutions that endows human memory with many of its positive properties, particularly rapid retrieval on the basis of unreliable search clues. Parallel Distributed Processing (PDP) systems offer some insight into how human memory systems may work, as they also have to solve the binding problem by knowledge-rich methods. Experimental analysis and statistical models of Memory for Individuals Task (MIT) are presented, which provide evidence that the memory representations underlying human performance consist of sets of existential facts containing no referential terms. It is shown that the proposed representations can be incorporated directly into a PDP simulation of the inference from representation to response, and that the resulting system produces human-like errors when subjected to noisy input. The PDP simulation captures some of the asymmetries between stimulus and response which the statistical model cannot.}
}
@article{DAHL20231039,
title = {A Learning Approach for Future Competencies in Manufacturing using a Learning Factory},
journal = {Procedia CIRP},
volume = {118},
pages = {1039-1043},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.178},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004055},
author = {Håkon Dahl and Nina Tvenge and Carla Susana A Assuad and Kristian Martinsen},
keywords = {Learning factory, Work Related Learning, Industry 4.0, Learning Method, Manufacturing, Future Work Competencies},
abstract = {This paper describes a study on future competence needs in manufacturing and how a learning factory utilising a Connective Model for Didactic Design can be used in teaching and learning of these competencies. The paper briefly reports on a literature study, and a set of interviews in Norwegian manufacturing companies to get a better understanding on the expected future competence needs. This was used to design a learning process with four steps: 1: Exploration, 2: Product and process design, 3: Problem solving and 4: Debriefing. The method was tested in a case study where undergraduate students are learners following the 4-step method. The approach was evaluated through feedback from the learners. The case utilised a Festo CP-Factory learning factory at NTNU.}
}
@incollection{AKAN20253,
title = {Chapter 0 - From the ground up!},
editor = {Aydin Akan and Luis F. Chaparro},
booktitle = {Signals and Systems Using MATLAB ® (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
pages = {3-62},
year = {2025},
isbn = {978-0-443-15709-7},
doi = {https://doi.org/10.1016/B978-0-44-315709-7.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157097000094},
author = {Aydin Akan and Luis F. Chaparro},
keywords = {Signals and systems, mathematical models, digital signal processing applications, concrete mathematics, complex variables, system dynamics, MATLAB},
abstract = {This chapter provides an overview of the material in the book, briefly illustrates the applications and highlights the mathematical background needed to understand the analysis of signals and systems. A signal is a function of time like a voice signal, or of space like an image, or of time and space like a video. A system then is a mathematical model of a device, just like the ordinary differential equations representing circuits. We illustrate the importance of the theory of signals and systems by means of practical applications, hint to how to implement them, and connect concepts in Calculus with more concrete mathematics from a computational point of view—using computers. A review of complex variables and their connection with the dynamics of systems is given. We end the chapter with a soft introduction to MATLAB®, a widely used high-level computational tool for analysis and design.}
}
@article{PROCTOR2021142852,
title = {Gateway to the perspectives of the Food-Energy-Water nexus},
journal = {Science of The Total Environment},
volume = {764},
pages = {142852},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.142852},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720363828},
author = {Kyle Proctor and Seyed M.H. Tabatabaie and Ganti S. Murthy},
keywords = {Food-energy–water nexus, Water security, Life cycle assessment, Problem archetype, Resource governance, Systems thinking},
abstract = {The Food-Energy–Water (FEW) nexus has been promoted as a tool for improving food, energy, and water resource security via an interdisciplinary approach that acknowledges the inherent synergies and tradeoffs involved in managing these resources. Over the past decade discussion of the nexus has increased rapidly, along with research funding and output. However, because the nexus encompasses so many different disciplines, researchers engage with and study the nexus from differing perspectives with distinct motivations and analytical methodologies. Understanding these motivations is critical to understanding the value of a given work. This paper first uses a narrative review to identify the motivations and toolsets of five key perspectives used to view the nexus, including: ecosystem health, waste management, public and private institutional change, stakeholder trust, and the learning process. Then, a systematic review is conducted to examine how publication trends have changed over the past decade, both generally and for each of these perspectives. The Food-Energy-Water nexus is not the first systems-based approach for addressing resource management and critiques of the nexus as a “Buzzword” or simply a reinvention of previous systems are growing in the literature. Challenging authors to explicitly define the role and motivations of their research within the broader category of the FEW nexus can improve the actionability of the research, better allow researchers to build from each other's work, and help reduce the ambiguity surrounding the nexus.}
}
@article{LISANA2025100896,
title = {Playing to learn: Game-based approach to financial literacy for generation Z},
journal = {Entertainment Computing},
volume = {52},
pages = {100896},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100896},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002647},
author = {Lisana Lisana and Hendra Dinata and Gabriela {Valencia Tanudjaja}},
keywords = {Gamification, Digital learning, Simulation, Evaluation},
abstract = {This quantitative study assessed the effectiveness of game-based learning in improving financial literacy among Generation Z. Conducted with 32 urban participants, the study involved the use of a board game and a mobile application, designed with input from financial literacy experts. Participants underwent a pretest to gauge their initial financial knowledge, engaged with the game, and completed a posttest to measure learning outcomes. Statistical analysis, including paired sample t-tests, compared pretest and posttest scores, revealing a significant enhancement in financial literacy post-gameplay. Furthermore, a questionnaire evaluated user satisfaction regarding the game, assessing metrics like enjoyment, ease of use, and perceived usefulness. Results demonstrated not only an improvement in financial knowledge but also high satisfaction among users, indicating that game-based learning can be a valuable tool for teaching financial concepts to Generation Z. These findings contribute to the understanding of game-based learning’s potential in financial education and provide insights for educators and parents. However, the study’s limitations suggest areas for future research to explore and refine the use of educational games in financial literacy.}
}
@article{LI2023103984,
title = {Improving short-term bike sharing demand forecast through an irregular convolutional neural network},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {147},
pages = {103984},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103984},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22003977},
author = {Xinyu Li and Yang Xu and Xiaohu Zhang and Wenzhong Shi and Yang Yue and Qingquan Li},
keywords = {Bike sharing, Deep learning, Travel demand forecast, Spatial–temporal analysis, Irregular convolution},
abstract = {As an important task for the management of bike sharing systems, accurate forecast of travel demand could facilitate dispatch and relocation of bicycles to improve user satisfaction. In recent years, many deep learning algorithms have been introduced to improve bicycle usage forecast. A typical practice is to integrate convolutional (CNN) and recurrent neural network (RNN) to capture spatial–temporal dependency in historical travel demand. For typical CNN, the convolution operation is conducted through a kernel that moves across a “matrix-format” city to extract features over spatially adjacent urban areas. This practice assumes that areas close to each other could provide useful information that improves prediction accuracy. However, bicycle usage in neighboring areas might not always be similar, given spatial variations in built environment characteristics and travel behavior that affect cycling activities. Yet, areas that are far apart can be relatively more similar in temporal usage patterns. To utilize the hidden linkage among these distant urban areas, the study proposes an irregular convolutional Long-Short Term Memory model (IrConv+LSTM) to improve short-term bike sharing demand forecast. The model modifies traditional CNN with irregular convolutional architecture to leverage the hidden linkage among “semantic neighbors”. The proposed model is evaluated with a set of benchmark models in five study sites, which include one dockless bike sharing system in Singapore, and four station-based systems in Chicago, Washington, D.C., New York, and London. We find that IrConv+LSTM outperforms other benchmark models in the five cities. The model also achieves superior performance in areas with varying levels of bicycle usage and during peak periods. The findings suggest that “thinking beyond spatial neighbors” can further improve short-term travel demand prediction of urban bike sharing systems.}
}
@article{LIN2024122254,
title = {Reinforcement learning and bandits for speech and language processing: Tutorial, review and outlook},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122254},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122254},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027562},
author = {Baihan Lin},
keywords = {Reinforcement learning, Bandits, Speech processing, Natural language processing, Speech recognition, Large language models, Survey, Perspective},
abstract = {In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits including those in the large language models, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.}
}
@article{WU2025109151,
title = {Examining the role and neural electrophysiological mechanisms of adjective cues in size judgment},
journal = {Neuropsychologia},
volume = {213},
pages = {109151},
year = {2025},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2025.109151},
url = {https://www.sciencedirect.com/science/article/pii/S0028393225000867},
author = {Yihan Wu and Ronglian Zheng and Huili Xing and Yining Kou and Yufeng Wang and Xin Wu and Feng Zou and Yanyan Luo and Meng Zhang},
keywords = {Size judgement, Language, ERP, EEG microstate},
abstract = {Numerous influential theories have attempted to elucidate the relationship between language and thought. The debate persists on whether language and thought are distinct entities or if language is deeply embedded in individual cognitive processes. This study employs adjective cues combined with a mental imagery size judgment task as an experimental paradigm, utilizing neurophysiological techniques to preliminarily explore the role of adjectives in size judgment tasks and their underlying neurophysiological mechanisms. Findings reveal that performance is best when adjectives are congruent with the size of the object, with EEG microstate results indicating strong activity in Class A, related to language networks under this condition. Additionally, when adjectives conflict with object size, the discovery of the Ni component suggests that individuals monitor and inhibit the conflict between adjectives and object size, leading to decreased task performance in this condition. Moreover, when object size is ambiguous, individuals' size judgments do not benefit significantly from clear adjective cues. Event-related potentials and EEG microstate results suggest that under this condition, top-down cognitive resources are recruited more extensively. In conclusion, language plays a more crucial role in simpler judgment tasks; as tasks become more complex, judgment processes engage a greater number of distributed brain regions to collaborate, while the language system remains active. This study provides initial cognitive neuroscience evidence for understanding the relationship between language and simple forms of thought, offering preliminary insights for future investigations into the connection between language and thought.}
}
@article{YU2022158,
title = {Bioinspired interactive neuromorphic devices},
journal = {Materials Today},
volume = {60},
pages = {158-182},
year = {2022},
issn = {1369-7021},
doi = {https://doi.org/10.1016/j.mattod.2022.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1369702122002413},
author = {Jinran Yu and Yifei Wang and Shanshan Qin and Guoyun Gao and Chong Xu and Zhong {Lin Wang} and Qijun Sun},
keywords = {Neuromorphic devices, Synaptic transistors, Interactive, Neuromorphic computing, Bioinspired},
abstract = {The performance of conventional computer based on von Neumann architecture is limited due to the physical separation of memory and processor. By synergistically integrating various sensors with synaptic devices, recently emerging interactive neuromorphic devices can directly sense/store/process various stimuli information from external environments and implement functions of perception, learning, memory, and computation. In this review, we present the basic model of bioinspired interactive neuromorphic devices and discuss the performance metrics. Next, we summarize the recent progress and development of bioinspired interactive neuromorphic devices, which are classified into neuromorphic tactile systems, visual systems, auditory systems, and multisensory system. They are discussed in detail from the aspects of materials, device architectures, operating mechanisms, synaptic plasticity, and potential applications. Additionally, the bioinspired interactive neuromorphic devices that can fuse multiple/mixed sensing signals are proposed to address more realistic and sophisticated problems. Finally, we discuss the pros and cons regarding to the computing neurons and integrating sensory neurons and deliver the perspectives on interactive neuromorphic devices at the material, device, network, and system levels. It is believed the neuromorphic devices can provide promising solutions to next generation of interactive sensation/memory/computation toward the development of multimodal, low-power, and large-scale intelligent systems endowed with neuromorphic features.}
}
@article{SALOMATIN2021582,
title = {Web user identification based on browser fingerprints using machine learning methods},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {582-587},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.512},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321019492},
author = {Alexander A. Salomatin and Andrey Y. Iskhakov and Anastasia O. Iskhakova},
keywords = {browser fingerprint, cybersecurity, identification, digital footprint, machine learning, web server},
abstract = {The article developed a method for identifying users on the network based on browser fingerprints using machine learning methods. The resulting method is a modification of the user identification method based on a digital footprint, which can be more efficient due to two components. First, the selection of attributes for a digital footprint is made from a limited set of attributes to form a user browser fingerprint. Secondly, the identification accuracy can be increased through the combined use of classification methods and the probabilistic-statistical approach. To check the successful operation of the method, a computational experiment is carried out on real data, which consists in solving the problem of classifying a user based on his browser fingerprint using the K nearest neighbors method.}
}
@article{MARTIN20102089,
title = {Integrating learning theories and application-based modules in teaching linear algebra},
journal = {Linear Algebra and its Applications},
volume = {432},
number = {8},
pages = {2089-2099},
year = {2010},
note = {Special issue devoted to the 15th ILAS Conference at Cancun, Mexico, June 16-20, 2008},
issn = {0024-3795},
doi = {https://doi.org/10.1016/j.laa.2009.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0024379509004704},
author = {William Martin and Sergio Loch and Laurel Cooley and Scott Dexter and Draga Vidakovic},
keywords = {Linear algebra, Learning theory, Curriculum, Pedagogy, Constructivist theories, APOS – Action-Process-Object-Schema, Theoretical framework, Encapsulated process, Thematicized schema, Triad – intra, Inter, Trans, Genetic decomposition, Vector addition, Matrix, Matrix multiplication, Matrix representation, Basis, Column space, Row space, Null space, Eigenspace, Transformation},
abstract = {The research team of The Linear Algebra Project developed and implemented a curriculum and a pedagogy for parallel courses in (a) linear algebra and (b) learning theory as applied to the study of mathematics with an emphasis on linear algebra. The purpose of the ongoing research, partially funded by the National Science Foundation, is to investigate how the parallel study of learning theories and advanced mathematics influences the development of thinking of individuals in both domains. The researchers found that the particular synergy afforded by the parallel study of math and learning theory promoted, in some students, a rich understanding of both domains and that had a mutually reinforcing effect. Furthermore, there is evidence that the deeper insights will contribute to more effective instruction by those who become high school math teachers and, consequently, better learning by their students. The courses developed were appropriate for mathematics majors, pre-service secondary mathematics teachers, and practicing mathematics teachers. The learning seminar focused most heavily on constructivist theories, although it also examined socio-cultural and historical perspectives. A particular theory, Action–Process–Object–Schema (APOS) [10], was emphasized and examined through the lens of studying linear algebra. APOS has been used in a variety of studies focusing on student understanding of undergraduate mathematics. The linear algebra courses include the standard set of undergraduate topics. This paper reports the results of the learning theory seminar and its effects on students who were simultaneously enrolled in linear algebra and students who had previously completed linear algebra and outlines how prior research has influenced the future direction of the project.}
}
@article{KOHLER2016212,
title = {On GPU acceleration of common solvers for (quasi-) triangular generalized Lyapunov equations},
journal = {Parallel Computing},
volume = {57},
pages = {212-221},
year = {2016},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2016.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167819116300436},
author = {Martin Köhler and Jens Saak},
keywords = {Lyapunov equations, BLAS level-3, Accelerator device},
abstract = {The solutions of Lyapunov and generalized Lyapunov equations are a key player in many applications in systems and control theory. Their stable numerical computation, when the full solution is sought, is considered solved since the seminal work of Bartels and Stewart [R. H. Bartels, G. W. Stewart, Solution of the matrix equation AX+XB=C: Algorithm 432, Comm. ACM 15 (1972) 820–826.]. A number of variants of their algorithm have been proposed, but none of them goes beyond BLAS level-2 style implementation. On modern computers, however, the formulation of BLAS level-3 type implementations is crucial to enable optimal usage of cache hierarchies and modern block scheduling methods based on directed acyclic graphs describing the interdependence of single block computations. In this contribution, we present the port of our recent BLAS level-3 algorithm [M. Köhler, J. Saak, On BLAS Level-3 implementations of common solvers for (quasi-) triangular generalized Lyapunov equations, SLICOT Working Note 2014-1, NICONET e.V., available from www.slicot.org (Sep. 2014).] to a GPU accelerator device.}
}
@article{FIELDS2022104714,
title = {Neurons as hierarchies of quantum reference frames},
journal = {Biosystems},
volume = {219},
pages = {104714},
year = {2022},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104714},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722000983},
author = {Chris Fields and James F. Glazebrook and Michael Levin},
keywords = {Activity-dependent remodeling, Bayesian inference, Bioelectricity, Computation, Learning, Memory},
abstract = {Conceptual and mathematical models of neurons have lagged behind empirical understanding for decades. Here we extend previous work in modeling biological systems with fully scale-independent quantum information-theoretic tools to develop a uniform, scalable representation of synapses, dendritic and axonal processes, neurons, and local networks of neurons. In this representation, hierarchies of quantum reference frames act as hierarchical active-inference systems. The resulting model enables specific predictions of correlations between synaptic activity, dendritic remodeling, and trophic reward. We summarize how the model may be generalized to nonneural cells and tissues in developmental and regenerative contexts.}
}
@incollection{DOLIVEIRACOELHO2020259,
title = {Chapter 5.1 - Osteomics: Decision support systems for forensic anthropologists},
editor = {Zuzana Obertová and Alistair Stewart and Cristina Cattaneo},
booktitle = {Statistics and Probability in Forensic Anthropology},
publisher = {Academic Press},
pages = {259-273},
year = {2020},
isbn = {978-0-12-815764-0},
doi = {https://doi.org/10.1016/B978-0-12-815764-0.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157640000058},
author = {João {d’Oliveira Coelho} and Francisco Curate and David Navega},
keywords = {Biological profile, Machine learning, Population data, Web-based applications, Age at death, Sex diagnosis, Biogeographic origins, Body parameters, Medicolegal contexts, Cross validation},
abstract = {The popularity of web-based analytical tools with an emphasis on improved statistical analyses within the landscape of forensic anthropology is increasing. Osteomics is a web-based platform composed of a suite of forensic decision support systems designed to contend with the challenges posed by the estimation of the biological profile of human skeletal remains and particularly the estimation of age at death, the diagnosis of sex, the calculation of body parameters, and the prediction of biogeographic origin. The web applications designed at Osteomics intend to make innovative and reliable statistical models freely available. The suggested models are grounded around traditional and advanced statistical thinking, data visualization and processing, and predictive modeling under the machine learning paradigm. This paper aims to introduce the potential of the web platforms as forensic decision support systems and to give a detailed description of the statistical techniques used in the web-based applications available at Osteomics.}
}
@article{KYRIAZOS2024105070,
title = {Quantum concepts in Psychology: Exploring the interplay of physics and the human psyche},
journal = {BioSystems},
volume = {235},
pages = {105070},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.105070},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723002459},
author = {Theodoros Kyriazos and Mary Poga},
keywords = {Quantum mechanics, Quantum psychology, Interdisciplinary, Human psyche},
abstract = {This paper delves into the innovative intersection of quantum mechanics and psychology, examining the potential of quantum principles to provide fresh insights into human emotions, cognition, and consciousness. Drawing parallels between quantum phenomena such as superposition, entanglement, tunneling, decoherence and their psychological counterparts, we present a quantum-psychological model that reimagines emotional states, cognitive breakthroughs, interpersonal relationships, and the nature of consciousness. The study uses computational models and simulations to explore this interdisciplinary fusion's implications and applications, highlighting its potential benefits and inherent challenges. While quantum concepts offer a rich metaphorical lens to view the intricacies of human experience, it is essential to approach this nascent framework with enthusiasm and skepticism. Rigorous empirical validation is paramount to realize its full potential in research and therapeutic contexts. This exploration stands as a promising thread in the tapestry of intellectual history, suggesting a deeper understanding of the human psyche through the lens of quantum mechanics.}
}
@incollection{GALLISTEL2017141,
title = {1.08 - Learning and Representation☆},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {141-154},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210092},
author = {Randy Gallistel},
keywords = {Associations, Cognitive map, Episodic memory, Information, Memory molecules, Path integration, Read–write memory, Signals, Sun compass, Symbols},
abstract = {Behavioral and electrophysiological evidence implies that brains compute representations of aspects of the experienced world. For example, they compute the animal's position in the world by integrating its velocity with respect to time. Other examples are the learning of the solar ephemeris, the construction of a cognitive map, and episodic memory in food caching. Representations require a symbolic read–write memory that carries information extracted from experience forward in time in a computationally accessible form. The analogy between the architecture of computer memory and the genetic architecture suggests the sort of memory structure to be looked for in the nervous system.}
}
@article{LIN2023103217,
title = {A novel personality detection method based on high-dimensional psycholinguistic features and improved distributed Gray Wolf Optimizer for feature selection},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103217},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103217},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322003181},
author = {Hao Lin and Chundong Wang and Qingbo Hao},
keywords = {Personality detection, Feature selection, Symmetric uncertainty, Grey Wolf Optimizer, Spark},
abstract = {Existing personality detection methods based on user-generated text have two major limitations. First, they rely too much on pre-trained language models to ignore the sentiment information in psycholinguistic features. Secondly, they have no consensus on the psycholinguistic feature selection, resulting in the insufficient analysis of sentiment information. To tackle these issues, we propose a novel personality detection method based on high-dimensional psycholinguistic features and improved distributed Gray Wolf Optimizer (GWO) for feature selection (IDGWOFS). Specifically, we introduced the Gaussian Chaos Map-based initialization and neighbor search strategy into the original GWO to improve the performance of feature selection. To eliminate the bias generated when using mutual information to select features, we adopt symmetric uncertainty (SU) instead of mutual information as the evaluation for correlation and redundancy to construct the fitness function, which can balance the correlation between features–labels and the redundancy between features–features. Finally, we improve the common Spark-based parallelization design of GWO by parallelizing only the fitness computation steps to improve the efficiency of IDGWOFS. The experiments indicate that our proposed method obtains average accuracy improvements of 3.81% and 2.19%, and average F1 improvements of 5.17% and 5.8% on Essays and Kaggle MBTI dataset, respectively. Furthermore, IDGWOFS has good convergence and scalability.}
}
@article{ALDULAIMY2024101272,
title = {The computing continuum: From IoT to the cloud},
journal = {Internet of Things},
volume = {27},
pages = {101272},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101272},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524002130},
author = {Auday Al-Dulaimy and Matthijs Jansen and Bjarne Johansson and Animesh Trivedi and Alexandru Iosup and Mohammad Ashjaei and Antonino Galletta and Dragi Kimovski and Radu Prodan and Konstantinos Tserpes and George Kousiouris and Chris Giannakos and Ivona Brandic and Nawfal Ali and André B. Bondi and Alessandro V. Papadopoulos},
keywords = {Computing continuum, Cloud computing, Fog computing, Edge computing, Mobile cloud computing, Multi-access edge computing, SDN, NFV, IoT, Use case, Reference architecture},
abstract = {In the era of the IoT revolution, applications are becoming ever more sophisticated and accompanied by diverse functional and non-functional requirements, including those related to computing resources and performance levels. Such requirements make the development and implementation of these applications complex and challenging. Computing models, such as cloud computing, can provide applications with on-demand computation and storage resources to meet their needs. Although cloud computing is a great enabler for IoT and endpoint devices, its limitations make it unsuitable to fulfill all design goals of novel applications and use cases. Instead of only relying on cloud computing, leveraging and integrating resources at different layers (like IoT, edge, and cloud) is necessary to form and utilize a computing continuum. The layers’ integration in the computing continuum offers a wide range of innovative services, but it introduces new challenges (e.g., monitoring performance and ensuring security) that need to be investigated. A better grasp and more profound understanding of the computing continuum can guide researchers and developers in tackling and overcoming such challenges. Thus, this paper provides a comprehensive and unified view of the computing continuum. The paper discusses computing models in general with a focus on cloud computing, the computing models that emerged beyond the cloud, and the communication technologies that enable computing in the continuum. In addition, two novel reference architectures are presented in this work: one for edge–cloud computing models and the other for edge–cloud communication technologies. We demonstrate real use cases from different application domains (like industry and science) to validate the proposed reference architectures, and we show how these use cases map onto the reference architectures. Finally, the paper highlights key points that express the authors’ vision about efficiently enabling and utilizing the computing continuum in the future.}
}
@article{LI2022111937,
title = {Detection method of timber defects based on target detection algorithm},
journal = {Measurement},
volume = {203},
pages = {111937},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111937},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122011332},
author = {Dongjie Li and Zilei Zhang and Baogang Wang and Chunmei Yang and Liwei Deng},
keywords = {Wood defect detection, YOLOX, Target detection, Feature fusion},
abstract = {Deep learning has achieved certain results in the field of wood surface defect detection. To address the problems of low accuracy of the detection results of surface defects on boards, slow detection speed and large number of model parameters, this article take advantage of computer vision to improve the feature fusion module of YOLOX target detection algorithm, by adding efficient channel attention (ECA) mechanism, adaptive spatial feature fusion mechanism (ASFF) and improve the confidence loss and localization loss functions as Focal loss and Efficient Intersection over Union (EIoU) loss, to enhance the feature extraction ability and detection accuracy of the algorithm. Considering the depth and width of the model, the depth-separable convolution and optional multi-version algorithm are used to reduce the model parameters and computational effort to seek the optimal model. Experiments show that the improved model detects four types of defects in rubber timber with a considerable improvement and has significant advantages over other target detection algorithms.}
}
@article{PINEDA2024101204,
title = {The mode of computing},
journal = {Cognitive Systems Research},
volume = {84},
pages = {101204},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101204},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001389},
author = {Luis A. Pineda},
keywords = {Mode of computing, Natural computing, Representation, Interpretation, Consciousness},
abstract = {The Turing Machine is the paradigmatic case of computing machines, but there are others such as analogical, connectionist, quantum and diverse forms of unconventional computing, each based on a particular intuition of the phenomenon of computing. This variety can be captured in terms of system levels, re-interpreting and generalizing Newell’s hierarchy, which includes the knowledge level at the top and the symbol level immediately below it. In this re-interpretation the knowledge level consists of human knowledge and the symbol level is generalized into a new level that here is called The Mode of Computing. Mental processes performed by natural brains are often thought of informally as computing processes and that the brain is alike to computing machinery. However, if natural computing does exist it should be characterized on its own. A proposal to such an effect is that natural computing appeared when interpretations were first made by biological entities, so natural computing and interpreting are two aspects of the same phenomenon, or that consciousness and experience are the manifestations of computing/interpreting. By analogy with computing machinery, there must be a system level at the top of the neural circuitry and directly below the knowledge level that is named here The mode of Natural Computing. If it turns out that such putative object does not exist the proposition that the mind is a computing process should be dropped; but characterizing it would come with solving the hard problem of consciousness.}
}
@article{IGELSTROM201770,
title = {The inferior parietal lobule and temporoparietal junction: A network perspective},
journal = {Neuropsychologia},
volume = {105},
pages = {70-83},
year = {2017},
note = {Special Issue: Concepts, Actions and Objects: Functional and Neural Perspectives},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0028393217300015},
author = {Kajsa M. Igelström and Michael S.A. Graziano},
keywords = {Angular gyrus, Supramarginal gyrus, Ventral parietal cortex, Posterior superior temporal sulcus, Internal cognition, Frontoparietal executive control network},
abstract = {Information processing in specialized, spatially distributed brain networks underlies the diversity and complexity of our cognitive and behavioral repertoire. Networks converge at a small number of hubs – highly connected regions that are central for multimodal integration and higher-order cognition. We review one major network hub of the human brain: the inferior parietal lobule and the overlapping temporoparietal junction (IPL/TPJ). The IPL is greatly expanded in humans compared to other primates and matures late in human development, consistent with its importance in higher-order functions. Evidence from neuroimaging studies suggests that the IPL/TPJ participates in a broad range of behaviors and functions, from bottom-up perception to cognitive capacities that are uniquely human. The organization of the IPL/TPJ is challenging to study due to the complex anatomy and high inter-individual variability of this cortical region. In this review we aimed to synthesize findings from anatomical and functional studies of the IPL/TPJ that used neuroimaging at rest and during a wide range of tasks. The first half of the review describes subdivisions of the IPL/TPJ identified using cytoarchitectonics, resting-state functional connectivity analysis and structural connectivity methods. The second half of the article reviews IPL/TPJ activations and network participation in bottom-up attention, lower-order self-perception, undirected thinking, episodic memory and social cognition. The central theme of this review is to discuss how network nodes within the IPL/TPJ are organized and how they participate in human perception and cognition.}
}
@incollection{YACKINOUS2015193,
title = {Chapter 11 - Cellular Automata Investigations and Emerging Complex System Principles},
editor = {William S. Yackinous},
booktitle = {Understanding Complex Ecosystem Dynamics},
publisher = {Academic Press},
address = {Boston},
pages = {193-212},
year = {2015},
isbn = {978-0-12-802031-9},
doi = {https://doi.org/10.1016/B978-0-12-802031-9.00011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128020319000115},
author = {William S. Yackinous},
keywords = {Cellular automata, Cellular automata investigations, Explicit experimentation, Cellular automata classes, Simple programs/simple rules, Complex system principles, Computational view of systems, Computational universality, Principle of Computational Equivalence},
abstract = {This chapter is primarily about Stephen Wolfram's innovative cellular automata investigations and his associated ideas on emerging complex system principles. The chapter begins with some cellular automata history and background, and then provides a description of Wolfram's cellular automata “explicit experimentation” work. The experimentation work shows that simple programs with simple rules, repeated over and over, can yield highly complex behavior. Wolfram has identified four classes of cellular automata. Those classes and their characteristics are discussed. The correspondence between cellular automata classes and the attractors of nonlinear dynamics theory is also discussed. Another of Wolfram's important insights is that the behavior of cellular automata is indicative of the behavior of systems in general. That idea is addressed in some detail. The latter part of the chapter addresses Wolfram's computational view of systems. The topics covered include computation as a framework for system principles, the concept of computational universality, and the identification of computationally universal cellular automata. Wolfram's Principle of Computational Equivalence is then described and discussed. The chapter concludes with a summary of my perspectives on the emerging complex system principles.}
}
@article{LIMASILVA2024109436,
title = {Dynamical homotopy transient-based technique to improve the convergence of ill-posed power flow problem},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {155},
pages = {109436},
year = {2024},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109436},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523004933},
author = {Alisson Lima-Silva and Francisco Damasceno Freitas},
keywords = {Euler method, Homotopy, Newton–Raphson method, Numerical integration, Power flow problem, Ill-posed problem},
abstract = {This paper proposes a hybrid technique to solve the ill-posed Power Flow Problem (PFP), considering a homotopy approach. The primary proposal is to solve large-scale problems where the traditional Newton–Raphson (NR) fails to converge, as in the case of ill-posed systems. The method explores a dynamical homotopy transient-based technique to improve the convergence of the ill-conditioned problem instead of using the classical static method. Depending on the integration selected scheme and the integration step, the result furnished by the dynamical homotopy method has low accuracy. Then, the NR method is employed to refine the low-accuracy result and accurately determine the ill-posed PFP solution. The proposed approach can be implemented efficiently using only one Jacobian matrix computation and LU factorization per point of the homotopy path. In the static homotopy problem, a PFP using previous results must be solved per path point. In this case, some LU factorizations are necessary for each path point. The technique’s performance was evaluated through experiments, including a 70,000-bus large-scale system. The approximate dynamical homotopy result used as an initial estimate provided appropriate convergence quality for the NR method to determine a high-precision solution to the PFP.}
}
@article{RAVISHANKAR20211,
title = {Time dependent network resource optimization in cyber–physical systems using game theory},
journal = {Computer Communications},
volume = {176},
pages = {1-12},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001857},
author = {Monica Ravishankar and Thompson Stephan and Thinagaran Perumal},
keywords = {Critical infrastructures, Cyber–physical systems, Game theory, Reinforcement learning technique, Linguistic fuzzy variables},
abstract = {The social and economic stability of a country is dependent on critical infrastructures (CIs) whose services range from financial to healthcare and power to transportation and communications. Most of these CIs are cyber–physical systems (CPSs), which integrate the network’s computational and communication capabilities to facilitate the monitoring and controlling of physical processes. Such systems are vulnerable to damage due to natural disasters, physical incidents, or cyber-attacks impacting the CPS organizations managing complex industrial control systems and data acquisition systems. When these CPSs are exposed to systemic cyber risks and cascaded network failures, network administrators need to recover from the compromise under limited resources. This is formulated as an attacker-defender game model to emulate the decision-making process in choosing an appropriate attack/defence mechanism in response to cybersecurity incidents using game theory. To further improve the assumptions made in the pure game-theoretic model, we relax the constraints on the rationality of the players, monetary payoff, and completeness of information by incorporating learning in games using reinforcement learning technique and compute the expected payoff using linguistic fuzzy variables.}
}
@article{RAY2020106679,
title = {A framework for probabilistic model-based engineering and data synthesis},
journal = {Reliability Engineering & System Safety},
volume = {193},
pages = {106679},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106679},
url = {https://www.sciencedirect.com/science/article/pii/S0951832018312754},
author = {Douglas Ray and Jose Ramirez-Marquez},
keywords = {Modeling and Simulation (M&S), Design of experiments (DOE), Deterministic computer experiments, Space filling designs, Uncertainty Quantification (UQ), Probabilistic optimization, Verification, Validation, Calibration, Trade space, Sensitivity analysis, Statistical engineering},
abstract = {Modern computing resources provide scientists, engineers, and system design teams the ability to study phenomena, such as system behavior, in a virtual setting. Computational modeling and simulation (M&S) enables engineers to avoid many of the challenges encountered in traditional design engineering, including the design, manufacture, and testing of expensive prototypes prior to having an optimized design. However, the use of M&S carries its own challenges, such as the computational time and resources required to execute effective studies, and uncertainties arising from simplifying assumptions inherent to computer models, which are intended to be an approximate representation of reality. In recent year advances have been made in a number of areas related to the efficient and reliable use of M&S for system evaluations, including design & analysis of computer experiments, uncertainty quantification, probabilistic analysis, response optimization, and data synthesis techniques. In this review paper, a general framework for systematically executing efficient M&S studies at the component-level, product-level, system-level, and system-of-systems-level is described. A case study is used to demonstrate how statistical and probabilistic techniques can be integrated with M&S to address those challenges inherent to model-based engineering, and how this aligns with the proposed workflow. The example is a gun-launch dynamics model of an artillery projectile developed by US Army engineers, and illustrates the application of this workflow in the study of subsystem system reliability, performance, and end-to-end system-level characterization.}
}
@incollection{FRANTZ202025,
title = {3 - The “Big 3.” Simon, Katona, Leibenstein},
editor = {Roger Frantz},
booktitle = {The Beginnings of Behavioral Economics},
publisher = {Academic Press},
pages = {25-45},
year = {2020},
series = {Perspectivs in Behavioral Economics and the Economics of Beh},
isbn = {978-0-12-815289-8},
doi = {https://doi.org/10.1016/B978-0-12-815289-8.00003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152898000034},
author = {Roger Frantz},
keywords = {ECONS and HUMANS, Behavioral macroeconomics, Intervening variables, Gestalt psychology, Tit-for-tat, Parable of the ant, Bounded rationality, Satisficing, X-efficiency, Non-allocative efficiency, Das John Maynard Keynes rationality problem},
abstract = {Katona, Leibenstein, and Simon are the “Big 3” of the old behavioral economics. Why are they the Big 3? Their names are most often mentioned by others in terms of “early” behavioral economics. They wrote convincingly about homo economicus, and in doing so they began knocking him off his pedestal. Without this behavioral economicus would never exist. With respect to the Big 3’s writings, Leibenstein wrote about, among other things, multiple-selves, gift exchange, social norms, consumer interdependence, non-allocative efficiency, and less than perfect rationality. Katona wrote about, among other things, ECONS vs HUMANS, expectations, aspirations, adaptive behavior, macro-behavioral theory, procedural rationality, and less than perfect rationality. Among other things, Herbert Simon wrote about bounded rationality, intuition (System 1) and logical thinking (System 2), ECONS vs HUMANS, satisficing, rejection of as if theorizing, learning theories in economics and psychology, rationality in economics and psychology, the nature of human knowledge (tacit knowledge), and less than perfect rationality.}
}
@article{XIONG2020180,
title = {Construction of approximate reasoning model for dynamic CPS network and system parameter identification},
journal = {Computer Communications},
volume = {154},
pages = {180-187},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.073},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420301225},
author = {Juxia Xiong and Jinzhao Wu},
keywords = {Cyber physical system, Network, Event message modeling, Interactive multi-model algorithm},
abstract = {CPS (Cyber Physical System) is a large and complex real-time feedback system that integrates computing processes, physical processes, communication networks, sensor networks, and control systems. It has the powerful function of sensing and controlling the physical environment, which is a big wave following the Internet technology. Because the forms of communication, interaction, and collaboration between heterogeneous units within the CPS are intricate and complex, a comprehensive model needs to be established to describe and analyze the CPS. This paper analyzes the CPS architecture and proposes a new and more complete CPS architecture, decomposes according to this architecture, and classifies the physical entities in the CPS. At the same time, event-based modeling thinking is used to define, classify and formalize event messages. Considering the higher real-time requirements of CPS, an event weighting algorithm was designed according to the different priorities of real-time events. In order to reduce the congestion caused by the limited network bandwidth in the CPS system, improve the ability to identify abnormal data with great uncertainty, and fully guarantee the response rate of the CPS system to emergencies, this paper analyzes the complexity of the CPS system from the perspective of information theory. The average dynamic complexity of the CPS system is set as a threshold to determine the level of information entropy of the sensor data in a certain period of time. The CPS system selects high information entropy data to send first. The effectiveness is analyzed through experiments.}
}
@article{LIBERATORE2024103456,
title = {The ghosts of forgotten things: A study on size after forgetting},
journal = {Annals of Pure and Applied Logic},
volume = {175},
number = {8},
pages = {103456},
year = {2024},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2024.103456},
url = {https://www.sciencedirect.com/science/article/pii/S016800722400054X},
author = {Paolo Liberatore},
keywords = {Logical forgetting, Boolean minimization},
abstract = {Forgetting is removing variables from a logical formula while preserving the constraints on the other variables. In spite of reducing information, it does not always decrease the size of the formula and may sometimes increase it. This article discusses the implications of such an increase and analyzes the computational properties of the phenomenon. Given a propositional Horn formula, a set of variables and a maximum allowed size, deciding whether forgetting the variables from the formula can be expressed in that size is Dp-hard in Σ2p. The same problem for unrestricted CNF propositional formulae is D2p-hard in Σ3p.}
}
@article{DEBOER2010502,
title = {Frame-based guide to situated decision-making on climate change},
journal = {Global Environmental Change},
volume = {20},
number = {3},
pages = {502-510},
year = {2010},
note = {Governance, Complexity and Resilience},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2010.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959378010000245},
author = {Joop {de Boer} and J. Arjan Wardekker and Jeroen P. {van der Sluijs}},
keywords = {Climate change, Adaptation, Decision-making, Frames},
abstract = {The present paper describes a frame-based approach to situated-decision-making on climate change. Building on the multidisciplinary literature on the relationship between frames and decision-making, it argues that decision-makers may gain from making frames more explicit and using them for generating different visions about the central issues. Frames act as organizing principles that shape in a “hidden” and taken-for-granted way how people conceptualize an issue. Science-related issues, such as climate change, are often linked to only a few frames, which consistently appear across different policy areas. Indeed, it appears that there are some very contrasting ways in which climate change may be framed. These frames can be characterized in terms of a simple framework that highlights specific interpretations of climate issues. A second framework clarifies the built-in frames of decision tools. Using Thompson's two basic dimensions of decision, it identifies the main uncertainties that should be considered in developing a decision strategy. The paper characterizes four types of decision strategy, focusing on (1) computation, (2) compromise, (3) judgment, or (4) inspiration, and links each strategy to the appropriate methods and tools, as well as the appropriate social structures. Our experiences show that the frame-based guide can work as an eye-opener for decision-makers, particularly where it demonstrates how to add more perspectives to the decision.}
}
@article{DWYER20111021,
title = {An approach to quantitatively measuring collaborative performance in online conversations},
journal = {Computers in Human Behavior},
volume = {27},
number = {2},
pages = {1021-1032},
year = {2011},
note = {Web 2.0 in Travel and Tourism: Empowering and Changing the Role of Travelers},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210003730},
author = {Paul Dwyer},
keywords = {Collaboration, Cognitive modeling, Collective thinking},
abstract = {Interpersonal dynamics often hinder people from optimizing collaboration. Researchers who monitor the intellectual activity of people as they converse online receive less value when such collaboration is impaired. How can they detect suboptimal collaboration? This study builds on a new metric for measuring collaborative value from the information content of participant contributions to propose a measure of collaborative efficiency, and demonstrates its utility by assessing collaboration around a sample of weblogs. The new collaborative value metric can augment qualitative research by highlighting for deeper investigation conversational themes that triggered elevated collaborative production. Identifying these themes may also define the cognitive box people have built within a collaborative venue. Challenging people to consider fresh ideas by deliberately introducing them into collaborative venues is recommended as the key to overcoming collaborative dysfunction.}
}
@article{BARLOW1983107,
title = {Vision: A computational investigation into the human representation and processing of visual information: David Marr. San Francisco: W. H. Freeman, 1982. pp. xvi + 397},
journal = {Journal of Mathematical Psychology},
volume = {27},
number = {1},
pages = {107-110},
year = {1983},
issn = {0022-2496},
doi = {https://doi.org/10.1016/0022-2496(83)90030-5},
url = {https://www.sciencedirect.com/science/article/pii/0022249683900305},
author = {H.B. Barlow}
}
@article{BEIGZADEH2025107877,
title = {Mental stress detection and performance enhancement using fNIRS and wrist vibrator biofeedback},
journal = {Biomedical Signal Processing and Control},
volume = {107},
pages = {107877},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2025.107877},
url = {https://www.sciencedirect.com/science/article/pii/S174680942500388X},
author = {Anita Beigzadeh and Vahid Yazdnian and Seyed Kamaledin Setarehdan},
keywords = {Stress management, Performance enhancement, Machine learning, Learning model for real-time stress classification, Brain signal processing, Biofeedback, Brain–computer interface, Functional near infrared spectroscopy},
abstract = {Daily life activities frequently expose individuals to varying levels of mental stress, which can adversely affect their performance. Therefore, it is crucial to develop effective strategies for stress management and performance improvement. This paper presents a comprehensive, portable, and real-time biofeedback system aimed at improving individuals’ stress management capabilities, ultimately leading to enhanced mental task performance. The system consists of a real-time brain signal acquisition device, a wireless vibration biofeedback unit, and a software-based program for stress level classification. Notably, the system is designed to minimize the time delay by efficiently integrating all components. Various signal processing and feature extraction techniques combined with machine learning have been employed for online stress detection. The experimental results demonstrate an accuracy of 83% and a recall of 92% in detecting true levels of mental stress in the stress classification module. In addition, the complete biofeedback system is tested on 20 participants in a controlled experimental setup, revealing a 55% reduction in stress levels and a 24.5% improvement in task accuracy. These findings support the effectiveness of the proposed system in stress management and performance improvement, validating the core premises of stress reduction and performance improvement through reward-based learning.}
}
@article{MENG2025105534,
title = {Application of a boundary-type algorithm to the inverse problems of convective heat and mass transfer},
journal = {Progress in Nuclear Energy},
volume = {179},
pages = {105534},
year = {2025},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2024.105534},
url = {https://www.sciencedirect.com/science/article/pii/S0149197024004840},
author = {Xiangyuan Meng and Mei Huang and Jianghao Yang and Xiaoping Ouyang and Boxue Wang and Yanping Huang and Hiroshi Matsuda and Bo Cao},
keywords = {Half boundary method, Inverse problems, Convection-diffusion, Discontinuous coefficient, The Gaussian plume model},
abstract = {The inverse problems of the convection-diffusion equation (ICDE) have received extensive attention in incomplete boundary conditions and uncertain source terms. They can be applied in thermally stratified pipe elbows and so on. Many algorithms need to combine with optimization algorithms to repeatedly calculate the direct problem in the solution process. To solve such problems, this paper employs a boundary-type algorithm named the half-boundary method (HBM). The HBM does not require additional repeated optimization of the direct problem. To test the performance of the method, the numerical simulations of some problems have been carried out, including the inverse problems of heat convection, river pollution and air pollution. The results show that the HBM has the desired accuracy by comparing with the exact solution. If there are errors in the measurement process, the solution doesn't generate a large deviation from the result. It is worth noting that the placement of internal measurement points minimally impacts the numerical results within the solution domain. And the method is also able to handle with discontinuous problems. Because the Gaussian plume model verifies the accuracy of HBM, the HBM can quickly calculate the atmospheric diffusion of the non-Gaussian plume model.}
}
@article{CRAIG2018300,
title = {Metaphors of knowing, doing and being: Capturing experience in teaching and teacher education},
journal = {Teaching and Teacher Education},
volume = {69},
pages = {300-311},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2017.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17301841},
author = {Cheryl J. Craig},
keywords = {Metaphors, Teachers' experiences, Narrative inquiry, School reform},
abstract = {In this article, Bateson's idea of human beings thinking with metaphors and learning through stories is examined as it played out within accumulated educational research studies. Five storied metaphors illuminating knowing, doing and being are highlighted from five investigations involving different research teams. In the cross-case analysis, the importance of narrative exemplars emerges, along with the significance of metaphors serving as proxies for teachers' experiences. The plotlines of the metaphors, the morals of the metaphors and the truths of the metaphors are also discussed. In the end result, the value of metaphors in surfacing teachers' embedded, embodied knowledge of experience is affirmed as well as the deftness of the narrative inquiry research method in metaphorically capturing pre-service and inservice teachers' storied experiences.}
}
@article{BONSIGNORE2017298,
title = {Present and future approaches to lifetime prediction of superelastic nitinol},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {92},
pages = {298-305},
year = {2017},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167844217300587},
author = {Craig Bonsignore}
}
@article{MATLI2024100286,
title = {Extending the theory of information poverty to deepfake technology},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100286},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100286},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000752},
author = {Walter Matli},
keywords = {Deepfake technology, Information poverty theory, Artificial intelligence (AI), Synthetic media, Societal implications, Technological advancements},
abstract = {The advent of deepfake technology has introduced complex challenges to the information technology landscape, simultaneously presenting benefits and novel risks and ethical considerations. This paper delves into the evolution of deepfakes through the prism of information poverty theory, scrutinising how deepfakes may contribute to a growing information access/use inequality. The research focuses on the risks of misinformation and the ensuing expansion of digital divides, particularly when manipulative media could delude individuals lacking access to legitimate information sources. The study outlines the potential exacerbation of information asymmetries and examines the societal implications across various demographics. By integrating an analytical discussion on the risks associated with deepfakes, the study aligns the observed trends with the theoretical underpinnings of information poverty. As part of its contribution, the paper offers actionable policy-making recommendations and educational strategies to combat the proliferation of harmful deepfake content. The article aims to ensure a more equitable distribution of authentic information and foster media literacy. Through a multifaceted approach, this study endeavours to provide a foundational understanding for stakeholders to navigate the ethical minefield posed by deepfakes and to instil a framework for information equity in the digital era. The article provides critical insights into the discourse on deepfake technology and its relation to information poverty, underscoring the urgent need for equitable access to informed digital spaces. As deepfake technology evolves and more data emerges, a societal demand exists for comprehensive knowledge about deepfakes to promote discernment, decision-making and awareness. Policymakers are tasked with recognising the significance of widening access to sophisticated information technologies whilst addressing their negative repercussions. Their efforts will be particularly crucial for disseminating knowledge about deepfakes to those with limited or non-existent information and communication awareness and infrastructures. Learning from past successes and failures becomes pivotal in shaping effective strategies to address the challenges posed by deepfakes and fostering accessible, informed digital communities.}
}
@article{SZYMANSKI201284,
title = {Information retrieval with semantic memory model},
journal = {Cognitive Systems Research},
volume = {14},
number = {1},
pages = {84-100},
year = {2012},
note = {Cognitive Systems Research: Special Issue on Modeling and Application of Cognitive Systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2011.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041711000179},
author = {Julian Szymański and Włodzisław Duch},
abstract = {Psycholinguistic theories of semantic memory form the basis of understanding of natural language concepts. These theories are used here as an inspiration for implementing a computational model of semantic memory in the form of semantic network. Combining this network with a vector-based object-relation-feature value representation of concepts that includes also weights for confidence and support, allows for recognition of concepts by referring to their features, enabling a semantic search algorithm. This algorithm has been used for word games, in particular the 20-question game in which the program tries to guess a concept that a human player thinks about. The game facilitates lexical knowledge validation and acquisition through the interaction with humans via supervised dialog templates. The elementary linguistic competencies of the proposed model have been evaluated assessing how well it can represent the meaning of linguistic concepts. To study properties of information retrieval based on this type of semantic representation in contexts derived from on-going dialogs experiments in limited domains have been performed. Several similarity measures have been used to compare the completeness of knowledge retrieved automatically and corrected through active dialogs to a “golden standard”. Comparison of semantic search with human performance has been made in a series of 20-question games. On average results achieved by human players were better than those obtained by semantic search, but not by a wide margin.}
}
@article{ZHU2024111294,
title = {Grey wolf optimizer based deep learning mechanism for music composition with data analysis},
journal = {Applied Soft Computing},
volume = {153},
pages = {111294},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111294},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624000681},
author = {Qian Zhu and Achyut Shankar and Carsten Maple},
keywords = {Music composition, LSTM, GWO, MIDI, Data analysis},
abstract = {Music composition using artificial intelligence has gained increasing research attention recently. However, existing methods often generate music that needs more coherence and authenticity. This paper proposes an evolutionary computation-based deep learning approach for music composition with data analysis. Specifically, we utilize long short-term memory (LSTM) networks for generating melodic sequences and adopt a grey wolf optimizer to optimize LSTM hyperparameters. The training data is first converted to musical instrument digital interface (MIDI) format for data analysis, and melody lines are extracted using a similarity matrix method. The MIDI data is then encoded for input into the LSTM networks. The generated music is evaluated using objective metrics like mean squared error and subjective methods, including surveys of music professionals. Comparisons made to benchmark algorithms like generative adversarial networks demonstrate the advantages of our approach in accurately capturing tone, rhythm, artistic conception, and other attributes of high-quality music. The proposed mechanism provides a practical framework for AI-based music generation while ensuring authenticity.}
}
@article{YETISEN2015724,
title = {Bioart},
journal = {Trends in Biotechnology},
volume = {33},
number = {12},
pages = {724-734},
year = {2015},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S016777991500205X},
author = {Ali K. Yetisen and Joe Davis and Ahmet F. Coskun and George M. Church and Seok Hyun Yun},
keywords = {genetics, transgenic art, tissue engineering, ethics, aesthetics},
abstract = {Bioart is a creative practice that adapts scientific methods and draws inspiration from the philosophical, societal, and environmental implications of recombinant genetics, molecular biology, and biotechnology. Some bioartists foster interdisciplinary relationships that blur distinctions between art and science. Others emphasize critical responses to emerging trends in the life sciences. Since bioart can be combined with realistic views of scientific developments, it may help inform the public about science. Artistic responses to biotechnology also integrate cultural commentary resembling political activism. Art is not only about ‘responses’, however. Bioart can also initiate new science and engineering concepts, foster openness to collaboration and increasing scientific literacy, and help to form the basis of artists’ future relationships with the communities of biology and the life sciences.}
}
@article{YAMAKAWA2021478,
title = {The whole brain architecture approach: Accelerating the development of artificial general intelligence by referring to the brain},
journal = {Neural Networks},
volume = {144},
pages = {478-495},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003543},
author = {Hiroshi Yamakawa},
keywords = {Brain reference architecture, Structure-constrained interface decomposition method, Brain information flow, Hypothetical component diagram, Brain-inspired artificial general intelligence, Whole-brain architecture},
abstract = {The vastness of the design space that is created by the combination of numerous computational mechanisms, including machine learning, is an obstacle to creating artificial general intelligence (AGI). Brain-inspired AGI development; that is, the reduction of the design space to resemble a biological brain more closely, is a promising approach for solving this problem. However, it is difficult for an individual to design a software program that corresponds to the entire brain as the neuroscientific data that are required to understand the architecture of the brain are extensive and complicated. The whole-brain architecture approach divides the brain-inspired AGI development process into the task of designing the brain reference architecture (BRA), which provides the flow of information and a diagram of the corresponding components, and the task of developing each component using the BRA. This is known as BRA-driven development. Another difficulty lies in the extraction of the operating principles that are necessary for reproducing the cognitive–behavioral function of the brain from neuroscience data. Therefore, this study proposes structure-constrained interface decomposition (SCID), which is a hypothesis-building method for creating a hypothetical component diagram that is consistent with neuroscientific findings. The application of this approach has been initiated for constructing various regions of the brain. In the future, we will examine methods for evaluating the biological plausibility of brain-inspired software. This evaluation will also be used to prioritize different computational mechanisms, which should be integrated and associated with the same regions of the brain.}
}
@article{PITTNAUER2023382,
title = {Observing the creation of new knowledge in the economics laboratory—Do participants discover how to learn from outcome feedback in a dynamic decision problem?},
journal = {Journal of Economic Behavior & Organization},
volume = {215},
pages = {382-405},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2023.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167268123003311},
author = {Sabine Pittnauer and Martin Hohnisch},
keywords = {Learning, Outcome feedback, Discovery, Conjecture, Heuristic simplification, Dynamic decision making},
abstract = {Domain-general learning rules often enable decision makers to learn from outcome feedback which actions tend to achieve a desired goal. However, in novel and complex environments decision makers must explore how to learn, i.e., acquire procedural knowledge of how to elicit and evaluate outcome feedback that will enable them to navigate toward a desired goal despite the vastness of the set of possible policies. Using a dynamic business simulation, this study investigated: (1) whether and how frequently participants discovered an effective procedure to learn from outcome feedback that allowed them to navigate toward a policy that maximizes long-term business profit (and hence their monetary payoff from the experiment), and (2) whether high monetary incentives affected learning procedures and performance. We found that a number of participants discovered an effective learning procedure and succeeded in approximating the optimal policy. In line with the heuristic method, this learning procedure involved a simplification of the search space and the application of domain-general learning rules to this simplified space. Although the decision histories of about half of the participants feature the key aspect of the effective learning procedure—search among the different steady states of the dynamical system—implementation errors prevented many of the participants from realizing the full potential of the learning procedure. We found no evidence to suggest that high monetary incentives affect the effectiveness of learning. Overall, the study illustrates that a “prepared mind” can discover new, effective learning procedures, although their initial implementation may require substantial refinement.}
}
@incollection{CHORAFAS200760,
title = {4 - Stress analysis and its tools},
editor = {Dimitris N. Chorafas},
booktitle = {Stress Testing for Risk Control Under Basel II},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {60-79},
year = {2007},
isbn = {978-0-7506-8305-0},
doi = {https://doi.org/10.1016/B978-075068305-0.50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780750683050500051},
author = {Dimitris N. Chorafas},
abstract = {Publisher Summary
This chapter explains the need for stress testing to take a scientific approach as an advanced analytical methodology for commendable results. The scientific method of investigation is the only basis for conducting tests and experiments. The chapter examines relatively novel approaches to surveys targeting a qualitative evaluation by experts, such as the Delphi method. The chapter discusses the contributions of the scientific method and financial technology to analytical thinking and testing. The characteristics of a sound methodology are discussed and the fundamentals of stress analysis under normal conditions or under stress are described. The chapter also discusses case studies with scenario analysis and talks about stress evaluation through sensitivity analysis and about the fundamentals of statistical analysis.}
}
@article{LU2017138,
title = {Quasi-generalized least squares regression estimation with spatial data},
journal = {Economics Letters},
volume = {156},
pages = {138-141},
year = {2017},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2017.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165176517301441},
author = {Cuicui Lu and Jeffrey M. Wooldridge},
keywords = {Quasi-GLS, Spatial correlation, Covariance tapering, Spatial HAC estimator},
abstract = {We use a particular quasi-generalized least squares (QGLS) approach to study a linear regression model with spatially correlated error terms. The QGLS estimator is consistent, asymptotically normal, computationally easier than GLS, and it appears to not lose much efficiency. A variance–covariance estimator for QGLS, which is robust to heteroskedasticity, spatial correlation and general variance–covariance misspecification is provided.}
}
@article{GARAVAGLIA2010258,
title = {Modelling industrial dynamics with “History-friendly” simulations},
journal = {Structural Change and Economic Dynamics},
volume = {21},
number = {4},
pages = {258-275},
year = {2010},
issn = {0954-349X},
doi = {https://doi.org/10.1016/j.strueco.2010.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0954349X10000573},
author = {Christian Garavaglia},
keywords = {Simulation, Industrial dynamics, Evolutionary economics, “History-Friendly” models, Complexity},
abstract = {The use of simulation techniques has increased greatly in recent years. In economics the industrial dynamics approach makes use of simulation techniques to understand the complexity of the industrial process of continuous change. Among these models, a new branch of studies known as “History-friendly” models aims at establishing a close link between formal theory, developing stand-alone theoretical simulation models, and empirical evidence. In this paper, we study “History-friendly” analyses and counterfactuals. Some examples of “History-friendly” models are widely examined. Finally, the paper makes a critical contribution to “History-friendly” methodology and defines the role of “History-friendly” models in the debate on the empirical validation of simulations.}
}
@article{SMOLENTSEV2020111671,
title = {On the role of integrated computer modelling in fusion technology},
journal = {Fusion Engineering and Design},
volume = {157},
pages = {111671},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111671},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620302192},
author = {Sergey Smolentsev and Gandolfo Alessandro Spagnuolo and Arkady Serikov and Jens Juul Rasmussen and Anders H. Nielsen and Volker Naulin and Jaime Marian and Matti Coleman and Lorenzo Malerba},
keywords = {Fusion technology, Computer modelling, Neutronics, Materials, Plasma, MHD thermofluids, Model integration},
abstract = {Computer modelling is expected to play an increasingly important role in fusion design and technology, where the complexity of the physical processes involved (plasma, materials, engineering), and the highly interconnected nature of systems and components (“system of systems” design), call for support from sophisticated and integrated computer simulation tools. In this paper, we review the contribution of coupled computer modelling to the design of the reactor, breeding blanket and integrated first wall in terms of neutronics, materials behaviour (including plasma-materials interaction, radiation effects and compatibility with fluids), magnetohydrodynamics thermofluid issues and thermo-hydraulic aspects, as well as simulations of plasma transport out of the confinement region to determine heat and particle loads on plasma facing components. The current capabilities and levels of maturity of existing simulation tools are critically analysed, having in mind the possibility of integrating several tools in a single computational suite in the future and highlighting the perspectives and difficulties of such an endeavour.}
}
@article{VALLE2025105242,
title = {Task-value motivational prompts in a descriptive dashboard can increase anxiety among anxious learners},
journal = {Computers & Education},
volume = {229},
pages = {105242},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105242},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000107},
author = {Natercia Valle and Pavlo Antonenko and Denis Valle and Benjamin Baiser},
keywords = {Data science applications in education, Distance education and online learning, Human-computer interface, Pedagogical issues, Post-secondary education},
abstract = {Despite the ubiquitous use of learning analytics dashboards in computer-mediated learning environments, there is still a knowledge gap on how these tools can support learners’ academic performance and motivation. This article describes an experimental study that investigated the influence of motivational prompts (task-value scaffolding) in a descriptive learning analytics dashboard on learners’ motivation, statistics anxiety, and learning performance in an authentic semester-long online statistics course. The study was based on a two-group experimental design during two semesters (Fall 2020 and Spring 2021). A total of 122 graduate students completed the study. The results showed that despite learners’ mostly positive perceptions of the dashboard, the use of motivational prompts did not influence learners’ cognitive outcomes. Test anxiety was the only affective outcome influenced by the intervention, with motivational prompts having a negative effect on learners who started the course with a higher level of test anxiety. This study provides needed empirical evidence on how the design of these tools can influence learners’ affective outcomes, with implications for theory and practice. However, additional experimental studies that account for sources of heterogeneity (e.g., intrapersonal characteristics, contextual factors) are necessary to uncover theoretical gaps and opportunities in the design of effective learning analytics dashboards.}
}
@article{GIERISCH200972,
title = {Factors associated with annual-interval mammography for women in their 40s},
journal = {Cancer Epidemiology},
volume = {33},
number = {1},
pages = {72-78},
year = {2009},
issn = {1877-7821},
doi = {https://doi.org/10.1016/j.cdp.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0361090X0900021X},
author = {Jennifer M. Gierisch and Suzanne C. O’Neill and Barbara K. Rimer and Jessica T. DeFrank and J. Michael Bowling and Celette Sugg Skinner},
keywords = {Breast neoplasms, Guideline adherence, Health behavior, Middle aged, Attitude to health, Patient compliance, Mass screening, Female, Risk factor, Health knowledge},
abstract = {Background: Evidence is mounting that annual mammography for women in their 40s may be the optimal schedule to reduce morbidity and mortality from breast cancer. Few studies have assessed predictors of repeat mammography on an annual interval among these women. Methods: We assessed mammography screening status among 596 insured Black and Non-Hispanic white women ages 43–49. Adherence was defined as having a second mammogram 10–14 months after a previous mammogram. We examined socio-demographic, medical and healthcare-related variables on receipt of annual-interval repeat mammograms. We also assessed barriers associated with screening. Results: 44.8% of the sample were adherent to annual-interval mammography. A history of self-reported abnormal mammograms, family history of breast cancer and never having smoked were associated with adherence. Saying they had not received mammography reminders and reporting barriers to mammography were associated with non-adherence. Four barrier categories were associated with women's non-adherence: lack of knowledge/not thinking mammograms are needed, cost, being too busy, and forgetting to make/keep appointments. Conclusions: Barriers we identified are similar to those found in other studies. Health professionals may need to take extra care in discussing mammography screening risk and benefits due to ambiguity about screening guidelines for women in their 40s, especially for women without family histories of breast cancer or histories of abnormal mammograms. Reminders are important in promoting mammography and should be coupled with other strategies to help women maintain adherence to regular mammography.}
}
@article{LIU2024122986,
title = {Tackling fuel poverty and decarbonisation in a distributed heating system through a three-layer whole system approach},
journal = {Applied Energy},
volume = {362},
pages = {122986},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.122986},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924003696},
author = {Xinyao Liu and Floris Bierkens and Ishanki {De Mel} and Matthew Leach and Michael Short and Mona Chitnis and Boyue Zheng and Lirong Liu},
keywords = {Residential heating decarbonisation, Fuel poverty, Cambridge housing model, Mixed-integer linear programming, Input-output-simulation},
abstract = {Residential heating displays huge decarbonisation potential towards Net-Zero. The complexity of heating system and socio-economic system appeals for a systematic design to avoid exacerbating fuel poverty. This study develops a three-layer heat-for-all model which integrates building stocks analysis, distributed heating system optimisation, economic and environmental impacts simulation to tackle heating decarbonisation and fuel poverty simultaneously. This whole system model is a powerful decision support tool that can help conceive heating decarbonisation strategies for wider regions and countries. More than 400,000 scenarios are created, considering the effects of future policy schemes (No Grant, Business as Usual, Proposed), minimum emission reduction target, carbon intensity of grid, future natural gas, and electricity prices. Results show that optimised heating system decarbonisation plan heavily relies on future energy prices. In the case study, only air source heat pumps are chosen when electricity price is lower than 3 times gas price. Secondly, investment in heating system could stimulate the greenhouse gas emission of whole supply chain, hedging the emission reduction achieved in heating system. This further reveals that life cycle thinking is imperative in GHG emission mitigation. Thirdly, electricity decarbonisation plays a vital role in achieving whole system emission reduction. The grid carbon intensity reduction makes substantial contribution to the emission reduction of heating system and industry system. In tackling fuel poverty, it's worth noticing that the fuel poverty is aggravated with more grant support under certain scenarios, since current policy schemes focus on capital investment in heating system but overlook the increased energy bills. It appeals for a more comprehensive policy design considering all stakeholders.}
}
@article{GAO2019242,
title = {Expert knowledge recommendation systems based on conceptual similarity and space mapping},
journal = {Expert Systems with Applications},
volume = {136},
pages = {242-251},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419304130},
author = {Li Gao and Kun Dai and Liping Gao and Tao Jin},
keywords = {Conceptual similarity, Space mapping, Core resource database (CRD), Institutional repository (IR), Expert Knowledge Recommendation System (EKRS)},
abstract = {The semantic analysis method of structured big data generated based on human knowledge is important in expert recommendation systems and scientific and technological information analysis. In these fields, the most important problem is the calculation of concept similarity. The study aims to explore the spatial mapping relationship between the general knowledge base and the professional knowledge base for the application of the general knowledge map in professional fields. With the core resource database (CRD) as the main body of the general knowledge and the institutional repository (IR) as the main body of the professional knowledge, the conceptual features of institutional expert knowledge were firstly abstracted from IR and inferred from small-scale datasets and the mathematical model was established based on the similarity of text concepts and related ranking results. Then, a two-set concept space mapping algorithm between CRD and IR was designed. In the algorithm, the more granular concept nodes were extracted from the information on the shortest paths among concepts to obtain a new knowledge set, the Expert Knowledge Recommendation System (EKRS). Finally, the simulation experiment was carried out with open datasets to verify the algorithm. The simulation results showed that the algorithm reduced the structural complexity in the calculation of large datasets. The proposed system model had a clear knowledge structure and the recommended accuracy of the text similarity was high. For small-scale knowledge base datasets with different sparsity, the system showed the stable performance, indicating the better convergence and robustness of the algorithm.}
}
@article{MALINVERNI2021100305,
title = {Educational Robotics as a boundary object: Towards a research agenda},
journal = {International Journal of Child-Computer Interaction},
volume = {29},
pages = {100305},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100305},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000349},
author = {Laura Malinverni and Cristina Valero and Marie Monique Schaper and Isabel Garcia {de la Cruz}},
keywords = {Educational robotics, Children, Robots, Boundary object, Intelligent technologies},
abstract = {Educational robotics has become each time more present in the educational experiences of children and young people. Nonetheless, often, the way in which robotics is introduced in educational settings has been considered as unnecessarily narrow. The paper aims at widening the scope of Educational Robotics and expanding the pedagogical possibilities of this field. To this end, the paper draws on the outcomes of two case studies carried out with primary and secondary school children aimed at investigating their views about robots. These studies allow framing and identifying five themes we believe are particularly relevant to rethink the pedagogy of Educational Robotics. Using these themes as cornerstones for reflection, we delineate a set of dimensions and paths to move Educational Robotics beyond the focus on technical skills but instead explore its potential as a boundary object to involve children in reflective processes around the ethical, social and cultural implications of emerging intelligent technologies.}
}
@article{SCHWAB2018500,
title = {A Robust Fault Detection Method using a Zonotopic Kaucher Set-membership Approach},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {24},
pages = {500-507},
year = {2018},
note = {10th IFAC Symposium on Fault Detection, Supervision and Safety for Technical Processes SAFEPROCESS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.623},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318323358},
author = {Stefan Schwab and Vicenç Puig and Soeren Hohmann},
keywords = {Robust fault detection, set-membership approach, Kaucher arithmetic},
abstract = {This paper presents a robust fault detection method using a zonotopic Kaucher set-membership method. The fault detection approach is based on checking the consistency between the model and the data. Consistency is given if there is an intersection between the feasible parameter set and the nominal parameter set. To allow efficient computation the feasible set is approximated by a zonotope. Due to the usage of Kaucher interval arithmetic the results are mathematically guaranteed. The proposed approach is assessed using an illustrative application based on a well-known four-tank case study. The study shows that it is possible to detect even small errors in a noisy setting.}
}
@article{SHUKLA2024e31397,
title = {AI as a user of AI: Towards responsible autonomy},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e31397},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31397},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024074280},
author = {Amit K. Shukla and Vagan Terziyan and Timo Tiihonen},
keywords = {Artificial Intelligence (AI), Autonomy, Responsible AI, ChatGPT, Prompt engineering, AI accountability},
abstract = {Recent advancements in Artificial Intelligence (AI), particularly in generative language models and algorithms, have led to significant impacts across diverse domains. AI capabilities to address prompts are growing beyond human capability but we expect AI to perform well also as a prompt engineer. Additionally, AI can serve as a guardian for ethical, security, and other predefined issues related to generated content. We postulate that enforcing dialogues among AI-as-prompt-engineer, AI-as-prompt-responder, and AI-as-Compliance-Guardian can lead to high-quality and responsible solutions. This paper introduces a novel AI collaboration paradigm emphasizing responsible autonomy, with implications for addressing real-world challenges. The paradigm of responsible AI-AI conversation establishes structured interaction patterns, guaranteeing decision-making autonomy. Key implications include enhanced understanding of AI dialogue flow, compliance with rules and regulations, and decision-making scenarios exemplifying responsible autonomy. Real-world applications envision AI systems autonomously addressing complex challenges. We have made preliminary testing of such a paradigm involving instances of ChatGPT autonomously playing various roles in a set of experimental AI-AI conversations and observed evident added value of such a framework.}
}
@article{BIENVENU202049,
title = {On low for speed oracles},
journal = {Journal of Computer and System Sciences},
volume = {108},
pages = {49-63},
year = {2020},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022000018305828},
author = {Laurent Bienvenu and Rod Downey},
keywords = {Oracle computations, Lowness for speed},
abstract = {Relativizing computations of Turing machines to an oracle is a central concept in the theory of computation, both in complexity theory and in computability theory(!). Inspired by lowness notions from computability theory, Allender introduced the concept of “low for speed” oracles. An oracle A is low for speed if relativizing to A has essentially no effect on computational complexity, meaning that if a decidable language can be decided in time f(n) with access to oracle A, then it can be decided in time poly(f(n)) without any oracle. The existence of non-computable such A's was later proven by Bayer and Slaman, who even constructed a computably enumerable one, and exhibited a number of properties of these oracles. In this paper, we pursue this line of research, answering the questions left by Bayer and Slaman and give further evidence that the class of low for speed oracles is a very rich one.}
}