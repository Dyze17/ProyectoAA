@article{FRANCIS2022103521,
title = {A framework for dynamic life cycle sustainability assessment and policy analysis of built environment through a system dynamics approach},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103521},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103521},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721007873},
author = {Ann Francis and Albert Thomas},
keywords = {Sustainability assessment, System dynamics, Dynamic life cycle sustainability assessment (D-LCSA), Computational modelling, Life cycle assessment},
abstract = {Sustainability is gaining attention, particularly in the building sector, owing to its significant influence on economy, society and environment. However, most assessment methods/frameworks available for this sector focus solely or dominantly on the environmental dimension of sustainability. Hence, a sustainability assessment framework for buildings that accounts for the interdependencies amongst social, economic and environmental aspects is essential. Further, buildings also undergo several time-induced changes in their characteristics, such as changes in electricity consumption, material properties, surrounding infrastructure and energy mix that can influence their sustainability. Therefore, this paper introduces a system dynamics-based methodological framework for Dynamic Life Cycle Sustainability Assessment (D-LCSA) capable of incorporating the dynamic changes in the building characteristics with time and capturing the interactions amongst different sustainability indicators. The usability and utility of the framework is demonstrated using a case study residential project in India. The case study results show that ignoring time-dependant dynamic aspects in sustainability assessment of buildings leads to underestimating the overall sustainability impacts by about 50 per cent and specific environmental impacts by about 12 per cent. Therefore, the study reinforces the need to adopt dynamic thinking through modelling and simulation to predict sustainability performance in the built environment.}
}
@article{XU2024167,
title = {Towards carbon neutrality in China: A systematic identification of China's sustainable land-use pathways across multiple scales},
journal = {Sustainable Production and Consumption},
volume = {44},
pages = {167-178},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2023.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S235255092300283X},
author = {Zhenci Xu},
keywords = {Carbon neutrality, Land use, Multi-scales, System thinking, China},
abstract = {Sustainable land use is crucial for achieving Carbon Neutrality goals, which requires a scientific identification of optimized pathways for land use patterns across multiple scales. Yet, current land use studies predominantly focus on single scales but lack system thinking and fail to establish complementary cross-regional carbon neutrality collaboration schemes. Applying life-cycle thinking to analyze land use sustainability and carbon neutrality potential at multiple scales could address this challenge. This study aims to present China's first multi-scale spatiotemporal optimization pathway for sustainable land use to improve carbon neutrality potential. It systematically integrates the complex spatial coupling relationships between land use intensity and efficiency. We integrate multi-scale sustainable land use pathways, spanning grid, basin, and administrative levels, and unveil significant variations in land use sustainability and carbon neutrality potential across China. Sixty-three percent of China's land is in low sustainability, and the overall carbon neutrality potential in China is relatively low, with regions accounting for <30 % facing more carbon neutrality missions. Implementing sequential and partitioned governance modes can effectively support China in achieving sustainable land use and advancing Carbon Neutrality goals. Our sustainable land use pathways for China provide valuable insights for systematically undertaking carbon neutrality actions across different scales.}
}
@article{JOKONYA20141533,
title = {Towards a Big Data Framework for the Prevention and Control of HIV/AIDS, TB and Silicosis in the Mining Industry},
journal = {Procedia Technology},
volume = {16},
pages = {1533-1541},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.175},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314004022},
author = {Osden Jokonya},
keywords = {Tuberculosis, Big Data, HIV/AIDS, Silicosis, Systems Approach, Viable Systems Model, Organizational Cybernetics, Hard Systems Thinking, Soft Systems Thinking, Emancipatory Systems Thinking, Critical Systems Thinking, Epidemiology},
abstract = {This paper proposes a big data integrated framework to assist with prevention and control of HIV/AIDS, TB and silicosis (HATS) in the mining industry. The linkage between HATS presents a major challenge to the mining industry globally. When the immune system is compromised by HIV/AIDS and silicosis, it makes it easier for tuberculosis to infect the body. In addition, the silica dust which affects the lungs may also cause silicosis and tuberculosis. The objective of this paper is to posit a big data integrated framework to assist in the prevention and control of HATS in the mining industry. Literature was reviewed in order to build a conceptual framework. Although this study is not the first to apply big data in healthcare, to the researcher's knowledge, it is the first to apply big data in understanding the linkage between HATS in the mining industry. The literature review indicates only a few studies using big data in healthcare with no research found on big data and HATS. It therefore makes a contribution to existing body of literature on the control of HATS. The proposed big data framework has the potential of addressing the needs of predictive epidemiology which is important in forecasting and disease control in the mining industry. The paper therefore lays a foundation for the use of viable systems model and big data to address the challenges of HATS in the mining industry. As part of future work, the framework will be validated using sequential explanatory mixed methods case study approach in mining organizations.}
}
@article{MARTINEZMINGO2023101154,
title = {Quantum projections on conceptual subspaces},
journal = {Cognitive Systems Research},
volume = {82},
pages = {101154},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101154},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000827},
author = {Alejandro Martínez-Mingo and Guillermo Jorge-Botana and José Ángel Martinez-Huertas and Ricardo {Olmos Albacete}},
keywords = {Quantum similarity model, Semantic-vector space models, Computational linguistics, Similarity},
abstract = {One of the main challenges of cognitive science is to explain the representation of conceptual knowledge and the mechanisms involved in evaluating the similarities between these representations. Theories that attempt to explain this phenomenon should account for the fact that conceptual knowledge is not static. In line with this thinking, many studies suggest that the representation of a concept changes depending on context. Traditionally, concepts have been studied as vectors within a geometric space, sometimes called Semantic-Vector Space Models (S-VSMs). However, S-VSMs have certain limitations in emulating human biases or context effects when the similarity of concepts is judged. Such limitations are related to the use of a classical geometric approach that represents a concept as a point in space. Recently, some theories have proposed the use of sequential projections of subspaces based on Quantum Probability Theory (Busemeyer and Bruza, 2012; Pothos et al., 2013). They argue that this theoretical approach may facilitate accounting for human similarity biases and context effects in a more natural way. More specifically, Pothos and Busemeyer (2011) proposed the Quantum Similarity Model (QSM) to determine expectation in conceptual spaces in a non-monotonic logic frame. To the best of our knowledge, previous data-driven studies have used the QSM subspaces in a unidimensional way. In this paper, we present a data-driven method to generate these conceptual subspaces in a multidimensional manner using a traditional S-VSM. We present an illustration of the method taking Tversky’s classical examples to explain the effects of Asymmetry, Triangular Inequality, and the Diagnosticity by means of sequential projections of those conceptual subspaces.}
}
@article{LIBEROS2019319,
title = {Phase singularity point tracking for the identification of typical and atypical flutter patients: A clinical-computational study},
journal = {Computers in Biology and Medicine},
volume = {104},
pages = {319-328},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2018.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0010482518303901},
author = {A. Liberos and M. Rodrigo and I. Hernandez-Romero and A. Quesada and F. Fernandez-Aviles and F. Atienza and A.M. Climent and M.S. Guillem},
keywords = {Atrial flutter, Phase map, Cardiac model, Body surface potential mapping},
abstract = {Atrial Flutter (AFL) termination by ablating the path responsible for the arrhythmia maintenance is an extended practice. However, the difficulty associated with the identification of the circuit in the case of atypical AFL motivates the development of diagnostic techniques. We propose body surface phase map analysis as a noninvasive tool to identify AFL circuits. Sixty seven lead body surface recordings were acquired in 9 patients during AFL (i.e. 3 typical, 6 atypical). Computed body surface phase maps from simulations of 5 reentrant behaviors in a realistic atrial structure were also used. Surface representation of the macro-reentrant activity was analyzed by tracking the singularity points (SPs) in surface phase maps obtained from band-pass filtered body surface potential maps. Spatial distribution of SPs showed significant differences between typical and atypical AFL. Whereas for typical AFL patients 70.78 ± 16.17% of the maps presented two SPs simultaneously in the areas defined around the midaxialliary lines, this condition was only satisfied in 5.15 ± 10.99% (p < 0.05) maps corresponding to atypical AFL patients. Simulations confirmed these results. Surface phase maps highlights the reentrant mechanism maintaining the arrhythmia and appear as a promising tool for the noninvasive characterization of the circuit maintaining AFL. The potential of the technique as a diagnosis tool needs to be evaluated in larger populations and, if it is confirmed, may help in planning ablation procedures.}
}
@article{HUANG2022209,
title = {A Framework for Collaborative Artificial Intelligence in Marketing},
journal = {Journal of Retailing},
volume = {98},
number = {2},
pages = {209-223},
year = {2022},
issn = {0022-4359},
doi = {https://doi.org/10.1016/j.jretai.2021.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022435921000142},
author = {Ming-Hui Huang and Roland T. Rust},
keywords = {Artificial intelligence, Collaborative AI, Collaborative intelligence, Augmentation, Replacement},
abstract = {We develop a conceptual framework for collaborative artificial intelligence (AI) in marketing, providing systematic guidance for how human marketers and consumers can team up with AI, which has profound implications for retailing, which is the interface between marketers and consumers. Drawing from the multiple intelligences view that AI advances from mechanical, to thinking, to feeling intelligence (based on how difficult for AI to mimic human intelligences), the framework posits that collaboration between AI and HI (human marketers and consumers) can be achieved by 1) recognizing the respective strengths of AI and HI, 2) having lower-level AI augmenting higher-level HI, and 3) moving HI to a higher intelligence level when AI automates the lower level. Implications for marketers, consumers, and researchers are derived. Marketers should optimize the mix and timing of AI-HI marketing team, consumers should understand the complementarity between AI and HI strengths for informed consumption decisions, and researchers can investigate innovative approaches to and boundary conditions of collaborative intelligence.}
}
@article{JOHNSON20013201,
title = {Methods for 3D computation of fluid–object interactions in spatially periodic flows},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {190},
number = {24},
pages = {3201-3221},
year = {2001},
note = {Advances in Computational Methods for Fluid-Structure Interaction},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(00)00389-3},
url = {https://www.sciencedirect.com/science/article/pii/S0045782500003893},
author = {Andrew Johnson and Tayfun Tezduyar},
abstract = {We present computational methods for 3D simulation of fluid–object interactions in spatially periodic flows. These methods include a stabilized space-time finite element formulation for incompressible flows with spatial periodicity, automatic mesh generation and update techniques for fluid–object mixtures with spatial periodicity, and parallel implementations. The methods can be applied to uni-periodic (i.e., periodic in one direction), bi-periodic, or tri-periodic flows. The methods are described here in the context of tri-periodic flows with fluid–object interactions, and are applied to the simulation of sedimentation of particles in a fluid. We present several case studies where the results obtained provide notable insight into the behavior of fluid–particle mixtures during sedimentation.}
}
@article{USMANI20241044,
title = {The Digital Age: Exploring the Intersection of AI/CI and Human Cognition and Social Interactions},
journal = {Procedia Computer Science},
volume = {239},
pages = {1044-1052},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.268},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924015114},
author = {Usman Ahmad Usmani and Ari Happonen and Junzo Watada},
keywords = {Artificial intelligence, Computational intelligence, Digitalization, Digital transformation, Human cognition, Social interaction, Industry 4.0, Digital capability, Social transformation, Human computer interaction},
abstract = {Although solutions based on artificial and computational intelligence have made life easier, the fast development of technology also raises questions about near future and log term human cognition and social interaction. Through a survey of the literature and qualitative analysis, our work examined current research on how the AI/CI affects human cognitive functions and social interactions. We discuss how AI and CI are influencing e.g. how we humans gather information, build relationships, and communicate with others, with and without the new frontline technologies. Additionally, proposals for future advances are discussed along with the ethical and societal ramifications these technologies have, could and might bring into our lives. We think that by developing a deeper knowledge of how AI/CI affects human cognition and social interaction, new contributions are made to a positive conversation and encourage a responsible approach to incorporating new technologies into our daily lives.}
}
@article{HARTMANN2021112902,
title = {Model development for evidence-based prioritisation of policy action on emerging chemical and microbial drinking water risks},
journal = {Journal of Environmental Management},
volume = {295},
pages = {112902},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.112902},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721009646},
author = {Julia Hartmann and Juan Carlos Chacon-Hurtado and Eric Verbruggen and Jack Schijven and Emiel Rorije and Susanne Wuijts and Ana Maria {de Roda Husman} and Jan Peter {van der Hoek} and Lisa Scholten},
keywords = {Multi criteria analysis, MCA, Stakeholder consultation, Water contaminants, Pathogen},
abstract = {While the burden of disease from well-studied drinking water contaminants is declining, risks from emerging chemical and microbial contaminants arise because of social, technological, demographic and climatological developments. At present, emerging chemical and microbial drinking water contaminants are not assessed in a systematic way, but reactively and incidence based. Furthermore, they are assessed separately despite similar pollution sources. As a result, risks might be addressed ineffectively. Integrated risk assessment approaches are thus needed that elucidate the uncertainties in the risk evaluation of emerging drinking water contaminants, while considering risk assessors’ values. This study therefore aimed to (1) construct an assessment hierarchy for the integrated evaluation of the potential risks from emerging chemical and microbial contaminants in drinking water and (2) develop a decision support tool, based on the agreed assessment hierarchy, to quantify (uncertain) risk scores. A multi-actor approach was used to construct the assessment hierarchy, involving chemical and microbial risk assessors, drinking water experts and members of responsible authorities. The concept of value-focused thinking was applied to guide the problem-structuring and model-building process. The development of the decision support tool was done using Decisi-o-rama, an open-source Python library. With the developed decision support tool (uncertain) risk scores can be calculated for emerging chemical and microbial drinking water contaminants, which can be used for the evidence-based prioritisation of actions on emerging chemical and microbial drinking water risks. The decision support tool improves existing prioritisation approaches as it combines uncertain indicator levels with a multi-stakeholder approach and integrated the risk assessment of chemical and microbial contaminants. By applying the concept of value-focused thinking, this study addressed difficulties in evidence-based decision-making related to emerging drinking water contaminants. Suggestions to improve the model were made to guide future research in assisting policy makers to effectively protect public health from emerging drinking water risks.}
}
@article{LIN2021103499,
title = {Informational cues or content? Examining project funding decisions by crowdfunders},
journal = {Information & Management},
volume = {58},
number = {7},
pages = {103499},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103499},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000732},
author = {Yan Lin and Wai Fong Boh},
keywords = {Experience, Elaboration Likelihood Model, Information Asymmetry, Crowdfunding},
abstract = {We examine how crowdfunder experience affects their reliance on information available on projects. Drawing on elaboration likelihood model and using data from Kickstarter, we apply machine learning techniques and choice modeling to examine the information provided by creators, investigating not only the descriptions, but also the pictures and the videos. We found that more experienced crowdfunders react positively to descriptions exhibiting higher analytical thinking, while less experienced crowdfunders rely more on cues that arouse attention (e.g., number of pictures and positive emotions in videos). We highlight the importance of considering how experience influences crowdfunders’ interpretation of different types of information.}
}
@article{WANG202428,
title = {Exploring the interplay between core and mood symptoms in schizophrenia: A network analysis},
journal = {Schizophrenia Research},
volume = {269},
pages = {28-35},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424001695},
author = {Yucheng Wang and Yixiao Xu and Peiyi Wu and Yang Zhou and Huanrui Zhang and Zijia Li and Yanqing Tang},
keywords = {Schizophrenia, Core symptoms, Mood symptoms, Network analysis, Symptom interactions},
abstract = {Background
Schizophrenia is a complex neuropsychiatric disorder characterized by positive symptoms, negative symptoms, cognitive deficits, and co-occurring mood symptoms. Network analysis offers a novel approach to investigate the intricate relationships between these symptom dimensions, potentially informing personalized treatment strategies.
Methods
A cross-sectional study was conducted from November 2019 to October 2021, involving 1285 inpatients with schizophrenia in Liaoning Province, China. Symptom severity was assessed using the Positive and Negative Syndrome Scale (PANSS), Hamilton Depression Rating Scale (HAMD-17), Hamilton Anxiety Rating Scale (HAMA-14), and Montreal Cognitive Assessment (MoCA). Network analysis was conducted to investigate the network structure, central symptoms, and bridge symptoms.
Results
The network analysis uncovered profound interconnectivity between core symptoms and the anxiety-depression community. Central symptoms, such as psychic anxiety, poor rapport, delusions, and attention, were identified as potential therapeutic targets. Bridge symptoms, including insomnia, depressed mood, anxiety-somatic, conceptual disorganization, and stereotyped thinking, emerged as key nodes facilitating interactions between symptom communities. The stability and reliability of the networks were confirmed through bootstrapping procedures.
Discussion
The findings highlight the complex interplay between schizophrenia symptoms, emphasizing the importance of targeting affective symptoms and cognitive impairment in treatment. The identification of central and bridge symptoms suggests potential pathways for personalized interventions aimed at disrupting self-reinforcing symptom cycles. The study underscores the need for a transdiagnostic, personalized approach to schizophrenia treatment.}
}
@incollection{PARRY2016255,
title = {Chapter Ten - Using Data Mining and Computational Approaches to Study Intermediate Filament Structure and Function},
editor = {M. Bishr Omary and Ronald K.H. Liem},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {568},
pages = {255-276},
year = {2016},
booktitle = {Intermediate Filament Proteins},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2015.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0076687915004152},
author = {David A.D. Parry},
keywords = {IF chain assembly, Sequence periodicities, Heptad and hendecad substructure, Interchain ionic interactions, IF secondary and tertiary structure, Structural/functional motifs, Mutations},
abstract = {Experimental and theoretical research aimed at determining the structure and function of the family of intermediate filament proteins has made significant advances over the past 20 years. Much of this has either contributed to or relied on the amino acid sequence databases that are now available online, and the data mining approaches that have been developed to analyze these sequences. As the quality of sequence data is generally high, it follows that it is the design of the computational and graphical methodologies that are of especial importance to researchers who aspire to gain a greater understanding of those sequence features that specify both function and structural hierarchy. However, these techniques are necessarily subject to limitations and it is important that these be recognized. In addition, no single method is likely to be successful in solving a particular problem, and a coordinated approach using a suite of methods is generally required. A final step in the process involves the interpretation of the results obtained and the construction of a working model or hypothesis that suggests further experimentation. While such methods allow meaningful progress to be made it is still important that the data are interpreted correctly and conservatively. New data mining methods are continually being developed, and it can be expected that even greater understanding of the relationship between structure and function will be gleaned from sequence data in the coming years.}
}
@incollection{PRATT198497,
title = {A Theoretical Framework for Thinking About Depiction},
editor = {W. Ray Crozier and Antlony J. Chapman},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {19},
pages = {97-109},
year = {1984},
booktitle = {Cognitive Processes in the Perception of Art},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)62347-X},
url = {https://www.sciencedirect.com/science/article/pii/S016641150862347X},
author = {Francis Pratt},
abstract = {Publisher Summary
This chapter provides a chronological account of the steps which describes the present theoretical framework for thinking about depiction. The experimental results provides good evidence for the following assertions: (1) knowledge is a necessary part of all acts of depiction done by people of all ages and of all levels of skill, (2) knowledge is a main determinant of looking strategies, (3) the role of knowledge in the organization of looking strategies is one of determining the level of description to be used as the basis of analytic processes, and (4) "good" copying performance (i.e., "accurate" in terms of scene-specific and view-specific relations) can be equated with level of description accessed. The chapter emphasizes on: (1) each descending level of description implies an increasing disintegration of the analytic task. (2) Analysis for depiction is concerned with variance. It is concerned with relations that change according to viewing circumstances. In effect, they can be considered as novel relations. There is much evidence that people's ability to maintain "novel" relations in memory is severely limited. (3) The model consisting of a group of straight lines is only capable of being analyzed at the lowest levels of description.}
}
@article{GENT202336,
title = {Computing comes to life},
journal = {New Scientist},
volume = {258},
number = {3442},
pages = {36-39},
year = {2023},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(23)01054-0},
url = {https://www.sciencedirect.com/science/article/pii/S0262407923010540},
author = {Edd Gent},
abstract = {Nature is capable of astonishing feats of computation. Now, we are re-engineering molecules, cells and even whole organisms into living processors, says Edd Gent}
}
@article{SONOBE2022101560,
title = {Development and validation of machine learning prediction model for post-rehabilitation functional outcome after intracerebral hemorrhage},
journal = {Interdisciplinary Neurosurgery},
volume = {29},
pages = {101560},
year = {2022},
issn = {2214-7519},
doi = {https://doi.org/10.1016/j.inat.2022.101560},
url = {https://www.sciencedirect.com/science/article/pii/S2214751922000743},
author = {Shinya Sonobe and Tetsuo Ishikawa and Kuniyasu Niizuma and Eiryo Kawakami and Takuya Ueda and Eichi Takaya and Carlos {Makoto Miyauchi} and Junya Iwazaki and Ryuzaburo Kochi and Toshiki Endo and Arun Shastry and Vijayananda Jagannatha and Ajay Seth and Atsuhiro Nakagawa and Masahiro Yoshida and Teiji Tominaga},
keywords = {Intracerebral hemorrhage, Machine learning prediction, Post-rehabilitation functional outcome, Design thinking},
abstract = {Objective
Predicting outcomes after intracerebral hemorrhage (ICH) may help improve patient outcomes. We developed and validated a machine learning prediction model for post-rehabilitation functional outcomes after ICH. Patient selection and explanatory variable settings were based on clinical significance. Functional outcomes were predicted using ternary classification.
Methods
The subjects were patients aged > 18 years without pre-onset severe disability who developed primary putaminal and/or thalamic hemorrhage and underwent an inpatient rehabilitation program. As explanatory variables, 43 values related to patient background, imaging-related findings, systemic conditions, neurological findings, and blood tests were acquired within 10 days of onset. As an objective variable, the functional outcome at discharge to home or nursing home was acquired using a ternary classification. The dataset consisting of the collected information was split into a training dataset and a test dataset with a ratio of 2:1. A predictive model using a balanced random forest algorithm was created using supervised learning from the training dataset. The predictive performance was validated using a test dataset.
Results
Between January 2018 and June 2019, 100 consecutive patients were included in the study. The areas under the receiver operating characteristic curves for predictions of good, moderate, and poor outcomes were 0.952, 0.790, and 0.921, respectively.
Conclusions
The predictive performance of the model was comparable to that of previous models. Patient selection and variable settings from a clinical perspective may contribute to accurate and detailed predictions. These study designs are based on design thinking and may meet the needs of clinical practice.}
}
@article{LLOYD2019167,
title = {You make it and you try it out: Seeds of design discipline futures},
journal = {Design Studies},
volume = {65},
pages = {167-181},
year = {2019},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2019.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X19300675},
author = {Peter Lloyd},
keywords = {design methods, design studies, design research, design process, design thinking},
abstract = {This paper takes a narrative seam through the design discipline, attempting to explain how design methodology, one of the three types of Nigel Cross' designerly ways of knowing, has changed over the 40 years of Design Studies. Specifically, the paper identifies the point when a ‘social turn’ in the discipline occurred, allowing more nuanced and critical studies of designing, and shifting the balance from an objective (‘scientific’) perspective to one more based on relativist approaches. The paper concludes by noting the plurality of present-day study, arguably enabled by design thinking, and sketches what this holds for the future of the discipline. The references in the paper are mainly restricted to those published in, or strongly relating to, Design Studies.}
}
@article{MCDERMOTT20071183,
title = {Level-headed},
journal = {Artificial Intelligence},
volume = {171},
number = {18},
pages = {1183-1186},
year = {2007},
note = {Special Review Issue},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001488},
author = {Drew McDermott},
keywords = {Speculation, Methodology, Natural language},
abstract = {I don't believe that human-level intelligence is a well defined goal. As the cognitive-science community learns more about thinking and computation, the mileposts will keep changing in ways that we can't predict, as will the esteem we assign to past accomplishments. It would be fun to have a computer that could solve brain teasers as well as the average scientist, but focusing on such things, besides being parochial, overlooks the crucial role language plays in everything humans do, a role we understand hardly at all on a computational level. I am optimistic that we will eventually figure language out, but not without new ideas. Plus, when we can talk to machines, will we understand each other?}
}
@article{MIRA2009793,
title = {Sensory representation spaces in neuroscience and computation},
journal = {Neurocomputing},
volume = {72},
number = {4},
pages = {793-805},
year = {2009},
note = {Brain Inspired Cognitive Systems (BICS 2006) / Interplay Between Natural and Artificial Computation (IWINAC 2007)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2008.04.054},
url = {https://www.sciencedirect.com/science/article/pii/S0925231208004682},
author = {J. Mira and A.E. Delgado},
keywords = {Representation space, Perception, Cortical maps, Semantic gap},
abstract = {Physics, Neuroscience and Computation are concerned with finding the most appropriate representation spaces to describe the interaction of a dynamic system with its environment. In this work first we review the two basic conceptual approaches to the problem of representing an environment, Marr's ascending “constructivism” and Gibson's “direct perception” hypothesis. Later we review the basic neural mechanisms associated with creating meaning in both approaches: lateral inhibition and the creation of cortical maps by resonance to patterns of stimuli of families of spatially ordered neurons. We end by considering the usefulness in artificial intelligence of knowledge about the way in which biological systems construct their representation spaces. We stress the idea regarding events as representation entities and, consequently, using an event time, different from physical time. Semantics emerges from the mechanisms that detect these relevant events in each organisational level and their composition rules to specify the constitutive entities of the next level. This semantic is distributed in the cortical maps of the neuron groups that resound to the corresponding events.}
}
@article{PEREZLOPEZ2024105162,
title = {Cartographic analysis as spatial determinant for climate change adaptation in the Hunter River Estuary, Australia},
journal = {Cities},
volume = {152},
pages = {105162},
year = {2024},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2024.105162},
url = {https://www.sciencedirect.com/science/article/pii/S0264275124003767},
author = {Irene {Perez Lopez} and Sandra Carrasco and Cesar {Mariscal Madrigal}},
keywords = {Ecological design, Estuary urbanism, Climate adaptation, Living infrastructures, Hunter River Australia},
abstract = {This paper explores the hydrological history of the Hunter River and Estuary (Newcastle, Australia), to identify pathways for incorporating climate-sensitive adaptation approaches into urban development and planning. The research method utilises mapping as a methodological discovery tools to visually articulate the correlation of pre-colonial hydrological landscapes, the transformation of the estuary over two centuries, the areas identified as at risk, and the opportunities for developing a climate-resilient estuary. This research aims to contribute to the redefinition of the discourse on the role of estuary planning for changing climate, focusing on four critical aspects: identify the impacts of urbanisation and industrialisation on ecosystems and its correlation with climate hazard at the estuary; visualise such transformations over time and space to identify critical spatial and climate factors threatening inhabitation; propose strategic spatial practices towards adaptation and resilience; and synthesising the options to foster reflective thinking and establish a correlation with novel policies, governance and practices. The study highlights that adopting new urbanism aligned with cultural and ecological principles can mitigate future climate impacts through re-naturalisation and urban adaptation to sea-level rise by focusing on proactive approaches to building resilient communities. This paper also acknowledges the need for site-specific adaptive design and planning strategies at multiple scales and governance levels.}
}
@article{GANO201556,
title = {Starting with Universe: Buckminster Fuller's Design Science Now},
journal = {Futures},
volume = {70},
pages = {56-64},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714002055},
author = {Gretchen Gano},
keywords = {Comprehensiveness, Big data, Design science, Buckminster Fuller, Worldviews Network},
abstract = {Increasingly, decision makers seek to harness “big data” to guide choices in management and policy settings as well as in professions that manufacture, build, and innovate. Scholars examining this trend tend to diagnose it at once as techno positivist in its insistence on design yoked to quantifiable variables and computational modeling and, alternatively, as an imperative integral to realizing ecologically sustainable innovation. This article investigates this tension. It reflects on the role of futurists, designers, architects, urban planners, social scientists, and artists in interpreting and utilizing comprehensiveness as a design frame. Among nine experimental foresight workshops at the inaugural Emerge conference at Arizona State University, many focused on producing physical objects or media, one modeled and expanded upon a method pioneered by architect and polymath R. Buckminster Fuller. At a time when many of the capabilities to realize Fuller's specifications for big data have matured, I investigate whether comprehensive design as framed by Fuller's method shows promise as a trend enabling ecologically sustainable innovations. A historical look at Fuller's Design Science and the reflection on it in the Emerge workshop marks an opportunity to highlight and interpret the resurgence of comprehensive thinking in design while navigating the contradictions this orientation engenders.}
}
@article{RUPESH20213320,
title = {Computational investigation of heat transfer on the surface of engine cylinder with fins of different shapes and materials},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {3320-3326},
year = {2021},
note = {International Conference on Materials, Manufacturing and Mechanical Engineering for Sustainable Developments-2020 (ICMSD 2020)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.11.471},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320391355},
author = {P.L. Rupesh and K. Raja and N.V. {Sai Deepak Raj} and M. {Pruthviraj Bharmal} and Pandey {Aditya Ramjatan}},
keywords = {Two stroke engine, Fin, Circular fin, Tapered fin, Silumin, Thermal Conductivity},
abstract = {In everyday life the use of vehicles has expanded immensely for some ventures and house hold applications, likewise the running time of engine cycle is exceptionally long. Thus because of the consistent running enormous measure of heat is produced. At the point when this heat isn't appropriately disseminated, the engine gets more fragile very soon and life of the engine declines because of the heat development. To build the life of the engine, heat dispersal is expanded by giving fins at external of engine chamber. The shape of the fins and the material used for the fin increases its heat dissipation capacity and in turn increases the cooling of the engine for proper functioning. The present work focuses on the design of fins of circular and tapered shapes for a 2-stroke engine. The temperature distribution and the heat dissipation along the fin surface of two shapes has been observed by a steady state thermal analysis. Alusil and Silumin has been selected as the fin materials and a computational evaluation has also been done using FEM. A better shape of the fin along with a suitable material has been selected based on the results observed by FEM and on comparison with the existing shape and material of the fin.}
}
@article{MACHKROL2023259,
title = {An ML-extended conceptual framework for implementing temporal big data analytics in organizations to support their agility},
journal = {Procedia Computer Science},
volume = {225},
pages = {259-268},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923011687},
author = {Maria Mach-Król and Bartłomiej Hadasik},
keywords = {temporal big data analytics, temporal knowledge, machine learning, organizational agility, feedback loop},
abstract = {The main aim of this paper is to present the machine learning (ML) extension to the authors’ original conceptual framework for implementing temporal big data analytics (TBDA) in organizations. The framework has been also supplemented with a ML-supported feedback loop aimed at ongoing verification of the organization's maturity for TBDA in light of changing needs, requirements, and the company's environment. Such extension is needed to make the TBDA more flexible and adaptable to market environment, thus augmenting organizational agility. The research has been carried following the Design Science Research in Information Systems (DSRIS) methodological approach with the addition of creative thinking. As a result, the extended framework is elaborated, and further improvements and research directions are identified.}
}
@article{SALMON2022105511,
title = {Bicycle crash contributory factors: A systematic review},
journal = {Safety Science},
volume = {145},
pages = {105511},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105511},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521003544},
author = {Paul M. Salmon and Mitch Naughton and Adam Hulme and Scott McLean},
keywords = {Cyclists, Cyclist crashes, Systems thinking, Road safety, Crash causation},
abstract = {There is a growing body of road safety research that seeks to identify crash contributory factors beyond road users, their vehicles, and the immediate road environment. Although cyclist safety represents a critical research area, this ‘systems thinking’ approach has received less attention in bicycle crash analysis. This article presents the findings from a systematic literature review which aimed to synthesise the peer reviewed literature regarding bicycle crash contributory factors (defined as factors which play a contributory role in bicycle crashes, as opposed to risk factors which are factors which may increase the probability of crashes). Crash contributory factors were extracted from included articles and mapped onto a systems thinking framework comprising seven hierarchical road transport system levels. The findings show that a majority of the included studies identified contributory factors relating to the road environment, cycling infrastructure, and cyclist and driver behaviour. No studies identified contributory factors outside of cyclists and road users, bicycles and vehicles, and the road environment and few specifically examined causal relationships between contributory factors. It is concluded that there are gaps in the knowledge base regarding the broader transport system features that play a role in bicycle crashes and how contributory factors interact to create crashes. We argue that more expansive research into the systemic factors involved in bicycle crashes is required and that initial work should focus on the development of new data sources and analysis methods.}
}
@article{GEORGIEV20181,
title = {Enhancing user creativity: Semantic measures for idea generation},
journal = {Knowledge-Based Systems},
volume = {151},
pages = {1-15},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118301394},
author = {Georgi V. Georgiev and Danko D. Georgiev},
keywords = {Creativity, Divergence, Semantic networks, Similarity, WordNet},
abstract = {Human creativity generates novel ideas to solve real-world problems. This thereby grants us the power to transform the surrounding world and extend our human attributes beyond what is currently possible. Creative ideas are not just new and unexpected, but are also successful in providing solutions that are useful, efficient and valuable. Thus, creativity optimizes the use of available resources and increases wealth. The origin of human creativity, however, is poorly understood, and semantic measures that could predict the success of generated ideas are currently unknown. Here, we analyze a dataset of design problem-solving conversations in real-world settings by using 49 semantic measures based on WordNet 3.1 and demonstrate that a divergence of semantic similarity, an increased information content, and a decreased polysemy predict the success of generated ideas. The first feedback from clients also enhances information content and leads to a divergence of successful ideas in creative problem solving. These results advance cognitive science by identifying real-world processes in human problem solving that are relevant to the success of produced solutions and provide tools for real-time monitoring of problem solving, student training and skill acquisition. A selected subset of information content (IC Sánchez–Batet) and semantic similarity (Lin/Sánchez–Batet) measures, which are both statistically powerful and computationally fast, could support the development of technologies for computer-assisted enhancements of human creativity or for the implementation of creativity in machines endowed with general artificial intelligence.}
}
@article{BOWMAN201834,
title = {Big questions, informative data, excellent science},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {34-36},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300622},
author = {Adrian W. Bowman},
keywords = {Big data, Statistical models},
abstract = {The expression big data is often used in a manner which implies that immediate insight is readily available. Unfortunately, this raises unrealistic expectations. A model which encapsulates the powerful concepts of statistical thinking remains an invaluable component of good analysis.}
}
@article{HASKOVA2025102515,
title = {Fuzzy calculator – A tool for management needs},
journal = {Journal of Computational Science},
volume = {85},
pages = {102515},
year = {2025},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2024.102515},
url = {https://www.sciencedirect.com/science/article/pii/S1877750324003089},
author = {Simona Hašková and Petr Šuleř and Martin Smrt},
keywords = {Fuzzy calculator, Computer program, Multi-criteria evaluation, Fuzzy logic},
abstract = {Fuzzy logic and fuzzy system models have become popular tools in the field of management as they enable efficient handling of uncertainty. We present a tool based on the authors´ original approach focused on solving complex managerial problems affected by the vagueness or uncertainty caused by the human factor. For this purpose, we show the connection between the functioning principle of the tool and processes occurring in the human mind including a description of its structure as perceived by an external observer. This is followed by an overview of selected fragments of fuzzy propositional logic, the theory of fuzzy sets, and the conclusions derived from it. The main part consists of formulating an algebraic description of the computational process of multi-criteria evaluation of the considered alternative performed by a fuzzy system, which serves as the executive unit of a Fuzzy calculator. This is supplemented by a flowchart diagram illustrating the algorithm of its functioning. The Fuzzy calculator distinguishes itself from other fuzzy systems by standardizing all linguistic variables, regardless of the number of linguistic values, into a unified framework comprising three terms L, M, and H, which are represented using trapezoidal fuzzy numbers, ensuring precise mathematical characterization. During the transformation, the original linguistic terms are preserved by incorporating the positions of their support intervals, thereby maintaining the specificity of the input information. This approach establishes the Fuzzy calculator as a universal and highly adaptable tool, capable of addressing a wide range of practical managerial problems with improved consistency and control.}
}
@article{ALON2025104829,
title = {Leveraging natural language processing to elucidate real-world clinical decision-making paradigms: A proof of concept study},
journal = {Journal of Biomedical Informatics},
volume = {166},
pages = {104829},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104829},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000589},
author = {Yaniv Alon and Etti Naimi and Chedva Levin and Hila Videl and Mor Saban},
keywords = {Clinical decision-making, Natural language processing (NLP), Heuristics-based reasoning, Shared decision-making, Healthcare informatics, AI in medicine},
abstract = {Background
Understanding how clinicians arrive at decisions in actual practice settings is vital for advancing personalized, evidence-based care. However, systematic analysis of qualitative decision data poses challenges.
Methods
We analyzed transcribed interviews with Hebrew-speaking clinicians on decision processes using natural language processing (NLP). Word frequency and characterized terminology use, while large language models (ChatGPT from OpenAI and Gemini by Google) identified potential cognitive paradigms.
Results
Word frequency analysis of clinician interviews identified experience and knowledge as most influential on decision-making. NLP tentatively recognized heuristics-based reasoning grounded in past cases and intuition as dominant cognitive paradigms. Elements of shared decision-making through individualizing care with patients and families were also observed. Limited Hebrew clinical language resources required developing preliminary lexicons and dynamically adjusting stopwords. Findings also provided preliminary support for heuristics guiding clinical judgment while highlighting needs for broader sampling and enhanced analytical frameworks.
Conclusions
This study represents the first use of integrated qualitative and computational methods to systematically elucidate clinical decision-making. Findings supported experience-based heuristics guiding cognition. With methodological enhancements, similar analyses could transform global understanding of tailored care delivery. Standardizing interdisciplinary collaborations on developing NLP tools and analytical frameworks may advance equitable, evidence-based healthcare by elucidating real-world clinical reasoning processes across diverse populations and settings.}
}
@article{TEZDUYAR199997,
title = {CFD methods for three-dimensional computation of complex flow problems},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {81},
number = {1},
pages = {97-116},
year = {1999},
issn = {0167-6105},
doi = {https://doi.org/10.1016/S0167-6105(99)00011-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167610599000112},
author = {Tayfun E. Tezduyar},
keywords = {CFD methods, T*AFSM, Three-dimensional flow simulations},
abstract = {This paper provides an overview of some of the CFD methods developed by the Team for Advanced Flow Simulation and Modeling (T*AFSM) [http://www.mems.rice.edu/TAFSM/]. The paper also provides many examples of three-dimensional flow simulations carried out with these CFD methods and advanced parallel supercomputers. The methods and tools described in this paper include: stabilized finite element formulations; formulations for flows with moving boundaries and interfaces; mesh update methods; iterative solution techniques for large nonlinear equation systems; and parallel implementation of these methods. Our target is to be able to address effectively certain classes of flow simulation problems. These include: unsteady flows with interfaces; fluid–object interactions; fluid–structure interactions; airdrop systems; aerodynamics of complex shapes; and contaminant dispersion.}
}
@article{SUPPES201295,
title = {Phase-oscillator computations as neural models of stimulus–response conditioning and response selection},
journal = {Journal of Mathematical Psychology},
volume = {56},
number = {2},
pages = {95-117},
year = {2012},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2012.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S002224961200003X},
author = {P. Suppes and J. Acacio {de Barros} and G. Oas},
keywords = {Learning, Neural oscillators, Three-oscillator Kuramoto model, Stability points of the Kuramoto model, Stimulus–response theory, Phase representation, Continuum of responses},
abstract = {The activity of collections of synchronizing neurons can be represented by weakly coupled nonlinear phase oscillators satisfying Kuramoto’s equations. In this article, we build such neural-oscillator models, partly based on neurophysiological evidence, to represent approximately the learning behavior predicted and confirmed in three experiments by well-known stochastic learning models of behavioral stimulus–response theory. We use three Kuramoto oscillators to model a continuum of responses, and we provide detailed numerical simulations and analysis of the three-oscillator Kuramoto problem, including an analysis of the stability points for different coupling conditions. We show that the oscillator simulation data are well-matched to the behavioral data of the three experiments.}
}
@incollection{KATZ2016123,
title = {Chapter 6 - Development of Counting Ability: An Evolutionary Computation Point of View},
editor = {Avishai Henik},
booktitle = {Continuous Issues in Numerical Cognition},
publisher = {Academic Press},
address = {San Diego},
pages = {123-145},
year = {2016},
isbn = {978-0-12-801637-4},
doi = {https://doi.org/10.1016/B978-0-12-801637-4.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128016374000068},
author = {Gali Barabash Katz and Amit Benbassat and Moshe Sipper},
keywords = {numerical cognition, size perception, counting, evolutionary algorithms, genetic algorithms, artificial neural networks},
abstract = {Examination of numerical cognition encompasses multiple facets (eg, discrete vs. continuous properties, subitizing, estimation, counting, etc.). Many models have been suggested to explain these features. By looking into the basic ability to perceive size, against the complex one of counting, we hypothesize that counting system evolved on the basis of a primitive size perception system rather than the two systems evolved separately. In this chapter, we present a novel way of using evolutionary computation techniques to evolve artificial neural networks (ANNs) first to perceive size and then to count, and compare their counting skills to a different group of ANNs who evolved to count from scratch. The results revealed better counting skills when evolving first to perceive size (or other classification task) and then to count over those who evolved just to count. In addition, ANNs who evolved with continuous stimuli presented better counting skills than those evolved with discrete stimuli.}
}
@article{LIU2025101587,
title = {Application of continuous threat detection based on computer thermal energy consumption optimization in enterprise cloud finance platform},
journal = {Journal of Radiation Research and Applied Sciences},
volume = {18},
number = {3},
pages = {101587},
year = {2025},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2025.101587},
url = {https://www.sciencedirect.com/science/article/pii/S1687850725002997},
author = {Lei Liu},
keywords = {Computer optimization of thermal energy consumption continuous threat detection enterprise finance cloud finance platform},
abstract = {With the increasing popularity of enterprise cloud financial platforms, cyber security threats are becoming increasingly complex and persistent. Traditional security defense methods are difficult to effectively deal with these new types of threats. Large-scale threat detection systems consume a huge amount of computing resources, which in turn leads to significant thermal energy emissions, posing challenges to the environment and operational costs. This study aims to propose a continuous threat detection method based on computer thermal energy consumption optimization, which is applied to the enterprise cloud financial platform to enhance security defense capabilities and reduce energy consumption. This paper proposes a continuous threat detection method based on computational neural networks. By optimizing the neural network algorithm, its computational complexity is reduced, thereby reducing the thermal energy consumption in the computational process. This method designs a continuous threat detection algorithm, which is capable of monitoring potential threats in the cloud financial platform in real time. The experimental results show that the proposed continuous threat detection method effectively reduces the thermal energy consumption of the computer on the premise of ensuring the detection accuracy. Compared with traditional threat detection methods, this method significantly reduces thermal energy consumption under the same detection performance. In the practical application of the enterprise cloud financial platform, this method can promptly detect and respond to various security threats, significantly enhancing the security of the platform. Therefore, the continuous threat detection method based on computer thermal energy consumption optimization proposed in this paper can effectively enhance the security of the enterprise cloud financial platform and reduce energy consumption at the same time.}
}
@article{WOOD199740,
title = {Thinking about Networks in the Control of Male Hamster Sexual Behavior},
journal = {Hormones and Behavior},
volume = {32},
number = {1},
pages = {40-45},
year = {1997},
issn = {0018-506X},
doi = {https://doi.org/10.1006/hbeh.1997.1403},
url = {https://www.sciencedirect.com/science/article/pii/S0018506X97914033},
author = {Ruth I. Wood},
abstract = {Motivated social behaviors such as mating are controlled by a complex network of limbic nuclei. Concepts of network organization derived from computational neuroscience may aid our understanding of the links between the neuroanatomical circuitry and what is represented by the anatomy. Research in my laboratory uses mating behavior in the male Syrian hamster as a model to elucidate how chemosensory and steroid cues are integrated in the brain. An interaction of odors and hormones is required for mating in this species. These two essential stimuli are transmitted through separate parallel pathways in the limbic system. The functional organization of the hamster mating behavior circuit is characterized by distributed representation, divergent and convergent neural pathways, and recurrent feedback. Odors and hormones have different modes of action on this neural network. While chemosensory cues stimulate the input units of the network, steroids facilitate behavior through the hidden units. In this manner, steroids appear to create a permissive environment for subsequent activation by odor cues.}
}
@article{HONG201611,
title = {Ontology-based conceptual design for ultra-precision hydrostatic guideways with human–machine interaction},
journal = {Journal of Industrial Information Integration},
volume = {2},
pages = {11-18},
year = {2016},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2016.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X16300188},
author = {Haibo Hong and Yuehong Yin},
keywords = {Human machine integrated conceptual design, Information integration, High dimensional information integration, Ontology},
abstract = {This paper proposed a human–machine integrated conceptual design method based on ontology, aiming at eliminating the uncertainties and blindness during the design process of ultra-precision grinding machine, especially for its key component–the ultra-precision hydrostatic guideways. Both the required knowledge and the database of hydrostatic guideways are modelled using ontologies to provide a consensual understanding among collaborators. Moreover, a formalized knowledge searching interface is developed to obtain similar instances as references according to the design principles and rules. Based on the imaginal thinking theory, the search process and the results are attempted to be presented in the form of image in order to fit human's customary intuitive thinking frame, facilitating the decision making process. Finally, our design of hydrostatic guideways for an ultra-precision grinding machine is used to validate the effectiveness of the method.}
}
@article{ZHOU2024108310,
title = {The neuroanatomical correlates of daily habitual tendencies and mediating effect on the association between daily habitual tendencies and symptoms of behavioral addictions},
journal = {Computers in Human Behavior},
volume = {158},
pages = {108310},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108310},
url = {https://www.sciencedirect.com/science/article/pii/S074756322400178X},
author = {Xinqi Zhou and Qi Liu and Lan Wang and Xianyang Gan and Ran Zhang and Xiqin Liu and Guojuan Jiao and Christian Montag and Weihua Zhao and Benjamin Becker},
keywords = {Habit, Gray matter, vmPFC, Precuneus, Internet gaming disorder, Smartphone use},
abstract = {Habitual behaviors significantly shape our daily actions. Furthermore, habit formation is proposed as a key mechanism contributing to the development and maintenance of addiction. However, the neural substrates underlying daily habitual tendencies and their contribution to behavioral addiction symptoms in everyday life remain poorly understood. To explore these questions, we conducted a comprehensive analysis of data from 219 individuals who underwent neuroimaging (structural MRI) assessments alongside evaluations of their daily habitual tendencies and symptoms of Internet Gaming Disorder (IGD) and Problematic Smartphone Use (PSU). Using voxel-based morphometry, meta-analytic decoding, and mediation analysis, we found that daily habitual tendencies were positively correlated with larger gray matter volumes in the ventromedial prefrontal cortex (vmPFC), precuneus, superior frontal gyrus (SFG), inferior temporal gyrus (ITG), and supplementary motor area (SMA). Notably, the midline regions, including the vmPFC and precuneus, play a crucial role in value-based computation, emotional regulation, social cognition, and self-referential thinking. Individual variations in gray matter volumes within these regions served as mediators, influencing the bidirectional relationship between daily habitual tendencies and IGD symptoms. However, vmPFC variations were specifically found to mediate the pathway from PSU to daily habitual tendencies. Our findings suggest that the morphological architecture of the vmPFC and precuneus is associated with habitual tendencies in daily life and may mediate the development of addictive behaviors. This study contributes to a more nuanced understanding of the neuroanatomical basis of daily habitual tendencies and their role in addictive behaviors.}
}
@article{EGUCHI2016692,
title = {RoboCupJunior for promoting STEM education, 21st century skills, and technological advancement through robotics competition},
journal = {Robotics and Autonomous Systems},
volume = {75},
pages = {692-699},
year = {2016},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2015.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015001281},
author = {Amy Eguchi},
keywords = {Educational robotics, Robotics competitions, STEM education, Computational thinking, Engineering skills, 21st century skills},
abstract = {RoboCupJunior is an international educational robotics initiative, aiming to promote STEM content and skill learning among participating youth through educational robotics competition inaugurated in 2000. What makes RoboCupJunior quite unique is its relationship with RoboCup which aims to promote robotics and AI research, by offering a publicly appealing, but formidable challenge including development of soccer robots, search and rescue robots, and robots functions at home and at work. This paper introduces a case of RoboCupJunior and the effectiveness of its practice for enhancing learning of STEM contents and skills for innovation and creativity among participating students. It presents the survey results from one of the World Championships held in 2012, the anecdotal and personal account of participating US students on their learning experience from their participation in 2013 World Championship, and participating students’ technological and innovative contributions to highlight the impacts RoboCupJunior has had through over a decade of its practice.}
}
@incollection{NUNES2010457,
title = {Learning Outside of School},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {457-463},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00525-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044894700525X},
author = {T. Nunes},
keywords = {Guided participation, Informal learning, Informal mathematics, Learning outside school, Nonformal learning, Oral arithmetic, Situated learning, Street mathematics, Thinking in action, Work-based learning},
abstract = {Learning can take place everywhere: in the home, the community, or at work. Learning outside school is often invisible because it is taken for granted, as common sense or cultural knowledge. It happens in the course of activities not designed for learning, so it can be described as thinking in action. The representational tools (number systems, graphs) and objects (crates, coins, bills) used outside school become part of our thinking as we act and think with them. A major process in learning outside school is guided participation, where learners take responsibility for accomplishing tasks guided by a more experienced person.}
}
@incollection{PROCHAZKOVA2020121,
title = {Chapter 6 - Altered states of consciousness and creativity},
editor = {David D. Preiss and Diego Cosmelli and James C. Kaufman},
booktitle = {Creativity and the Wandering Mind},
publisher = {Academic Press},
pages = {121-158},
year = {2020},
series = {Explorations in Creativity Research},
isbn = {978-0-12-816400-6},
doi = {https://doi.org/10.1016/B978-0-12-816400-6.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128164006000067},
author = {Luisa Prochazkova and Bernhard Hommel},
keywords = {Altered states of consciousness (ASC), Cannabis, Convergent thinking, Creativity, Divergent thinking, Hallucinations, Meditation, Metactontrol, Psychedelics},
abstract = {Increasing evidence suggests that altered states of consciousness (ASC) are associated with both positive and negative effects on components of creative performance, and convergent and divergent thinking in particular. We provide a metacontrol framework that allows characterizing factors that induce ASC in terms of their general impact on the information processing style of problem solvers. We discuss behavioral and neuronal findings from three areas that reflect strong connections between ASC and the underlying effects on metacontrol on the one hand and components of creativity on the other hand: drug-induced ASC, meditation-induced ASC, and hallucinations. While more, and especially more systematic research is needed, we identify a general trend, suggesting that factors that induce ASC are likely to alter the metacontrol state by biasing it toward either persistence, which is beneficial for convergent thinking and other persistence-heavy operations, or flexibility, which is beneficial for divergent thinking and other flexibility-heavy operations.}
}
@article{ZHANG2023100528,
title = {Foreign language effect in accounting uncertainty expressions: Interpretation and probabilistic estimation},
journal = {Journal of International Accounting, Auditing and Taxation},
volume = {50},
pages = {100528},
year = {2023},
issn = {1061-9518},
doi = {https://doi.org/10.1016/j.intaccaudtax.2023.100528},
url = {https://www.sciencedirect.com/science/article/pii/S1061951823000071},
author = {Yuqian Zhang and Anura {De Zoysa} and Corinne Cortese},
keywords = {Foreign language effect, Uncertainty expressions, Probability estimation, Accounting judgement, Interpretation},
abstract = {The foreign language effect, or thinking in a foreign language, reduces judgment bias under uncertainty. This study investigates how language use (native versus foreign) affects accounting judgment on uncertainty expressions. We conducted two separate experiments: between-subjects and within-subjects, both of which included tasks requiring interpretations and probability estimations based on accounting standard uncertainty expressions. The results demonstrated that foreign language use affected the interpretation of uncertainty expressions and reduced judgment bias in probability estimation, particularly in the context of asset recognition. These findings have important implications for accounting research and reporting.}
}
@article{GARDNER201854,
title = {SMLXL: Scaling the smart city, from metropolis to individual},
journal = {City, Culture and Society},
volume = {12},
pages = {54-61},
year = {2018},
note = {Innovation and identity in next generation smart cities},
issn = {1877-9166},
doi = {https://doi.org/10.1016/j.ccs.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877916617301315},
author = {Nicole Gardner and Luke Hespanhol},
keywords = {Smart cities, Architecture, Design, Physical computing, Proxemics, Computational design},
abstract = {The ‘smart city’ is an oft-cited techno-urban imaginary promoted by businesses and governments alike. It thinks big, and is chiefly imagined in terms of large-scale information communications systems that hinge on the collection of real-time and so-called ‘big data’. Less talked about are the human-scale implications and user-experience of the smart city. Much of the current academic scholarship on smart cities offers synoptic and technical perspectives, leaving the users of smart systems curiously unaccounted for. While they purport to empower citizens, smart cities initiatives are rarely focused at the citizen-scale, nor do they necessarily attend to the ways initiatives can be user-led or co-designed. Drawing on the outcomes of a university studio, this article rethinks the smart city as a series of urban scales—metropolis, community, individual, and personal—and proposes an analytical model for classifying smart city initiatives in terms of engagement. Informed by the theory of proxemics, the model proposed analyses smart city initiatives in terms of the scope of their features and audience size; the actors accountable for their deployment and maintenance; their spatial reach; and the ability of design solutions to re-shape and adapt to different urban scenarios and precincts. We argue that the significance of this model lies in its potential to facilitate modes of thinking across and between scales in ways that can gauge the levels of involvement in the design of digitally mediated urban environments, and productively re-situate citizens as central to the design of smart city initiatives.}
}
@article{LAING2011306,
title = {Computational approaches to RNA structure prediction, analysis, and design},
journal = {Current Opinion in Structural Biology},
volume = {21},
number = {3},
pages = {306-318},
year = {2011},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2011.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X11000674},
author = {Christian Laing and Tamar Schlick},
abstract = {RNA molecules are important cellular components involved in many fundamental biological processes. Understanding the mechanisms behind their functions requires RNA tertiary structure knowledge. Although modeling approaches for the study of RNA structures and dynamics lag behind efforts in protein folding, much progress has been achieved in the past two years. Here, we review recent advances in RNA folding algorithms, RNA tertiary motif discovery, applications of graph theory approaches to RNA structure and function, and in silico generation of RNA sequence pools for aptamer design. Advances within each area can be combined to impact many problems in RNA structure and function.}
}
@incollection{GLENBERG199977,
title = {4 Why mental models must be embodied},
editor = {Gert Rickheit and Christopher Habel},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {128},
pages = {77-90},
year = {1999},
booktitle = {Mental Models in Discourse Processing and Reasoning},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(99)80048-X},
url = {https://www.sciencedirect.com/science/article/pii/S016641159980048X},
author = {Arthur Glenberg},
abstract = {Publisher Summary
Mental models are related to the concept of meaning and language comprehension; in other words, comprehending a linguistic message means that an appropriate mental model has been formed. The manipulation of mental models corresponds to thinking, and it is the manipulation that generates emergent ideas. The chapter discusses the importance of considering the ways ideas combine and presents the data from two experiments that illustrate the combination of ideas. The chapter illustrates the major implications for the theories of mental models. The first implication is that the computational theories cannot account for the data. The second implication is that something like embodiment is needed, and the chapter outlines one account of embodied mental models. The third implication is the most important and most controversial. It is that the human cognition is not a computational phenomenon.}
}
@article{SCHMITT2025111332,
title = {Relationships and representations of brain structures, connectivity, dynamics and functions},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {138},
pages = {111332},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2025.111332},
url = {https://www.sciencedirect.com/science/article/pii/S0278584625000867},
author = {Oliver Schmitt},
keywords = {Brain theory, Functions, Hierarchies, Ontologies, Connectomics, Imaging, Modeling, Simulation, Computational neuroscience, Neuronal dynamics, Behavior},
abstract = {The review explores the complex interplay between brain structures and their associated functions, presenting a diversity of hierarchical models that enhances our understanding of these relationships. Central to this approach are structure-function flow diagrams, which offer a visual representation of how specific neuroanatomical structures are linked to their functional roles. These diagrams are instrumental in mapping the intricate connections between different brain regions, providing a clearer understanding of how functions emerge from the underlying neural architecture. The study details innovative attempts to develop new functional hierarchies that integrate structural and functional data. These efforts leverage recent advancements in neuroimaging techniques such as fMRI, EEG, MEG, and PET, as well as computational models that simulate neural dynamics. By combining these approaches, the study seeks to create a more refined and dynamic hierarchy that can accommodate the brain’s complexity, including its capacity for plasticity and adaptation. A significant focus is placed on the overlap of structures and functions within the brain. The manuscript acknowledges that many brain regions are multifunctional, contributing to different cognitive and behavioral processes depending on the context. This overlap highlights the need for a flexible, non-linear hierarchy that can capture the brain’s intricate functional landscape. Moreover, the study examines the interdependence of these functions, emphasizing how the loss or impairment of one function can impact others. Another crucial aspect discussed is the brain’s ability to compensate for functional deficits following neurological diseases or injuries. The investigation explores how the brain reorganizes itself, often through the recruitment of alternative neural pathways or the enhancement of existing ones, to maintain functionality despite structural damage. This compensatory mechanism underscores the brain’s remarkable plasticity, demonstrating its ability to adapt and reconfigure itself in response to injury, thereby ensuring the continuation of essential functions. In conclusion, the study presents a system of brain functions that integrates structural, functional, and dynamic perspectives. It offers a robust framework for understanding how the brain’s complex network of structures supports a wide range of cognitive and behavioral functions, with significant implications for both basic neuroscience and clinical applications.}
}
@incollection{VARGAS201945,
title = {Cell Adhesion: Basic Principles and Computational Modeling},
editor = {Roger Narayan},
booktitle = {Encyclopedia of Biomedical Engineering},
publisher = {Elsevier},
address = {Oxford},
pages = {45-58},
year = {2019},
isbn = {978-0-12-805144-3},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.99930-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383999306},
author = {Diego A. Vargas and Hans {Van Oosterwyck}},
keywords = {Adherens junction, Adhesion dynamics, Bond lifetime, Cell adhesion, Cellularized material, Focal adhesion, Force spectroscopy, Mathematical modeling, Mechanotransduction, Multiscale modeling, Rate constant, Vertex model},
abstract = {A cell interacts with its environment through adhesion complexes. These are protein complexes that form through noncovalent interactions between adhesion receptors in the cell membrane and similar receptors in neighboring cells or ligand molecules in the surrounding extracellular matrix. Cell adhesions are crucial to maintain tissue integrity and cellular communication. Communication and sensing occur through the transmittal of forces through adhesions. This relevant role motivated researchers to develop theoretical models of adhesion. Initial models were based on studies of association kinetics of proteins, which later were expanded to explicitly include the role of force in determining bond strength. The introduction of techniques that allowed measurements of force in the range of a single adhesion produced models that describe the inner workings of the adhesion molecules themselves. Despite the relative simplicity of these models, they are still relevant. Not only were these studies novel and creative, they have been integrated into models describing larger cellular aggregates, unraveling the role of mechanics in biology. These models have been used in the study of cell migration, developmental biology, and cancer biology.}
}
@article{DUCH1996136,
title = {Computational physics of the mind},
journal = {Computer Physics Communications},
volume = {97},
number = {1},
pages = {136-153},
year = {1996},
note = {High-Performance Computing in Science},
issn = {0010-4655},
doi = {https://doi.org/10.1016/0010-4655(96)00027-6},
url = {https://www.sciencedirect.com/science/article/pii/0010465596000276},
author = {Włodzisław Duch},
abstract = {In the XIX century and earlier physicists such as Newton, Mayer, Hooke, Helmholtz and Mach were actively engaged in the research on psychophysics, trying to relate psychological sensations to intensities of physical stimuli. Computational physics allows to simulate complex neural processes giving a chance to answer not only the original psychophysical questions but also to create models of the mind. In this paper several approaches relevant to modeling of the mind are outlined. Since direct modeling of the brain functions is rather limited due to the complexity of such models a number of approximations is introduced. The path from the brain, or computational neurosciences, to the mind, or cognitive sciences, is sketched, with emphasis on higher cognitive functions such as memory and consciousness. No fundamental problems in understanding of the mind seem to arise. From a computational point of view realistic models require massively parallel architectures.}
}
@article{CAMERON2019102,
title = {Education in Process Systems Engineering: Why it matters more than ever and how it can be structured},
journal = {Computers & Chemical Engineering},
volume = {126},
pages = {102-112},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418311773},
author = {Ian T. Cameron and Sebastian Engell and Christos Georgakis and Norbert Asprion and Dominique Bonvin and Furong Gao and Dimitrios I. Gerogiorgis and Ignacio E. Grossmann and Sandro Macchietto and Heinz A. Preisig and Brent R. Young},
abstract = {This position paper is an outcome of discussions that took place at the third FIPSE Symposium in Rhodes, Greece, between June 20–22, 2016 (http://fi-in-pse.org). The FIPSE objective is to discuss open research challenges in topics of Process Systems Engineering (PSE). Here, we discuss the societal and industrial context in which systems thinking and Process Systems Engineering provide indispensable skills and tools for generating innovative solutions to complex problems. We further highlight the present and future challenges that require systems approaches and tools to address not only ‘grand’ challenges but any complex socio-technical challenge. The current state of Process Systems Engineering (PSE) education in the area of chemical and biochemical engineering is considered. We discuss approaches and content at both the unit learning level and at the curriculum level that will enhance the graduates’ capabilities to meet the future challenges they will be facing. PSE principles are important in their own right, but importantly they provide significant opportunities to aid the integration of learning in the basic and engineering sciences across the whole curriculum. This fact is crucial in curriculum design and implementation, such that our graduates benefit to the maximum extent from their learning.}
}
@article{RIVEST19931,
title = {On Choosing between Experimenting and Thinking when Learning},
journal = {Information and Computation},
volume = {106},
number = {1},
pages = {1-25},
year = {1993},
issn = {0890-5401},
doi = {https://doi.org/10.1006/inco.1993.1047},
url = {https://www.sciencedirect.com/science/article/pii/S0890540183710473},
author = {R.L. Rivest and R.H. Sloan},
abstract = {We introduce a model of inductive inference, or learning, that extends the conventional Bayesian approach by explicitly considering the computational cost of formulating predictions to be tested. We view the learner as a scientist who must divide her time between doing experiments and deducing predictions from promising theories, and we wish to know how she can do so most effectively. We explore several approaches based on the cost of making a prediction relative to the cost of performing an experiment. The resulting strategies share many qualitative characteristics with "real" science. This model is significant for the following reasons: •It allows us to study how a scientist might go about acquiring knowledge in a world where (as in real life) both performing experiments and making predictions from theories require time and effort.•It lays the foundation for a rigorous machine-implementable notion of "subjective probability." Good (1959, , 443-447) argues persuasively that subjective probability is at the heart of probability theory. Previous treatments of subjective probability do not handle the complication that the learner′s subjective probabilities may change as the result of pure thinking; our model captures this and other effects in a realistic manner. In addition, we begin to answer the question of how to trade off versus -a question that is fundamental for computers that must exist in the world and learn from their experience.}
}
@article{OSINGA2022103298,
title = {Big data in agriculture: Between opportunity and solution},
journal = {Agricultural Systems},
volume = {195},
pages = {103298},
year = {2022},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2021.103298},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X21002511},
author = {Sjoukje A. Osinga and Dilli Paudel and Spiros A. Mouzakitis and Ioannis N. Athanasiadis},
keywords = {Big data solutions, Precision Agriculture, Case study, Stakeholders, Technological maturity level, Mixed-method approach},
abstract = {CONTEXT
Big data applications in agriculture evolve fast, as more experience, applications, good practices and computational power become available. Actual solutions to real-life problems are scarce. What characterizes the adoption of big data problems to solutions and to what extent is there a match between them?
OBJECTIVE
We aim to assess the conditions of the adoption of big data technologies in agricultural applications, based on the investigation of twelve real-life practical use cases in the precision agriculture and livestock domain.
METHODS
We use a mixed method approach: a case study research around the twelve use cases of Horizon 2020 project CYBELE, varying from precision arable and livestock farming to fishing and food security, and a stakeholder survey (n = 56). Our analysis focuses on four perspectives: (1) the drivers of change that initiated the use cases; (2) the big data characteristics of the problem; (3) the technological maturity level of the solution both at start and end of the project; (4) the stakeholder perspective.
RESULTS AND CONCLUSIONS
Results show that the use cases’ drivers of change are a combination of data-, technology, research- and commercial interests; most have at least a research drive. The big data characteristics (volume, velocity, variety, veracity) are well-represented, with most emphasis on velocity and variety. Technology readiness levels show that the majority of use cases started at experimental or lab environment stage and aims at a technical maturity of real-world small-scale deployment. Stakeholders’ main concern is cost, user friendliness and to embed the solution within their current work practice. The adoption of better-matching big data solutions is modest. Big data solutions do not work out-of-the-box when changing application domains. Additional technology development is needed for addressing the idiosyncrasies of agricultural applications.
SIGNIFICANCE
We add a practical, empirical assessment of the current status of big data problems and solutions to the existing body of mainly theoretical knowledge. We considered the CYBELE research project as our laboratory for this. Our strength is that we interviewed the use case representatives in person, and that we included the stakeholders’ perspective in our results. Large-scale deployments need effective interdisciplinary approaches and long-term project horizons to address issues emerging from big data characteristics, and to avoid compartmentalization of agricultural sciences. We need both an engineering perspective – to make things work in practice – and a systems thinking perspective – to offer holistic, integrated solutions.}
}
@article{BEHARA2009195,
title = {Parallel finite element computation of incompressible flows},
journal = {Parallel Computing},
volume = {35},
number = {4},
pages = {195-212},
year = {2009},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2008.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819108001348},
author = {Suresh Behara and Sanjay Mittal},
keywords = {Navier–Stokes equations, Parallel computing, Superlinear speedup, Wake, Transition, Wake instabilities},
abstract = {A stabilized finite element formulation for three-dimensional unsteady incompressible flows is implemented on a distributed memory parallel computer. A matrix-free version of the GMRES algorithm is utilized to solve the equation systems in an implicit manner. The scalability of the computations on a 64-processor Linux cluster is evaluated for moderate to large size problems. A method for estimating the speedup for large-scale problems, where computations on a single processor is not possible, is proposed. Superlinear speedup is observed, perhaps for the first time, for a large-scale problem that is associated with more than 44 million nodes and 176 million equations. The performance of the various subactivities of the program is monitored to investigate the cause. It is found that the formation of the RHS vector and the preconditioner achieves a very high level of superlinear speedup as the number of processors increase. As a result, even though the network time for interprocessor communication increases with increase in processors, an overall superlinear speedup is realized for large-scale problems. The superlinear speedup is attributed to cache related effects. A comparison between the performance of matrix and matrix-free versions of the GMRES algorithm is carried out. It is found that for large-scale applications the matrix-free version outperforms its counterpart for reasonable dimensions of the Kyrylov subspace. The effect of mesh partitioning on the scalability is also studied. A significant reduction in communication time is observed with partitioning that leads to an overall improvement of speedup. The parallel implementation is utilized to study the wake instabilities in flow past a stationary circular cylinder at Re=150, 200 and 300. The Re=150 flow is found to be two-dimensional while mode-A and mode-B instabilities are observed at Re=200 and 300, respectively. The Re=300 flow is associated with a low frequency modulation in addition to the vortex shedding frequency.}
}
@article{CHENG2013267,
title = {Shape-anisotropic particles at curved fluid interfaces and role of Laplace pressure: A computational study},
journal = {Journal of Colloid and Interface Science},
volume = {402},
pages = {267-278},
year = {2013},
issn = {0021-9797},
doi = {https://doi.org/10.1016/j.jcis.2013.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S0021979713003056},
author = {Tian-Le Cheng and Yu U. Wang},
keywords = {Capillary forces, Surface tension, Laplace pressure, Diffuse interface field approach, Gibbs–Duhem relation, Shape anisotropy, Pickering emulsions},
abstract = {The self-assembly behavior of shape-anisotropic particles at curved fluid interfaces is computationally investigated by diffuse interface field approach (DIFA). A Gibbs–Duhem-type thermodynamic formalism is introduced to treat heterogeneous pressure within the phenomenological model, in agreement with Young–Laplace equation. Computer simulations are performed to study the effects of capillary forces (interfacial tension and Laplace pressure) on particle self-assembly at fluid interfaces in various two-dimensional cases. For isolated particles, it is found that the equilibrium liquid interface remains circular and particles of different shapes do not disturb the homogeneous curvature of liquid interface, while the equilibrium position, orientation and stability of a particle at the liquid interface depend on its shape and initial location with respect to the liquid interface. For interacting particles, the curvature of local liquid interfaces is different from the apparent curvature of the particle shell; nevertheless, irrespective of the particle shapes, a particle-coated droplet always tends to deform into a circular morphology under positive Laplace pressure, loses mechanical stability and collapses under negative Laplace pressure, while adapts to any morphology and stays in neutral equilibrium under zero Laplace pressure. Finally, the collective behaviors of particles and Laplace pressure evolution in bicontinuous interfacially jammed emulsion gels (bijels) are investigated.}
}
@article{VAHLDICK2020100037,
title = {A blocks-based serious game to support introductory computer programming in undergraduate education},
journal = {Computers in Human Behavior Reports},
volume = {2},
pages = {100037},
year = {2020},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2020.100037},
url = {https://www.sciencedirect.com/science/article/pii/S2451958820300373},
author = {Adilson Vahldick and Paulo Roberto Farah and Maria José Marcelino and António José Mendes},
keywords = {Computer programming learning, Blocks-based approach, Serious games},
abstract = {Blocks-based environments have been used to promote computational thinking (CT) and programming learning mostly in elementary and middle schools. In many countries, like Brazil and Portugal, isolated initiatives have been launched to promote CT learning, but until now there is no evidence of a widespread use of this type of environments. Consequently, it is not common that students that reach higher education nowadays are familiar with CT and programming. This paper presents the development of a serious game to support the learning of basic computer programming. It is a blocks-based environment including also resources that allow the teacher to follow the student’s progress and customize in-game tasks. Four cycles of experiments were conducted, improving both the game and how it was used. Based on the results of these experiences, the key contribution of this paper is a set of fourteen findings and recommendations to the creation and use of a game-based approach to support introductory computer programming learning for novices.}
}
@article{VAHDANJOO2025101287,
title = {Digital transformation of the agri-food system},
journal = {Current Opinion in Food Science},
volume = {63},
pages = {101287},
year = {2025},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2025.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2214799325000177},
author = {Mahdi Vahdanjoo and Claus Grøn Sørensen and Michael Nørremark},
abstract = {The purpose of this paper is to examine the role of digital transformation in the agri-food sector. The study emphasizes digitalization as both an enabler of production efficiency and a radical innovator, redesigning business models and agricultural practices. The study explores the development of applications and products that connect consumers, supply chain actors, and producers, leading to customized food products. It highlights the notion of circular agri-food systems for feedback loops in the value chain, minimizing waste and integrating environmental and social values. Also, the paper explores the challenges in digital adoption, including technical barriers, privacy, and security concerns. To overcome these challenges, an interdisciplinary approach is proposed, merging technological, ecological, economic, and governance insights. Key needs identified for successful digital transformation include enhanced data processing, technological convergence, sustainability awareness, interoperability, and user adoption. The conclusion stresses the importance of invoking systemic thinking, user-friendly designs, and interdisciplinary collaboration in making sure that digital innovations enable a sustainable and resilient food production system.}
}
@article{AIZENBERG199987,
title = {One computational approach in support of the Riemann hypothesis},
journal = {Computers & Mathematics with Applications},
volume = {37},
number = {1},
pages = {87-94},
year = {1999},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(98)00244-2},
url = {https://www.sciencedirect.com/science/article/pii/S0898122198002442},
author = {L. Aizenberg and V. Adamchik and V.E. Levit},
keywords = {ς-function, Riemann Hypothesis, Analytic continuation of a function given on a part of its boundary, Holomorphic functions, Conformal mappings, Unit disk, Computational experiments},
abstract = {Some of the results on the criteria for the existence of an analytic continuation into a domain of a function given on a part of its boundary obtained by one of the authors are applied to the Riemann Hypothesis on the zeta-function zeroes. We include all of the basic structural information needed on the previous results on analytic continuation. Some comprehensive numerical experiments have been performed. We have found two important trends in the associated numerical results. The first one is that these findings favor the view that the Riemann Hypothesis is valid. The second one corresponds to a new conjecture on monotonic behavior of some sequences of integrals. The computational experiments have been performed with the Mathematica V3.0.}
}
@article{NIKIFORIDOU20124830,
title = {Risk Literacy in Early Childhood Education Under a Lifelong Perspective},
journal = {Procedia - Social and Behavioral Sciences},
volume = {46},
pages = {4830-4833},
year = {2012},
note = {4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.06.343},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812020794},
author = {Zoi Nikiforidou and Jenny Pange and Theodore Chadjipadelis},
keywords = {First keywords, second keywords, third keywords, forth keywords},
abstract = {Risk entails every action, every level and every perspective of our lives. The ability to make advantageous decisions, to deal with uncertainties, to infer and estimate more or less probable outcomes, to manage risky or riskless situations compose the wider notion of risk literacy and may be inserted from formal preschool education. The current paper aims to enlighten the notion of risk literacy and safety education as a necessity in establishing pupils ready to accept failure, to achieve success, to take initiatives, to become self-competent, to develop probabilistic and statistical thinking, to confront uncertainty and in turn to face the challenges of modern risk society. It is argued that within the formal settings of preschool education, through developmentally appropriate activities, opportunities may be implemented in order to encourage children as future citizens to construct risk literate personalities. It is concluded that risk perception and management imply awareness, assessment, avoidance and adaptation and are connected with growth, maturity, practice, experiences, intuitions and computations.}
}
@article{MEDINAOLIVA201338,
title = {PRM-based patterns for knowledge formalisation of industrial systems to support maintenance strategies assessment},
journal = {Reliability Engineering & System Safety},
volume = {116},
pages = {38-56},
year = {2013},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2013.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S0951832013000616},
author = {G. Medina-Oliva and P. Weber and B. Iung},
keywords = {Maintenance strategies, Performances analysis, Decision-making, Bayesian Networks (BN), Probabilistic Relational Model (PRM)},
abstract = {The production system and its maintenance system must be now developed on “system thinking” paradigm in order to guarantee that Key Performance Indicators (KPI) will be optimized all along the production system (operation) life. In a recursive way, maintenance system engineering has to integrate also KPI considerations with regards to its own enabling systems. Thus this paper develops a system-based methodology wherein a set of KPIs is computed in order to verify if the objectives of the production and maintenance systems are satisfied. In order to help the decision-making process for maintenance managers, a “unified” generic model have been developed. This model integrates (a) the interactions of the maintenance system with its enabling systems, (b) the impact of the maintenance strategies through the computation of some key performance indicators, and (c) different kinds of knowledge regarding the maintenance system and the system of interest, including quantitative and qualitative knowledge. This methodology is based on an executable unified model built with Probabilistic Relational Model (PRM). PRM allows a modular representation and inferences computation of large size models. The methodology added-value is shown on a test-bench.}
}
@article{TETEWSKY1986202,
title = {Conceptual and lexical determinants of nonentrenched thinking},
journal = {Journal of Memory and Language},
volume = {25},
number = {2},
pages = {202-225},
year = {1986},
issn = {0749-596X},
doi = {https://doi.org/10.1016/0749-596X(86)90030-6},
url = {https://www.sciencedirect.com/science/article/pii/0749596X86900306},
author = {Sheldon J Tetewsky and Robert J Sternberg},
abstract = {Two experiments investigating information-processing consequences of entrenched and nonentrenched concepts are reported. An attempt is made to distinguish between these two kinds of concepts by using two variables—the naturalness of the occurrence described by a concept and the familiarity of the name used to refer to that occurrence. In each experiment a given conceptual system was expressed in four alternative forms by crossing concept familiarity (naturalness) with lexical familiarity. The experiments used a concept-selection task in which subjects were required to characterize an event based on a preliminary piece of information and a final, confirmatory piece of information. The results indicated that the locus of nonentrenchment lies in using a familiar name to identify an unfamiliar occurrence or in using an unfamiliar name to identify a familiar occurrence. An information-processing model of task performance provided a very good account of the latency data and scores from the concept-selection task correlated with scores from a set of psychometric reasoning tests. The distinction between entrenched and nonentrenched concepts can be interpreted in terms of interference theory, and it also has implications for the way we think about induction and human intelligence.}
}
@incollection{ADAMS2016283,
title = {Chapter 16 - Brain Computations in Schizophrenia},
editor = {Ted Abel and Thomas Nickl-Jockschat},
booktitle = {The Neurobiology of Schizophrenia},
publisher = {Academic Press},
address = {San Diego},
pages = {283-295},
year = {2016},
isbn = {978-0-12-801829-3},
doi = {https://doi.org/10.1016/B978-0-12-801829-3.00024-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128018293000240},
author = {R.A. Adams and K.J. Friston},
keywords = {Schizophrenia, Bayesian brain, precision, aberrant salience, reversal, predictive coding, NMDAR, GABA, delusions},
abstract = {Because the brain performs Bayesian inference for the causes of its sensory data, the synaptic gain could encode the precision (inverse variance) of its beliefs using a hierarchical generative model and predictive coding. Several neurobiological risk factors for schizophrenia (NMDAR and GABAergic interneuron hypofunction) reduce both synaptic and “oscillatory” gain at high hierarchical areas. This could impair the encoding of precision at higher levels of the brain’s hierarchical model and increase expected precision at lower levels. This imbalance can account for many neurobiological and phenomenological findings in schizophrenia. Striatal D2R hyperactivity may increase the precision of current policies by inhibiting behavioral or cognitive switching. This could be a (dysfunctional) consequence of or even an attempt to compensate for prefrontal or hippocampal pathology. This D2R hyperactivity may also reduce learning from positive outcomes and affect the encoding of motivational (or informational) salience.}
}
@article{MUNSON2019100736,
title = {After eliciting: Variation in elementary mathematics teachers’ discursive pathways during collaborative problem solving},
journal = {The Journal of Mathematical Behavior},
volume = {56},
pages = {100736},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2019.100736},
url = {https://www.sciencedirect.com/science/article/pii/S073231231930046X},
author = {Jen Munson},
keywords = {Classroom discourse, Eliciting, Responsiveness, Student understanding},
abstract = {Mathematics teachers are called on to craft instruction that centers students’ mathematical ideas and creates consistent, pervasive opportunities for meaning-making through discourse. In the context of collaborative problem solving, teachers can use eliciting and probing to uncover student thinking while students work together to develop mathematical ideas and strategies. After eliciting and probing, teachers can further respond to the student thinking that has been revealed. This study explored the discursive pathways two fourth grade mathematics teachers used after eliciting student thinking, when their aim was to be responsive to and advance student thinking. Drawing on interactions (n = 97) from nine lessons, qualitative analysis identified five distinct discursive pathways after eliciting, two of which, praise and funneling, were associated with the nature of student understanding uncovered during eliciting. Implications for future research and professional development on teacher-student discourse are discussed.}
}
@article{MA2024103893,
title = {Secure outsourced decryption for FHE-based privacy-preserving cloud computing},
journal = {Journal of Information Security and Applications},
volume = {86},
pages = {103893},
year = {2024},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2024.103893},
url = {https://www.sciencedirect.com/science/article/pii/S2214212624001959},
author = {Xirong Ma and Chuan Li and Yuchang Hu and Yunting Tao and Yali Jiang and Yanbin Li and Fanyu Kong and Chunpeng Ge},
keywords = {Privacy-preserving computation, Outsourced computing, Homomorphic encryption},
abstract = {The demand for processing vast volumes of data has surged dramatically due to the advancement of machine learning technology. Large-scale data processing necessitates substantial computational resources, prompting individuals and enterprises to turn to cloud services. Accompanying this trend is a growing concern regarding data leakage and misuse. Homomorphic encryption (HE) is one solution for safeguarding data privacy, enabling encrypted data to be processed securely in the cloud. However, the encryption and decryption routines of some HE schemes require considerable computational resources, presenting non-trivial work for clients. In this paper, we propose an outsourced decryption protocol for the prevailing RLWE-based fully homomorphic encryption schemes. The protocol splits the original decryption into two routines, with the computationally intensive part executed remotely by the cloud. Its security relies on an invariant of the NTRU-search problem with a newly designed blinding key distribution. Cryptographic analyses are conducted to configure protocol parameters across varying security levels. Our experiments demonstrate that the proposed protocol achieves up to a 67% acceleration in the client-side computation, accompanied by a 50% reduction in space usage.}
}
@article{PANANGADEN201410,
title = {Causality in physics and computation},
journal = {Theoretical Computer Science},
volume = {546},
pages = {10-16},
year = {2014},
note = {Models of Interaction: Essays in Honour of Glynn Winskel},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2014.02.041},
url = {https://www.sciencedirect.com/science/article/pii/S0304397514001674},
author = {Prakash Panangaden},
keywords = {Causal structure, Event structure, Spacetime, Petri nets},
abstract = {Glynn Winskel has had enormous influence on the study of causal structure in computer science. In this brief note, I discuss analogous concepts in relativity where also causality plays a fundamental role. I discuss spacetime structure in a series of layers and emphasize the role of causal structure. I close with some comparisons between causality in relativity and in distributed computing systems.}
}
@article{SUTHAR202431,
title = {Practical exercises of computer-aided process synthesis for chemical engineering undergraduates},
journal = {Education for Chemical Engineers},
volume = {48},
pages = {31-43},
year = {2024},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2024.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1749772824000071},
author = {Krunal J. Suthar and Aesha Mehta and Swapna Rekha Panda and Hitesh Panchal and Rakesh Sinha},
keywords = {computational tools, lifelong learning, laboratory learning, process synthesis},
abstract = {The study presents ten different exercises covering various computational tools. These exercises are practical applications presented to improve the understanding and skills of students in important concepts of chemical-aided process synthesis. A few exercises aim to build a foundation in computational techniques for chemical engineering undergraduates. The exercises are based on a spreadsheet that covers the design of regression analysis to find the optimum Antoine constants, array calculation for multicomponent distillation material balance, and the generation of a Gantt chart to plan and study the activities of batch processes. The other exercises included an introduction to process simulation, simulation, and reactor rating, and a simulation of multicomponent shortcut distillation. These exercises provide students with hands-on experience in utilizing process simulation software essential for analysing and optimizing chemical processes in real-world scenarios. The exercises also included the design of a heat exchanger network and solving a linear programming problem. An anonymous survey was collected from the cohort that had undergone the exercises, and the practical grades were compared with the batch that did not study the proposed exercises. Additionally, student feedback on practical exercises was collected. Based on the experience of the course coordinator and the collected feedback from participants, it was clear that the exercises helped students to inculcate critical thinking and self-learning abilities. An article essentially sheds light on the computer-aided practical exercises that enable chemical engineering graduates to engage in lifelong learning.}
}
@article{KUDARIYAWAR2016193,
title = {Computational study of instabilities in a rectangular natural circulation loop using 3D CFD simulation},
journal = {International Journal of Thermal Sciences},
volume = {101},
pages = {193-206},
year = {2016},
issn = {1290-0729},
doi = {https://doi.org/10.1016/j.ijthermalsci.2015.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1290072915003440},
author = {Jayaraj Yallappa Kudariyawar and Abhijeet Mohan Vaidya and Naresh Kumar Maheshwari and Polepalle Satyamurthy},
keywords = {Natural circulation loop, 3D CFD simulation, Instability},
abstract = {Steady state and transient characteristics of a natural circulation loop working with water are obtained. For this purpose, 3D steady state and transient CFD simulations are performed. The CFD model includes pipe thickness as well as secondary side coolant passage apart from primary side. Steady state and transient characteristics are computed for various configurations i.e. Vertical Heater Vertical Cooler (VHVC), Horizontal Heater Horizontal Cooler (HHHC), etc. Steady state data was compared with available correlations. Flow initiation transients were compared with experimental data. Both the steady state and transient results are found to be in good agreement with previously published data. The reason for formation of unidirectional and bi-directional pulsing in HHHC configuration at different powers is explained with the help of temperature fields at different instants of time. Effect of sudden power rise/power step back on instability in HHHC configuration is estimated using CFD simulations.}
}
@article{SANCHEZTORRUBIA201212177,
title = {An approach to automatic learning assessment based on the computational theory of perceptions},
journal = {Expert Systems with Applications},
volume = {39},
number = {15},
pages = {12177-12191},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.04.069},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412006665},
author = {M. Gloria Sánchez-Torrubia and Carmen Torres-Blanc and Gracian Trivino},
keywords = {Automatic learning assessment, Computing with words and perceptions, Granular linguistic model of a phenomenon},
abstract = {E-learning systems output a huge quantity of data on a learning process. However, it takes a lot of specialist human resources to manually process these data and generate an assessment report. Additionally, for formative assessment, the report should state the attainment level of the learning goals defined by the instructor. This paper describes the use of the granular linguistic model of a phenomenon (GLMP) to model the assessment of the learning process and implement the automated generation of an assessment report. GLMP is based on fuzzy logic and the computational theory of perceptions. This technique is useful for implementing complex assessment criteria using inference systems based on linguistic rules. Apart from the grade, the model also generates a detailed natural language progress report on the achieved proficiency level, based exclusively on the objective data gathered from correct and incorrect responses. This is illustrated by applying the model to the assessment of Dijkstra’s algorithm learning using a visual simulation-based graph algorithm learning environment, called GRAPHs.}
}
@article{NIKNAM20112805,
title = {Non-smooth economic dispatch computation by fuzzy and self adaptive particle swarm optimization},
journal = {Applied Soft Computing},
volume = {11},
number = {2},
pages = {2805-2817},
year = {2011},
note = {The Impact of Soft Computing for the Progress of Artificial Intelligence},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2010.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S1568494610002875},
author = {Taher Niknam and Hasan Doagou Mojarrad and Hamed Zeinoddini Meymand},
keywords = {Economic dispatch, New adaptive particle swarm optimization (NAPSO), Mutation operator, Multi-fuel effects, Self-adaptive parameter control},
abstract = {Economic dispatch (ED) problem is a nonlinear and non-smooth optimization problem when valve-point effects, multi-fuel effects and prohibited operating zones (POZs) have been considered. This paper presents an efficient evolutionary method for a constrained ED problem using the new adaptive particle swarm optimization (NAPSO) algorithm. The original PSO has difficulties in premature convergence, performance and the diversity loss in optimization process as well as appropriate tuning of its parameters. In the proposed algorithm, to improve the global searching capability and prevent the convergence to local minima, a new mutation is integrated with adaptive particle swarm optimization (APSO). In APSO, the inertia weight is tuned by using fuzzy IF/THEN rules and the cognitive and the social parameters are self-adaptively adjusted. The proposed NAPSO algorithm is validated on test systems consisting of 6, 10, 15, 40 and 80 generators with the objective functions possessing prohibited zones, multi-fuel effects and valve-point loading effects. The research results reveal the effectiveness and applicability of the proposed algorithm to the practical ED problem.}
}
@incollection{JOHNSON201435,
title = {Chapter 3 - Computational and Process Models of Decision Making in Psychology and Behavioral Economics},
editor = {Paul W. Glimcher and Ernst Fehr},
booktitle = {Neuroeconomics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {35-47},
year = {2014},
isbn = {978-0-12-416008-8},
doi = {https://doi.org/10.1016/B978-0-12-416008-8.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160088000036},
author = {Eric J. Johnson and Roger Ratcliff},
keywords = {Computation Process Models, Decision Neuroscience, Drift-Diffusion Models, economic theory, Intertemporal Choice, Riskless Choice, Risky Choice},
abstract = {This chapter reviews models of choice on two levels: The first concerns the descriptions of choice and their evolution from normative models of how choices should be make to more behaviorally realistic models, more consistent with data showing that choice depends heavily on context. We present brief overviews of risky and riskless choice models and data and for choice over time. We then turn to computational process models, a more recent class of models that make prediction for multiple properties of the decision process beyond simply what is chosen, including predicting the distribution of errors and decision times.These models are typically applied to simpler choices, but have found great use in contemporary neuroscience.}
}
@article{MARTINRAMOS201751,
title = {First exposure to Arduino through peer-coaching: Impact on students' attitudes towards programming},
journal = {Computers in Human Behavior},
volume = {76},
pages = {51-58},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217304193},
author = {Pablo Martín-Ramos and Maria João Lopes and M. Margarida {Lima da Silva} and Pedro E.B. Gomes and Pedro S. {Pereira da Silva} and José P.P. Domingues and Manuela {Ramos Silva}},
keywords = {Attitudes survey, Arduino, High school, Programming, Peer coaching},
abstract = {In this paper we report the work that jeKnowledge (Júnior Empresa da Faculdade de Ciências e Tecnologias da Universidade de Coimbra), a student-led initiative, has done in the ‘jeKnowledge academy’ courses to actively engage Portuguese high-school students in STEM education through hands-on projects based on the low-cost Arduino platform. F2F activities, based on a peer-assisted learning strategy, were complemented with tutorials and more advanced project suggestions in a blog. Pre and post surveys on students' attitudes towards programming and peer-coaching were administered to pre-university and first year college participants, finding an overall increase in the Likert scale for all the programming-related constructs under study (confidence, interest, gender, usefulness and professional) after the introductory course. As regards the peer-based learning approach, younger students seemed to be more eager to be taught in a less formal way than their older counterparts. The course resulted in high degrees of satisfaction for both the student tutors and their tutees.}
}
@article{JONES2000571,
title = {Unstructured mesh computations on CCMs},
journal = {Advances in Engineering Software},
volume = {31},
number = {8},
pages = {571-580},
year = {2000},
issn = {0965-9978},
doi = {https://doi.org/10.1016/S0965-9978(00)00012-0},
url = {https://www.sciencedirect.com/science/article/pii/S0965997800000120},
author = {M.T Jones and K Ramachandran},
keywords = {Configurable computing, Floating point, Finite element},
abstract = {Configurable Computing Machines (CCMs) have been able to provide orders of magnitude increases in execution rates for applications such as image processing, signal processing, and automatic target recognition. This paper describes the use of CCMs to accelerate complex, large-scale scientific computations. These applications present a challenge for CCMs because of their large size, hundreds of thousands of lines of code, and the unstructured nature of the computations. This paper describes strategies for accelerating scientific computations on CCMs and demonstrates the effectiveness of one such strategy on the Annapolis Micro Systems WildForce board. Results from this implementation are analyzed.}
}
@article{CRONIN2022100987,
title = {Analysis of tutors’ responses to students’ queries in a second linear algebra course at a mathematics support center},
journal = {The Journal of Mathematical Behavior},
volume = {67},
pages = {100987},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100987},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000554},
author = {Anthony Cronin and Sepideh Stewart},
keywords = {Mathematics tutors, Second courses in linear algebra, Mathematics support center, Feedback, Tutors’ tactics, Advanced mathematical thinking},
abstract = {This paper analyses six years of tutor feedback produced after inquiries made by students in a second linear algebra course at a university mathematics support center (MSC). We utilized Mason’s (2002) pedagogical tactics to build a model to analyze MSC tutors' feedback responding to these students’ queries. The aim of this research was to investigate the nature of students’ difficulties with concepts in a second linear algebra course that emphasizes theories and proof, in addition to examining the tactics employed by tutors to resolve student difficulties. We analyzed 227 feedback comments from 44 tutors based on their interactions with 82 students over six years. Our findings indicated that the most common areas of difficulty were basis, vector space, subspace, span, and proof. Tutor tactics deployed included ‘being mathematical’, ‘simplifying and complexifying’, and ‘worked examples’. We also discuss some implications for linear algebra tutor training.}
}
@incollection{BARBAROSSA2018419,
title = {Chapter 16 - The Edge Cloud: A Holistic View of Communication, Computation, and Caching},
editor = {Petar M. Djurić and Cédric Richard},
booktitle = {Cooperative and Graph Signal Processing},
publisher = {Academic Press},
pages = {419-444},
year = {2018},
isbn = {978-0-12-813677-5},
doi = {https://doi.org/10.1016/B978-0-12-813677-5.00016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012813677500016X},
author = {Sergio Barbarossa and Stefania Sardellitti and Elena Ceci and Mattia Merluzzi},
keywords = {5G networks, Wireless communications, Graph-based learning},
abstract = {The evolution of communication networks shows a clear shift of focus from just improving the communications aspects to enabling new important services, from Industry 4.0 to automated driving, virtual/augmented reality, the Internet of Things (IoT), and so on. This trend is evident in the roadmap planned for the deployment of the fifth-generation (5G) communication networks. This ambitious goal requires a paradigm shift toward a vision that looks at communication, computation, and caching (3C) resources as three components of a single holistic system. The further step is to bring these 3C resources closer to the mobile user, at the edge of the network, to enable very low latency and high reliability services. The scope of this chapter is to show that signal processing techniques can play a key role in this new vision. In particular, we motivate the joint optimization of 3C resources. Then we show how graph-based representations can play a key role in building effective learning methods and devising innovative resource allocation techniques.}
}
@article{CASTRO20242377,
title = {Product Customization based on Digital Twin and Cloud Manufacturing within a Decentralized Production System},
journal = {Procedia Computer Science},
volume = {239},
pages = {2377-2384},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.431},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924016752},
author = {Hélio Castro and Fernando Câmara and Paulo Ávila and Luís Ferreira and Manuela Cruz-Cunha},
keywords = {Industry 4.0, Digital Twin, Cyber-Physical System, Smart Factory, Product Customization, Cloud Manufacturing},
abstract = {Industry 4.0 represents a turning point in the thinking of the production model since it is based on digitalized production systems with the aim of improving productivity, product quality, and delivery time to the customer. The digitalization and evolution of information technology allowed the emulation of production system virtual models, namely in the concept of Digital Twin (DT), with the ability to simulate different scenarios providing support for better decision making. This concept not only represents a virtual copy of the physical world that obtains information about the state of the value chain but also illustrates a system capable of changing the development of the production activity according to the fulfillment of the intended business goals. In literature, the concept of the Digital Twin is exhaustively treated as a stand-alone factory (one digital factory represents one physical factory) and underestimates the possibility of a DT oriented to a customized product (a project) that requires decentralized production systems. This paper brings to discussion the relevance of product customized applying DT to smart customization, and the inclusion of decentralized production systems supported by Cloud Manufacturing.}
}
@article{CIRAOLO201378,
title = {A computational method for the Helmholtz equation in unbounded domains based on the minimization of an integral functional},
journal = {Journal of Computational Physics},
volume = {246},
pages = {78-95},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113002258},
author = {Giulio Ciraolo and Francesco Gargano and Vincenzo Sciacca},
keywords = {Helmholtz equation, Transparent boundary conditions, Minimization of integral functionals},
abstract = {We study a new approach to the problem of transparent boundary conditions for the Helmholtz equation in unbounded domains. Our approach is based on the minimization of an integral functional arising from a volume integral formulation of the radiation condition. The index of refraction does not need to be constant at infinity and may have some angular dependency as well as perturbations. We prove analytical results on the convergence of the approximate solution. Numerical examples for different shapes of the artificial boundary and for non-constant indexes of refraction will be presented.}
}
@article{TEZDUYAR19992039,
title = {Methods for parallel computation of complex flow problems},
journal = {Parallel Computing},
volume = {25},
number = {13},
pages = {2039-2066},
year = {1999},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(99)00080-0},
url = {https://www.sciencedirect.com/science/article/pii/S0167819199000800},
author = {Tayfun Tezduyar and Yasuo Osawa},
keywords = {Computational fluid dynamics, Flow simulation, Stabilization methods, Compressible flow, Incompressible flow, Multidomain computational methods},
abstract = {This paper is an overview of some of the methods developed by the Team for Advanced Flow Simulation and Modeling (T★AFSM) [http://www.mems.rice.edu/TAFSM/] to support flow simulation and modeling in a number of “Targeted Challenges”. The “Targeted Challenges” include unsteady flows with interfaces, fluid–object and fluid–structure interactions, airdrop systems, and air circulation and contaminant dispersion. The methods developed include special numerical stabilization methods for compressible and incompressible flows, methods for moving boundaries and interfaces, advanced mesh management methods, and multi-domain computational methods. We include in this paper a number of numerical examples from the simulation of complex flow problems.}
}
@article{AIMONE2009187,
title = {Computational Influence of Adult Neurogenesis on Memory Encoding},
journal = {Neuron},
volume = {61},
number = {2},
pages = {187-202},
year = {2009},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2008.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S0896627308010192},
author = {James B. Aimone and Janet Wiles and Fred H. Gage},
keywords = {SYSNEURO, MOLNEURO, STEMCELL},
abstract = {Summary
Adult neurogenesis in the hippocampus leads to the incorporation of thousands of new granule cells into the dentate gyrus every month, but its function remains unclear. Here, we present computational evidence that indicates that adult neurogenesis may make three separate but related contributions to memory formation. First, immature neurons introduce a degree of similarity to memories learned at the same time, a process we refer to as pattern integration. Second, the extended maturation and change in excitability of these neurons make this added similarity a time-dependent effect, supporting the possibility that temporal information is included in new hippocampal memories. Finally, our model suggests that the experience-dependent addition of neurons results in a dentate gyrus network well suited for encoding new memories in familiar contexts while treating novel contexts differently. Taken together, these results indicate that new granule cells may affect hippocampal function in several unique and previously unpredicted ways.}
}
@article{MCKELVEY2009476,
title = {Designing an electronic auction market for complex ‘smart parts’ logistics: Options based on LeBaron's computational stock market},
journal = {International Journal of Production Economics},
volume = {120},
number = {2},
pages = {476-494},
year = {2009},
note = {Special Issue on Introduction to Design and Analysis of Production Systems},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2009.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925527309000899},
author = {Bill McKelvey and Christine Wycisk and Michael Hülsmann},
keywords = {Supply chain management, Electronic auction market, I&C technologies, Complexity theory, Neural networks},
abstract = {Modern technologies, such as RFID, offer never-before seen learning abilities to parts moving in supply chains. Logistics systems may be understood as complex adaptive logistics systems (CALS). They also may be conceived as electronic auction markets as ‘smart parts’ bid for the best routing and pricing from transportation firms. To ensure the world-wide functionality and efficiency of CALS transportation markets, we suggest the utility of an agent-based computational market design based on Blake LeBaron's stock-market model. Given that parts may be more or less smart, markets more or less complex, and self-organizing CALS systems probabilistically subject to the bullwhip effect, we suggest nine different computational CALS market-design options, offering more adaptivity to unexpected environmental contingencies.}
}
@article{GOMEZCARRILLO2023296,
title = {Integrating neuroscience in psychiatry: a cultural–ecosocial systemic approach},
journal = {The Lancet Psychiatry},
volume = {10},
number = {4},
pages = {296-304},
year = {2023},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(23)00006-8},
url = {https://www.sciencedirect.com/science/article/pii/S2215036623000068},
author = {Ana Gómez-Carrillo and Laurence J Kirmayer and Neil Krishan Aggarwal and Kamaldeep S Bhui and Kenneth Po-Lun Fung and Brandon A Kohrt and Mitchell G Weiss and Roberto Lewis-Fernández},
abstract = {Summary
Psychiatry has increasingly adopted explanations for psychopathology that are based on neurobiological reductionism. With the recognition of health disparities and the realisation that someone's postcode can be a better predictor of health outcomes than their genetic code, there are increasing efforts to ensure cultural and social–structural competence in psychiatric practice. Although neuroscientific and social–cultural approaches in psychiatry remain largely separate, they can be brought together in a multilevel explanatory framework to advance psychiatric theory, research, and practice. In this Personal View, we outline how a cultural–ecosocial systems approach to integrating neuroscience in psychiatry can promote social–contextual and systemic thinking for more clinically useful formulations and person-centred care.}
}
@article{RAJ2021474,
title = {Assessment of antiviral potencies of cannabinoids against SARS-CoV-2 using computational and in vitro approaches},
journal = {International Journal of Biological Macromolecules},
volume = {168},
pages = {474-485},
year = {2021},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2020.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0141813020351783},
author = {Vinit Raj and Jae Gyu Park and Kiu-Hyung Cho and Pilju Choi and Taejung Kim and Jungyeob Ham and Jintae Lee},
keywords = {Cannabinols,  antiviral assay, SARS-CoV-2 and M enzyme},
abstract = {Effective treatment choices to the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) are limited because of the absence of effective target-based therapeutics. The main object of the current research was to estimate the antiviral activity of cannabinoids (CBDs) against the human coronavirus SARS-CoV-2. In the presented research work, we performed in silico and in vitro experiments to aid the sighting of lead CBDs for treating the viral infections of SARS-CoV-2. Virtual screening was carried out for interactions between 32 CBDs and the SARS-CoV-2 Mpro enzyme. Afterward, in vitro antiviral activity was carried out of five CBDs molecules against SARS-CoV-2. Interestingly, among them, two CBDs molecules namely Δ9 -tetrahydrocannabinol (IC50 = 10.25 μM) and cannabidiol (IC50 = 7.91 μM) were observed to be more potent antiviral molecules against SARS-CoV-2 compared to the reference drugs lopinavir, chloroquine, and remdesivir (IC50 ranges of 8.16–13.15 μM). These molecules were found to have stable conformations with the active binding pocket of the SARS-CoV-2 Mpro by molecular dynamic simulation and density functional theory. Our findings suggest cannabidiol and Δ9 -tetrahydrocannabinol are possible drugs against human coronavirus that might be used in combination or with other drug molecules to treat COVID-19 patients.}
}
@article{YOUNG201719,
title = {Technology-enhanced mathematics instruction: A second-order meta-analysis of 30 years of research},
journal = {Educational Research Review},
volume = {22},
pages = {19-33},
year = {2017},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X1730026X},
author = {Jamaal Young},
keywords = {Meta-analysis, Mathematics achievement, Technology, Calculators, Computer-assisted instruction},
abstract = {It is important to assess the cumulative effects of technology on student achievement captured in the last 30 years of technologyenhanced mathematics instruction. Synthesizing the thousands of articles and gray literature on this subject is necessary but would require a considerable commitment of academic resources. A second-order metaanalysis or meta-analysis of meta-analyses is an alternative that is reasonable and effective. Thus, a second-order meta-analysis of 19 prior meta-analyses with minimum overlap between primary studies was conducted. The results represent 663 primary studies (approximately 141,733 participants) and 1,263 effect sizes. The random effects' mean effect size of .38 was statistically significantly different from zero. The results provide a historical and contextualized summary of 30 years of meta-analytic research, which supports meta-analytic thinking and better interpretation of future effect sizes. Results indicate that technology function and study quality are major contributors to effect size variation. Specifically, computation enhancement technologies were most effective, while studies that examine combinations of enhancements were least effective. Implications for technology-enhanced mathematics instruction and meta-analytic research are provided.}
}
@article{GUERRAMACIAS2025e41099,
title = {Development of transversal skills in higher education programs in conjunction with online learning: relationship between learning strategies, project-based pedagogical practices, e-learning platforms, and academic performance},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e41099},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e41099},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024171307},
author = {Yolanda Guerra-Macías and Sergio Tobón},
keywords = {Generic competencies, 21st-century skills, Virtual education, Higher education, Socioformation, Generic skills, Socioformative rubrics},
abstract = {This study investigates the development of transversal skills and their association with academic performance in university students enrolled in on-campus programs with online activities. A cross-sectional, descriptive, and quantitative research was conducted with 252 students from a public university in Mexico. Transversal skills, socioformative project-based practices, learning strategies, and the relevance of online activities were assessed using validated rubrics. The results indicated a low level of development in three transversal skills: research, entrepreneurship, and English, with the latter being the poorest rated. Critical and creative thinking exhibited the highest level of development. In the didactic component, socioformative project-based pedagogical practices and learning strategies showed acceptable levels. Students expressed satisfaction with complementary online activities, showing a preference for interactive videos and short videos under 4 min. Regression analysis and structural equations were used to examine the relationships between various factors. Results demonstrated that socioformative project-based pedagogical practices, learning strategies, and online education positively correlated with the development of transversal skills. Furthermore, a higher level of transversal skills was associated with better academic averages among students. Socioformative project-based pedagogical practices also correlated with academic performance through transversal skills. The study concludes that integrating online activities into on-campus programs, based on the socioformative pedagogical model, can enhance the development of transversal skills and improve academic performance. Further research into the implementation of this educational model and its long-term impact on university education and professional success is recommended.}
}
@article{FIORE2006S248,
title = {Multi-scale computational analysis of fluid dynamics in the Toraymyxin adsorption cartridge},
journal = {Journal of Biomechanics},
volume = {39},
pages = {S248},
year = {2006},
note = {Abstracts of the 5th World Congress of Biomechanics},
issn = {0021-9290},
doi = {https://doi.org/10.1016/S0021-9290(06)83940-0},
url = {https://www.sciencedirect.com/science/article/pii/S0021929006839400},
author = {G.B. Fiore and G. Guadagni and M. Soncini and S. Vesentini and A. Redaelli}
}
@article{HSU2011380,
title = {The probabilistic analysis of language acquisition: Theoretical, computational, and experimental analysis},
journal = {Cognition},
volume = {120},
number = {3},
pages = {380-390},
year = {2011},
note = {Probabilistic models of cognitive development},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2011.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010027711000734},
author = {Anne S. Hsu and Nick Chater and Paul M.B. Vitányi},
keywords = {Child language acquisition, Poverty of the stimulus, No negative evidence, Bayesian models, Minimum description length, Simplicity principle, Natural language, Probabilistic models, Identification in the limit},
abstract = {There is much debate over the degree to which language learning is governed by innate language-specific biases, or acquired through cognition-general principles. Here we examine the probabilistic language acquisition hypothesis on three levels: We outline a novel theoretical result showing that it is possible to learn the exact generative model underlying a wide class of languages, purely from observing samples of the language. We then describe a recently proposed practical framework, which quantifies natural language learnability, allowing specific learnability predictions to be made for the first time. In previous work, this framework was used to make learnability predictions for a wide variety of linguistic constructions, for which learnability has been much debated. Here, we present a new experiment which tests these learnability predictions. We find that our experimental results support the possibility that these linguistic constructions are acquired probabilistically from cognition-general principles.}
}
@article{CUMMINGS2003369,
title = {Computational challenges in high angle of attack flow prediction},
journal = {Progress in Aerospace Sciences},
volume = {39},
number = {5},
pages = {369-384},
year = {2003},
issn = {0376-0421},
doi = {https://doi.org/10.1016/S0376-0421(03)00041-1},
url = {https://www.sciencedirect.com/science/article/pii/S0376042103000411},
author = {Russell M. Cummings and James R. Forsythe and Scott A. Morton and Kyle D. Squires},
abstract = {Aircraft aerodynamics have been predicted using computational fluid dynamics for a number of years. While viscous flow computations for cruise conditions have become commonplace, the non-linear effects that take place at high angles of attack are much more difficult to predict. A variety of difficulties arise when performing these computations, including challenges in properly modeling turbulence and transition for vortical and massively separated flows, the need to use appropriate numerical algorithms if flow asymmetry is possible, and the difficulties in creating grids that allow for accurate simulation of the flowfield. These issues are addressed and recommendations are made for further improvements in high angle of attack flow prediction. Current predictive capabilities for high angle of attack flows are reviewed, and solutions based on hybrid turbulence models are presented.}
}
@article{CRAWFORD202180,
title = {Efficient mechanisms for level-k bilateral trading},
journal = {Games and Economic Behavior},
volume = {127},
pages = {80-101},
year = {2021},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2021.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899825621000282},
author = {Vincent P. Crawford},
keywords = {Mechanism design, Bilateral trading, Level- thinking, Behavioral game theory},
abstract = {This paper revisits Myerson and Satterthwaite's (1983) classic analysis of mechanism design for bilateral trading, replacing equilibrium with a level-k model of strategic thinking and focusing on direct mechanisms. The revelation principle fails for level-k models, so restricting attention to direct mechanisms and imposing incentive-compatibility are not without loss of generality. If, however, only direct, level-k-incentive-compatible mechanisms are feasible and traders' levels are observable, Myerson and Satterthwaite's characterization of mechanisms that maximize traders' total surplus subject to incentive constraints generalizes qualitatively to level-k models. If only direct, level-k-incentive-compatible mechanisms are feasible but traders' levels are not observable, generically a particular posted-price mechanism maximizes traders' total expected surplus subject to incentive constraints. If direct, non-level-k-incentive-compatible mechanisms are feasible and traders best respond to them, total expected surplus-maximizing mechanisms may take completely different forms.}
}
@incollection{1991344,
title = {Appendix A - Scientific chaos: a new way of thinking about dynamics},
editor = {Ralph D. Stacey},
booktitle = {The Chaos Frontier},
publisher = {Butterworth-Heinemann},
pages = {344-365},
year = {1991},
isbn = {978-0-7506-0139-9},
doi = {https://doi.org/10.1016/B978-0-7506-0139-9.50021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780750601399500212}
}
@article{MARINIER200948,
title = {A computational unification of cognitive behavior and emotion},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {48-69},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000302},
author = {Robert P. Marinier and John E. Laird and Richard L. Lewis},
abstract = {Existing models that integrate emotion and cognition generally do not fully specify why cognition needs emotion and conversely why emotion needs cognition. In this paper, we present a unified computational model that combines an abstract cognitive theory of behavior control (PEACTIDM) and a detailed theory of emotion (based on an appraisal theory), integrated in a theory of cognitive architecture (Soar). The theory of cognitive control specifies a set of required computational functions and their abstract inputs and outputs, while the appraisal theory specifies in more detail the nature of these inputs and outputs and an ontology for their representation. We argue that there is a surprising functional symbiosis between these two independently motivated theories that leads to a deeper theoretical integration than has been previously obtained in other computational treatments of cognition and emotion. We use an implemented model in Soar to test the feasibility of the resulting integrated theory, and explore its implications and predictive power in several task domains.}
}
@article{MURRAY202483,
title = {Brain mechanisms of rumination and negative self-referential processing in adolescent depression},
journal = {Journal of Affective Disorders},
volume = {366},
pages = {83-90},
year = {2024},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2024.08.114},
url = {https://www.sciencedirect.com/science/article/pii/S0165032724013466},
author = {Laura Murray and Nigel M. Jaffe and Anna O. Tierney and Kristina Pidvirny and Emma G. Balkind and Batool S. Abbasi and Miranda Brown and Christian A. Webb},
keywords = {Depression, Adolescence, Ecological momentary assessment, fMRI, Rumination, Self-referent encoding task},
abstract = {Background
Depression is linked to cognitive biases towards more negative and less positive self-relevant information. Rumination, perseverative negative thinking about the past and the self, may contribute to these biases.
Methods
159 adolescents (12–18 years), with a range of depression symptoms, completed the SRET during fMRI. Multiple regressions tested associations between conventional self-report and ecological momentary assessment (EMA) measured rumination, and neural and behavioral responses during a self-referent encoding task (SRET).
Results
Higher rumination (conventional self-report and EMA) was associated with more negative and fewer positive words endorsed and recalled. Higher self-reported (but not EMA) rumination was associated with higher accuracy in recognizing negative words and greater insula and dorsal anterior cingulate activity to negative versus positive words.
Limitations
The sample included mostly non-Hispanic White participants with household incomes above the national average, highlighting the need for replication in more diverse samples. Word endorsement discrepancies required fMRI analyses to model neural response to viewing negative versus positive words.
Conclusions
Adolescents with higher rumination endorsed and recalled more negative and fewer positive words and recognized more negative words during the SRET. Higher insula reactivity, a key region for modulating externally-oriented attention and internally-oriented self-referential processes, may contribute to links between rumination and negative memory biases. These findings provide insight into neurocognitive mechanisms underlying depression.}
}
@article{KARAGIANNIS2022103631,
title = {The OMiLAB Digital Innovation environment: Agile conceptual models to bridge business value with Digital and Physical Twins for Product-Service Systems development},
journal = {Computers in Industry},
volume = {138},
pages = {103631},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103631},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000264},
author = {Dimitris Karagiannis and Robert Andrei Buchmann and Wilfrid Utz},
keywords = {Digital twin, Physical twin, Smart Product-service Systems, Agile modeling method engineering, OMiLAB, Domain-specific conceptual modeling},
abstract = {OMiLAB is a community of practice which offers a digital ecosystem bringing together open technologies to investigate and apply conceptual modeling methods for varying purposes and domains. One of the core value propositions is a dedicated Digital Innovation environment comprising several toolkits and workspaces, designed to support Product-Service Systems (PSS) prototyping – a key ingredient for PSS lifecycle management. At the core of this environment is a notion of Agile Digital Twin – a conceptual representation that can be tailored with knowledge engineering means to bridge the semantic and functional gap between a business perspective (focusing on value creation) and an engineering perspective (focusing on cyber-physical proofs-of-concept). To facilitate this bridging, the hereby proposed environment orchestrates, across three abstraction layers, methods such as Design Thinking, Agile Modeling Method Engineering and Model-driven Engineering to turn Ideation into smart Product-Service Systems experiments, in a laboratory setting. The proposed environment was built following Design Science principles. It addresses the problem of historically-disconnected skills required for Digital Innovation projects and it provides a testbed for feasibility experimentation. For design-oriented, artifact building research, a higher Technology Readiness Level can thus be achieved (compared to the level that idea development methods typically attain).}
}
@article{STRASS201534,
title = {Analyzing the computational complexity of abstract dialectical frameworks via approximation fixpoint theory},
journal = {Artificial Intelligence},
volume = {226},
pages = {34-74},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000776},
author = {Hannes Strass and Johannes Peter Wallner},
keywords = {Abstract dialectical frameworks, Computational complexity, Approximation fixpoint theory},
abstract = {Abstract dialectical frameworks (ADFs) have recently been proposed as a versatile generalization of Dung's abstract argumentation frameworks (AFs). In this paper, we present a comprehensive analysis of the computational complexity of ADFs. Our results show that while ADFs are one level up in the polynomial hierarchy compared to AFs, there is a useful subclass of ADFs which is as complex as AFs while arguably offering more modeling capacities. As a technical vehicle, we employ the approximation fixpoint theory of Denecker, Marek and Truszczyński, thus showing that it is also a useful tool for complexity analysis of operator-based semantics.}
}
@article{PARK20151,
title = {A Neuro-educational Study of the Development of the Creativity-based Teaching Program and its Effect},
journal = {Procedia - Social and Behavioral Sciences},
volume = {186},
pages = {1-8},
year = {2015},
note = {The Proceedings of 5th World Conference on Learning, Teaching and Educational Leadership},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.200},
url = {https://www.sciencedirect.com/science/article/pii/S187704281502460X},
author = {Sun-Hyung Park and Kwang-Ki Kim and Kyung-Hwa Lee},
keywords = {Creativity-based teaching programs, Divergent thinking, The TTCT, FMRI},
abstract = {This study aims at exploring a possibility of developing a creativity-based teaching program needed for enhancing prospective teachers’ creative potentials based on the theories of Sawyer and Renzulli. Neuroimaging tools such as fMRI were used to identify effects of the program on pre-service teachers’ neural activations on divergent thinking measured primarily by the Torrance Tests of Creative Thinking (TTCT). Since the research is still in progress, we present a theoretical model for the teaching program, and preliminary test results of comparing changes of neural recruitments in students’ brain participated in fMRI with the TTCT.}
}
@article{SILVEIRA1980165,
title = {Generic masculine words and thinking},
journal = {Women's Studies International Quarterly},
volume = {3},
number = {2},
pages = {165-178},
year = {1980},
note = {The voices and words of women and men},
issn = {0148-0685},
doi = {https://doi.org/10.1016/S0148-0685(80)92113-2},
url = {https://www.sciencedirect.com/science/article/pii/S0148068580921132},
author = {Jeanette Silveira},
abstract = {Synopsis
It has been alleged that, in appropriate verbal contexts, man and he are generic, i.e. that the words include women as well as men, as for example in, Man is mortal, or One must watch his language. Many feminists argue for the elimination of this generic use of man and he and the substitution of such non-male words as people and they. Others argue on various grounds that these changes are unnecessary. This paper isolates the issues involved in such arguments and provisionally concludes that a reduction in the generic use of man and he would result in a long term reduction in sexist thinking. Recent feminist research on man and he is carefully reviewed. In its final section, the paper develops the implication that women experience more alienation than men in the presence of the generic man and he.}
}
@article{EMILYESTHERRANI2025107032,
title = {Alzheimer disease classification using optimal clustering based pre-trained SqueezeNet model},
journal = {Biomedical Signal Processing and Control},
volume = {100},
pages = {107032},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.107032},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424010905},
author = {K. {Emily Esther Rani} and S. Baulkani},
keywords = {Alzheimer disease, Optimal center, Clustering, Adaptive reptile search algorithm, Pre-trained squeezenet},
abstract = {Alzheimer’s disease (AD) is an irreversible neurological illness identified by deficits in thinking, behavior, and memory. Early detection and prevention of AD is a crucial and difficult task. DL (Deep Learning) has gained significant attention recently as a potential tool for early AD detection. However, traditional diagnostic methods such as cognitive tests and manual analysis of brain imaging are time consuming and prone to error. Hence, there is a need for an automated model which shows better classification performance. To tackle these issues, this study presents a system to improve AD recognition performance. Initially, the pre-processing and skull stripping processes are performed. Then, for segmenting the grey, whiter matters and Cerebrospinal Fluid (CSF), the optimal clustering process is carried out. Here, the optimal center of clusters is selected by the metaheuristic optimization Adaptive Reptile Search Algorithm-Clustering Approach (ARSA-CA) is utilized. The proposed ARSA is the integration of the optimization RSA and simulated annealing (SA). Finally, for classifying the different classes of AD, the DL approach pre-trained SqueezeNet is utilized. The experimentation is carried out on the ADNI and OASIS datasets and achieved better accuracies of 98.3% and 98.2% respectively. Thus, it is proved that the proposed model is suitable for identifying AD.}
}
@incollection{MYUNG20012453,
title = {Computational Approaches to Model Evaluation},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2453-2457},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00589-1},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767005891},
author = {I.J. Myung},
abstract = {The induction problem of inferring a predictive function (i.e., model) from finite data is a central component of the scientific enterprise in cognitive science, computer science and statistics, and yet the problem is fundamentally ill posed. Many models can often provide equally good fits to a given observed data set but they may differ considerably in their ability to generalize to new, as yet unseen, data sets generated from the same underlying process. To make this inductive inference problem well posed one needs to define a justifiable measure of generalizability and then use the measure to choose among a set of competing models. Many such measures have been proposed in the past, notably by scientists in the fields of machine learning and algorithmic coding theory. A representative list of such approaches includes the structural risk minimization method and Vapnik-Chervonenkis dimension, the regularization theory, and the minimum description length principle. This article presents a review of these computational approaches to model evaluation. Also discussed are the interesting connections between the computational approaches and some of the statistical approaches to model evaluation such as the Akaike information criterion, the Bayesian information criterion and Bayesian model selection.}
}
@incollection{DEAN2014157,
title = {Chapter 7 - Decorrelation Learning in the Cerebellum: Computational Analysis and Experimental Questions},
editor = {Narender Ramnani},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {210},
pages = {157-192},
year = {2014},
booktitle = {Cerebellar Learning},
issn = {0079-6123},
doi = {https://doi.org/10.1016/B978-0-444-63356-9.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444633569000078},
author = {Paul Dean and John Porrill},
keywords = {Cerebellum, eye blink conditioning, vestibulo-ocular reflex, spike-timing dependent plasticity, avoidance learning, long-term depression, long-term potentiation, supervised learning, reafference, least mean squares},
abstract = {Many cerebellar models use a form of synaptic plasticity that implements decorrelation learning. Parallel fibers carrying signals positively correlated with climbing-fiber input have their synapses weakened (long-term depression), whereas those carrying signals negatively correlated with climbing input have their synapses strengthened (long-term potentiation). Learning therefore ceases when all parallel-fiber signals have been decorrelated from climbing-fiber input. This is a computationally powerful rule for supervised learning and can be cast in a spike-timing dependent plasticity form for comparison with experimental evidence. Decorrelation learning is particularly well suited to sensory prediction, for example, in the reafference problem where external sensory signals are interfered with by reafferent signals from the organism’s own movements, and the required circuit appears similar to the one found to mediate classical eye blink conditioning. However, for certain stimuli, avoidance is a much better option than simple prediction, and decorrelation learning can also be used to acquire appropriate avoidance movements. One example of a stimulus to be avoided is retinal slip that degrades visual processing, and decorrelation learning appears to play a role in the vestibulo-ocular reflex that stabilizes gaze in the face of unpredicted head movements. Decorrelation learning is thus suitable for both sensory prediction and motor control. It may also be well suited for generic spatial and temporal coordination, because of its ability to remove the unwanted side effects of movement. Finally, because it can be used with any kind of time-varying signal, the cerebellum could play a role in cognitive processing.}
}
@article{JONES20171,
title = {Diversity not quantity in caregiver speech: Using computational modeling to isolate the effects of the quantity and the diversity of the input on vocabulary growth},
journal = {Cognitive Psychology},
volume = {98},
pages = {1-21},
year = {2017},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2017.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028516302274},
author = {Gary Jones and Caroline F. Rowland},
keywords = {Input quantity, Lexical diversity, Vocabulary acquisition, CLASSIC, Language acquisition},
abstract = {Children who hear large amounts of diverse speech learn language more quickly than children who do not. However, high correlations between the amount and the diversity of the input in speech samples makes it difficult to isolate the influence of each. We overcame this problem by controlling the input to a computational model so that amount of exposure to linguistic input (quantity) and the quality of that input (lexical diversity) were independently manipulated. Sublexical, lexical, and multi-word knowledge were charted across development (Study 1), showing that while input quantity may be important early in learning, lexical diversity is ultimately more crucial, a prediction confirmed against children’s data (Study 2). The model trained on a lexically diverse input also performed better on nonword repetition and sentence recall tests (Study 3) and was quicker to learn new words over time (Study 4). A language input that is rich in lexical diversity outperforms equivalent richness in quantity for learned sublexical and lexical knowledge, for well-established language tests, and for acquiring words that have never been encountered before.}
}
@article{HEYLIGHEN2014113,
title = {Designing in the absence of sight: Design cognition re-articulated},
journal = {Design Studies},
volume = {35},
number = {2},
pages = {113-132},
year = {2014},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000926},
author = {Ann Heylighen and Greg Nijs},
keywords = {design cognition, design research, epistemology},
abstract = {Starting from the study of an architect who designs in the absence of sight, we question to what extent prevailing notions of design may be complemented with alternative articulations. In doing so, we point to the cognitivist understanding of human cognition underlying design researchers' strong attention to ‘visual thinking’, and contrast this with more situated understandings of human cognition. The ontological and epistemological differences between both raise questions about how design research is produced, and consequently what design can also be. By accounting for how a blind architect re-articulates prevailing notions of design, we invite researchers to keep the discussion open and call for an ontological and epistemological re-articulation in design research.}
}
@article{GOMEZTALAL2024108109,
title = {Understanding the disparities in Mathematics performance: An interpretability-based examination},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108109},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108109},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624002677},
author = {Ismael Gómez-Talal and Luis Bote-Curiel and José Luis Rojo-Álvarez},
keywords = {Programme for International Student Assessment, Interpretable machine learning, Shapley additive explanations, Explainable black-box models},
abstract = {Problem:
Educational disparities in Mathematics performance are a persistent challenge. This study aims to unravel the complex factors contributing to these disparities among students internationally, with a focus on the interpretability of the contributing factors.
Methodology:
Utilizing data from the Programme for International Student Assessment (PISA), we conducted rigorous preprocessing and variable selection to prepare for applying binary classification interpretability models. These models were trained using the Stratified K-Fold technique to ensure balanced representation and assessed using six key metrics.
Solution:
By applying interpretability models such as Shapley Additive Explanations (SHAP) analysis, we identified critical factors impacting student performance, including reading accessibility, critical thinking skills, gender, and geographical location.
Results:
Our findings reveal significant disparities linked to resource availability, with students from lower socioeconomic backgrounds possessing fewer books and demonstrating lower performance in Mathematics. The geographical analysis highlighted regional educational disparities, with certain areas consistently underperforming in PISA assessments. Gender also emerged as a determinant, with females contributing differently to performance levels across the spectrum.
Conclusion:
The study provides insights into the multifaceted determinants of student Mathematics performance and suggests potential avenues for future research to explore global interpretability models and further investigate the socioeconomic, cultural, and educational factors at play.}
}
@article{GISIGER2000250,
title = {Computational models of association cortex},
journal = {Current Opinion in Neurobiology},
volume = {10},
number = {2},
pages = {250-259},
year = {2000},
issn = {0959-4388},
doi = {https://doi.org/10.1016/S0959-4388(00)00075-1},
url = {https://www.sciencedirect.com/science/article/pii/S0959438800000751},
author = {Thomas Gisiger and Stanislas Dehaene and Jean-Pierre Changeux},
abstract = {Recent computational models, or mathematical realizations of neurobiological theories, are providing insights into the organization and workings of the association cortex. Such models concern the construction of cortical maps, the neural basis of cognitive functions such as visual perception, reward-motivated learning and some aspects of consciousness.}
}
@article{CORREABAENA20181410,
title = {Accelerating Materials Development via Automation, Machine Learning, and High-Performance Computing},
journal = {Joule},
volume = {2},
number = {8},
pages = {1410-1420},
year = {2018},
issn = {2542-4351},
doi = {https://doi.org/10.1016/j.joule.2018.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S2542435118302289},
author = {Juan-Pablo Correa-Baena and Kedar Hippalgaonkar and Jeroen {van Duren} and Shaffiq Jaffer and Vijay R. Chandrasekhar and Vladan Stevanovic and Cyrus Wadia and Supratik Guha and Tonio Buonassisi},
keywords = {accelerated materials development, machine learning, artificial intelligence, energy materials},
abstract = {Successful materials innovations can transform society. However, materials research often involves long timelines and low success probabilities, dissuading investors who have expectations of shorter times from bench to business. A combination of emergent technologies could accelerate the pace of novel materials development by ten times or more, aligning the timelines of stakeholders (investors and researchers), markets, and the environment, while increasing return on investment. First, tool automation enables rapid experimental testing of candidate materials. Second, high-performance computing concentrates experimental bandwidth on promising compounds by predicting and inferring bulk, interface, and defect-related properties. Third, machine learning connects the former two, where experimental outputs automatically refine theory and help define next experiments. We describe state-of-the-art attempts to realize this vision and identify resource gaps. We posit that over the coming decade, this combination of tools will transform the way we perform materials research, with considerable first-mover advantages at stake.}
}
@article{REYNANTE2024102287,
title = {Reducing the cognitive abstractness of climate change through an “engineering fiction” learning experience: A natural language processing study},
journal = {Journal of Environmental Psychology},
volume = {95},
pages = {102287},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102287},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424000604},
author = {Brandon Reynante and Nicole M. Ardoin and Roy Pea},
keywords = {Artificial intelligence, Climate change education, Climate fiction},
abstract = {The lackluster societal response to the climate crisis is partially attributed to the abstractness of people's mental construals of climate change given its vast spatial and temporal dimensions, which fail to evoke urgency to act. Prior efforts to measure mental construal levels of climate change are inconsistent, insufficient, and labor-intensive. This study developed and implemented learning experiences for integrating engineering design and climate fiction writing to engage 48 high school students in concrete climate change thinking. A novel measure of cognitive abstractness overcomes previous methodological shortcomings by automatically quantifying the linguistic abstractness of participant-authored stories using natural language processing. Comparing participant stories written at the beginning and end of the intervention reveals a significant decrease in linguistic abstractness (Cohen's d = 1.01, p = 0.03). This study contributes to the nascent movement for greater use of narratives as data sources in environmental psychology research, which may uncover new insights into human behavior and decision making.}
}
@article{PEZZULO2011275,
title = {Computational explorations of perceptual symbol systems theory},
journal = {New Ideas in Psychology},
volume = {29},
number = {3},
pages = {275-297},
year = {2011},
note = {Special Issue: Cognitive Robotics and Reevaluation of Piaget Concept of Egocentrism},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2009.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X09000336},
author = {Giovanni Pezzulo and Gianguglielmo Calvi},
keywords = {Perceptual symbol systems, Schemas, Embodiment, Anticipation, Simulation},
abstract = {The aim of this paper is twofold. First, we provide a methodological pathway from theories of situated, embodied cognition to simulations with an eye to empirical evidence, and suggest a possible cross-fertilization between cognitive robotics and psychology. Psychological theories, in particular those formulated at an abstract level, include models which are often severely underspecified at the level of mechanisms. This is true in the synchronic, constructive perspective (how can the effects observed in experiments be concretely generated by the model's mechanisms?) and in the diachronic, developmental perspective (how can such mechanisms be learned and developed?). The synthetic method of artificial cognitive systems research, and in particular of cognitive robotics, can complement research in psychology (and neurosciences) by exploring the constructive and developmental aspects of theories. Our second aim is to provide an example of such a methodology by describing simulations aiming at developing a perceptual symbol system (PSS) (Barsalou, 1999). We then describe the two main theoretical constructs of the PSS, perceptual symbols and simulators, illustrate their development in an artificial system, and test the system in prediction, categorization, and abstraction tasks.}
}
@article{2004263,
title = {Computational Statistics and Data Analysis},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {73},
number = {2},
pages = {263},
year = {2004},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2004.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169743904001820}
}
@article{AMADORHIDALGO2021103694,
title = {Cognitive abilities and risk-taking: Errors, not preferences},
journal = {European Economic Review},
volume = {134},
pages = {103694},
year = {2021},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2021.103694},
url = {https://www.sciencedirect.com/science/article/pii/S0014292121000477},
author = {Luis Amador-Hidalgo and Pablo Brañas-Garza and Antonio M. Espín and Teresa García-Muñoz and Ana Hernández-Román},
keywords = {Decision making under uncertainty, Cognitive abilities, Online experiment, Risk and loss aversion, Factor analysis},
abstract = {There is an intense debate whether risk-taking behavior is partially driven by cognitive abilities. The critical issue is whether choices arising from subjects with lower cognitive abilities are more likely driven by errors or lack of understanding than pure preferences for risk. The latter implies that the often-argued link between risk preferences and cognitive abilities (a common finding is that abilities relate negatively to risk aversion and positively to loss aversion) might be a spurious correlation. This experiment reports evidence from a sample of 556 participants who made choices in two risk-related tasks and completed three cognitive tasks, all with real monetary incentives: number-additions (including incentive-compatible expected number of correct additions), the Cognitive Reflection Test (to measure analytical/reflective thinking) and the Remote Associates Test (for convergent thinking). Results are unambiguous: none of our cognition measures plays any systematic role on risky decision making. Using structural equation modeling and factor analysis, we show that cognitive abilities are negatively associated with noisy, inconsistent choices and this effect may make higher ability individuals appear to be less risk averse and more loss averse. Yet we show that errors are more likely to appear when the two payoffs in a given decision exhibit similar probability. Therefore, our results suggest that failing to account for noisy decision making might have led to erroneously inferring a correlation between cognitive abilities and risk preferences in previous studies.}
}