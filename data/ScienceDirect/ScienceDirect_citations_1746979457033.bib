@article{TRICHKOVAKASHAMOVA2024123,
title = {Criteria and Approaches for Optimization of Innovative Methods for STEM Education},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {3},
pages = {123-128},
year = {2024},
note = {22nd IFAC Conference on Technology, Culture and International Stability TECIS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.137},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324002209},
author = {E. Trichkova-Kashamova and E. Paunova-Hubenova and Y. Boneva and S. Dimitrov},
keywords = {STEM education, innovative educational methods, optimisation, data analyses, technology-based learning},
abstract = {The proposed research aims to evaluate the modern learning process in STEM subjects in a technology-rich environment. The study examines contemporary teaching methods and evaluates their application in different educational levels in Bulgaria. The aim is to provide information for developing a concept of a modern technology-based learning process and integrating innovative methods with appropriate technological tools.}
}
@article{ELMAIZI2019126,
title = {A novel information gain based approach for classification and dimensionality reduction of hyperspectral images},
journal = {Procedia Computer Science},
volume = {148},
pages = {126-134},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930016X},
author = {Asma Elmaizi and Hasna Nhaila and Elkebir Sarhrouni and Ahmed Hammouch and Chafik Nacir},
keywords = {Hyperspectral images, dimentionality reduction, information gain, classification accuracy},
abstract = {Recently, the hyperspectral sensors has improved our ability to monitor the earth surface with high spectral resolution. However, the high dimensionality of spectral data brings challenges for the image processing. Consequently, the dimensionality reduction is a necessary step in order to reduce the computational complexity and increase the classification accuracy. In this paper, we propose a new filter approach based on information gain for dimensionality reduction and classification of hyperspectral images. A special strategy based on hyperspectral bands selection is adopted to pick the most informative bands and discard the irrelevant and noisy ones. The algorithm evaluates the relevancy of the bands based on the information gain function with the support vector machine classifier. The proposed method is compared using two benchmark hyperspectral datasets (Indiana, Pavia) with three competing methods. The comparison results showed that the information gain filter approach outperforms the other methods on the tested datasets and could significantly reduce the computation cost while improving the classification accuracy.}
}
@article{FUSHING2023129227,
title = {Multiscale major factor selections for complex system data with structural dependency and heterogeneity},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {630},
pages = {129227},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.129227},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123007823},
author = {Hsieh Fushing and Elizabeth P. Chou and Ting-Li Chen},
keywords = {Broken symmetry, Conditional entropy, Contingency table, Major factor selection, Multiclass Classification, Pitching dynamics},
abstract = {The unknown multiscale structure hidden in large complex systems is explored bottom-up through discovered heterogeneity under structural dependency embedded within structured data sets. Via two real complex systems, we demonstrate computed hierarchical structures with broken symmetry constituting data’s information content. Through graphic displays, such information content indirectly, but efficiently resolves system-related scientific issues that are difficult to resolve directly. All bottom-up explorations and computations are based on conditional entropy and mutual information evaluated upon contingency table platforms after categorizing all quantitative features. Categorical Exploratory Data Analysis (CEDA) first extracts global major factors that share significant mutual information with the targeted response (Re) variable against many covariate (Co) features under the presence of structural dependency. Then each global major factor is taken as one perspective of heterogeneity to subdivide the entire data set according to its categories into sub-collections. This simple “de-associating” protocol significantly reduces structural dependency among the rest of the features such that another run of major factor selection performed on the sub-collection scale can precisely identify which feature sets could provide extra information beyond the global major factor. Finally, informative patterns collected from multiple perspectives of heterogeneity are displayed to explicitly resolve issues of prediction, classification, and detecting minute dynamic changes.}
}
@incollection{ESCHE2014699,
title = {Systematic Modeling for Optimization},
editor = {Mario R. Eden and John D. Siirola and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {34},
pages = {699-704},
year = {2014},
booktitle = {Proceedings of the 8th International Conference on Foundations of Computer-Aided Process Design},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63433-7.50101-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634337501012},
author = {Erik Esche and David Müller and Günter Wozny},
keywords = {Multiple-Scale Modeling, Optimization, Convexification, Linearization},
abstract = {Optimization usually requires models, which are computationally speaking less expensive than models commonly used for simulations. At the same time, process optimization and model predictive control etc. require dependable accuracies in addition to the fastness. To demystify the art of preparing process models for optimization, a workflow is presented in this contribution, which systematically deduces models based on simplification of existing models and experiment based deduction of computationally inexpensive correlations.}
}
@article{NEDIC2019224,
title = {OP0284 PARE THE INFLUENCE OF FUNCTIONAL TRAINING AND PSYCHO-SOCIAL SUPPORT},
journal = {Annals of the Rheumatic Diseases},
volume = {78},
pages = {224},
year = {2019},
issn = {0003-4967},
doi = {https://doi.org/10.1136/annrheumdis-2019-eular.4256},
url = {https://www.sciencedirect.com/science/article/pii/S0003496724042614},
author = {Nenad Nedić and Mirjana Lapčević},
abstract = {Background
Treatment of chronic noncontagious diseases in which we include RMD implies usage of medicines and non medicine (changing bad lifestyle habits like losing nutrition, physical activity, smoking…).1 Unwanted cardiovascular and cerebrovascular states are the most common cause of shortening of live of people with RMD.2 None of the chemical drugs can replace physical activity. Physical activity dosage is individual, and depends on aerobic capability and heart rate increase, taking in account age, type of noncontagious diseases, level of tissue and organs damage as well as the type of work the person is engaged in. Patients with RMD have common symptoms, such as stiffness, fatigue, poor mobility, joint pain and muscle pain, anxiety and depression, and lack of fitness. In addition to physical medicine and rehabilitation and balneoklimatology, various forms of physical activity are recommended, such as walking, swimming, functional training. Today, moderate physical activity is known to help reduce fatigue, strengthen muscles and bones, improve flexibility and endurance of the joints, and improve general health. It is necessary to find the best combination of rest, activities and exercise programs to prevent deformities of the joints, the development of disability, improve the quality of life, and the mental health of patients with RMD.3
Objectives
1. By practicing Cigong, an increase in the volume of movement in the joint, the strengthening of joint muscles and the improvement of general condition, pain relief, fatigue reduction is achieved and also, it helps patients look and feel better. 2. The goal of Yoga is to neutralize and remove all obstacles that stand in the way and disturb the function of the body and the mind and achieves inner peace. 3. Changing the psychological state during exercise also led to a positive way of thinking. 4. Psycho-physical support for the people with RMD
Methods
From 2011, we organize two times a week functional training of Cigong.4 Since January 2016, twice a week persons with RMD have been practicing Yoga.5 From 2015, one per week four psychologists volunteer hold workshops for psycho-social support for persons with RMD.6,7 Participants of the training and psycho-social sessions took a survey.
Results
1. Joint pain reduction – 50% of total number of participants 2. Joint mobility increase – 95% of total number of participants 3. General fitness improvement – 73% of total number of participants 4. Less pronounced negative emotions – 59% of total number of participants
Conclusion
Practicing cigong, yoga and psycho-social support workshops help patients look better and feel better. Changing the psychological state during exercise also led to a positive way of thinking. All of this increased the effectiveness of drug treatment and improved the quality of life of patients with RMD.
References
[1] Dragojević R. Vodič za zdrav život: preporuke medicine i pouke mudrosti, Beograd 2017. [2] Lapcević M., Vuković M. Dimitrijevic I. Et all Uticaj medikamentnog I nemedikamentnog lečenja na smanjenje faktora rizika za kardiovaskularne I cerebrovaskularne događaje u interventnoj studiji. Srp Arh Celok Lek 2007. [3] Lapčević M, Prvanov D, Đorđević S. Procena kvaliteta života obolelih od hroničnih reumatskih oboljenja. Opšta medicina 2010. [4] Ilinka Acimovic, “The Influence of Health Qigong on the Subjectively Expressed Psychophysical State of Patients with Rheumatoid Arthritis, Rheum, Osteoporosis, Osteopenia” CHINESE MEDICINE AND CULTURE [5] Swami Kriyananda, “Demystifying Patanjali: The Yoga Sutras - The Wisdom of Paramhansa Yogananda”. Crystal Clarity Publishers, Nevada City, CA, 2013. [6] Lapčević M, et al. Socioeconomic and therapy factor influence on self-reported fatigue, anxiety and depression in rheumatoid arthritis patients. Rev Bras Reumatol. 2017. [7] Milić V. Analiza ličnosti I uticaj optimizma na pozitivan ishod lečenja; 2018.
Disclosure of Interests
None declared}
}
@article{YANG2024106650,
title = {Integrating parcel delivery schedules with public transport networks in urban co-modality systems},
journal = {Computers & Operations Research},
volume = {167},
pages = {106650},
year = {2024},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2024.106650},
url = {https://www.sciencedirect.com/science/article/pii/S0305054824001229},
author = {Xuan Yang and Xinyao Nie and Hao Luo and George Q. Huang},
keywords = {Logistics, Public transport, Co-modality, Parcel assignment},
abstract = {Co-modality transportation advocates using urban public transport to support urban freight operations. This study considers the implementation of co-modality in a fixed-route transit network comprising multiple lines following predetermined routes and schedules. We first develop a schedule-based parcel assignment model to formulate the synchronized co-modality transportation problem (SCTP). The effectiveness of the proposed arc-based meta-heuristic algorithm is substantiated through a comprehensive computational analysis, comparing its performance with that of an exact approach and genetic algorithm. Our findings reveal a nuanced trade-off between transportation efficiency and co-modal stop utilization, identifying a threshold beyond which additional stops do not improve efficiency but increase costs. We also discover a 'buckets effect' in co-modal capacities, suggesting that balanced vehicle and stop capacities are crucial for optimizing system performance. A case study with real urban transit data validates our model's potential for significant efficiency gains in co-modality transportation systems, offering actionable insights for urban logistics.}
}
@incollection{PETROVIC2025163,
title = {A Historical and Current Look at Chemical Design for Reduced Hazard},
editor = {Béla Török},
booktitle = {Encyclopedia of Green Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {163-172},
year = {2025},
isbn = {978-0-443-28923-1},
doi = {https://doi.org/10.1016/B978-0-443-15742-4.00072-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157424000727},
author = {Predrag V. Petrovic and Philip Coish and Paul T. Anastas},
keywords = {Benign by design, CAMD, First do no harm, Green chemistry, Hazard reduction, Safer chemical design, Sustainable chemistry},
abstract = {Green chemistry aims to design chemical products and processes that reduce or eliminate the use and generation of hazardous substances. The design of chemicals for reduced hazard selects the properties and attributes that provide the desired function while avoiding unintended consequences for human health and the environment. The tools and knowledge that are available to designers have increased considerably in recent years. Importantly, the approach and goals of design have evolved and become more holistic, multi-disciplinary, and inclusive. This is notable as the global population׳s continued economic and social well-being cannot be maintained with measures that destroy our natural environment. Sustainable chemistry promotes, advances, enables, and empowers the implementation of the chemistry of sustainability and includes chemical design for reduced hazard. In this chapter, we consider the design of chemicals for reduced hazard (safer chemical design) focusing on the past and present, and importantly, looking to the future on what lies ahead.}
}
@incollection{LIANG2023263,
title = {Teacher skills and knowledge for technology integration},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {263-271},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.04037-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305040379},
author = {Leming Liang and Nancy Law},
keywords = {Technology-enhanced learning, Teacher skills, Teacher knowledge, TPACK, Teacher learning, AR/VR, AI, Ethics, Pedagogical innovation, Teacher leadership},
abstract = {With the pervasive use of digital technology in all aspects of our lives, and the rapid advances and deployment of new technologies in education, the use and teaching of technology in education is a necessary professional repertoire for teachers to ensure students' well-being. Influential education policy frameworks posit that teachers need competence in the integration of technology for agile delivery of teaching in blended/fully online modes and for fostering students' digital literacy. Future-oriented professional development programs need to go beyond individual learning and engage leadership in creating the necessary conditions for technology-enhanced learning innovations at institutional and system levels.}
}
@article{ANDERSON2024108366,
title = {Trichotomy revisited: A monolithic theory of attentional control},
journal = {Vision Research},
volume = {217},
pages = {108366},
year = {2024},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2024.108366},
url = {https://www.sciencedirect.com/science/article/pii/S0042698924000105},
author = {Brian A. Anderson},
keywords = {Attentional control, Visual attention, Selection history, Memory, Learning},
abstract = {The control of attention was long held to reflect the influence of two competing mechanisms of assigning priority, one goal-directed and the other stimulus-driven. Learning-dependent influences on the control of attention that could not be attributed to either of those two established mechanisms of control gave rise to the concept of selection history and a corresponding third mechanism of attentional control. The trichotomy framework that ensued has come to dominate theories of attentional control over the past decade, replacing the historical dichotomy. In this theoretical review, I readily affirm that distinctions between the influence of goals, salience, and selection history are substantive and meaningful, and that abandoning the dichotomy between goal-directed and stimulus-driven mechanisms of control was appropriate. I do, however, question whether a theoretical trichotomy is the right answer to the problem posed by selection history. If we reframe the influence of goals and selection history as different flavors of memory-dependent modulations of attentional priority and if we characterize the influence of salience as a consequence of insufficient competition from such memory-dependent sources of priority, it is possible to account for a wide range of attention-related phenomena with only one mechanism of control. The monolithic framework for the control of attention that I propose offers several concrete advantages over a trichotomy framework, which I explore here.}
}
@article{ANDIC2023490,
title = {A robust crow search algorithm based power system state estimation},
journal = {Energy Reports},
volume = {9},
pages = {490-501},
year = {2023},
note = {Proceedings of 2022 7th International Conference on Renewable Energy and Conservation},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.09.075},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723013124},
author = {Cenk Andic and Ali Ozturk and Belgin Turkay},
keywords = {Crow search algorithm, Power systems, State estimation},
abstract = {The State Estimation (SE) computational procedure plays a crucial role in modern electric power system security control by monitoring and analyzing operational conditions and predicting any emergency. In order to estimate state variables, Power System State Estimation (PSSE) takes into account the magnitudes and phases of voltage on each bus. To address the state estimation challenges in power systems, in this paper, we propose a novel application of the Crow Search Algorithm (CSA) specifically tailored for the state estimation problem. We have assessed the introduced algorithm using the frameworks of both the IEEE 14-bus and IEEE 30-bus test systems. The first formulation is the Weighted Least Square (WLS) method, and the second is the Weighted Least Absolute Value (WLAV) method, both of which are objective function formulations. By comparing the results, it is clear that CSA-based SE is superior to the other metaheuristic algorithms considered, namely Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Artificial Bee Swarm Optimization (ABSO). As a point of comparison, we use the Newton–Raphson method for calculating load flow. It has been shown that the proposed CSA-based SE technique has better accuracy than the other two algorithms in all different test systems. With this study, the power system is operated more accurately and reliably by the operators operating the system.}
}
@incollection{GOUGH2024547,
title = {25 - Mycelium-based materials for the built environment: a case study on simulation, fabrication and repurposing myco-materials},
editor = {Emina Kristina Petrović and Morten Gjerde and Fabricio Chicca and Guy Marriage},
booktitle = {Sustainability and Toxicity of Building Materials},
publisher = {Woodhead Publishing},
pages = {547-571},
year = {2024},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-323-98336-5},
doi = {https://doi.org/10.1016/B978-0-323-98336-5.00025-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032398336500025X},
author = {Phillip Gough and Anastasia Globa and Dagmar Ingrid Elfriede Reinhardt},
keywords = {Biodegradable building materials, mycelium, circular economy, materials science, case study research, digital printing},
abstract = {Remanufacturing organic waste for composite materials is an opportunity to create circular economic approaches in construction. Research shows how the mycelium (root network) of some fungi, such as Reishi mushrooms, can be guided and formed to create sustainable, biodegradable composite myco-materials for a range of applications. As they degrade wood, paper or coffee waste, Reishi mushrooms create a new material with potential applications in architecture. Research into biocomposites and eco-materials such as mycelium integrates advanced digital fabrication processes, complex structures and algorithmically generated forms for packaging, thermal and sound insulation or as cladding and structural materials. Significantly, the capacity of mycelium composites to seamlessly return as resource material after use in a sustainable ecological cycle indicates the potential for a new approach to sustainability in building and construction processes across the lifespan of buildings. The adaptation of a living organism as an interactive architecture building module could allow strategies for signalling and negotiating climatic variations and local site conditions around the building. Moreover, the capacity of mycelia to not only thrive in contaminated industrial wastelands but to actively detoxify and colonise could further be employed for bioremediation. The research discussed here presents an empirical study into mycelium composites, using computational design and desktop 3D printing to identify strengths and limitations of material, moulds, growth and form. The case study demonstrates how substrate inclusions impact the formation of the material, opportunities to re-awaken inert myco-material for new growth and the quality of fine-detailed elements, using domestic technology and readily available materials. We contribute a taxonomy of existing uses and applications of myco-materials and a range of implications for the process of designing with myco-materials in architecture.}
}
@incollection{ZELINSKY2005395,
title = {CHAPTER 65 - Specifying the Components of Attention in a Visual Search Task},
editor = {Laurent Itti and Geraint Rees and John K. Tsotsos},
booktitle = {Neurobiology of Attention},
publisher = {Academic Press},
address = {Burlington},
pages = {395-400},
year = {2005},
isbn = {978-0-12-375731-9},
doi = {https://doi.org/10.1016/B978-012375731-9/50069-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123757319500690},
author = {Gregory J. Zelinsky},
abstract = {ABSTRACT
Although commonly treated as a unitary process, attention is more likely a collection of task-related but separable operations. Three components of attention (set, selection, and movement) are identified and defined within the context of a computationally explicit model of eye movements during visual search. The model compares filter-based representations of the target and search displays to derive a salience map indicating likely target candidates in a scene. Eye position is defined as the centroid of activity on this saliency map. As this map is thresholded over time, the changing centroid produces a sequence of movements that eventually cause simulated gaze to become aligned with the target. By adopting a more computational language and making explicit the underlying operations of the task, visual search, a behavior that has long been hobbled to the concept of attention, can be well described without appeal to an abstracted attention theory.}
}
@article{PLUZHNIKOVA202334,
title = {The Human Factor and the Problem of Transport Safety in Modern Conditions},
journal = {Transportation Research Procedia},
volume = {68},
pages = {34-39},
year = {2023},
note = {XIII International Conference on Transport Infrastructure: Territory Development and Sustainability},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523000078},
author = {N.N. Pluzhnikova},
keywords = {Transport, transport robotics, intelligence, IT-technologies, human},
abstract = {The article is devoted to the analysis of the interaction between man and artificial intelligence to ensure the safety of transport. The author analyzes intelligence and IT technologies on the example of the development of transport robotics. To consider this problem the author refers to cognitive developments in this area, and also indicates the philosophical problems of the development of transport robotics. The article uses such methods as comparative and structural analysis.}
}
@article{ROBINSON2009310,
title = {Children's understanding of the inverse relation between multiplication and division},
journal = {Cognitive Development},
volume = {24},
number = {3},
pages = {310-321},
year = {2009},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2008.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0885201408000889},
author = {Katherine M. Robinson and Adam K. Dubé},
keywords = {Arithmetic, Inversion, Conceptual knowledge, Procedural knowledge, Factual knowledge, Multiplication, Division},
abstract = {Children's understanding of the inversion concept in multiplication and division problems (i.e., that on problems of the form d * e/e no calculations are required) was investigated. Children in Grades 6, 7, and 8 completed an inversion problem-solving task, an assessment of procedures task, and a factual knowledge task of simple multiplication and division. Application of the inversion concept in the problem-solving task was low and constant across grades. Most participants approved of the inversion-based shortcut but only a slight majority preferred it. Three clusters of children were identified based on their performance on the three tasks. The inversion cluster used and approved of the inversion shortcut the most and had high factual knowledge. The negation cluster used the negation strategy, had lower approval of the inversion shortcut, and had medium factual knowledge. The computation cluster used computation and had the lowest approval and the weakest factual knowledge. The findings highlight the importance of addressing the multiplication and division inversion concept in theories of children's mathematical competence.}
}
@incollection{YADEN2023849,
title = {Reintroducing “development” into theories of the acquisition and growth of early literacy: developmental science approaches and the cultural-historical perspective of L. S. Vygotsky},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {849-865},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.07103-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305071037},
author = {David B. Yaden and Camille Martinez-Yaden},
keywords = {Microgenetic, Developmental science, Optimizing equilibration, Process-relational, Relational-developmental-system, Backward transition, Early writing, Overlapping waves theory, Prospective models, Retrospective models, Hyperbolic geometry},
abstract = {This chapter contrasts the differences between computational “retrospective” models of early writing achievement whose elements represent static states of being and “prospective” models based upon the principles of developmental science and a process-relational-developmental framework which characterizes early writing performances always in the process of “becoming.” The chapter highlights these differences using examples from Spanish-speaking and Chinese/English emergent bilinguals to illustrate the various patterns of writing development captured in a Piagetian/Vygotskian-inspired early writing assessment. The children's simultaneous display of multiple conceptualizations of the notational system in Spanish, English, and Chinese is interpreted as reflecting aspects of Siegler's “overlapping waves theory” and Piaget's “optimizing equilibration.”}
}
@article{CRONIN2022100213,
title = {A review of in silico toxicology approaches to support the safety assessment of cosmetics-related materials},
journal = {Computational Toxicology},
volume = {21},
pages = {100213},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000019},
author = {Mark T.D. Cronin and Steven J. Enoch and Judith C. Madden and James F. Rathman and Andrea-Nicole Richarz and Chihae Yang},
keywords = {Cosmetics, Risk assessment, , Computational, Read-across, Quantitative structure-activity relationship},
abstract = {In silico tools and resources are now used commonly in toxicology and to support the “Next Generation Risk Assessment” (NGRA) of cosmetics ingredients or materials. This review provides an overview of the approaches that are applied to assess the exposure and hazard of a cosmetic ingredient. For both hazard and exposure, databases of existing information are used routinely. In addition, for exposure, in silico approaches include the use of rules of thumb for systemic bioavailability as well as physiologically-based kinetics (PBK) and multi-scale models for estimating internal exposure at the organ or tissue level. (Internal) Thresholds of Toxicological Concern are applicable for the safety assessment of ingredients at low concentrations. The use of structural rules, (Quantitative) Structure-Activity Relationships ((Q)SARs) and read-across are the most typically applied modelling approaches to predict hazard. Data from exposure and hazard assessment are increasingly being brought together in NGRA to provide an overall assessment of the safety of a cosmetic ingredient. All in silico approaches are reviewed in terms of their maturity and robustness for use.}
}
@article{SAENZROYO2024121922,
title = {Ordering vs. AHP. Does the intensity used in the decision support techniques compensate?},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121922},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121922},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423024247},
author = {Carlos Sáenz-Royo and Francisco Chiclana and Enrique Herrera-Viedma},
keywords = {AHP, IBR, Decision Support Systems, Expertise, Intensity Judgment},
abstract = {The manifestation of the intensity in the judgment of one alternative versus another in the peer comparison processes is a central element in some decision support techniques, such as the Analytical Hierarchy Process (AHP). However, its contribution regarding quality (expected performance) with respect to the priority vector has not been evaluated so far. Using the Intentional Bounded Rationality Methodology (IBRM), this work analyzes the gains obtained from requiring the decision-maker to report an intensity judgment in pairs (AHP) with respect to a technique that only requires expressing a preference (Ordering). The results show that when decision-makers have low levels of expertise, it is possible that a less informative and computational cheap technique (Ordering) performs better than a more informative and computational expensive one (AHP). When decision-makers have medium and high levels of expertise, AHP technique obtains modest gains with respect to the Ordering technique. This study proposes a cost-benefit analysis of decision support techniques contrasting the gains of a technique that requires more resources (AHP) against other that require less resources (Ordering). Our results can change the managing approach of the information obtained from experts’ judgments.}
}
@article{MCCRADDEN2023100864,
title = {A normative framework for artificial intelligence as a sociotechnical system in healthcare},
journal = {Patterns},
volume = {4},
number = {11},
pages = {100864},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100864},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923002489},
author = {Melissa D. McCradden and Shalmali Joshi and James A. Anderson and Alex John London},
abstract = {Summary
Artificial intelligence (AI) tools are of great interest to healthcare organizations for their potential to improve patient care, yet their translation into clinical settings remains inconsistent. One of the reasons for this gap is that good technical performance does not inevitably result in patient benefit. We advocate for a conceptual shift wherein AI tools are seen as components of an intervention ensemble. The intervention ensemble describes the constellation of practices that, together, bring about benefit to patients or health systems. Shifting from a narrow focus on the tool itself toward the intervention ensemble prioritizes a “sociotechnical” vision for translation of AI that values all components of use that support beneficial patient outcomes. The intervention ensemble approach can be used for regulation, institutional oversight, and for AI adopters to responsibly and ethically appraise, evaluate, and use AI tools.}
}
@article{JOHANN2016420,
title = {Soil moisture modeling based on stochastic behavior of forces on a no-till chisel opener},
journal = {Computers and Electronics in Agriculture},
volume = {121},
pages = {420-428},
year = {2016},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2015.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0168169915004020},
author = {André L. Johann and Augusto G. {de Araújo} and Hevandro C. Delalibera and André R. Hirakawa},
keywords = {Soil physics, Computational models, Precision agriculture, Soft computing, Force sensors},
abstract = {Crop-yield variability is frequently associated with soil moisture and its real-time measurement can be an alternative for the automatic control of no-till seeding to improve soil–crop conditions. Soil moisture has a significant influence on soil behavior, markedly on its temporal and spatial variability; however, the measurement of soil moisture is generally time consuming and expensive. Many studies employ electric, electromagnetic, optical, or radiometric sensors for the direct measurement of soil moisture. It is also possible to develop an estimation method employing existing machinery components using mechanical sensors such as load cells. Auto-regressive error function (AREF) combined with computational models is applied in this study for estimating soil moisture using a data set of forces acting on a chisel and speed as inputs to assess the feasibility of achieving more accurate results than previously obtained by Sakai et al. (2005). AREF is a stochastic method that can be applied to the analysis of soil-force patterns acting on a tool. Three computational models are developed, including two artificial neural networks (a Multi-Layer Perceptron (MLP) and a Radial Basis Function (RBF)) and one Neuro-Fuzzy model (ANFIS). These are compared with two multiple linear regression (MLR) models with two and six independent variables. The models’ performances are evaluated using root mean square error (RMSE), determination coefficient (R2), and average percentage error (APE). The computational models demonstrated superior performance compared to MLR, confirming the hypothesis. The neural network models had similar performances with RMSE between 1.27% and 1.30%, R2 around 0.80, and APE between 3.77% and 3.75% for testing data. These results indicate that using AREF parameters combined with computational models may be a suitable technique to estimate soil moisture and has potential to be used in control systems applied to no-till machinery.}
}
@article{JALALIAN2023103602,
title = {Learning about me and you: Only deterministic stimulus associations elicit self-prioritization},
journal = {Consciousness and Cognition},
volume = {116},
pages = {103602},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103602},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023001393},
author = {Parnian Jalalian and Marius Golubickis and Yadvi Sharma and C. {Neil Macrae}},
keywords = {Self, Instrumental learning, Probabilistic selection task, Self-prioritization, Drift diffusion model},
abstract = {Self-relevant material has been shown to be prioritized over stimuli relating to others (e.g., friend, stranger), generating benefits in attention, memory, and decision-making. What is not yet understood, however, is whether the conditions under which self-related knowledge is acquired impacts the emergence of self-bias. To address this matter, here we used an associative-learning paradigm in combination with a stimulus-classification task to explore the effects of different learning experiences (i.e., deterministic vs. probabilistic) on self-prioritization. The results revealed an effect of prior learning on task performance, with self-prioritization only emerging when participants acquired target-related associations (i.e., self vs. friend) under conditions of certainty (vs. uncertainty). A further computational (i.e., drift diffusion model) analysis indicated that differences in the efficiency of stimulus processing (i.e., rate of information uptake) underpinned this self-prioritization effect. The implications of these findings for accounts of self-function are considered.}
}
@article{FRENCH2023100030,
title = {Reflections on 50 years of MCDM: Issues and future research needs},
journal = {EURO Journal on Decision Processes},
volume = {11},
pages = {100030},
year = {2023},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2023.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2193943823000031},
author = {Simon French},
keywords = {Behavioural decision studies, Bayesian analysis, Conflicting objectives, Cynefin, Multiple criteria decision-making (MCDM), Uncertainty},
abstract = {Modern discussions of multiple criteria decision-making extend back about half a century. I reflect on key developments, schools of thought and controversies that have taken place over the period, arguing that perhaps those of us in different schools focus too much on our differences and do not capitalise enough on what we share in common. Moreover, the differences between schools are indications of their respective weaknesses and can drive improvements in each. The discussion points to a number of issues and research needs that the community needs to address.}
}
@incollection{SINGH2025427,
title = {Chapter Seventeen - Internet of Things legal landscape: Privacy, security, liability, and regulatory issues},
editor = {Pethuru Raj and Kavita Saini and Brij B. Gupta},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {138},
pages = {427-447},
year = {2025},
booktitle = {Post-Quantum Cryptography Algorithms and Approaches for IoT and Blockchain Security},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2025.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245825000269},
author = {Sushma Singh and Ravi Chandra Prakash and M. Arvindhan},
keywords = {Internet of Things, Legal landscape, Privacy, Security, Liability, Regulatory challenges, Intellectual property, Emerging trends, Case studies},
abstract = {Rapid adoption of Internet of Things (IoT) technology has created a new era of physical-digital convergence. This chapter examines the complex legal landscape of the Internet of Things, including privacy, security, liability, and regulatory issues. The goal is to help policymakers and industry professionals comprehend the complex IoT legal landscape. The chapter opens by contextualizing the extraordinary convergence of technology and law and emphasizing the need for a proactive legal framework. A full examination of IoT privacy risks follows. This includes an overview of broad data collecting, informed consent issues, and the changing role of individuals in personal data control. Existing data protection rules are evaluated to ensure privacy in the IoT context. The next part examines the legal ramifications of IoT ecosystem security breaches. Manufacturing, software, and service providers’ roles in device and data security are examined. The debate includes cybersecurity rules and the changing legal attitude needed to combat security breaches. In the next section, IoT-related legal frameworks are examined for liability and accountability. To explain IoT liability, the section navigates the complex relationships between manufacturers, software developers, and service providers using case studies and precedents. The next part discusses IoT stakeholders’ regulatory compliance challenges. A review of present legislation’ ability to meet the dynamic nature of IoT technology is provided, along with suggestions for new regulatory frameworks that balance innovation and ethical and legal standards. The chapter examines IoT patents, trademarks, and copyrights. Exploring interoperability, licencing, and IP rights challenges provides practical insights for stakeholders navigating the legal complexity of the quickly evolving IoT. New IoT trends and their legal ramifications are examined throughout the chapter. An adaptive legal framework is needed to suit the changing technology landscape of artificial intelligence, blockchain, and IoT. Strategically interwoven real-world case studies demonstrate how legal principles apply to IoT implementations. These case studies demonstrate how legal principles are used and teach us from important cases. Internet of Things, Legal Landscape, Privacy, Security, Liability, Regulatory Challenges, IP, Emerging Trends, Case Studies.}
}
@article{KIM2025,
title = {Machine Learning–Based Prediction of Substance Use in Adolescents in Three Independent Worldwide Cohorts: Algorithm Development and Validation Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/62805},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125002699},
author = {Soeun Kim and Hyejun Kim and Seokjun Kim and Hojae Lee and Ahmed Hammoodi and Yujin Choi and Hyeon Jin Kim and Lee Smith and Min Seo Kim and Guillaume Fond and Laurent Boyer and Sung Wook Baik and Hayeon Lee and Jaeyu Park and Rosie Kwon and Selin Woo and Dong Keon Yon},
keywords = {adolescents, machine learning, substance, prediction, XGBoost, random forest, ML, substance use, adolescents, adolescent, South Korea, United States, Norway, web-based survey, survey, risk behavior, smoking, alcohol, intervention, interventions},
abstract = {Background
To address gaps in global understanding of cultural and social variations, this study used a high-performance machine learning (ML) model to predict adolescent substance use across three national datasets.
Objective
This study aims to develop a generalizable predictive model for adolescent substance use using multinational datasets and ML.
Methods
The study used the Korea Youth Risk Behavior Web-Based Survey (KYRBS) from South Korea (n=1,098,641) to train ML models. For external validation, we used the Youth Risk Behavior Survey (YRBS) from the United States (n=2,511,916) and Norwegian nationwide Ungdata surveys (Ungdata) from Norway (n=700,660). After developing various ML models, we evaluated the final model’s performance using multiple metrics. We also assessed feature importance using traditional methods and further analyzed variable contributions through SHapley Additive exPlanation values.
Results
The study used nationwide adolescent datasets for ML model development and validation, analyzing data from 1,098,641 KYRBS adolescents, 2,511,916 YRBS participants, and 700,660 from Ungdata. The XGBoost model was the top performer on the KYRBS, achieving an area under receiver operating characteristic curve (AUROC) score of 80.61% (95% CI 79.63-81.59) and precision of 30.42 (95% CI 28.65-32.16) with detailed analysis on sensitivity of 31.30 (95% CI 29.47-33.20), specificity of 99.16 (95% CI 99.12-99.20), accuracy of 98.36 (95% CI 98.31-98.42), balanced accuracy of 65.23 (95% CI 64.31-66.17), F1-score of 30.85 (95% CI 29.25-32.51), and area under precision-recall curve of 32.14 (95% CI 30.34-33.95). The model achieved an AUROC score of 79.30% and a precision of 68.37% on the YRBS dataset, while in external validation using the Ungdata dataset, it recorded an AUROC score of 76.39% and a precision of 12.74%. Feature importance and SHapley Additive exPlanation value analyses identified smoking status, BMI, suicidal ideation, alcohol consumption, and feelings of sadness and despair as key contributors to the risk of substance use, with smoking status emerging as the most influential factor.
Conclusions
Based on multinational datasets from South Korea, the United States, and Norway, this study shows the potential of ML models, particularly the XGBoost model, in predicting adolescent substance use. These findings provide a solid basis for future research exploring additional influencing factors or developing targeted intervention strategies.}
}
@article{HAO2025115545,
title = {Temperature history reconstruction in steel box girders using limited data and proper orthogonal decomposition-based dimension reduction representation},
journal = {Measurement},
volume = {240},
pages = {115545},
year = {2025},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2024.115545},
url = {https://www.sciencedirect.com/science/article/pii/S0263224124014301},
author = {Jing Hao and Hailin Lu and Hongyou Cao and Yunlai Zhou},
keywords = {Temperature history reconstruction, High-frequency temperature component, Stochastic vector processes, Dimension reduction, Proper orthogonal decomposition},
abstract = {The monitoring temperature history of steel box girders inevitably contains gaps or anomalies due to the malfunctions of the structural health monitoring system. Traditional methods for reconstructing temperature histories face the challenge in accounting for the randomness of temperature variations caused by the uncertainty of complex environmental factors. This research proposes a framework for reconstructing the long-term temperature of steel box girders by combining the limited measurements with a proper orthogonal decomposition (POD) based dimension reduction representation approach, to account for the stochastic nature of daily temperature variations. Unlike the conventional Monte Carlo-based POD method, the developed POD-based dimension reduction representation approach effectively reduces the number of elementary random variables from thousands to two by introducing random functions serving as constraints, overcoming the challenge of high-dimensional random variables inherent in the Monte Carlo methods. The proposed approach divides the measured temperature histories into random high-frequency (HF) and deterministic low-frequency (LF) components and establishes the theoretical models of power spectral density and coherence functions of HF temperature components to accommodate the generation of HF temperature component samples, and finally reconstructs temperature samples by superimposing the LF components and generated HF component samples. The results from a practical example demonstrate that the statistical characteristics of representative HF temperature component samples generated by the POD-based dimension reduction representation align well with the corresponding targeted values, and the proposed method outperforms the traditional POD method, yielding a 60% efficiency enhancement without compromising computational accuracy. The developed framework owns apparent superiority in accuracy compared to the traditional POD and the long short-term memory methods, particularly in continuous and extensive missing data. Moreover, the reconstructed temperature samples with assigned probabilities present complete probability information from the level of total probability. These results advance the probabilistic methods in tackling long-term temperature history reconstruction.}
}
@article{CHAVAS2024476,
title = {Bridging the microscopic divide: a comprehensive overview of micro-crystallization and in vivo crystallography},
journal = {IUCrJ},
volume = {11},
number = {4},
pages = {476-485},
year = {2024},
issn = {2052-2525},
doi = {https://doi.org/10.1107/S205225252400513X},
url = {https://www.sciencedirect.com/science/article/pii/S205225252400054X},
author = {Leonard Michel Gabriel Chavas and Fasséli Coulibaly and Damià Garriga and E. N. Baker},
keywords = {micro-crystallization,  crystallography, structural biology, macromolecular research, X-ray diffraction, XFELs, MicroED},
abstract = {The 26th IUCr congress held in Melbourne brought discussions on micro-crystallization and in vivo crystallography within structural biology to the forefront, highlighting innovative approaches and collaborative efforts to advance macromolecular research.
A series of events underscoring the significant advancements in micro-crystallization and in vivo crystallography were held during the 26th IUCr Congress in Melbourne, positioning microcrystallography as a pivotal field within structural biology. Through collaborative discussions and the sharing of innovative methodologies, these sessions outlined frontier approaches in macromolecular crystallography. This review provides an overview of this rapidly moving field in light of the rich dialogues and forward-thinking proposals explored during the congress workshop and microsymposium. These advances in microcrystallography shed light on the potential to reshape current research paradigms and enhance our comprehension of biological mechanisms at the molecular scale.}
}
@incollection{WANG2017259,
title = {Chapter 14 - Reason and Emotion in Xunzi’s Moral Psychology},
editor = {T.-W. Hung and T.J. Lane},
booktitle = {Rationality},
publisher = {Academic Press},
address = {San Diego},
pages = {259-276},
year = {2017},
isbn = {978-0-12-804600-5},
doi = {https://doi.org/10.1016/B978-0-12-804600-5.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046005000143},
author = {E.H. Wang},
keywords = {Xunzi, moral rationalism, emotion},
abstract = {In this paper I explore the extent to which Xunzi may, or may not, be a moral rationalist by investigating the roles reason and emotion play in Xunzi’s moral psychology. To this end, I address Soek’s and Slingerland’s recent work on this subject. Seok (2013) recently characterized two contrasting models of moral psychology: “reason based” and “emotion based”; the former takes the reflective and conscious reasoning ability to be the essence of one’s moral judgment and action, while emotions and affective mechanisms play only minor roles (if any); the latter takes emotional states to be essential or at least necessary. Soek understands Confucian ethics in general to operate with the emotion-based model, but his argument mainly concerns Mencius’ work. Slingerland (2010), on the other hand, categorizes Xunzi’s moral psychology as a theory that presumes what he calls the “high reason model,” which significantly resembles the reason-based model in Soek’s account. Slingerland understands that, on Xunzi’s account, rational faculties and emotional faculties are competitive in the reasoning process. Moreover, he takes Xunzi to prioritize the rational faculties, thinking that they can and should monitor emotional responses, and override them when needed. Slingerland also cites recent empirical studies to criticize this model. I argue that Xunzi’s moral psychology cannot be captured by either of the two models Soek characterizes, but presents to us a third alternative: it gives us a good example of a hybrid model of these two. Indeed, Xunzi’s emphasis on ritual practices in the cultivation of xin and qing toward sagehood sheds light on a possible interplay between reason and emotion in ideal moral judgment/decision. This discussion inspires further consideration of what a moral rationalist may be, and the extent to which Xunzi may, or may not, be a moral rationalist.}
}
@article{SCHOLL201856,
title = {Understanding psychiatric disorder by capturing ecologically relevant features of learning and decision-making},
journal = {Behavioural Brain Research},
volume = {355},
pages = {56-75},
year = {2018},
note = {SI: MCC 2016},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2017.09.050},
url = {https://www.sciencedirect.com/science/article/pii/S0166432817305673},
author = {Jacqueline Scholl and Miriam Klein-Flügge},
keywords = {Reinforcement learning, Decision-making, Computational psychiatry},
abstract = {Recent research in cognitive neuroscience has begun to uncover the processes underlying increasingly complex voluntary behaviours, including learning and decision-making. Partly this success has been possible by progressing from simple experimental tasks to paradigms that incorporate more ecological features. More specifically, the premise is that to understand cognitions and brain functions relevant for real life, we need to introduce some of the ecological challenges that we have evolved to solve. This often entails an increase in task complexity, which can be managed by using computational models to help parse complex behaviours into specific component mechanisms. Here we propose that using computational models with tasks that capture ecologically relevant learning and decision-making processes may provide a critical advantage for capturing the mechanisms underlying symptoms of disorders in psychiatry. As a result, it may help develop mechanistic approaches towards diagnosis and treatment. We begin this review by mapping out the basic concepts and models of learning and decision-making. We then move on to consider specific challenges that emerge in realistic environments and describe how they can be captured by tasks. These include changes of context, uncertainty, reflexive/emotional biases, cost-benefit decision-making, and balancing exploration and exploitation. Where appropriate we highlight future or current links to psychiatry. We particularly draw examples from research on clinical depression, a disorder that greatly compromises motivated behaviours in real-life, but where simpler paradigms have yielded mixed results. Finally, we highlight several paradigms that could be used to help provide new insights into the mechanisms of psychiatric disorders.}
}
@article{FRY2010218,
title = {On the nature of tetraalkylammonium ions in common electrochemical solvents: General and specific solvation – Quantitative aspects},
journal = {Journal of Electroanalytical Chemistry},
volume = {638},
number = {2},
pages = {218-224},
year = {2010},
issn = {1572-6657},
doi = {https://doi.org/10.1016/j.jelechem.2009.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0022072809004288},
author = {Albert J. Fry and L. Kraig Steffen},
keywords = {Computational electrochemistry, Tetraalkylammonium ions, Specific solvation, Inner sphere solvation, General solvation},
abstract = {The free energies of each of 80 tetraalkylammonium ion/solvent complexes [R4N+/(solv)n], with R ranging from methyl through butyl and n ranging from 1 through 4, were computed by density functional theory (DFT) in five common electrochemical solvents: dimethylformamide (DMF), dimethylsulfoxide (DMSO), acetonitrile (AN), dichloromethane (DCM), and methanol (MeOH). The energies of the complexes were computed both with and without their solvation energies. Additional computations of the energies of the individual components, both solvated and unsolvated, were also carried out. The resulting data permit construction of a thermodynamic cycle for each R4N+/solvent pair that in turn allows the determination of the extent of general and specific solvation energies for that pair. An additional series of computations for pentane as solvent were carried out. Since this solvent should not coordinate with tetraalkylammonium ions, these computations provide a test of the validity of the computational method. This work represents a useful new general protocol for assessing the relative importance of general and specific solvation in chemical systems.}
}
@article{LI2021711,
title = {Prediction of BLEVE blast loading using CFD and artificial neural network},
journal = {Process Safety and Environmental Protection},
volume = {149},
pages = {711-723},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2021.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0957582021001324},
author = {Jingde Li and Qilin Li and Hong Hao and Ling Li},
keywords = {ANN, BLEVE, Blast wave, Peak pressure, CFD, Neural networks},
abstract = {Boiling Liquid Expanding Vapour Explosions (BLEVEs) are extreme explosions driven by nonlinear physical processes associated with explosively expanded vapour and flashed liquid. Blast loading generated from BLEVEs may severely harm structures and people. Prediction of such strong explosions is not currently feasible using simple tools. Physics-based Computational Fluid Dynamics (CFD) methods are commonly utilized to predict the blast loading of BLEVE by going through many empirical formulas that map input variables to the target progressively. The calculation is often time-consuming, and it is therefore impractical to apply these methods to predict explosion loads from BLEVE in normal design analysis. Thinking of the composition of empirical relations in CFD models as a complex and nonlinear function, it is necessary to find an approximation of this function that can be efficiently calculated. The Artificial Neural Network (ANN) is a data-driven computational model that is capable of approximating any functions by learning from training data. Once properly trained, ANN can produce accurate predictions even for unseen inputs. This article presents the development of an ANN model to predict blast loading of BLEVEs in an open environment. A rigorous validation process is presented for the design of ANN structure, and the selected ANN is trained using validated simulation data from CFD models. Extensive evaluation of the network predictive performance is conducted, and it shows that the developed ANN can reproduce the result of CFD models effectively and efficiently, not only on simulation data but also on real experimental data. The prediction of ANN has a percentage error around 6 % and R2 value over 0.99 with the result of CFD simulated data. It speeds up the processing time from hours to seconds and only increases the error from 26.3 %–27.6 %, compared to the CFD simulations of real experimental data. Therefore, the developed ANN model can be potentially applied in the process engineering to generate a large number of reliable data for safety and risk assessment of BLEVEs in a more efficient way.}
}
@article{PANTALEON201579,
title = {Taylor series expansion using matrices: An implementation in MATLAB®},
journal = {Computers & Fluids},
volume = {112},
pages = {79-82},
year = {2015},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2015.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045793015000183},
author = {Carlos Pantaleón and Amitabha Ghosh},
keywords = {Taylor series, Finite differences, Truncation error, Modified equation, Symbolic computation},
abstract = {Taylor series expansions are widely used in engineering approximations, for instance, to develop finite differences schemes or numerical integration methods. This technical note presents a novel technique to generate, display and manipulate Taylor series expansion by using matrices. The resulting approach allows algebraic manipulation as well as differentiation in a very intuitive manner in order to experiment with different numerical schemes, their truncation errors and their structures, while avoiding manual calculation errors. A detailed explanation of the mathematical procedure to generate a matrix form of the Taylor series expansion for a function of two variables is presented along with the algorithm of an implementation in MATLAB®. Example cases of different orders are tabulated to illustrate the generation and manipulation capabilities of this technique. Additionally, an extended application is developed to determine the modified equations of finite difference schemes for partial differential equations, with one-dimensional examples of the wave equation and the heat equation using explicit and implicit schemes.}
}
@article{YANG2023105120,
title = {Thoughts of brain EEG signal-to-text conversion using weighted feature fusion-based Multiscale Dilated Adaptive DenseNet with Attention Mechanism},
journal = {Biomedical Signal Processing and Control},
volume = {86},
pages = {105120},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105120},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423005530},
author = {Jing Yang and Muhammad Awais and Md. Amzad Hossain and Por {Lip Yee} and Ma. Haowei and Ibrahim M. Mehedi and A.I.M. Iskanderani},
keywords = {Thought-to-text conversion, Electroencephalography signal, Optimal weighted feature fusion, Eurasian oystercatcher wild geese migration optimization, Multiscale Dilated Adaptive DenseNet with Attention Mechanism},
abstract = {Individuals with visual inefficiencies or different abilities face difficulties using their hands to operate smartphones and computers, necessitating reliance on others to enter data. Such dependence may lead to security and privacy issues, especially when sensitive information is shared with helpers. To address this problem, we present Think2Type, an efficient Brain-Computer Interface (BCI) that enables users to translate their active intentions into text format based on Morse code. BCI leverages brain activity to facilitate interaction with computers, often captured via Electroencephalography (EEG). This work proposes an enhanced attention-based deep learning strategy to develop an efficient text conversion mechanism from EEG signals. We begin by collecting EEG signals from standard benchmark datasets and extracting spectral and statistical features in phase 1, concatenating them into concatenated feature set 1 (F1). In phase 2, we extract spatial and temporal features via a One-Dimensional Convolutional Neural Network (1DCNN) and a Recurrent Neural Network (RNN), respectively, concatenating them into concatenated feature set 2 (F2). Weighted feature fusion is performed on concatenated features F1 and F2, with the hybrid optimization algorithm Eurasian Oystercatcher Wild Geese Migration Optimization (EOWGMO) optimizing the weight for improved fusion efficiency. The text conversion phase utilizes the Multiscale Dilated Adaptive DenseNet with Attention Mechanism (MDADenseNet-AM) to obtain the converted text information. The MDADenseNet-A's parameters are optimized to improve thought-to-text conversion performance. The developed model's performance is evaluated via experimental analysis and compared to conventional techniques, resulting in a higher accuracy value of 96.41%, facilitating appropriate text conversion.}
}
@article{LIU2024100129,
title = {Extracting multi-objective multigraph features for the shortest path cost prediction: Statistics-based or learning-based?},
journal = {Green Energy and Intelligent Transportation},
volume = {3},
number = {1},
pages = {100129},
year = {2024},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2023.100129},
url = {https://www.sciencedirect.com/science/article/pii/S2773153723000658},
author = {Songwei Liu and Xinwei Wang and Michal Weiszer and Jun Chen},
keywords = {Multi-objective multigraph, Feature extraction, Shortest path cost prediction, Node patterns, Node embeddings, Regression},
abstract = {Efficient airport airside ground movement (AAGM) is key to successful operations of urban air mobility. Recent studies have introduced the use of multi-objective multigraphs (MOMGs) as the conceptual prototype to formulate AAGM. Swift calculation of the shortest path costs is crucial for the algorithmic heuristic search on MOMGs, however, previous work chiefly focused on single-objective simple graphs (SOSGs), treated cost enquires as search problems, and failed to keep a low level of computational time and storage complexity. This paper concentrates on the conceptual prototype MOMG, and investigates its node feature extraction, which lays the foundation for efficient prediction of shortest path costs. Two extraction methods are implemented and compared: a statistics-based method that summarises 22 node physical patterns from graph theory principles, and a learning-based method that employs node embedding technique to encode graph structures into a discriminative vector space. The former method can effectively evaluate the node physical patterns and reveals their individual importance for distance prediction, while the latter provides novel practices on processing multigraphs for node embedding algorithms that can merely handle SOSGs. Three regression models are applied to predict the shortest path costs to demonstrate the performance of each. Our experiments on randomly generated benchmark MOMGs show that (i) the statistics-based method underperforms on characterising small distance values due to severe overestimation; (ii) A subset of essential physical patterns can achieve comparable or slightly better prediction accuracy than that based on a complete set of patterns; and (iii) the learning-based method consistently outperforms the statistics-based method, while maintaining a competitive level of computational complexity.}
}
@incollection{BARBER20251,
title = {Cultural contributions to cognitive aging},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {1-16},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00042-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000425},
author = {Nicolette Barber and Ioannis Valoumas and Chaipat Chunharas and Sirawaj Itthipuripat and Angela Gutchess},
keywords = {Aging, Attention, Autobiographical memory, Cognition, Cognitive aging, Cognitive neuroscience, Cross-cultural, Culture, Long-term memory, Memory},
abstract = {In this article, we review research on the influences of culture on cognitive aging, with a focus on long-term memory and attention. Given the small number of studies directly investigating cognitive aging across cultures, we draw on existing cross-cultural studies comparing brain and behavior in young adult samples. We outline the potential for future research and discuss the importance of adopting a cross-cultural lens to support cognition and well-being in older adults in diverse cultural contexts.}
}
@article{NOORMAN2017677,
title = {Biochemical engineering’s grand adventure},
journal = {Chemical Engineering Science},
volume = {170},
pages = {677-693},
year = {2017},
note = {13th International Conference on Gas-Liquid and Gas-Liquid-Solid Reactor Engineering},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2016.12.065},
url = {https://www.sciencedirect.com/science/article/pii/S0009250916307266},
author = {Henk J. Noorman and Joseph J. Heijnen},
keywords = {Lifeline modeling, Bioprocess design, Scale-down, Bio-economy, Renewable feedstocks, Bio-products},
abstract = {Building on the recent revolution in molecular biology, enabling a wealth of bio-product innovations made from renewable feedstocks, the biotechnology field is in a transition phase to bring the products to the market. This requires a shift from natural sciences to engineering sciences with first conception of new, efficient large-scale bioprocess designs, followed by implementation of the most promising design in practice. Inspired by a former publication by O. Levenspiel in 1988, an outline is presented of main challenges that the field of biochemical engineering is currently facing, in a context of major global sustainability trends. The critical stage is the conceptual design phase. Issues can best be addressed and overcome by adopting an attitude where one begins with the end in mind. This applies to three principal components: 1. the bioprocess value chain, where the product specifications and downstream purification schemes should be set before defining the upstream sections, 2. the time perspective, starting in the future assuming that feedstock and product-market combinations are already in place and then going back to today, and 3. the scale of operation, where the industrial operation sets the boundaries for all labscale research and development, and not vice versa. In this way, and ideal process is defined taking constraints from anticipated manufacturing into account. For illustration, three bioprocess design examples are provided, that show how new, ideal conceptual designs can be generated. These also make clear that the engineering sciences are undergoing a revolution, where bio-based approaches replace fossil routes, and gross simplification is replaced by highly detailed computational methods. For biochemical processes, lifeline modeling frameworks are highlighted as powerful means to reconcile the competing needs for high speed and high quality in biochemical engineering, both in the design and implementation stages, thereby enabling significant growth of the bio-based economy.}
}
@incollection{HERLIHY20211,
title = {Chapter 1 - Introduction},
editor = {Maurice Herlihy and Nir Shavit and Victor Luchangco and Michael Spear},
booktitle = {The Art of Multiprocessor Programming (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {1-18},
year = {2021},
isbn = {978-0-12-415950-1},
doi = {https://doi.org/10.1016/B978-0-12-415950-1.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124159501000094},
author = {Maurice Herlihy and Nir Shavit and Victor Luchangco and Michael Spear},
keywords = {parallelism, concurrent programming, shared-memory multiprocessors, safety, liveness, mutual exclusion, coordination protocol, producer—consumer problem, readers–writers problem, deadlock-freedom, starvation-freedom, Amdahl's law},
abstract = {This chapter introduces and motivates the study of shared-memory multiprocessor programming, or concurrent programming. It describes the overall plan of the book, and then presents some basic concepts of concurrent computation, and presents some of the fundamental problems—mutual exclusion, the producer–consumer problem, and the readers–writers problem—and some simple approaches to solve these problems. It ends with a brief discussion of Amdahl's law.}
}
@article{SCHMID2019178,
title = {Representing stuff in the human brain},
journal = {Current Opinion in Behavioral Sciences},
volume = {30},
pages = {178-185},
year = {2019},
note = {Visual perception},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300816},
author = {Alexandra C Schmid and Katja Doerschner},
abstract = {Our experience of materials does not merely comprise judgments of single properties such as glossiness or roughness but is rather made up of a multitude of simultaneous impressions of qualities. To understand the neural mechanisms yielding such complex impressions, we suggest that it is necessary to extend existing experimental approaches to those that view material perception as a distributed and dynamic process. A distributed representations framework not only fits better with our perceptual experience of material qualities, it is commensurate with recent psychophysics and neuroimaging results.}
}
@article{LASHKARY2025420,
title = {Detection of Alzheimer’s Disease progression from Structural MRIs using ConvMixer based Deep Learning Model},
journal = {Procedia Computer Science},
volume = {258},
pages = {420-429},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.278},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925013808},
author = {Mahak Lashkary and Kanika Bansal and Samyak Jain and Praveen Kumar Shukla and Hitesh Tekchandani and Vijaypal Singh Dhaka},
keywords = {Alzheimer’s Disease, Cognitive Impairment (CI), Cognitively Normal (CN), ConvMixer, deep learning, structural MRI, neuroimaging, Classification, early detection, machine learning},
abstract = {Alzheimer’s Disease (AD) is a progressive neurodegenerative disorder that significantly impacts cognitive functions and quality of life. Affecting over 50 million people worldwide, early and accurate detection of AD progression is crucial for timely intervention and management. This paper presents a reliable method for classifying Alzheimer’s stages using Deep Learning based ConvMixer model on structural MRI data. The proposed deep learning based convmixer model significantly utilizes the patch based learning with depthwise separable convolution to achieve improved performance. The dataset used in this work has been extracted from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and comprises of three classes: Alzheimer’s Disease (AD), Cognitive Impairment (CI), and Cognitively Normal (CN). The proposed approach demonstrated exceptional performance, achieving an accuracy of 99.88%, specificity of 99.87%, sensitivity of 99.90 and precision of 99.95%. The achieved results demonstrate the proposed model’s accuracy in distinguishing Alzheimer’s stages, making it a promising tool for early diagnosis and disease progression monitoring.}
}
@article{JOHNSON2022105743,
title = {Metacognition for artificial intelligence system safety – An approach to safe and desired behavior},
journal = {Safety Science},
volume = {151},
pages = {105743},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105743},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522000832},
author = {Bonnie Johnson},
keywords = {Metacognition, Artificial intelligence systems, Machine learning, System safety, Complexity},
abstract = {Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human–machine teaming operations. In parallel, the increasing volume, velocity, variety, veracity, value, and variability of data is confounding the complexity of these new systems – creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.}
}
@article{VERDUZCO2022103189,
title = {CALRECOD — A software for Computed Aided Learning of REinforced COncrete structural Design},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103189},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103189},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000965},
author = {Luis Fernando Verduzco and Jaime Horta and Miguel A. Pérez Lara y Hernández and Juan Bosco Hernández},
keywords = {CALRECOD, Reinforced concrete structures, High education, Computed aided learning, Optimization methods},
abstract = {It is presented the development and implementation of a new computed aided learning MatLab Toolbox for the design of reinforced concrete structures named as CALRECOD for their abbreviation Computer Aided Learning of Reinforced Concrete Design. Such development emerges as the result of a series of research works in the Autonomous University of Queretaro with the main purpose of improving the way in which the design of reinforced concrete structures is taught in high education institutions. CALRECOD uses optimization methods and algorithms to aid students in their design interaction learning so that they are able to compare their own designs and what commercial software delivers with optimal ones given certain load conditions on the elements or structures. The software consists almost entirely of MatLab functions (.m files) and the ACI 318-19 code is taken as their main design reference to make it internationally useful, although in some cases the Mexican code NTC-17 specifications are used. Besides MatLab functions, the software consists as well of ANSYS SpaceClaim script functions (.scscript files) as an additional tool for the aid in the visualization of design results in a 3D space in the software ANSYS SpaceClaim. CALRECOD has proven to be versatile, flexible and of easy use with a huge potential to increase learning outcomes for students in high education programs related with the design of reinforced concrete structures as well as to enhance the creation of efficient interactive environments for researchers and academics focused on the development of new design and analysis methods for such structures. With their optimization design functions, a solid comparison platform of designs’ performance could be laid out, and with its extended function design packages for structural systems, reinforced concrete design courses could be enhanced in a great deal regarding their program content’s scope. The software can be found at: https://github.com/calrecod/CALRECOD.}
}
@article{CORTENBACH2024104869,
title = {The Dial-a-Ride problem with meeting points: A problem formulation for shared demand–responsive transit},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {169},
pages = {104869},
year = {2024},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2024.104869},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X24003905},
author = {L.E. Cortenbach and K. Gkiotsalitis and E.C. {van Berkum} and E. Walraven},
keywords = {Dial-a-Ride problem, Meeting points, Demand–responsive transit, Tabu search},
abstract = {In this paper, a formulation for the Dial-a-Ride Problem with Meeting Points (DARPmp) is introduced. The problem consists of defining routes that satisfy trip requests between pick-up and drop-off points while complying with time window, ride time, vehicle load, and route duration constraints. A set of meeting points is defined, and passengers may be asked to use these meeting points as alternative pickup or drop-off points if this results in routes with lower costs. Incorporating meeting points into the DARP is achieved by formulating a mixed-integer linear program. Two preprocessing steps and three valid inequalities are introduced, which improve the computational performance when solving the DARPmp to global optimality. Two versions of the Tabu Search metaheuristic are proposed to approximate the optimal solution in large-scale networks due to the NP-hardness of DARPmp. Performing numerical experiments with benchmark instances, this study demonstrates the benefits of DARPmp compared to DARP in terms of reducing vehicle running costs.}
}
@article{READ201952,
title = {Using neural networks as models of personality process: A tutorial},
journal = {Personality and Individual Differences},
volume = {136},
pages = {52-67},
year = {2019},
note = {Dynamic Personality Psychology},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2017.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0191886917306724},
author = {Stephen J. Read and Vita Droutman and Benjamin J. Smith and Lynn C. Miller},
keywords = {Neural networks, Computational modeling, Within-subjects variability, Connectionist modeling, Personality dynamics},
abstract = {This paper presents a tutorial for creating neural network models of personality processes. Such models enable researchers to create explicit models of both personality structure and personality dynamics, and to address issues of recent concern in personality, such as, “If personality is stable, then how is it possible that within subject variability in personality states can be as large as or larger than between subject variability in personality?” or “Is it possible to understand personality dynamics and personality structure within a common framework?” We discuss why one should want to use neural networks, review what a neural network model is, review a previous model we have constructed, discuss how to conceptualize issues in such a way that they can be computationally modeled, show how that conceptualization can be translated into a model, and discuss the utility of such models for understanding personality structure and personality dynamics. To build our model we use a neural network modeling package called emergent that is freely available, and a specific architecture called Leabra to build a runnable model that addresses one of the questions posed above: How can within subject variability in personality related states be as large as between subject variability in personality?}
}
@article{BREIGER2018104,
title = {Capturing distinctions while mining text data: Toward low-tech formalization for text analysis},
journal = {Poetics},
volume = {68},
pages = {104-119},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X17301584},
author = {Ronald L. Breiger and Robin Wagner-Pacifici and John W. Mohr},
keywords = {Text mining, Hermeneutics, National security, Computational sociology, Big data, Close reading},
abstract = {In this article we consider some low-tech approaches to text mining. Our goal is to articulate a RiCH (Reader in Control of Hermeneutics) style of text analysis that takes advantage of the digital affordances of modern reading practices and easily deployable computational tools while also preserving the primacy of the interpretive lens of the human reader. In the article we offer three analytical interventions that are suitable to the low-tech formalizations we propose: the first and most developed intervention tracks the (normally computationally ignored) “stop” words; the second identifies the use of strategic anxiety terms in the texts; and the third (less developed in this article) introduces the grammatical features of modality (including modalization statements of probability and usuality, and modulation statements regarding degrees of obligation and inclination). All three analytical interventions provide a productive tracking of various modes and degrees of strategic decisiveness, contradiction, uncertainty and indeterminacy in a corpus of recent U.S. National Security Strategy reports.}
}
@article{DELLACORTE2016209,
title = {Referential description of the evolution of a 2D swarm of robots interacting with the closer neighbors: Perspectives of continuum modeling via higher gradient continua},
journal = {International Journal of Non-Linear Mechanics},
volume = {80},
pages = {209-220},
year = {2016},
note = {Dynamics, Stability, and Control of Flexible Structures},
issn = {0020-7462},
doi = {https://doi.org/10.1016/j.ijnonlinmec.2015.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0020746215001468},
author = {Alessandro {Della Corte} and Antonio Battista and Francesco dell׳Isola},
keywords = {Swarm robot, Second gradient continua, Generalized continua, Deformable bodies},
abstract = {In the present paper a discrete robotic system model whose elements interact via a simple geometric law is presented and some numerical simulations are provided and discussed. The main idea of the work is to show the resemblance between the cases of first and second neighbors interaction with (respectively) first and second gradient continuous deformable bodies. Our numerical results showed indeed that the interaction and the evolution process described is suitable to closely reproduce some basic characteristics of the behavior of bodies whose deformation energy depends on first or on higher gradients of the displacement. Moreover, some specific qualitative characteristics of the continuous deformation are also reproduced. The model introduced here will need further investigation and generalization in both theoretical and numerical directions.}
}
@article{UMAIR2024106224,
title = {Emotion Fusion-Sense (Emo Fu-Sense) – A novel multimodal emotion classification technique},
journal = {Biomedical Signal Processing and Control},
volume = {94},
pages = {106224},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106224},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424002829},
author = {Muhammad Umair and Nasir Rashid and Umar {Shahbaz Khan} and Amir Hamza and Javaid Iqbal},
keywords = {EEG, ECG, GSR, Respiration amplitude, Body temperature, LSTM, Feature fusion, Modality biasing, Multimodal emotion classification},
abstract = {Human emotions play a vital role in overall well-being. With the advent of advance technologies growing interest has been observed in developing a multimodal emotion classification system that can accurately interpret human emotions. The article presents a comprehensive overview of a multimodal emotion classification system (Emo Fu-Sense) designed to capture the rich and nuanced nature of human emotions. Objective of Emo Fu-Sense is to integrates information from Electrocardiogram (ECG), Galvanic Skin Response (GSR), Electroencephalograph (EEG), respiration amplitude and body temperature to achieve holistic understanding of emotional states. To effectively extract information from multimodal data, designed system employs conventional methods with sophisticated machine learning algorithms including Long Short-Term Memory (LSTM), a variety of Recurrent Neural Network (RNN). Recommended solution extracts column wise features independently from different modalities based on the windowing operation. Finally, feature fusion and modality biasing were used to combine the information from different modalities. The proposed method has not only highlighted the limitations of unimodal system but has achieved a classification accuracy of 92.62 %, with an average F1-Score of 93 % and 9.2 % of Mean Absolute Error (MAE). Obtained results are better than existing state-of-the-art approaches. Evaluation of the multimodal emotion classification system was conducted on MAHNOB-HCI dataset, which encompasses a wide range of emotional expressions across various contexts and individuals. The integration of multiple modalities and advanced machine learning techniques enables a more comprehensive understanding of emotional states and highlights the significance of research and development in the field of affective computing.}
}
@article{2024100678,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100678},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100678},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000473}
}
@article{KOTYRA2023105613,
title = {High-performance watershed delineation algorithm for GPU using CUDA and OpenMP},
journal = {Environmental Modelling & Software},
volume = {160},
pages = {105613},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105613},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222003139},
author = {Bartłomiej Kotyra},
keywords = {Watershed delineation, GIS, Parallel algorithms, GPU, CUDA, OpenMP},
abstract = {Watershed delineation is one of the fundamental tasks in hydrological studies. Tools for extracting watersheds from digital elevation models and flow direction rasters are commonly implemented in GIS software packages. However, the performance of available techniques and algorithms often turns out to be far from sufficient, especially when working with large datasets. While modern hardware offers high computing performance through massive parallelism, there is still a need for algorithms that can effectively use these capabilities. This paper proposes an algorithm for rapid watershed delineation directly from flow direction rasters, using the possibilities offered by modern GPU devices. Performance measurements show a significant reduction in execution time compared to other parallel solutions proposed for this task in the literature. Moreover, this implementation makes it possible to delineate multiple watersheds from the same dataset simultaneously, each having one or more outlet cells, with virtually no additional computational cost.}
}
@incollection{GINSBURGH2006947,
title = {Chapter 27 The Computation of Prices Indices},
editor = {Victor A. Ginsburg and David Throsby},
series = {Handbook of the Economics of Art and Culture},
publisher = {Elsevier},
volume = {1},
pages = {947-979},
year = {2006},
issn = {1574-0676},
doi = {https://doi.org/10.1016/S1574-0676(06)01027-1},
url = {https://www.sciencedirect.com/science/article/pii/S1574067606010271},
author = {Victor Ginsburgh and Jianping Mei and Michael Moses},
keywords = {prices indices, repeat sales, hedonic pricing, auctions},
abstract = {While there are no significant investment characteristics that inhibit art from being considered as an asset, a major hurdle has long been the lack of a systematic measure of its financial performance. Due to its heterogeneity (each piece is different) and its infrequency of trading (the exact same piece does not come to the market very often), the determination of changes in market value is difficult to ascertain. Two estimation methods are commonly used to construct indices. Repeat-sales regression (RSR) uses prices of individual objects traded at two distinct moments in time. If the characteristics of an object do not change (which is usually so for collectibles), the heterogeneity issue is bypassed. The basic idea of the hedonic regression (HR) method is to regress prices on various attributes of objects (dimensions, artist, subject matter, etc.) and to use the residuals of the regression which can be considered as “characteristic-free prices” to compute the price index. The chapter deals with the basics of hedonic and repeat-sales estimators, and tries to interpret in economic terms what both are trying to achieve. It also goes into some more technical details which may be useful for researchers who want to construct such indices, and gives some guidelines on how to go about collecting data, and the choice between RSR and HR that this induces. Both methods are compared using simulated returns, pointing to which method should be used given the data at hand.}
}
@article{GUPTA2022103,
title = {The interaction between technology, business environment, society, and regulation in ICT industries},
journal = {IIMB Management Review},
volume = {34},
number = {2},
pages = {103-115},
year = {2022},
issn = {0970-3896},
doi = {https://doi.org/10.1016/j.iimb.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0970389622000465},
author = {Subhashish Gupta},
keywords = {ICT, Technology, Disruption, Convergence, Interaction, Big data, Network effects, Innovation, Competition law, Strategy},
abstract = {This paper focusses on the interaction between technology, the business environment, regulation, and society in ICT industries. The role of technological advances in communication (e.g., cellular mobile, 5 G, spectrum allocation) and in computational advances (e.g., cloud, Internet of Things, artificial intelligence) along with developments in the business environment (e.g., disruption, convergence, Industry 4.0) and the regulatory environment (e.g., competition law and market regulation) in the model is explained. The economics of network industries and competition law and strategies such as vertical integration, bundling, and tying are described. The role of regulation and innovation is discussed along with some cases.}
}
@article{DISESSA198067,
title = {Computation as a physical and intellectual environment for learning physics},
journal = {Computers & Education},
volume = {4},
number = {1},
pages = {67-75},
year = {1980},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(80)90009-3},
url = {https://www.sciencedirect.com/science/article/pii/0360131580900093},
author = {A.A. DiSessa}
}
@article{HALL1996115,
title = {The role of creativity within best practice manufacturing},
journal = {Technovation},
volume = {16},
number = {3},
pages = {115-121},
year = {1996},
issn = {0166-4972},
doi = {https://doi.org/10.1016/0166-4972(95)00050-X},
url = {https://www.sciencedirect.com/science/article/pii/016649729500050X},
author = {David J. Hall},
abstract = {‘Best practice’ manufacturing is linked directly to aspects of creativity, through an appreciation of the operation of the practitioner's brain. It is suggested that the introduction of techniques such as benchmarking or business process re-engineering cannot succeed in the long term, unless the correct understanding is developed within the management team. Concepts of mind set and lateral thinking are related to the top-down introduction of step change, whilst ‘total quality’ programmes develop the culture necessary for bottom-up continuous improvement. Successful companies will run the two approaches in parallel.}
}
@incollection{BAREISS1993157,
title = {The Evolution of a Case-Based Computational Approach to Knowledge Representation, Classification, and Learning},
editor = {Glenn V. Nakamura and Roman Taraban and Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {29},
pages = {157-186},
year = {1993},
booktitle = {Categorization by Humans and Machines},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60139-5},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108601395},
author = {Ray Bareiss and Brian M. Slator}
}
@article{ISMAILOVA2020291,
title = {Hereditary information processes with semantic modeling structures},
journal = {Procedia Computer Science},
volume = {169},
pages = {291-296},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303045},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {semantic information processing, computational model, variable domains},
abstract = {In practice, when developing an information model, inheritance and composition mechanisms are used, which allows the model developer to extend the properties of the class. In this paper, we establish and use the difference between these two closely related representations when applied in aspect-oriented modeling. In particular, when an aspect is applied to extend the base class of the original model, the designer must choose to use composition. Depending on the composition order, indexing occurs, which can lead to the expansion of the base class by dynamic effects. With a different compositional order, a class narrowing occurs, since it becomes necessary to take into account an additional property. If you intend to define an alternative to a base class with advanced functionality, then inheritance should be used. The work demonstrates the power of the combined use of inheritance and composition, which allows us to develop an aspect-oriented modeling of a family of property transformations, in which a line of intermediate models of the working information process arises.}
}
@article{SPARKES202515,
title = {Quantum-inspired algorithm could give us better weather forecasts},
journal = {New Scientist},
volume = {265},
number = {3529},
pages = {15},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00218-0},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925002180},
author = {Matthew Sparkes}
}
@article{DAVIS2024307,
title = {AI rising in higher education: opportunities, risks and limitations},
journal = {Asian Education and Development Studies},
volume = {13},
number = {4},
pages = {307-319},
year = {2024},
issn = {2046-3162},
doi = {https://doi.org/10.1108/AEDS-01-2024-0017},
url = {https://www.sciencedirect.com/science/article/pii/S2046316224000154},
author = {Adrian John Davis},
keywords = {Human mind, Human intelligence, Human consciousness, Artificial intelligence (AI), Artificial consciousness, Quality teaching},
abstract = {Purpose
The aim of this paper is twofold: to explore the significance and implications of the rise of AI technology for the field of tertiary education in general and, in particular, to answer the question of whether teachers can be replaced by intelligent AI systems such as androids, what that requires in terms of human capabilities and what that might mean for teaching and learning in higher education.
Design/methodology/approach
Given the interdisciplinary nature of this conceptual paper, a literature review serves as a methodological tool to access data pertaining to the research question posed in the paper.
Findings
This exploratory paper gathers a range of evidence from the philosophy of mind (the mind-body problem), Kahneman’s (2011) System 1 and System 2 models of the mind, Gödel’s (1951) Two Incompleteness Theorems, Polanyi’s (1958, 1966) theory of tacit knowing and Searle’s (1980) Chinese Room thought experiment to the effect that no AI system can ever fully replace a human being because no machine can replicate the human mind and its capacity for intelligence, consciousness and highly developed social skills such as empathy and cooperation.
Practical implications
AI is rising, but there are inherent limits to what machines can achieve when compared to human capabilities. An android can at most attain “weak AI”, that is, it can be smart but lack awareness or empathy. Therefore, an analysis of good teaching at the tertiary level shows that learning, knowledge and understanding go far beyond any quantitative processing that an AI machine does so well, helping us to appreciate the qualitative dimension of education and knowledge acquisition. ChatGPT is robotic, being AI-generated, but human beings thrive on the human-to-human interface – that is, human relationships and meaningful connections – and that is where the true qualitative value of educational attainment will be gauged.
Social implications
This paper has provided evidence that human beings are irreplaceable due to our unique strengths as meaning-makers and relationship-builders, our capacity for morality and empathy, our creativity, our expertise and adaptability and our capacity to build unity and cooperate in building social structures and civilization for the benefit of all. Furthermore, as society is radically automated, the purpose of human life and its reevaluation will also come into question. For instance, as more and more occupations are replaced by ChatGPT services, more and more people will be freed up to do other things with their time, such as caring for relatives, undertaking creative projects, studying further and having children.
Originality/value
The investigation of the scope and limitations of AI is significant for two reasons. First, the question of the nature and functions of a mind becomes critical to the possibility of replication because if the human mind is like a super-sophisticated computer, then the relationship between a brain and mind is similar (if not identical) to the relationship between a computer as machine hardware and its programme or software (Dreyfus, 1979). [ ] If so, it should be theoretically possible to understand its mechanism and reproduce it, and then it is just a matter of time before AI research and development can replicate the human mind and eventually replace a human teacher, especially if an AI machine can teach just as intelligently yet more efficiently and economically. But if AI has inherent limitations that preclude the possibility of ever having a human-like mind and thought processes, then our investigation can at least clarify in what ways AI/AGI – such as ChatGPT – could support teaching and learning at universities.}
}
@article{LEE2024101413,
title = {Cognitive flexibility training for impact in real-world settings},
journal = {Current Opinion in Behavioral Sciences},
volume = {59},
pages = {101413},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101413},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624000640},
author = {Liz Y Lee and Máiréad P Healy and Nastassja L Fischer and Ke Tong and Annabel SH Chen and Barbara J Sahakian and Zoe Kourtzi},
abstract = {Interacting with complex and dynamic environments challenges the brain’s ability to adapt to change. This key ability known as cognitive flexibility involves learning the structure of the environment, switching attention between features, dimensions and tasks, and adopting new rules in the face of uncertainty. Training cognitive flexibility has strong potential to improve adaptive behavior across the lifespan with impact in real-world settings (e.g. educational, clinical). Here, we review evidence on the role of cognitive training in improving executive functions and the factors that may enhance the effectiveness of training programs. We propose that personalized and adaptive training programs that focus on the multifaceted abilities comprising cognitive flexibility are key for promoting adaptive behavior and lifelong learning in real-world settings.}
}
@article{LI2025124674,
title = {Aerodynamic analysis and hygrothermal transfer characteristics of cellulose evaporative cooling pads: A case study applied to protected agriculture},
journal = {Applied Thermal Engineering},
volume = {258},
pages = {124674},
year = {2025},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2024.124674},
url = {https://www.sciencedirect.com/science/article/pii/S1359431124023421},
author = {He Li and Chengji Zong and Jiarui Lu and Shumei Zhao and Weitang Song and Dongyan Yang},
keywords = {Direct evaporative cooling, CFD, Saturation efficiency, Pressure drop, Energy saving, Wind tunnel experiment},
abstract = {Evaporative cooling pads are clean media for cost-effective temperature management. However, the spatially integrated microclimate resulting from heat and mass transfer in the evaporative cooling pad is an uncertain and nonlinear complexity. Therefore, this purpose of this study is to present an innovative prediction model with computational fluid dynamics and evaluate the operating scenarios and geometrical properties of wet pads from quantitative metrics such as saturation efficiency and pressure drop. The reliability of the established numerical model of wet pads was verified by wind tunnel experiments and existing experiments, which predicted the outlet conditions in satisfactory conformity. The results showed that a wet pad with 8 mm ripple height and 19.6 mm ripple distance exhibits the best prospective, with energy consumption and specific water consumption reduced by 45.28 % and 26.26 %, respectively. Meanwhile, the cross strategy of 45_45° corrugation obliquity reduces the pressure drop by 18.95 %, providing uniform supply air distributions. The heat exchange of the wet pad is mainly concentrated within the first 35 mm from the inlet section, while the mass exchange of the wet pad is concentrated within the first 60 mm. The range of frontal air velocity with 0.9–2.5 m/s is recommend for the evaporative cooling system.}
}
@article{ALIJAH2007193,
title = {On the N3O2- paradigm},
journal = {Journal of Molecular Structure},
volume = {844-845},
pages = {193-199},
year = {2007},
note = {STUDIES IN HYDROGEN-BONDED SYSTEMS – A collection of Invited Papers in honour of Professor Lucjan Sobcyk, on the occasion of his 80th Birthday},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2007.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0022286007003316},
author = {Alexander Alijah and Eugene S. Kryachko},
keywords = {, Theoretical calculations, Isomers, Electron detachment},
abstract = {A survey of the existing experimental and theoretical data on the trinitrogen dioxide anion N3O2- that manifests a controversy as to the number of isomers and their chemical structures is presented. To resolve the controversy, new computational studies are performed at the MP2/aug-cc-pVTZ computational level. Two hitherto unknown isomers are predicted, one with singlet and one with triplet spin multiplicity. The singlet isomer, structurally characterized as N2·[ONO]−, is the most stable among all known isomers and accounts for fragmentation patterns observed in the recent dissociative photodetachment experiments.}
}
@article{ONKAL2013772,
title = {Scenarios as channels of forecast advice},
journal = {Technological Forecasting and Social Change},
volume = {80},
number = {4},
pages = {772-788},
year = {2013},
note = {Scenario Method: Current developments in theory and practice},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2012.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0040162512002090},
author = {Dilek Önkal and Kadire Zeynep Sayım and Mustafa Sinan Gönül},
keywords = {Forecast, Scenario, Group, Judgment, Advice taking},
abstract = {Today's business environment provides tougher competition than ever before, stressing the important role played by information and forecasts in decision-making. The scenario method has been popular for focused organizational learning, decision making and strategic thinking in business contexts, and yet, its use in communicating forecast information and advice has received little research attention. This is surprising since scenarios may provide valuable tools for communication between forecast providers and users in organizations, offering efficient platforms for information exchange via structured storylines of plausible futures. In this paper, we aim to explore the effectiveness of using scenarios as channels of forecast advice. An experimental study is designed to investigate the effects of providing scenarios as forecast advice on individual and group-based judgmental predictions. Participants are given time series information and model forecasts, along with (i) best-case, (ii) worst-case, (iii) both, or (iv) no scenarios. Different forecasting formats are used (i.e., point forecast, best-case forecast, worst-case forecast, and surprise probability), and both individual predictions and consensus forecasts are requested. Forecasts made with and without scenarios are compared for each of these formats to explore the potential effects of providing scenarios as forecast advice. In addition, group effects are investigated via comparisons of composite versus consensus predictions. The paper concludes with a discussion of results and implications for future research on scenario use in forecasting.}
}
@article{DESOYSA2025106885,
title = {Green with envy? The effects of inequality and equity within and across social groups on greenhouse gas emissions, 1990–2020},
journal = {World Development},
volume = {188},
pages = {106885},
year = {2025},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2024.106885},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X24003565},
author = {Indra {de Soysa}},
keywords = {Inequality, Equity, Horizontal inequality, Greenhouse gas, Climate change},
abstract = {The idea that inequality and inequities drive climate change forms a strong discourse in environmental politics. Reducing inequality is promoted as a win–win solution for reducing greenhouse gases. Others view egalitarian processes as a potential threat since increasing the consumption possibilities of the bottom-rungs of society relative to the top would drive up higher overall emissions. Using the latest available data on greenhouse gas emissions and the adoption of green energy technology measured over three decades, this study finds that a variety of measurements of vertical and horizontal inequality and inequitable access to political resources correlate with lower emissions per capita and greater adoption of green energy technologies. Inequality works in the opposite way than often thought. Per capita income levels, contrarily, are robustly and consistently associated with higher emissions, results that support the view that it is overall wealth (consumption) that drives climate change, not its distribution. Reducing inequality and poverty poses a moral and practical conundrum because levelling up incomes within and between countries, given current levels of technology, will worsen the climate crisis. The basic results hold up to a barrage of robustness tests, such as alternative estimating methods, models, and data, and to formal tests of omitted variables bias. Understanding how emissions might be reduced while addressing questions of equity demands calls for much harder thinking, and potentially fewer slogans, such as “eco-social contracts” and “new green deals” that peddle win–win solutions to a ‘wicked problem.’}
}
@incollection{BERNINGER2002273,
title = {Chapter 10 - Building a Computing Brain Pedagogically},
editor = {Virginia W. Berninger and Todd L. Richards},
booktitle = {Brain Literacy for Educators and Psychologists},
publisher = {Academic Press},
address = {San Diego},
pages = {273-294},
year = {2002},
series = {Practical Resources for the Mental Health Professional},
issn = {18730450},
doi = {https://doi.org/10.1016/B978-012092871-2/50011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780120928712500115},
author = {Virginia W. Berninger and Todd L. Richards},
abstract = {Publisher Summary
This chapter focuses on building a computing brain pedagogically. The brain, as it interacts with the world, constructs more concepts that are represented as mental models in distributed neural networks. Both the quantitative dimension and logical structures contribute to how these mental models are constructed and represented in the brain. The brain draws on both inductive thinking. During the construction process, the brain also uses multiple codes to represent and understand this emerging conceptual domain. The hand that is instrumental in development of the written language system also plays a major role in development of conceptual representations of the world. Working memory also plays an important role in conceptual development in the math domain. Like the writing brain, the computing brain also develops from both play and conscious work. The chapters conclude that development of the computing brain requires guided assistance in translating implicit knowledge based on experience into explicit knowledge that can be used for the hard work of math problem solving, which is conducted in resource-limited, temporally constrained working memory.}
}
@article{TSAI20251293,
title = {VR Games for Teaching Lean Manufacturing Tools: A Case Study of Stool Manufacturing},
journal = {Procedia Computer Science},
volume = {253},
pages = {1293-1302},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.191},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925001991},
author = {Tim Tsai and Zaneta Sarah Widjaja and Md Rakibul Hasan and Dhrumil Pithwa and Purna Sai {Teja Pinninti} and Rafiq Ahmad},
keywords = {Lean Manufacturing, Virtual Reality, Lean Tools, VR Games},
abstract = {This study investigates the efficacy of Virtual Reality (VR) in enhancing lean manufacturing training. By integrating VR with lean manufacturing principles, the aim is to compare performance and learning outcomes in traditional and lean scenarios. The research highlights the limitations of conventional training approaches in fully engaging learners and keeping pace with rapid technological advancements in manufacturing processes. Through the development of an interactive VR game focused on a stool manufacturing process, the study advances the use of PDCA framework to incorporate key lean manufacturing tools such as 5S Principles, Kanban, Poka-Yoke, and ergonomic improvements. The game development process is detailed, covering the preparation of 3D models, set up of virtual scenes, development of game function, design of user interface, and deployment. User testing reveals significant improvements in process efficiency and knowledge acquisition when employing lean-inspired scenarios within the VR environment. The study concludes with promising results, demonstrating the potential of VR in lean manufacturing training while also acknowledging the need for further research to validate these findings across a wider range of manufacturing processes.}
}
@article{MUTH1992278,
title = {Extraneous information and extra steps in arithmetic word problems},
journal = {Contemporary Educational Psychology},
volume = {17},
number = {3},
pages = {278-285},
year = {1992},
issn = {0361-476X},
doi = {https://doi.org/10.1016/0361-476X(92)90066-8},
url = {https://www.sciencedirect.com/science/article/pii/0361476X92900668},
author = {K.Denise Muth},
abstract = {To determine how middle school students cope with some of the demands imposed on them by arithmetic word problems, 140 eighth graders were asked to solve word problems modeled after those used by the National Assessment of Educational Progress. Processing demands were imposed on the students by adding extraneous information and extra steps to the problems. Results indicated that the presence of extraneous information and extra steps reduced the accuracy of students' solutions. Thinking-out-loud protocols also revealed several misconceptions that students have about solving word problems.}
}
@article{JYOTSNA20231270,
title = {IntelEye: An Intelligent Tool for the Detection of Stressful State based on Eye Gaze Data While Watching Video},
journal = {Procedia Computer Science},
volume = {218},
pages = {1270-1279},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.105},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001059},
author = {C. Jyotsna and J. Amudha and Amritanshu Ram and Giandomenico Nollo},
keywords = {Eye Tracking, Mental Health, Real Time Monitoring, K-Nearest Neighbour, Eye Gaze Measures, Welch Two Sample t-test},
abstract = {Technology to monitor mental health is gaining popularity as it helps to improve the cognitive and behavioral performance of an individual. Considering the growing need to monitor mental health, there is subsequent research in continuous and real-time monitoring technologies that can increase the quality of life by reducing the cost of health care. Eye tracking technology has played a significant role in monitoring a person's mental health. An intelligent system can apply several computational procedures to extract meaningful information from the massive physiological data obtained from eye tracking. The proposed model IntelEye is a tool to detect the stressful states of an individual while watching calm and stressful videos. The eye gaze measures based on pupil diameter, fixation, and blink were used for detecting stressful conditions. The data was collected from hospital employees, and the K Nearest Neighbor algorithm could successfully recognize the stressful states and the corresponding gaze location during stressful situations. IntelEye is not only identifying the stressful states but also has the novelty of identifying the scene and gaze location, making them stressful while watching the video.}
}
@article{URSINO2015234,
title = {A neural network for learning the meaning of objects and words from a featural representation},
journal = {Neural Networks},
volume = {63},
pages = {234-253},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002639},
author = {Mauro Ursino and Cristiano Cuppini and Elisa Magosso},
keywords = {Semantic memory, Lexical memory, Conceptual representation, Hebb rule, Dominant features, Category formation},
abstract = {The present work investigates how complex semantics can be extracted from the statistics of input features, using an attractor neural network. The study is focused on how feature dominance and feature distinctiveness can be naturally coded using Hebbian training, and how similarity among objects can be managed. The model includes a lexical network (which represents word-forms) and a semantic network composed of several areas: each area is topologically organized (similarity) and codes for a different feature. Synapses in the model are created using Hebb rules with different values for pre-synaptic and post-synaptic thresholds, producing patterns of asymmetrical synapses. This work uses a simple taxonomy of schematic objects (i.e., a vector of features), with shared features (to realize categories) and distinctive features (to have individual members) with different frequency of occurrence. The trained network can solve simple object recognition tasks and object naming tasks by maintaining a distinction between categories and their members, and providing a different role for dominant features vs. marginal features. Marginal features are not evoked in memory when thinking of objects, but they facilitate the reconstruction of objects when provided as input. Finally, the topological organization of features allows the recognition of objects with some modified features.}
}
@article{BARTELL20051283,
title = {How hydrides misled chemists},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {61},
number = {7},
pages = {1283-1286},
year = {2005},
note = {Honour Issue - Jim Durig},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2004.11.056},
url = {https://www.sciencedirect.com/science/article/pii/S1386142504006481},
author = {Lawrence S. Bartell},
keywords = {Models of molecular structure and force fields, Simple hydrides, Ligand-close-packing, Cautionary stories},
abstract = {Hydrogen-containing molecules are simple enough to be attractive subjects in experimental diffraction and spectroscopic studies and in quantum computations. Yet, the inferences about molecular structure and force fields originally drawn from studies of these subjects were significantly flawed. In recent developments the original models of structure invoked, such as hybridization, have been superseded. The reasons for this are briefly reviewed. What has emerged to account for molecular geometry, prevailing even over the popular VSEPR theory, is a model of geminal nonbonded interactions.}
}
@article{HARA20239703,
title = {Reorganizing Cyber-Physical Configurations using User Activities for Human-in-the-Loop Cyber-Physical Systems},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {9703-9708},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.281},
url = {https://www.sciencedirect.com/science/article/pii/S240589632300633X},
author = {Tatsunori Hara and Yuki Okada and Jun Ota},
keywords = {Design, modelling and analysis of HMS, human-centered automation and design, cyber-physical system, design structure matrix, data utilization, product service system, smart home},
abstract = {This paper proposes a “human-in-the-loop design structure matrix (DSM)” method for understanding and reorganizing the structures of multiple cyber-physical systems (CPSs), focusing on user activities. By incorporating user activities as integral components in system engineering techniques, this method contributes to the design literature on human-in-the-loop CPS. Using the illustrative case of a smart home, we obtained the following types of clusters to review the structure and cyber-physical configurations of CPSs: clusters that retain the target user activity, clusters decoupled from the target user activity, and clusters across two different user activities. In the second type, we found a hub cluster regarding the cyber process of notifications to users, which enabled diverse data utilization among the clusters.}
}
@article{SECCHI2024105891,
title = {Modeling and theorizing with agent-based sustainable development},
journal = {Environmental Modelling & Software},
volume = {171},
pages = {105891},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105891},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223002773},
author = {D. Secchi and V. Grimm and D.B. Herath and F. Homberg},
keywords = {Sustainable development, Theory development, Human–environment interaction, Agent-based modeling, Common language, ODD protocol},
abstract = {Sustainable development is an expression that permeates large areas of knowledge. For it to be meaningful, environmental aspects must be considered as intertwined with economic and social aspects. This is a multidisciplinary effort that is made challenging by the task of synthesizing the many emerging contributions. This has limited theory development where the definition of mechanisms, assumptions, dynamics and the determination of the entities involved are largely left to the reader’s imagination. We suggest to engage with the rationale of agent-based modeling to better define the assumptions, mechanisms, and boundaries of sustainable development. For this, the O-part of the widely used ODD protocol for describing agent-based models (ABM) provides a standardized structure, which we here augment to OsDD to specifically take into account sustainability issues. Even without formulating and implementing the full ABM, using OsDD requires to be explicit about the mechanisms, assumptions, dynamics and the entities involved and thereby provides a common language for theory development.}
}
@article{BENBOW19841643,
title = {Computational analysis of polymer processing: (Edited by J.R.A. Pearson and S.M. Richardson). Applied Science, 1983. £36.00. 343pp},
journal = {Chemical Engineering Science},
volume = {39},
number = {11},
pages = {1643},
year = {1984},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(84)80093-7},
url = {https://www.sciencedirect.com/science/article/pii/0009250984800937},
author = {J.J. Benbow}
}
@article{LU2025154,
title = {Simulation and classification optimization design of ultra-low temperature heat exchangers in dilution refrigerator using local thermal nonequilibrium model},
journal = {International Journal of Refrigeration},
volume = {174},
pages = {154-164},
year = {2025},
issn = {0140-7007},
doi = {https://doi.org/10.1016/j.ijrefrig.2025.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140700725000866},
author = {Yian Lu and Jun Shen and Ya'nan Zhao and Danyang Liu},
keywords = {Dilution refrigeration, Ultra-low temperature heat exchanger, Classification optimization design, Numerical simulation, Local thermal nonequilibrium},
abstract = {Dilution refrigeration is widely used in a variety of cutting-edge scientific fields such as quantum computations. The ultra-low temperature heat exchangers play a crucial role for the ultimate performance of dilution refrigerator. To study the classification optimization design of ultra-low temperature heat exchangers, an integrated local thermal nonequilibrium model have used. By considering the ultra-low temperature heat exchangers as an integrated unit consisting of a number of interconnected sub-modules, an optimized design study of the geometrical parameter configurations of continuous heat exchanger, three-stage step heat exchanger, and connecting tubes has been carried out. Using the optimized configuration for continuous heat exchanger with a length of 19 m, an inner tube diameter of 10 mm, and an outer tube diameter of 15 mm at a molar flow rate of 1.5 mmol/s, the classification optimization of step heat exchanger is carried out. By adjusting the ratio of the sintered volume to the whole sintered volume (g1=0.15, g2=0.3 and g3=0.55) and the ratio of concentrated heat transfer area to the total heat transfer area of this stage (f1=0.35, f2=0.4 and f3=0.45), the heat transfer efficiency of the three-stage step heat exchanger can be further improved without additional resources. And the impact of length and diameter of connecting tubes has also been discussed in this paper. This study provides quantitative guidance for the design of ultra-low temperature heat exchangers, which is important for improving the overall performance of dilution refrigerators.}
}
@article{VIJAYALAKSHMI20222382,
title = {Topological indices relating some nanostructures},
journal = {Materials Today: Proceedings},
volume = {68},
pages = {2382-2386},
year = {2022},
note = {4th International Conference on Advances in Mechanical Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.09.106},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322059144},
author = {K. {Vijaya Lakshmi} and N. Parvathi},
keywords = {Topological indices, Chemical graph theory, Molecular structure, Nanotechnology, Nanotubes, Nanosheet},
abstract = {The application of mathematics mainly graph theory has a significant role in the development of chemistry. Topological indices act as a bridge between mathematics and chemistry. The topological index is nothing but a numerical quantity that has yielded valuable insights in terms of molecular structure. In nanotechnology, the structural characterization of the molecular species and the nanoparticles are indicated by the indices. These indices identify the symmetry of molecular structures and provide them a scientific terminology to predict qualities like boiling temperatures, viscosity, and gyrating radius. The main objective of the current paper is to compute topological indices with the corresponding formulae, moreover, these indices help in predicting physicochemical properties without the involvement of the laboratory. The current research paper investigated the computation of degree-based topological indices for the nanotubes HAC5C6C7[a,b] and TU(C4C6C8[a,b].}
}
@article{ROY2022105849,
title = {EEG based stress analysis using rhythm specific spectral feature for video game play},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105849},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105849},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006023},
author = {Shidhartho Roy and Monira Islam and Md. Salah Uddin Yusuf and Nushrat Jahan},
keywords = {Beta–Alpha ratio, EEG, Stress-relaxation modeling, Topography, Video gameplay},
abstract = {Background and objective:
For the emerging significance of mental stress, various research directives have been established over time to understand better the causes of stress and how to deal with it. In recent years, the rise of video gameplay has been unprecedented, further triggered by the lockdown imposed due to the COVID-19 pandemic. Several researchers and organizations have contributed to the practical analysis of the impacts of such extended periods of gameplay, which lacks coordinated studies to underline the outcomes and reflect those in future game designing and public awareness about video gameplay. Investigations have mainly focused on the “gameplay stress” based on physical syndromes. Some studies have analyzed the effects of video gameplay with Electroencephalogram (EEG), Magnetic resonance imaging (MRI), etc., without concentrating on the relaxation procedure after video gameplay.
Methods:
This paper presents an end-to-end stress analysis for video gaming stimuli using EEG. The power spectral density (PSD) of the Alpha and Beta bands is computed to calculate the Beta-to-Alpha ratio (BAR). The Alpha and Beta band power is computed, and the Beta-to-Alpha band power ratio (BAR) has been determined. In this article, BAR is used to denote mental stress. Subjects are chosen based on various factors such as gender, gameplay experience, age, and Body mass index (BMI). EEG is recorded using Scan SynAmps2 Express equipment. There are three types of video gameplay: strategic, puzzle, and combinational. Relaxation is accomplished in this study by using music of various pitches. Two types of regression analysis are done to mathematically model stress and relaxation curve. Brain topography is rendered to indicate the stressed and relaxed region of the brain.
Results:
In the relaxed state, the subjects have BAR 0.701, which is considered the baseline value. Non-gamer subjects have an average BAR of 2.403 for 1 h of strategic video gameplay, whereas gamers have 2.218 BAR concurrently. After 12 minutes of listening to low-pitch music, gamers achieved 0.709 BAR, which is nearly the baseline value. In comparison to Quartic regression, the 4PL symmetrical sigmoid function performs regression analysis with fewer parameters and computational power.
Conclusion:
Non-gamers experience more stress than gamers, whereas strategic games stress the human brain more. During gameplay, the beta band in the frontal region is mostly activated. For relaxation, low pitch music is the most useful medium. Residual stress is evident in the frontal lobe when the subjects have listened to high pitch music. Quartic regression and 4PL symmetrical sigmoid function have been employed to find the model parameters of the relaxation curve. Among them, quartic regression performs better in terms of Akaike information criterion (AIC) and R2 measure.}
}
@article{FALCONE2020110269,
title = {Soft computing techniques in structural and earthquake engineering: a literature review},
journal = {Engineering Structures},
volume = {207},
pages = {110269},
year = {2020},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2020.110269},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619322540},
author = {Roberto Falcone and Carmine Lima and Enzo Martinelli},
keywords = {Structural engineering, Earthquake engineering, Fuzzy logic, Neural network, Swarm intelligence, Evolutionary computing},
abstract = {Although civil engineering problems are often characterized by significant levels of complexity, they are generally approached and solved by combining several practitioners’ skills, such as intuition, past experience, logical reasoning, mathematical elaborations, and physical sense. This is also the case of problems in structural and earthquake engineering whose solution is generally based on the so-called “engineer’s judgment”. However, heuristic theories and algorithms within the framework of “soft computing” can provide a more rational and systematic way to approach and solve problems in these areas. As a matter of fact, the aforementioned algorithms have been recently utilized in several branches of engineering and applied sciences. This paper proposes a state-of-the-art review of the main applications of soft computing techniques to relevant structural and earthquake engineering problems. Specifically, the applications of fuzzy computing, evolutionary computing, swarm intelligence, and neural networks, as well as their hybrid combinations, are analyzed with the aim to examine their capability and limitations in modeling, simulation, and optimization problems.}
}
@article{MA2024109594,
title = {Robust adaptive learning framework for semi-supervised pattern classification},
journal = {Signal Processing},
volume = {224},
pages = {109594},
year = {2024},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2024.109594},
url = {https://www.sciencedirect.com/science/article/pii/S0165168424002135},
author = {Jun Ma and Guolin Yu},
keywords = {Non-convex distance metric, Semi-supervised robust classification, Generalized adaptive robust loss function, Outliers, Kernel method},
abstract = {Hessian scatter regularized twin support vector machine (HSR-TSVM) employs hinge loss function and L2-distance metric, which makes it ineffective in dealing with outliers and noise data problems. Aiming to this problem, this paper a novel robust adaptive learning framework CL2,pHSR-TSVM is developed for semi-supervised classification tasks. In CL2,pHSR-TSVM, the generalized adaptive robust loss function Lδ(u) is first innovatively introduced to overcome the problem that hinge loss function is not sensitive to noise and outliers. Intuitively, Lδ(u) can improve the robustness of the model by selecting different robust loss functions for different learning tasks during the learning process via the adaptive parameter δ. Secondly, the robust distance metric capped L2,p-norm is introduced in CL2,pHSR-TSVM to reduce and eliminate the exaggerated influence of L2-distance metric on the learning process of outliers, especially when the outliers are far from the normal data distribution, by setting the appropriate parameters. Furthermore, to improve the computational efficiency of CL2,pHSR-TSVM, the fast CL2,pHSR-TSVM is presented for semi-supervised classification tasks. Finally, two effective algorithms are designed to solve our methods respectively, and the convergence and computational complexity are analyzed theoretically. Experimental results demonstrate the effectiveness and robustness of our methods.}
}
@article{JIA2011445,
title = {Evolutionary level set method for structural topology optimization},
journal = {Computers & Structures},
volume = {89},
number = {5},
pages = {445-454},
year = {2011},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045794910002567},
author = {Haipeng Jia and H.G. Beom and Yuxin Wang and Song Lin and Bo Liu},
keywords = {Evolutionary structure optimization, Structure topology optimization, Intelligent computation, Level set method},
abstract = {This paper proposes an evolutionary accelerated computational level set algorithm for structure topology optimization. It integrates the merits of evolutionary structure optimization (ESO) and level set method (LSM). Traditional LSM algorithm is largely dependent on the initial guess topology. The proposed method combines the merits of ESO techniques with those of LSM algorithm, while allowing new holes to be automatically generated in low strain energy within the nodal neighboring region during optimization. The validity and robustness of the new algorithm are supported by some widely used benchmark examples in topology optimization. Numerical computations show that optimization convergence is accelerated effectively.}
}
@article{ARNOLD2018581,
title = {Combining conscious and unconscious knowledge within human-machine-interfaces to foster sustainability with decision-making concerning production processes},
journal = {Journal of Cleaner Production},
volume = {179},
pages = {581-592},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618300787},
author = {Marlen Gabriele Arnold},
keywords = {Exploratory design, Structural systemic constellations, Cognitive human biases, HMI, Sustainable production contexts},
abstract = {At present, sustainability science is mainly based on conscious information and strongly focused on analytical tools or strategies. Neuroscience has made obvious that human decisions are prepared by the unconsciousness. Intuition plays an important role in early and late stages of learning processes and has a crucial impact on decision-making. Thus, intuitive and unconscious thinking is crucial for management processes in general and production planning processes in the main. However, unconscious knowledge and human behaviour is predominantly neglected in production research. Especially the addressing of human machine interfaces (HMI), human cognitive biases have a crucial impact on decision making processes. Constellation work is based on unconscious knowledge and intuition. Thus, systemic structural constellations are an innovative tool to integrate unconscious knowledge in a research context. In systemic structural constellations specific foci of complex systems, such as a production system, can be simulated and represented through spatial arrangements of persons or symbols. So, the method was used to reveal relevant patterns of relationships, structures, interaction, implicit knowledge, including hidden or underlying dynamics and influences that are relevant to and within a production system to understand how the raised problems in HMI can be better solved. The guiding research question is: How can the use of structural systemic constellations improve decision-making processes in HMI contexts in production environments in order to increase sustainability? Results show sustainability seems to be a matter of consciousness and is closely linked to the bias group not enough meaning. Sustainability and complexity resemble more than being linked by trade-offs. The recognition of human biases can be trained to improve human-machine-interfaces and sustainability. Constellation work contributes to decision theory by supporting effectuation.}
}
@article{SURESHBABU2006277,
title = {Modeling and simulation in signal transduction pathways: a systems biology approach},
journal = {Biochimie},
volume = {88},
number = {3},
pages = {277-283},
year = {2006},
issn = {0300-9084},
doi = {https://doi.org/10.1016/j.biochi.2005.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0300908405001999},
author = {C.V. {Suresh Babu} and Eun {Joo Song} and Young Sook Yoo},
keywords = {Biological systems, Signal transduction, Systems biology, Modeling and simulation},
abstract = {Modeling, the heart of systems biology, of complex processes (example: signal transduction) is a wide scientific discipline where many approaches from different areas are confronted with the aim of better understanding, identifying and modeling of complex data coming from various sources. The purpose of this paper is to introduce the basic steps of systems biology view towards signaling pathways, which mainly deals with the computational tools. The paper emphasizes the modeling and simulation approach in the signal transduction pathways using the topologies of the biochemical reactions with an overview of the different types of software platforms. Finally, we demonstrated the epidermal growth factor receptor signaling pathway model as an example to study the growth factor mediated signaling system with biological experiments. This paper will enables new comers to underline the strengths of the computational approaches towards signal transduction, as well as to highlight the systems biology research directions.}
}
@article{LIU2025103366,
title = {LoViT: Long Video Transformer for surgical phase recognition},
journal = {Medical Image Analysis},
volume = {99},
pages = {103366},
year = {2025},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103366},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524002913},
author = {Yang Liu and Maxence Boels and Luis C. Garcia-Peraza-Herrera and Tom Vercauteren and Prokar Dasgupta and Alejandro Granados and Sébastien Ourselin},
keywords = {Surgical phase recognition, Long videos, Temporally-rich spatial feature, Multi-scale, Phase transition-aware},
abstract = {Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT), emphasizing the development of a temporally-rich spatial feature extractor and a phase transition map. The temporally-rich spatial feature extractor is designed to capture critical temporal information within the surgical video frames. The phase transition map provides essential insights into the dynamic transitions between different surgical phases. LoViT combines these innovations with a multiscale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then leverages the temporally-rich spatial features and phase transition map to classify surgical phases using phase transition-aware supervision. Our approach outperforms state-of-the-art methods on the Cholec80 and AutoLaparo datasets consistently. Compared to Trans-SVNet, LoViT achieves a 2.4 pp (percentage point) improvement in video-level accuracy on Cholec80 and a 3.1 pp improvement on AutoLaparo. Our results demonstrate the effectiveness of our approach in achieving state-of-the-art performance of surgical phase recognition on two datasets of different surgical procedures and temporal sequencing characteristics. The project page is available at https://github.com/MRUIL/LoViT.}
}
@incollection{WOOLLISCROFT2020153,
title = {Chapter 12 - Precision medicine},
editor = {James O. Woolliscroft},
booktitle = {Implementing Biomedical Innovations into Health, Education, and Practice},
publisher = {Academic Press},
pages = {153-167},
year = {2020},
isbn = {978-0-12-819620-5},
doi = {https://doi.org/10.1016/B978-0-12-819620-5.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196205000126},
author = {James O. Woolliscroft},
keywords = {Precision medicine, Environment, Behavior, Microbiome, Genome, Pharmacogenomics},
abstract = {The convergence of computational, technologic and biomedical advances has enabled the development of precision medicine. Growing out of an understanding that there is a need for a new taxonomy of disease, the vision for precision medicine is to better understand the complex relationships in health and disease through the assemblage of massive databases that include individuals’ genomes, microbiomes, exposomes (a subsection of the environment), epigenomes, physiologic data, signs and symptoms, and other relevant information. Through the development of a holistic picture of genomic, microbiota, environmental and behavioral factors leading to disease, the intent is to intervene before disease becomes manifest to maintain or restore to health. Precision medicine will drive not only disruptive changes in the practice of clinical medicine, but also changes in our very conceptualization of health and disease.}
}
@article{CAPONNETTO2021104823,
title = {Examining nursing student academic outcomes: A forty-year systematic review and meta-analysis},
journal = {Nurse Education Today},
volume = {100},
pages = {104823},
year = {2021},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2021.104823},
url = {https://www.sciencedirect.com/science/article/pii/S0260691721000800},
author = {Valeria Caponnetto and Angelo Dante and Vittorio Masotta and Carmen {La Cerra} and Cristina Petrucci and Celeste Marie Alfes and Loreto Lancia},
keywords = {Academic failure, Academic success, Attrition, Bachelor's degree, Determinants, Factors, Nursing student},
abstract = {Objectives
To synthesize the definitions of nursing students' academic outcomes and provide a quantitative synthesis of their associated and predictive factors.
Design
Systematic review and meta-analysis.
Data sources
Four scientific databases were searched until January 2020.
Review methods
Observational studies describing undergraduate nursing students' academic outcomes were included. Studies were analytically synthesized and meta-analyses were performed utilizing the Odds Ratio or Cohen's d as effect sizes.
Results
Eighteen studies, published from 1979 to 2018, were included in the review, nine were meta-analyzed. Studies involved 10,024 undergraduate nursing students and were mostly retrospective cohort (55.6%). Students were mostly female (75.4%) with a mean age ranging from 21.3 to 27.0 years. Meta-analysis revealed that being female (OR = 1.65, 95% CI = 1.26 to 2.12), having attended a Classical, Scientific or Academic high school (OR = 1.30, 95% IC = 1.16 to 1.46), and having reported higher final grades at the upper-secondary high school (Cohen's d = 0.42, 95% CI = 0.18 to 0.65) was significantly associated with student's ability to graduate within the regular duration of the program. Sensitivity analyses confirmed meta-analytic results and meta-analyses heterogeneity depended on study design. Contrasting and limited evidence were found for other investigated factors, and for academic outcomes different from graduation within the regular duration of the program.
Conclusions
Despite meta-analytic results, gender and upper-secondary school would be unethical students' entry selection criteria. Final upper-secondary school grades should be considered for this scope and purpose. Conflicting and limited evidence found for other factors, such as students' background, suggested the influence of local contexts on the phenomenon and its investigation. Investigating the role of modifiable individual variables, such as empathy and critical thinking, could contribute to the open debate about students' entry selection strategies. An improvement in methodological quality of future studies is recommended and expected.}
}
@article{WU2024100295,
title = {Analyzing K-12 AI education: A large language model study of classroom instruction on learning theories, pedagogy, tools, and AI literacy},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100295},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100295},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000985},
author = {Di Wu and Meng Chen and Xu Chen and Xing Liu},
keywords = {AI education, Large language models, Pedagogical approaches, AI literacy},
abstract = {There is growing recognition among researchers and stakeholders about the significant impact of artificial intelligence (AI) technology on classroom instruction. As a crucial element in developing AI literacy, AI education in K-12 schools is increasingly gaining attention. However, most existing research on K-12 AI education relies on experiential methodologies and suffers from a lack of quantitative analysis based on extensive classroom data, hindering a comprehensive depiction of AI education's current state at these educational levels. To address this gap, this article employs the advanced semantic understanding capabilities of large language models (LLMs) to create an intelligent analysis framework that identifies learning theories, pedagogical approaches, learning tools, and levels of AI literacy in AI classroom instruction. Compared with the results of manual analysis, analysis based on LLMs can achieve more than 90% consistency. Our findings, based on the analysis of 98 classroom instruction videos in central Chinese cities, reveal that current AI classroom instruction insufficiently foster AI literacy, with only 35.71% addressing higher-level skills such as evaluating and creating AI. AI ethics are even less commonly addressed, featured in just 5.1% of classroom instruction. We classified AI classroom instruction into three categories: conceptual (50%), heuristic (18.37%), and experimental (31.63%). Correlation analysis suggests a significant relationship between the adoption of pedagogical approaches and the development of advanced AI literacy. Specifically, integrating Project-based/Problem-based learning (PBL) with Collaborative learning appears effective in cultivating the capacity to evaluate and create AI.}
}
@article{LEE201618,
title = {Affective Computing as Complex Systems Science},
journal = {Procedia Computer Science},
volume = {95},
pages = {18-23},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.288},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324607},
author = {William Lee and Michael D. Norman},
keywords = {Affective Computing, Computational Models, Complexity, Emotion, Apprasial},
abstract = {Pioneered in the early ‘90s by Rosalind Picard, a professor and IEEE Fellow of the MIT Media Lab, Affective Computing – rooted originally in artificial intelligence – now branches into wearable computing, big data, psychology, neuroscience, and modeling in order to advance the knowledge, understanding, and development of systems for sensing, recognizing, categorizing, and reacting to human emotion. Yet, the challenges of sensing multiple modalities simultaneously, disambiguating complex emotional states non-linearly, and modeling multiple individuals’ emotional states dynamically have continued to ring true, despite dramatic advances in affective computing. This paper seeks to serve two objectives. The first objective is to discuss how these three challenges are related to the three characteristics of complex systems – namely multiple components, non-linearity, and emergent behaviors. The second objective is to identify opportunities from the complex systems domain to address these challenges in novel and comprehensive ways. Recent advances in the utilization of Dynamical Systems Theory (an applied complexity science methodology) have shown that complex human interaction can be rigorously studied and modeled. Coupling the technological advances that cloud-based affective computing have brought with the emerging complex systems science-perspective may well catalyze a new era of human-machine and human-human collaboration.}
}
@article{ALGERAFI202461,
title = {Designing of an effective e-learning website using inter-valued fuzzy hybrid MCDM concept: A pedagogical approach},
journal = {Alexandria Engineering Journal},
volume = {97},
pages = {61-87},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824003867},
author = {Mohammed Abdulwahab Al-Gerafi and Shankha Shubhra Goswami and Mohammad Amir Khan and Quadri Noorulhasan Naveed and Ayodele Lasisi and Abdulaziz AlMohimeed and Ahmed Elaraby},
keywords = {E-Learning, Pedagogy, IVF, COPRAS, EDAS, PIV, MCDM},
abstract = {The demand for effective e-learning platforms requires prioritizing pedagogical excellence in online educational websites. Current approaches struggle with uncertainties, hindering optimal e-learning environments due to a lack of comprehensive evaluation in traditional methods. An integrated approach is crucial to avoid inefficiencies and incomplete understanding of learner needs. This research introduces a pioneering methodology integrating Inter-Valued Fuzzy (IVF) COPRAS-EDAS-PIV hybrid Multiple Criteria Decision-Making (MCDM) techniques, addressing existing limitations. Leveraging the IVF concept allows a holistic assessment of pedagogical parameters, ensuring a thorough understanding of the decision-making landscape. The study involves an extensive literature review, parameter identification, and data acquisition through group decision-making. The selection of a suitable e-learning website is based on seven conflicting parameters, and preference ranking orders are prescribed using EDAS, COPRAS, and PIV MCDM model. Rigorous analysis using these techniques facilitates precise ranking and informed decision-making. The findings underscore the efficacy of the proposed IVF-MCDM approach for the design of a pedagogical e-learning website. Final results reveal that alternative 5 as the most preferable, followed by alternative 2, while alternative 3 is the least favored option among the group. Comparative and sensitivity analyses validate the approach’s superiority, enabling stakeholders to make well-informed decisions for optimal e-learning websites that cater to diverse learner needs, thus enhancing the overall online learning experience.}
}
@article{YANG2024111470,
title = {A lattice-theoretic model of three-way conflict analysis},
journal = {Knowledge-Based Systems},
volume = {288},
pages = {111470},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111470},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124001059},
author = {Han Yang and Yiyu Yao and Keyun Qin},
keywords = {MS.F-bilattice, Three-way decision, Three-way conflict analysis},
abstract = {Pawlak conflict analysis uses a three-valued situation table for representing the ratings of a set of agents on a set of issues. This paper examines a lattice-theoretic basis of three-way conflict analysis. Qualitatively, we adopt a triangle, namely, an MS.F-bilattice, to characterize the structures of agents ratings, which gives an intuitive and effective tool for ordering a single agent and a pair of agents. We consider a strength ordering and a rating ordering to construct MS.F-bilattices. By applying the principles of three-way decision as thinking in threes, we trisect, according to the rating ordering, the nine pairs of ratings into three regions: potential opposition (PO), potential conflict (PC), and potential support (PS) regions. For each region, according to the strength ordering, we construct the weak, medium, and strong three subregions. Quantitatively, we introduce opposition-alliance and support-alliance measures based on the rating ordering for one issue to trisect these pairs of ratings into PO, PC, and PS regions. We study opposition strength, conflict strength, and support strength measures based on strength ordering for one issue to trisect each of the three regions into three subregions. Finally, we extend the five types of measures for a set of issues. The lattice-theoretic model of three-way conflict analysis clarifies the semantics of pairs of ratings by two agents and gives a different perspective on trisection methods in conflict analysis. To demonstrate the value of the proposed methods, we analyze a case study of the development planning of the Gansu Province of China.}
}
@article{LI202514,
title = {Paradigm shifts from data-intensive science to robot scientists},
journal = {Science Bulletin},
volume = {70},
number = {1},
pages = {14-18},
year = {2025},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2024.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S2095927324006807},
author = {Xin Li and Yanlong Guo}
}
@article{RAVALI2022100045,
title = {A systematic review of artificial intelligence for pediatric physiotherapy practice: Past, present, and future},
journal = {Neuroscience Informatics},
volume = {2},
number = {4},
pages = {100045},
year = {2022},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2022.100045},
url = {https://www.sciencedirect.com/science/article/pii/S2772528622000073},
author = {Ravula Sahithya Ravali and Thangavel Mahalingam Vijayakumar and Karunanidhi {Santhana Lakshmi} and Dinesh Mavaluru and Lingala Viswanath Reddy and Mervin Retnadhas and Tintu Thomas},
keywords = {Artificial intelligence, Systematic review, Pediatric physical therapy, Physiotherapy education},
abstract = {Background: Artificial intelligence (AI) is one of the active research fields to develop systems that mimic human intelligence and is helpful in many fields, particularly in medicine. (“Role of Artificial Intelligence Techniques ... - PubMed”) Physiotherapy is mainly involving in curing bone-related pain and injuries. The recent emergence of artificially intelligent machines has seen human cognitive capacity enhanced by computational agents that can recognize previously hidden patterns within massive data sets. (“(PDF) Artificial intelligence in clinical practice ...”) In this context, artificial intelligence in pediatric physiotherapy could be one of the most important modalities in delivering better medical and healthcare services to needy people. It is an attempt to identify the types, as well as to assess the effectiveness of interventions provided by artificial intelligence on pediatric physical therapy optimization-related outcomes. Methods: Data acquisition was carried out by systematic searches from various academic and research databases i.e., google scholar, PubMed, and IEEE from March 2011 to March 2021. Besides, numerous trial registries and grey literature resources were also explored. A total of 187 titles/abstracts were screened, and forty-eight full-text articles were assessed for eligibility. Conclusions: This research describes some of the possible influences of artificial intelligence technologies on pediatric physiotherapy practice, and the subsequent ways in which physiotherapy education will need to change to graduate professionals who are fit for practice in the 21st century health system for promoting safe and effective use of artificial intelligence and the delivery of Pediatric Physical Therapy care to people.}
}
@article{KASALICA20212157,
title = {APE in the Wild: Automated Exploration of Proteomics Workflows in the bio.tools Registry},
journal = {Journal of Proteome Research},
volume = {20},
number = {4},
pages = {2157-2165},
year = {2021},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.0c00983},
url = {https://www.sciencedirect.com/science/article/pii/S1535390721002031},
author = {Vedran Kasalica and Veit Schwämmle and Magnus Palmblad and Jon Ison and Anna-Lena Lamprecht},
keywords = {proteomics, scientific workflows, computational pipelines, workflow exploration, automated workflow composition, semantic tool annotation},
abstract = {The bio.tools registry is a main catalogue of computational tools in the life sciences. More than 17 000 tools have been registered by the international bioinformatics community. The bio.tools metadata schema includes semantic annotations of tool functions, that is, formal descriptions of tools’ data types, formats, and operations with terms from the EDAM bioinformatics ontology. Such annotations enable the automated composition of tools into multistep pipelines or workflows. In this Technical Note, we revisit a previous case study on the automated composition of proteomics workflows. We use the same four workflow scenarios but instead of using a small set of tools with carefully handcrafted annotations, we explore workflows directly on bio.tools. We use the Automated Pipeline Explorer (APE), a reimplementation and extension of the workflow composition method previously used. Moving “into the wild” opens up an unprecedented wealth of tools and a huge number of alternative workflows. Automated composition tools can be used to explore this space of possibilities systematically. Inevitably, the mixed quality of semantic annotations in bio.tools leads to unintended or erroneous tool combinations. However, our results also show that additional control mechanisms (tool filters, configuration options, and workflow constraints) can effectively guide the exploration toward smaller sets of more meaningful workflows.
}
}
@incollection{GILLESPIE2024124,
title = {Differentiation},
editor = {Samuel M. Scheiner},
booktitle = {Encyclopedia of Biodiversity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {124-133},
year = {2024},
isbn = {978-0-323-98434-8},
doi = {https://doi.org/10.1016/B978-0-12-822562-2.00167-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128225622001675},
author = {Rosemary G. Gillespie},
keywords = {Adaptive landscape, Cline, Coalescent process, Gene flow, Hybrid zone, Local adaptation, Natural selection, Neutral theory, Population structure and Speciation},
abstract = {Differentiation generally considers the accumulation of genetic differences between populations or species but can be applied more broadly to the diversification of genes, organisms, and populations. This article examines how elementary evolutionary and ecological processes lead to differentiation in both the narrow and broad senses and introduces successively more complex processes involved in differentiation. Most mutations are selectively neutral and neutral evolution can be considered the default process of genomic change, with natural selection being measurable from changes in allele frequencies that lead to deviation from neutrality. While the basic tenets of genetic differentiation are well established, the advent of new genomic technologies and associated computational tools have provided unprecedented insights into the mechanism of genetic differentiation and how these might lead to the formation of species. Recent work has highlighted in particular the importance of genetic admixture events and genomic interactions in shaping the process of differentiation.}
}
@article{LI20221,
title = {Computing for Chinese Cultural Heritage},
journal = {Visual Informatics},
volume = {6},
number = {1},
pages = {1-13},
year = {2022},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2021.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X21000644},
author = {Meng Li and Yun Wang and Ying-Qing Xu},
keywords = {Cultural computing, Chinese Cultural Heritage, Computable cultural ecosystem, Mogao caves, Guqin},
abstract = {Implementing computational methods for preservation, inheritance, and promotion of Cultural Heritage (CH) has become a research trend across the world since the 1990s. In China, generations of scholars have dedicated themselves to studying the country’s rich CH resources; there are great potential and opportunities in the field of computational research on specific cultural artefacts or artforms. Based on previous works, this paper proposes a systematic framework for Chinese Cultural Heritage Computing that consists of three conceptual levels which are Chinese CH protection and development strategy, computing process, and computable cultural ecosystem. The computing process includes three modules: (1) data acquisition and processing, (2) digital modeling and database construction, and (3) data application and promotion. The modules demonstrate the computing approaches corresponding to different phases of Chinese CH protection and development, from digital preservation and inheritance to presentation and promotion. The computing results can become the basis for the generation of cultural genes and eventually the formation of computable cultural ecosystem Case studies on the Mogao caves in Dunhuang and the art of Guqin, recognized as world’s important tangible and intangible cultural heritage, are carried out to elaborate the computing process and methods within the framework. With continuous advances in data collection, processing, and display technologies, the framework can provide constructive reference for building up future research roadmaps in Chinese CH computing and related fields, for sustainable protection and development of Chinese CH in the digital age.}
}
@article{FOUGERES2021100025,
title = {Fuzzy engineering design semantics elaboration and application},
journal = {Soft Computing Letters},
volume = {3},
pages = {100025},
year = {2021},
issn = {2666-2221},
doi = {https://doi.org/10.1016/j.socl.2021.100025},
url = {https://www.sciencedirect.com/science/article/pii/S2666222121000149},
author = {Alain-Jérôme Fougères and Egon Ostrosi},
keywords = {Fuzzy collaborative design, Fuzzy collaborative system, Natural language processing, Fuzzy requirement engineering, Fuzzy engineering design platform},
abstract = {Product design activities are predicated on fuzzy modelling, given that verbalising and interpreting engineering requirements are inherently fuzzy processes. The aim of this paper is to present a method for fuzzy intelligent requirement engineering from natural language to Computer-Aided Design (CAD) models. The field exploring the dynamics of computational processes from fuzzy linguistic modelling to fuzzy design modelling is complex and remains under-explored. No existing research has been identified which focuses specifically on fuzzy requirements engineering from natural language to CAD modelling. This paper seeks to address this by providing a design formalisation system based on five key principles. These principles are used to set out a computing procedure which follows a method broken up into six phases. The results of these six phases are fuzzy semantic graphs, which provide engineering requirements according to reliable design information. The approach is put into practice using the fuzzy agent-based tool developed by the authors, called F-EGEON (Fuzzy Engineering desiGn sEmantics elabOration and applicatioN). The proposed method is illustrated through an application from the automotive industry.}
}
@incollection{LUDLOW2025,
title = {I-Language and E-Language},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00558-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005585},
author = {Peter Ludlow},
keywords = {I-language, E-Language, Internalism, Externalism, Individualism, Intensional, Language faculty, Scientific method, Chomsky, Language organ},
abstract = {Chomsky (1986) introduced the distinction between I-language and E-language—a distinction that has consequences for how scientific investigation proceeds. In the former case we are investigating internal mechanisms of the language faculty. In the latter case we are investigating external, social constructs. In this essay we examine the distinguishing features of these approaches, raise questions about the coherence of each approach, and finally raise questions about whether the distinction itself stands up to close scrutiny.}
}
@article{PIANTADOSI2012199,
title = {Bootstrapping in a language of thought: A formal model of numerical concept learning},
journal = {Cognition},
volume = {123},
number = {2},
pages = {199-217},
year = {2012},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2011.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010027711002769},
author = {Steven T. Piantadosi and Joshua B. Tenenbaum and Noah D. Goodman},
keywords = {Number word learning, Bootstrapping, Cognitive development, Bayesian model, Language of thought, CP transition},
abstract = {In acquiring number words, children exhibit a qualitative leap in which they transition from understanding a few number words, to possessing a rich system of interrelated numerical concepts. We present a computational framework for understanding this inductive leap as the consequence of statistical inference over a sufficiently powerful representational system. We provide an implemented model that is powerful enough to learn number word meanings and other related conceptual systems from naturalistic data. The model shows that bootstrapping can be made computationally and philosophically well-founded as a theory of number learning. Our approach demonstrates how learners may combine core cognitive operations to build sophisticated representations during the course of development, and how this process explains observed developmental patterns in number word learning.}
}
@article{GIOVANNINI20143280,
title = {Approach for the rationalisation of product lines variety},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {3280-3291},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.02226},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016421130},
author = {A. Giovannini and A. Aubry and H. Panetto and H. El Haouzi and L. Pierrel and M. Dassisti},
keywords = {Mass customization, Product variety, Knowledge representation, Knowledge-based system},
abstract = {The product variety management is a key process to deal with the flexibility requested by the mass customisation. In this paper we show that current variety-modelling methods miss a customer representation: without a proper assessment of the customers is not possible to define the product variety that has to be developed to meet the requirements of a customer segment. Here we present an innovative approach to rationalise the product variety, i.e. to link each product variant to the customer profile who needs it. The aim is to optimise the product variety avoiding excesses (variants not related to a customer), lacks (customers not related to a variant) or redundancies (two or more variants proposed to a customer). An overview of customer modelling approaches in the classic product design (non-customisable) is presented. The innovative approach is here developed using system-thinking concepts. A knowledge-based system that uses this approach is designed. Finally the approach is explained using a real industrial case of a quasi-real coil design process.}
}
@article{BLACKSCHAFFER20162374289516665393,
title = {Training Pathology Residents to Practice 21st Century Medicine: A Proposal},
journal = {Academic Pathology},
volume = {3},
pages = {2374289516665393},
year = {2016},
issn = {2374-2895},
doi = {https://doi.org/10.1177/2374289516665393},
url = {https://www.sciencedirect.com/science/article/pii/S2374289521002189},
author = {W. Stephen Black-Schaffer and Jon S. Morrow and Michael B. Prystowsky and Jacob J. Steinberg},
keywords = {competency, progressive responsibility, residency training},
abstract = {Scientific advances, open information access, and evolving health-care economics are disrupting extant models of health-care delivery. Physicians increasingly practice as team members, accountable to payers and patients, with improved efficiency, value, and quality. This change along with a greater focus on population health affects how systems of care are structured and delivered. Pathologists are not immune to these disruptors and, in fact, may be one of the most affected medical specialties. In the coming decades, it is likely that the number of practicing pathologists will decline, requiring each pathologist to serve more and often sicker patients. The demand for increasingly sophisticated yet broader diagnostic skills will continue to grow. This will require pathologists to acquire appropriate professional training and interpersonal skills. Today’s pathology training programs are ill designed to prepare such practitioners. The time to practice for most pathology trainees is typically 5 to 6 years. Yet, trainees often lack sufficient experience to practice independently and effectively. Many studies have recognized these challenges suggesting that more effective training for this new century can be implemented. Building on the strengths of existing programs, we propose a redesign of pathology residency training that will meet (and encourage) a continuing evolution of American Board of Pathology and Accreditation Council for Graduate Medical Education requirements, reduce the time to readiness for practice, and produce more effective, interactive, and adaptable pathologists. The essence of this new model is clear definition and acquisition of core knowledge and practice skills that span the anatomic and clinical pathology continuum during the first 2 years, assessed by competency-based metrics with emphasis on critical thinking and skill acquisition, followed by individualized modular training with intensively progressive responsibility during the final years of training. We anticipate that implementing some or all aspects of this model will enable residents to attain a higher level of competency within the current time-based constraints of residency training.}
}
@article{URBINA2022100031,
title = {The commoditization of AI for molecule design},
journal = {Artificial Intelligence in the Life Sciences},
volume = {2},
pages = {100031},
year = {2022},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2022.100031},
url = {https://www.sciencedirect.com/science/article/pii/S2667318522000022},
author = {Fabio Urbina and Sean Ekins},
keywords = {Artificial intelligence, Design-make-test, Machine learning, Molecule design, Recurrent neural networks},
abstract = {Anyone involved in designing or finding molecules in the life sciences over the past few years has witnessed a dramatic change in how we now work due to the COVID-19 pandemic. Computational technologies like artificial intelligence (AI) seemed to become ubiquitous in 2020 and have been increasingly applied as scientists worked from home and were separated from the laboratory and their colleagues. This shift may be more permanent as the future of molecule design across different industries will increasingly require machine learning models for design and optimization of molecules as they become “designed by AI”. AI and machine learning has essentially become a commodity within the pharmaceutical industry. This perspective will briefly describe our personal opinions of how machine learning has evolved and is being applied to model different molecule properties that crosses industries in their utility and ultimately suggests the potential for tight integration of AI into equipment and automated experimental pipelines. It will also describe how many groups have implemented generative models covering different architectures, for de novo design of molecules. We also highlight some of the companies at the forefront of using AI to demonstrate how machine learning has impacted and influenced our work. Finally, we will peer into the future and suggest some of the areas that represent the most interesting technologies that may shape the future of molecule design, highlighting how we can help increase the efficiency of the design-make-test cycle which is currently a major focus across industries.}
}
@article{FIELDS2025310,
title = {Paradox or illusion? A comment on “The paradox of the self-studying brain” by Battaglia, Servajean, and Friston},
journal = {Physics of Life Reviews},
volume = {53},
pages = {310-311},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2025.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S1571064525000600},
author = {Chris Fields},
keywords = {Free energy principle, Introspection, Perception}
}
@incollection{SANCHEZSILVA2013437,
title = {17 - Risk assessment and management of civil infrastructure networks: a systems approach},
editor = {S. Tesfamariam and K. Goda},
booktitle = {Handbook of Seismic Risk Analysis and Management of Civil Infrastructure Systems},
publisher = {Woodhead Publishing},
pages = {437-464},
year = {2013},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-85709-268-7},
doi = {https://doi.org/10.1533/9780857098986.4.437},
url = {https://www.sciencedirect.com/science/article/pii/B9780857092687500177},
author = {M. Sánchez-Silva and C. Gómez},
keywords = {infrastructure, transportation networks, systems thinking, risk assessment, decision-making, optimization},
abstract = {Abstract:
Infrastructure networks are complex systems due to the large number of components that interact in a nonlinear way. Detecting and understanding the properties of such systems is of paramount importance to make effective decisions about risk management and sustainable development. This chapter presents a systems approach to risk management and risk-based decision making in infrastructure networks. In the proposed approach, the internal structure of a network is detected via pattern recognition (clustering), and structured information is used to enhance conceptual and computational analyses of reliability, vulnerability, damage propagation, and resource allocation. The approach can be applied to network analysis of complex infrastructure systems subjected to extreme events, such as earthquakes.}
}
@article{FARHADINIA2016135,
title = {Multiple criteria decision-making methods with completely unknown weights in hesitant fuzzy linguistic term setting},
journal = {Knowledge-Based Systems},
volume = {93},
pages = {135-144},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004359},
author = {B. Farhadinia},
keywords = {Multi-criteria decision making, Hesitant fuzzy linguistic term set, Entropy measure, Similarity measure, Distance measure},
abstract = {As for multi-criteria decision making problems with hesitant fuzzy linguistic information, it is common that the criteria involved in the problems are associated with the predetermined weights, whereas the information about criteria weights is generally incomplete. This is because of the complexity and the inherent subjective nature of human thinking. In this circumstance, the weights of criteria can be derived by means of information entropy from the evaluation values of criteria for alternatives. To the best of our knowledge, up to now, there is no work having introduced the concept of entropy measure for hesitant fuzzy linguistic term sets (HFLTSs). Hence, in this paper, we are going to fill in this gap by developing information about how entropy measures of HFLTSs can be designed.}
}
@article{RATTEN2023100857,
title = {Generative artificial intelligence (ChatGPT): Implications for management educators},
journal = {The International Journal of Management Education},
volume = {21},
number = {3},
pages = {100857},
year = {2023},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2023.100857},
url = {https://www.sciencedirect.com/science/article/pii/S1472811723000952},
author = {Vanessa Ratten and Paul Jones},
keywords = {Academic research, Teaching, Learning, Digital transformation, Management education, Artificial intelligence, ChatGPT},
abstract = {ChatGPT has been one of the most talked about computer programs amongst management educators in recent weeks due to its transformative ability to change how assessments are undertaken and graded. Unlike other educational technologies that can be tracked when used, ChatGPT has superior abilities that make it virtually untraceable when used. This creates a dilemma for management educators wanting to utilise the technology whilst staying relevant but also interested in authentic learning. Thus, it is critical for management educators to quickly implement policies regarding ChatGPT and subsequent new generative artificial intelligence because of its ease of use and affordability. This article is conceptual in nature and discusses ChatGPT as a generative form of artificial intelligence that presents challenges for management educators that need to be addressed through appropriate strategies. Thereby contributing to the literature on how technological innovations can be included in curriculum design and management learning practices. Practical and managerial implications are stated that highlight the critical need to re-examine existing education practices as a way of incorporating new technological innovation that can be utilised in a beneficial way.}
}
@article{MOHAMMED2025110933,
title = {Artificial intelligence approaches in predicting the mechanical properties of natural fiber-reinforced concrete: A comprehensive review},
journal = {Engineering Applications of Artificial Intelligence},
volume = {153},
pages = {110933},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110933},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625009339},
author = {Mohammed Mohammed and Jawad K. Oleiwi and Aeshah M. Mohammed and Azlin F. Osman and Tijjani Adam and Bashir O. Betar and Subash C.B. Gopinath},
keywords = {Implemented artificial intelligence, Natural fiber-reinforced concrete, Mechanical properties, Applications artificial intelligence, Sustainability},
abstract = {Implementing artificial intelligence (AI) techniques in predicting the mechanical properties of natural fiber-reinforced concrete (NFRC) has emerged as a transformative approach, offering significant advancements over traditional modeling methods. Construction materials science increasingly leverages advanced technologies to enhance composites like natural fiber-reinforced concrete (NFRC). However, the varied nature of natural fibers and their interactions within concrete matrices present significant challenges in accurately predicting the NFRC's mechanical properties. This comprehensive review highlights the innovative use of artificial intelligence (AI) techniques to address challenges in predicting material reliability. It explores the application of various AI methodologies, including machine learning (ML) techniques such as artificial neural networks (ANN) and support vector machines (SVM), along with pattern recognition (PR) and deep learning (DL). These approaches are utilized to tackle the challenges posed by the variability of natural fibers and their interactions within concrete matrices. These techniques have proven highly effective in accurately predicting the mechanical behavior of NFRC, marking significant advancements in the field. This review paper aims to summarize techniques for applying the AI methods mentioned in the NFRC. Firstly, we present a general introduction to AI and NFRC, highlighting the significance of AI in predicting the mechanical properties of NFRC. After that, a comparison between ML, PR, and DL in the field is discussed, and a review of recent applications of AI in the field is provided. Further, the advantages of employing such algorithmic methods are discussed in detail. Finally, future directions for employing ML, PR, and DL are presented, and their limitations are discussed.}
}
@article{XU2024100549,
title = {Prediction of environmental pollution hazard index of water conservancy system based on fuzzy logic},
journal = {International Journal of Thermofluids},
volume = {21},
pages = {100549},
year = {2024},
issn = {2666-2027},
doi = {https://doi.org/10.1016/j.ijft.2023.100549},
url = {https://www.sciencedirect.com/science/article/pii/S2666202723002641},
author = {Bingshu Xu},
keywords = {Water Conservancy System, Fuzzy Logic, Combined Forecasting, Environmental Pollution, Hazard Index Prediction},
abstract = {The water conservancy framework is a significant foundation for China's financial and social turn of events. Simultaneously monetary and social turn of events, likewise tremendously affect the biological climate. In the prediction method of the environmental pollution hazard index of water conservancy projects, a risk assessment must be carried out to enhance its applicability. This article mainly focuses on traditional mathematical prediction models and combines the characteristics of fuzzy logic mathematics (good nonlinear quality) to establish a new combination prediction method. By comparing the convergence errors of traditional methods with this method, the results show that the algorithm proposed in this paper can effectively reduce prediction errors with fewer iterations, making it stable at around 500 times, with good convergence and high computational accuracy. The fuzzy logic-based prediction method for environmental pollution hazards in water conservancy systems proposed in this article can overcome the difficulty of nonlinear combination prediction. This achieves real-time prediction of environmental pollution hazards in water conservancy systems and shortens response time, greatly improving the accuracy of prediction.}
}